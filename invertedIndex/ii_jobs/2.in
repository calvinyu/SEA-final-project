2<|###|>Document retrieval
'''Document retrieval''' is defined as the matching of some stated user query against a set of [[free-text]] records. These records could be any type of mainly [[natural language|unstructured text]], such as [[newspaper article]]s, real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words.

Document retrieval is sometimes referred to as, or as a branch of, '''Text Retrieval'''. Text retrieval is a branch of [[information retrieval]] where the information is stored primarily in the form of [[natural language|text]]. Text databases became decentralized thanks to the [[personal computer]] and the [[CD-ROM]]. Text retrieval is a critical area of study today, since it is the fundamental basis of all [[internet]] [[search engine]]s.

==Description==
Document retrieval systems find information to given criteria by matching text records (''documents'') against user queries, as opposed to [[expert system]]s that answer questions by [[Inference|inferring]] over a logical [[knowledge base|knowledge database]]. A document retrieval system consists of a database of documents, a [[classification algorithm]] to build a full text index, and a user interface to access the database.

A document retrieval system has two main tasks:
# Find relevant documents to user queries
# Evaluate the matching results and sort them according to relevance, using algorithms such as [[PageRank]].

Internet [[search engines]] are classical applications of document retrieval. The vast majority of retrieval systems currently in use range from simple Boolean systems through to systems using [[statistical]] or [[natural language processing]] techniques.

==Variations==
There are two main classes of indexing schemata for document retrieval systems: ''form based'' (or ''word based''), and ''content based'' indexing. The document classification scheme (or [[Search engine indexing|indexing algorithm]]) in use determines the nature of the document retrieval system.

===Form based===
Form based document retrieval addresses the exact syntactic properties of a text, comparable to substring matching in string searches. The text is generally unstructured and not necessarily in a natural language, the system could for example be used to process large sets of chemical representations in molecular biology. A [[suffix tree]] algorithm is an example for form based indexing.

===Content based===
The content based approach exploits semantic connections between documents and parts thereof, and semantic connections between queries and documents. Most content based document retrieval systems use an [[inverted index]] algorithm.

A ''signature file'' is a technique that creates a ''quick and dirty'' filter, for example a [[Bloom filter]], that will keep all the documents that match to the query and ''hopefully'' a few ones that do not. The way this is done is by creating for each file a signature, typically a hash coded version. One method is superimposed coding. A post-processing step is done to discard the false alarms. Since in most cases this structure is inferior to [[inverted file]]s in terms of speed, size and functionality, it is not used widely. However, with proper parameters it can beat the inverted files in certain environments.

==Example: PubMed==
The [[PubMed]]<ref>{{cite journal |author=Kim W, Aronson AR, Wilbur WJ |title=Automatic MeSH term assignment and quality assessment |journal=Proc AMIA Symp |pages=31923 |year=2001 |pmid=11825203 |pmc=2243528 }}
</ref> form interface features the "related articles" search which works through a comparison of words from the documents' title, abstract, and [[Medical Subject Headings|MeSH]] terms using a word-weighted algorithm.<ref>{{cite web|url=https://www.ncbi.nlm.nih.gov/books/NBK3827/#pubmedhelp.Computation_of_Related_Citati|title=Computation of Related Citations}}</ref><ref>{{cite journal|journal=BMC Bioinformatics|date=Oct 30, 2007|volume=8|pages=423|pmid=17971238|title=PubMed related articles: a probabilistic topic-based model for content similarity|author=Lin J1, Wilbur WJ|doi=10.1186/1471-2105-8-423|pmc=2212667}}</ref>

== See also ==

* [[Compound term processing]]
* [[Document classification]]
* [[Enterprise search]]
* [[Full text search]]
* [[Information retrieval]]
* [[Latent semantic indexing]]
* [[Search engine]]

== References ==

<references/>

==Further reading==
* {{cite journal|first1=Christos|last1=Faloutsos|first2=Stavros|last2=Christodoulakis|title=Signature files: An access method for documents and its analytical performance evaluation|journal=ACM Transactions on Information Systems (TOIS)|volume=2|issue=4|year=1984|pages=267288|doi=10.1145/2275.357411}}
* {{cite journal|author=Justin Zobel, Alistair Moffat and Kotagiri Ramamohanarao|title=Inverted files versus signature files for text indexing|journal=ACM Transactions on Database Systems (TODS)|volume=23|issue=4|year=1998|pages= 453490|url=http://www.cs.columbia.edu/~gravano/Qual/Papers/19%20-%20Inverted%20files%20versus%20signature%20files%20for%20text%20indexing.pdf|doi=10.1145/296854.277632}}
* {{cite journal|author=Ben Carterette and Fazli Can|title=Comparing inverted files and signature files for searching a large lexicon|journal=Information Processing and Management|volume= 41|issue=3|year=2005|pages= 613633|url=http://www.users.miamioh.edu/canf/papers/ipm04b.pdf|doi=10.1016/j.ipm.2003.12.003}}

== External links ==
* [http://cir.dcs.uni-pannon.hu/cikkek/FINAL_DOMINICH.pdf Formal Foundation of Information Retrieval], Buckinghamshire Chilterns University College

[[Category:Information retrieval]]
[[Category:Electronic documents]]
[[Category:Substring indices]]

[[zh:]]
>>EOP<<
8<|###|>National Centre for Text Mining
The '''National Centre for Text Mining''' (NaCTeM)
<ref name="ariadne">{{cite journal| author=Ananiadou S| title=The National Centre for Text Mining: A Vision for the Future | journal=Ariadne | year= 2007 | issue= 53 | url=http://www.ariadne.ac.uk/issue53/ananiadou/  }}</ref> is a publicly funded [[text mining]] (TM) centre. It was established to provide support, advice, and information on TM technologies and to disseminate information from the larger TM community, while also providing tailored services and tools in response to the requirements of the [[United Kingdom]] academic community. 

The [[software]] tools and services which NaCTeM supplies allow researchers to apply text mining techniques to problems within their specific areas of interest - examples of these tools are highlighted below. In addition to providing services, the Centre is also involved in, and makes significant contributions to, the text mining research community both nationally and internationally in initiatives such as [[Europe PubMed Central]].

The Centre is located in the [[Manchester Institute of Biotechnology]] and is operated and organized by the [[University of Manchester School of Computer Science]]. NaCTeM contributes expertise in [[information extraction]], [[natural language processing]] and parallel and distributed data mining systems in biomedical and clinical applications.

==Services==
[http://www.nactem.ac.uk/software/termine/ '''TerMine'''] is a domain independent method for automatic term recognition which can be used to help locate the most important terms in a document and automatically ranks them. <ref name="multi-word">{{cite journal| author=Frantzi, K., Ananiadou, S. and Mima, H.| title=Automatic recognition of multi-word terms | journal=International Journal of Digital Libraries | year= 2007 | volume=3 |issue= 2 | pages= 117132|  url=http://personalpages.manchester.ac.uk/staff/sophia.ananiadou/IJODL2000.pdf }}</ref> 

[http://www.nactem.ac.uk/software/acromine/ '''AcroMine'''] finds all known expanded forms of [[acronyms]] as they have appeared in [[Medline]] entries or conversely, it can be used to find possible acronyms of expanded forms as they have previously appeared in [[Medline]] and [[Disambiguation|disambiguates]] them.<ref name="pmid17050571">{{cite journal| author=Okazaki N, Ananiadou S| title=Building an abbreviation dictionary using a term recognition approach. | journal=Bioinformatics | year= 2006 | volume= 22 | issue= 24 | pages= 308995 | pmid=17050571 | doi=10.1093/bioinformatics/btl534 | pmc= | url=http://www.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&tool=sumsearch.org/cite&retmode=ref&cmd=prlinks&id=17050571  }} </ref>

[http://www-tsujii.is.s.u-tokyo.ac.jp/medie/ '''Medie'''] is  an intelligent search engine, for semantic retrieval of sentences containing biomedical correlations from [[Medline]] abstracts.

[http://refine1-nactem.mc.man.ac.uk/facta/ ''' Facta+'''] is a MEDLINE search engine for finding associations between biomedical concepts.<ref name="pmid18772154">{{cite journal| author=Tsuruoka Y, Tsujii J, Ananiadou S| title=FACTA: a text search engine for finding associated biomedical concepts | journal=Bioinformatics | year= 2008 | volume= 24 | issue= 21 | pages= 255960 | pmid=18772154 | doi=10.1093/bioinformatics/btn469 | pmc=2572701   }} </ref>

[http://www.nactem.ac.uk/software/kleio/ '''KLEIO'''] is a faceted semantic information retrieval system based on MEDLINE.

[https://www-tsujii.is.s.u-tokyo.ac.jp/info-pubmed/ '''Info-PubMed'''] provides information and graphical representation of biomedical interactions extracted from [[Medline]] using deep [[Semantic analysis (machine learning)|semantic parsing]] technology. This is supplemented with a term dictionary consisting of over 200,000 [[protein]]/[[gene]] names  and identification of [[disease]] types and [[organisms]].

==Resources==

[http://www.nactem.ac.uk/biolexicon/ '''BioLexicon'''] a large-scale terminological resource for the biomedical domain

[http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/wiki.cgi?page=GENIA+corpus '''GENIA'''] a collection of reference materials for the development of biomedical text mining systems

==References==
{{Reflist}}

==External links==
* http://www.nactem.ac.uk

[[Category:Computational linguistics]]
[[Category:Computer science organizations]]
[[Category:Information retrieval]]
[[Category:Linguistics organizations]]
[[Category:School of Computer Science, University of Manchester]]
>>EOP<<
14<|###|>Web search query
A '''web search query''' is a query that a user enters into a web [[search engine]] to satisfy his or her [[information needs]]. Web search queries are distinctive in that they are often plain text or [[hypertext]] with optional search-directives (such as "and"/"or" with "-" to exclude). They vary greatly from standard [[query language]]s, which are governed by strict syntax rules as [[command language]]s with keyword or positional [[Parameter (computer science)|parameters]].

== Types ==
There are three broad categories that cover most web search queries: informational, navigational, and transactional. These are often called "do, know, go."<ref>{{cite web|last=Gibbons|first=Kevin|title=Do, Know, Go: How to Create Content at Each Stage of the Buying Cycle|url=http://searchenginewatch.com/article/2235624/Do-Know-Go-How-to-Create-Content-at-Each-Stage-of-the-Buying-Cycle|publisher=Search Engine Watch|accessdate=24 May 2014}}</ref>

* '''Informational queries'''  Queries that cover a broad topic (e.g., ''colorado'' or ''trucks'') for which there may be thousands of relevant results.

* '''Navigational queries'''  Queries that seek a single website or web page of a single entity (e.g., ''youtube'' or ''delta air lines'').

* '''Transactional queries'''  Queries that reflect the intent of the user to perform a particular action, like purchasing a car or downloading a screen saver.

Search engines often support a fourth type of query that is used far less frequently:

* '''Connectivity queries'''  Queries that report on the connectivity of the indexed [[web graph]] (e.g., Which links point to this [[Uniform Resource Locator|URL]]?, and How many pages are indexed from this [[domain name]]?).<ref>{{cite web|last=Moore|first=Ross|title=Connectivity servers|url=http://nlp.stanford.edu/IR-book/html/htmledition/connectivity-servers-1.html|publisher=Cambridge University Press|accessdate=24 May 2014}}</ref>

== Characteristics ==

Most commercial web search engines do not disclose their search logs, so information about what users are searching for on the Web is difficult to come by.<ref>Dawn Kawamoto and Elinor Mills (2006), [http://news.cnet.com/AOL-apologizes-for-release-of-user-search-data/2100-1030_3-6102793.html AOL apologizes for release of user search data]</ref> Nevertheless, a study in 2001<ref>{{cite journal|author = Amanda Spink, Dietmar Wolfram, Major B. J. Jansen, Tefko Saracevic | year = 2001 | title = Searching the web: The public and their queries | journal = Journal of the American Society for Information Science and Technology | volume = 52 | issue = 3 | pages = 226234 | doi = 10.1002/1097-4571(2000)9999:9999<::AID-ASI1591>3.3.CO;2-I }}</ref> analyzed the queries from the [[Excite]] search engine showed some interesting characteristics of web search:

* The average length of a search query was 2.4 terms. 
* About half of the users entered a single query while a little less than a third of users entered three or more unique queries. 
* Close to half of the users examined only the first one or two pages of results (10 results per page).
* Less than 5% of users used advanced search features (e.g., [[boolean operators]] like AND, OR, and NOT).
* The top four most frequently used terms were , '' (empty search), and, of, ''and'' sex.

A study of the same Excite query logs revealed that 19% of the queries contained a geographic term (e.g., place names, zip codes, geographic features, etc.).<ref>{{cite conference | author = Mark Sanderson and Janet Kohler | year = 2004 | title = Analyzing geographic queries | booktitle = Proceedings of the Workshop on Geographic Information (SIGIR '04) | url =http://supremacyseo.com/analyzing-geographic-queries }}</ref>

A 2005 study of Yahoo's query logs revealed 33% of the queries from the same user were repeat queries and that 87% of the time the user would click on the same result.<ref>{{cite conference | author = Jaime Teevan, Eytan Adar, Rosie Jones, Michael Potts | year = 2005 | title = History repeats itself: Repeat Queries in Yahoo's query logs | booktitle = Proceedings of the 29th Annual ACM Conference on Research and Development in Information Retrieval (SIGIR '06) | pages = 703704 | url =http://www.csail.mit.edu/~teevan/work/publications/posters/sigir06.pdf | doi=10.1145/1148170.1148326 }}</ref> This suggests that many users use repeat queries to revisit or re-find information. This analysis is confirmed by a Bing search engine blog post telling about 30% queries are navigational queries <ref>http://www.bing.com/community/site_blogs/b/search/archive/2011/02/10/making-search-yours.aspx</ref>

In addition, much research has shown that query term frequency distributions conform to the [[power law]], or ''long tail'' distribution curves. That is, a small portion of the terms observed in a large query log (e.g. > 100 million queries) are used most often, while the remaining terms are used less often individually.<ref name="baezayates1">{{cite journal | author = Ricardo Baeza-Yates | year = 2005 | title = Applications of Web Query Mining | booktitle = Lecture Notes in Computer Science | pages = 722 | volume = 3408 | publisher = Springer Berlin / Heidelberg | url = http://www.springerlink.com/content/kpphaktugag5mbv0/ | ISBN = 978-3-540-25295-5}}</ref> This example of the [[Pareto principle]] (or ''8020 rule'') allows search engines to employ [[optimization techniques]] such as index or [[Partition (database)|database partitioning]], [[web cache|caching]] and pre-fetching.

But in a recent study in 2011 it was found that the average length of queries has grown steadily over time and average length of non-English languages queries had increased more than English queries.<ref>{{cite journal | author = Mona Taghavi, Ahmed Patel, Nikita Schmidt, Christopher Wills, Yiqi Tew | year = 2011 | title = An analysis of web proxy logs with query distribution pattern approach for search engines | booktitle = Journal of Computer Standards & Interfaces | pages = 162170 | volume = 34 | issue = 1 |publisher = Elsevier  | url = http://www.sciencedirect.com/science/article/pii/S0920548911000808 | doi=10.1016/j.csi.2011.07.001}}</ref> Google has implemented the [[Google_Hummingbird|hummingbird]] update in August 2013 to handle longer search queries since more searches are conversational (ie "where is the nearest coffee shop?").<ref>{{cite web|last=Sullivan|first=Danny|title=FAQ: All About The New Google Hummingbird Algorithm|url=http://searchengineland.com/google-hummingbird-172816|publisher=Search Engine Land|accessdate=24 May 2014}}</ref> 
For longer queries, [[Natural language processing]] helps, since parse trees of queries can be matched with that of answers and their snippets.<ref>{{vcite journal |author=Galitsky B|title=Machine learning of syntactic parse trees for search and classification of text|journal=Engineering Applications of Artificial Intelligence |volume=26 |issue=3 |date=2013 |pages=153-172|doi=10.1016/j.engappai.2012.09.017}}</ref> For multi-sentence queries where keywords statistics and [[Tfidf]] is not very helpful, [[Parse thicket]] technique comes into play to structurally represent complex questions and answers.<ref>{{vcite journal |author=Galitsky B, Ilvovsky D, Kuznetsov SO, Strok F|title=Finding Maximal Common Sub-parse Thickets
for Multi-sentence Search |journal=Lecture Notes In Artificial Intelligence |volume = 8323 |date=2013 |http://www.aclweb.org/anthology/R13-1037
}}</ref>

== Structured queries ==
With search engines that support Boolean operators and parentheses, a technique traditionally used by librarians can be applied. A user who is looking for documents that cover several topics or ''facets'' may want to describe each of them by a [[logical disjunction|disjunction]] of characteristic words, such as <code>vehicles OR cars OR automobiles</code>. A ''faceted query'' is a [[logical conjunction|conjunction]] of such facets; e.g. a query such as <code>(electronic OR computerized OR DRE) AND (voting OR elections OR election OR balloting OR electoral)</code> is likely to find documents about electronic voting even if they omit one of the words "electronic" and "voting", or even both.<ref>{{Cite web
|url=http://eprints.eemcs.utwente.nl/6918/01/TR-CTIT-06-57.pdf
|title=Exploiting Query Structure and Document Structure to Improve Document Retrieval Effectiveness
|author=Vojkan Mihajlovic, Djoerd Hiemstra, Henk Ernst Blok, Peter M.G. Apers
|postscript=<!--None-->}}</ref>

== See also ==
* [[Information retrieval]]
* [[Web search engine]]
* [[Web query classification]]
* [[Taxonomy for search engines]]

== References ==
{{reflist|2}}

{{Internet search}}

[[Category:Information retrieval]]
[[Category:Internet search]]
>>EOP<<
20<|###|>Semantic technology
{{no footnotes|date=March 2013}}
[[File:SemanticNetExample.jpg|thumb|Simplistic example of the sort of semantic net used in Semantic Web technology]]
In [[software]], '''semantic technology''' encodes meanings separately from data and content files, and separately from application code. 

This enables machines as well as people to understand, share and reason with them at execution time. With semantic technologies, adding, changing and implementing new relationships or interconnecting programs in a different way can be just as simple as changing the external model that these programs share.

With traditional [[information technology]], on the other hand, meanings and relationships must be predefined and hard wired into data formats and the application program code at design time. This means that when something changes, previously unexchanged information needs to be exchanged, or two programs need to interoperate in a new way, the humans must get involved.

Off-line, the parties must define and communicate between them the knowledge needed to make the change, and then recode the data structures and program logic to accommodate it, and then apply these changes to the database and the application. Then, and only then, can they implement the changes.

Semantic technologies are meaning-centered. They include tools for:

* autorecognition of topics and concepts, 
* information and meaning extraction, and
* categorization. 

Given a question, semantic technologies can directly search topics, concepts, associations that span a vast number of sources.

Semantic technologies provide an abstraction layer above existing IT technologies that enables bridging and interconnection of data, content, and processes. Second, from the portal perspective, semantic technologies can be thought of as a new level of depth that provides far more intelligent, capable, relevant, and responsive interaction than with information technologies alone.

== See also ==
* [[Business Intelligence 2.0]] (BI 2.0)
* [[Metadata]]
* [[Ontology (computer science)]]
* [[Semantic targeting]]
* [[Semantic web]]

==References==

* J.T. Pollock, R. Hodgson. ''Adaptive Information: Improving Business Through Semantic Interoperability, Grid Computing, and Enterprise Integration.'' [[J. Wiley and Sons]], October 2004
* R. Guha, R. McCool, and E. Miller. Semantic search. In ''WWW2003  Proc. of the 12th international conference on World Wide Web'', pp 700709. [[ACM Press]], 2003.
* I. Polikoff and D. Allemang. [https://lists.oasis-open.org/archives/regrep-semantic/200402/pdf00000.pdf Semantic technology.] ''TopQuadrant Technology Briefing'' v1.1, September 2003.
* [[Tim Berners-Lee|T. Berners-Lee]], J. Hendler, and O. Lassila. The Semantic Web: A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities. ''[[Scientific American]]'', May 2001.
* A.P. Sheth, C. Ramakrishnan. [http://corescholar.libraries.wright.edu/knoesis/970Technology%20In%20Action:%20Ontology%20Driven%20Information%20Systems%20For%20Search,%20Integration%20and%20Analysis. Semantic (Web) Technology In Action: Ontology Driven Information Systems For Search, Integration and Analysis.] ''[[IEEE Data Engineering Bulletin]]'', 2003.
* Steffen Staab, Rudi Studer  (Ed.), Handbook on Ontologies, Springer, 
* Mills Davis. The Business Value of Semantic Technologies. Presentation and Report. Semantic Technologies for E-Government, September
2004.
* P. Hitzler, M. Krotzsch, S. Rudolph, Foundations of Semantic Web Technologies, Chapman&Hall/CRC, 2009, ISBN 978-1-4200-9050-5

== External links ==
* [http://semtech2010.semanticuniverse.com Semantic Technology Conference]
* [http://www.semanticarts.com Semantic Technology and the Enterprise]

[[Category:Information retrieval]]
[[Category:Semantics]]
>>EOP<<
26<|###|>ZyLAB Technologies
{{advert|date=April 2012}}
{{Infobox company |
  name   = ZyLAB |
  logo   = <!--  Commented out because image was deleted: [[Image:zylab logo.jpg|center]] --> |
  slogan = "eDiscovery & Information Risk Management" |
  type   = Private |
  foundation     = 1983 |
  location       = [[McLean, Virginia]]<br>[[Amsterdam]] |
  key_people     = [[Pieter Varkevisser]], president & CEO<br>[[Dr. Johannes C. Scholtes]], chairman & chief strategy officer | Mary Mack, Enterprise Technology Counsel
  num_employees  = 140 |
  industry       = [[Software]], eDiscovery and Information Risk Management, Records Management, Email Archiving, SharePoint Archiving |
  products       = ZyLAB Information Management Platform and various bundles for eDiscovery, email & SharePoint archiving, text-analytics, visualization, contract management, and workflow. |

  homepage       = [http://www.zylab.com/ www.zylab.com]
}}

'''ZyLAB''' is a developer of software for [[Electronic discovery|e-discovery]], information risk management, email management, records, contract, and document management, knowledge management, and workflow. The company is headquartered in [[McLean, Virginia]] and in [[Amsterdam]], [[Netherlands]]. ZyLABs most important products are ZyLAB eDiscovery & Production System, the ZyLAB Information Management Platform and bundles that build systems for deployments.

== History ==
In 1983 ZyLAB was the first company providing a [[Full text search|full-text]] search program for electronic files stored in the file system of [[IBM PC compatible|IBM-compatible PCs]]. The program was called ZyINDEX. The first version of ZyINDEX was written in [[Pascal (programming language)|Pascal]] and worked on [[MS-DOS]]. Subsequent programs were written in [[C (programming language)|C]], [[C++]] and [[C Sharp (programming language)|C#]] and work on a variety of Microsoft operating systems.

In 1991, ZyLAB integrated ZyINDEX with an optical character recognition ([[Optical character recognition|OCR]]) program, Calera Wordscan, which was a spin-off from [[Raymond Kurzweil]]s first OCR implementation. This integration was called ZyIMAGE. ZyIMAGE was the first PC program to include a [[Fuzzy string searching|fuzzy string search]] algorithm to overcome scanning and OCR errors.

In 1998, the company developed support to full-text search email, including attachments.

In 2000, ZyLAB embraced the new [[XML]] standard and created a full content management and records management system based on the XML standard and build a full solution for e-discovery, historical archives, records management, document management, email archiving, contract management, and professional back-office solutions.

In 2003, the company invested in expanding the ZyIMAGE product suite with advanced [[text analytics]], [[text mining]], [[data visualization]], [[computational linguistics]], and [[Machine translation|automatic translation]].

2005: ZyIMAGE Information Access Platform was released, an integrated solution to address information access problems.

Platforms for ZyIMAGE e-Discovery and legal production, historical archiving, compliance, back-office records management and [[COMINT#COMINT|COMINT]] were launched in 2007.

2010: ZyLAB Information Management Platform was released, an integrated solution to address e-Discovery and information management problems.

==Customers==
Initial customers of ZyINDEX were organizations such as the [[FBI]] and other law enforcement agencies to investigate electronic data from seized PCs, the [[United States Navy|U.S. Navy]] for on-board manuals, and law firms around the world for [[Electronic discovery|e-Discovery]]. Over the years, ZyLAB received grants from the European Union (DG13).

Other well-known ZyLAB customers were [[O. J. Simpson murder case|O.J. Simpson's defense team]], war crime tribunals such as the [[trial of Slobodan Milosevic]], the [[Special Court for Sierra Leone]], the [[Extraordinary Chambers in the Courts of Cambodia|UN-AKRT-ECCC Cambodia Khmer Rouge trials]] and the [[International Criminal Tribunal for Rwanda|Rwanda tribunal]]. In 2007, the U.S. [[Executive Office of the President of the United States|Executive Office of the President]] selected ZyLAB for email archiving, basically for its open XML structures, which is endorsed by organizations such as the [[National Archives and Records Administration]]. ZyLABs software was used for many other high-profile investigations such as the [[Oklahoma City bombing]].

Public websites also use the ZyLAB Webserver.

[[Gartner]] positioned ZyLAB in the "Leaders" quadrant in its 2007, 2008 and 2009 Magic Quadrant for Information Access Solutions, gave it a strong positive rating in its 2007, 2008 and 2009 e-Discovery Marketscope and a Positive Rating in its 2007 and 2008 Records Management MarketScope.

ZyLABs chief strategy officer, Dr. Johannes C. Scholtes, is professor in [[text mining]] at [[Maastricht University|the University of Maastricht]] faculty of Humanities and Sciences and director in the board of AIIM.

==System overview and compatibility==
According to the companys website it delivers systems for deployments, product bundles and the core components is the ZyLAB Information Management platform include:

Systems:
*ZyLAB eDiscovery and Production
*ZyLAB Compliance and Litigation readiness
*ZyLAB Law Enforcement and Investigations
*ZyLAB Communications Intelligence
*ZyLAB Digital Print and Media Archiving
*ZyLAB Enterprise Information Management

Bundles:
*E-Mail Archiving Bundle
*Microsoft SharePoint Bundle
*Analytics Bundle
*eDiscovery EDRM Processing bundle
*DoD and Sox Compliant RMA Bundle
*TIFF Archiving and Production Bundle
*WebPublishing Bundle
*Commercial Publishing Bundle
*Business Process Automation Bundle
*Development and Integrators Bundle
*Scanning Bundle
*Digital Copier Bundle
*Professional Text Mining
*Machine translation

===Supported configurations===
*'''Server OS''': Windows 2003, Windows 2008
*'''Databases''': XML, MS SQL Server 2005, MS SQL Server 2008, Oracle 10g, Oracle 11g, mySQL
*'''Web Servers''': IIS
*'''Client OS''': Windows XP, Windows Vista, Windows 7
*'''Clustering''': Support for Active/Passive Failover.
*'''Authentication''': Active Directory, LDAP, XML, NTFS, IBM Tripoli.
*'''Virtualization''': VMware Infrastructure, VMware Workstation, VMware Server, VMware Fusion.

===Languages supported===
*'''Unicode'''. Support for documents in all languages.
*'''Internationalization'''. ZyLAB offers translated products for English, German, French, Dutch, Spanish, Italian, Danish, Swedish, Norwegian, Finnish, Portuguese, Arabic and [[Persian language|Persian]]. In addition to these languages, over 400 languages are supported by ZyLAB's recognition and full-text indexing technology, including all Western-European, Eastern European, Baltic, African, Asian and South American languages. ZyLAB's technical ability for broad language and character recognition enhances the accuracy of stored information searches and helps diminish the costs incurred by incorrect searches or text correction.

==Zy-IMAGE-nation Annual Conference==
The annual Zy-IMAGE-nation Conference is sponsored by ZyLAB. During this conference, seminars and interactive sessions from leading professionals about the advanced technologies and procedural enhancements that are driving new levels of operational efficiency in private and public sectors. The focus of the conference is on technologies that provide integrated capabilities for managing the accumulated knowledge of an organization, especially records and e-mail, as well as other business-critical processes. Related topics to be covered include best practices for e-discovery preparation and implementation, records management, email archiving, and knowledge management.

==See also==
* [[Electronic discovery|e-Discovery]]
* [[Optical character recognition|Optical Character Recognition (OCR)]]
* [[Document Imaging]]
* [[E-mail archiving|E-mail Archiving]]
* [[Knowledge Management]]
* [[Document management system|Document Management (System)]]
* [[Enterprise content management|Enterprise Content Management]]
* [[Records management|Records Management]]
* [[Contract management|Contract Management]]
* [[Workflow]]
* [[Text mining|Text Mining]]
* [[Text analytics|Text Analytics]]
* [[Machine translation|Automatic Machine Translation]]
* [[Data visualization|Data Visualization]]

==References==
{{Reflist}}
*[http://www.pcmag.com/encyclopedia_term/0,,t=zyindex&i=55248,00.asp Definition of ZyINDEX] in [[PC Magazine|''PCMAG.com'']]'s encyclopedia
*[http://www.pcmag.com/encyclopedia_term/0,2542,t=ZyIMAGE&i=55247,00.asp Definition of ZyIMAGE] in [[PC Magazine|''PCMAG.com'']]'s encyclopedia
*[http://www.informationweek.com/777/knowledge.htm Review] of ZyImage 3.0 in ''[[InformationWeek]]''
*[http://www.accessmylibrary.com/coms2/summary_0286-9201794_ITM Mac version of ZyINDEX made its debut on Comdex]
*[http://query.nytimes.com/gst/fullpage.html?res=940DE6DA1730F93AA35751C0A96E948260 Review] of ZyINDEX in the ''[[New York Times]]''
*[http://www.computerwoche.de/heftarchiv/1988/26/1155611/ Review] of ZyINDEX on ''Computerwoche.de'' (article in German)
*[http://www.computerwoche.de/index.cfm?pid=2123&pk=1096333 Review] of ZyIMAGE's webserver on ''Computerwoche.de'' (article in German)
*[http://nl.newsbank.com/nl-search/we/Archives?p_product=MH&s_site=miami&p_multi=MH&p_theme=realcities&p_action=search&p_maxdocs=200&p_topdoc=1&p_text_direct-0=0EB367D56736E685&p_field_direct-0=document_id&p_perpage=10&p_sort=YMD_date: Review] of ZyINDEX in the ''[[Miami Herald]]''
*[http://www.usdoj.gov/oig/special/0203/chapter3.htm ZyINDEX used in the Investigation of the Belated Production of Documents in the Oklahoma City Bombing Case]
*[http://www.fcw.com/print/6_31/news/70014-1.html Review] of ZyIMAGE on ''Federal Computer Week (FCW.com)''
*Zylab retrieval engine optimized for CD-ROM; Zylab, Progressive Technologies merge," Seybold Report on Desktop Publishing. vol. 8, No. 10, Jun. 6, 1994, p. 40.
*Knibbe, "ZyImage 2 boosts, OCR, batch duties," InfoWorld, vol. 15, Issue 51, Dec. 20, 1993, p.&nbsp;20.
*Knibbe, "ZyImage 3.0 will facilitate distribution on CD-ROMs; Boasts integration with WordScan OCR software," InfoWorld, vol. 16, No. 38, Sep. 19, 1994, p.&nbsp;22.
*Marshall, "Text retrieval alternatives: 10 more ways to pinpoint important information," Infoworld, vol. 14, No. 12, Mar. 23, 1992, pp.&nbsp;8889.
*Marshall, "ZyImage adds scanning access to ZyIndex," InfoWorld, vol. 16, No. 15, Apr. 11, 1994, pp.&nbsp;73, 76, and 77.
*Marshall, "ZyImage is ZyIndex plus a scan interface integrated," InfoWorld. vol. 15, Issue 10, Mar. 8, 1993, p.&nbsp;100.
*Marshall et al., "ZyIndex for Windows, Version 5.0," InfoWorld, v. 15, n. 21, May 1993, pp.&nbsp;127, 129, 133 and 137.
*Simon, "ZyImage: A Winning Combination of OCR And Text Indexing," PC Magazine. vol. 12, No. 6, Mar. 30, 1993, p.&nbsp;56.
*Rooney, "Text-retrieval veterans prepare Windows attack," PC Week, v. 9, n. 24, Jun. 1992, p.&nbsp;46.
*Rooney, "ZyLab partners with Calera: firms roll out document-image system," PC Week, vol. 10, No. 3, Jan. 25, 1993, p.&nbsp;22.
*Torgan, "ZyImage: Document Imaging and Retrieval System," PC Magazine. vol. 12, No. 3, Feb. 9, 1993, p.&nbsp;62.

===Gartner reports===
*Introduction to Investigative Case Management Products (18 April 2007)
*Hype Cycle for Legal and Regulatory Information Governance, 2007 (16 July 2007)
*MarketScope for Contract Management, 2007 (16 July 2007)
*Choosing an E-Discovery Solution in 2007 and 2008 (18 July 2007)
*Magic Quadrant for Information Access Technology, 2007 (5 September 2007)
*Magic Quadrant for Information Access Technology, 2008
*Magic Quadrant for Information Access Technology, 2009
*The Expanding Enterprise E-Discovery Marketplace (12 November 2007)
*MarketScope for E-Discovery and Litigation Support Vendors, 2007 (14 December 2007)
*MarketScope for E-Discovery Product Vendors, 2008
*MarketScope for E-Discovery Product Vendors, 2009
*MarketScope for Records Management (20 May 2008)
*Hype Cycle for Content Management, 2008 (8 July 2008)
*Using the Electronic Discovery Reference Model to Identify, Collect and Preserve Digital Evidence (11 July 2008)
*Using the Electronic Discovery Reference Model to Process, Review and Analyze Digital Evidence (11 July 2008)
*Hype Cycle for Governance, Risk and Compliance Technologies, 2009 (17 July 2009)

==External links==
*[http://www.zylab.com/ ZyLAB official website]
*[http://www.edrm.net/ The Electronic Discovery Reference Model (EDRM)]
*[http://www.aiim.org/ AIIM]

[[Category:Companies established in 1983]]
[[Category:Software companies of the United States]]
[[Category:Information retrieval]]
>>EOP<<
32<|###|>IR evaluation
{{multiple issues|
{{Orphan|date=February 2009}}
{{Unreferenced|date=March 2008}}
}}

== IR Evaluation ==
The evaluation of information retrieval system is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for [[Standard Boolean model|Boolean retrieval]] or top-k retrieval, include [[precision and recall]].

*'''Precision''' is the fraction of retrieved documents that are [[Relevance (information retrieval)|relevant]] to the query:

:<math> \mbox{precision}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{retrieved documents}\}|} </math>

*'''Recall''' is the fraction of the documents relevant to the query that are successfully retrieved:

:<math> \mbox{recall}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{relevant documents}\}|} </math>

*'''F-measure''' is the harmonic mean of precision and recall:

:<math> \mbox{F-measure}= 2 * \frac{\{\mbox{precision}\}*\{\mbox{recall}\}}{\{\mbox{precision}\}+\{\mbox{recall}\}} </math>

For modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. [[Precision and recall#Precision|Precision]] at k documents (P@k) is still a useful metric (e.g., P@10 corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k.

F-measure tends to be a better single metric when compared to precision and recall because both of them give different information that can complement each other when combined. If one of them excels more than the other, this metric will reflect it.

Virtually all modern evaluation metrics (e.g., [[Information retrieval#Mean average precision|mean average precision]], [[Information retrieval#Discounted cumulative gain|discounted cumulative gain]]) are designed for '''ranked retrieval''' without any explicit rank cutoff, taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks.

==See also==
* [[Information retrieval]]
* [[Precision and recall]]
* [[Web search engine]]

==Further reading==
* Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch&uuml;tze. [http://www-csli.stanford.edu/~hinrich/information-retrieval-book.html Introduction to Information Retrieval]. Cambridge University Press, 2008.
*Stefan B&uuml;ttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.

[[Category:Information retrieval]]
[[Category:Searching]]
>>EOP<<
38<|###|>Globrix
'''Globrix''' was a UK [[real estate]] [[Web search engine|search engine]] that was launched in January 2008. It was launched as a joint venture with [[News International]], publishers of ''[[The Sunday Times]]'', ''[[The Sun (newspaper)|The Sun]]'', ''[[The Times]]'', ''[[The News of the World]]'' and ''[[Thelondonpaper]]''.<ref>[http://www.nma.co.uk/news/news-international-invests-in-property-site-globrix/35492.article News International invests in property site, Globrix - NMA article]</ref>

[[Estate agent]]s and [[letting agent]]s could list their properties for free. This competed with traditional paid-listings sites such as [[Rightmove]] (originally a joint venture between four of the UK's largest property agents, now a [[public limited company]]), [[Zoopla|Propertyfinder]] (also partly backed by News International) and [[Primelocation]] (owned by [[Daily Mail and General Trust]]). Unlike most property websites, Globrix directed users to agent websites rather than hosting the property details and capturing the lead on Globrix itself. Globrix gathered its property listings in three different ways; crawling agent websites, taking data feeds and by agents manually uploading via the Globrix extranet. Because Globrix was 'free to list', Globrix was able to gain substantial market coverage and claimed to list more properties than any other UK property website. Unlike websites like [[Gumtree]] and [[Oodle]], private sellers and landlords were not allowed to list their properties on the site.

The website charged property professionals and property related services companies for geo-targeted [[Web banner|banner ads]]. There were also premium services available to estate and letting agents (such as [[Search Engine Optimization]] consultancy, branded email alerts and increased traffic) and [[Google Ads]] were displayed in unsold advertising positions on the right hand side of search results.

==Functionality==

The basic property search functionality is kept simple with just one text box on the homepage. Users can search for property by location (e.g. city, town, full postcode, partial postcode or, unusually for property portals, street name), places of interest (e.g. schools, stations, landmarks) or by key features (e.g. swimming pool, garden, double glazing, helipad).

Search results can then be refined further by changing the price parameters, number of bedrooms and bathrooms, property type (e.g. detached, bungalow, flat), outside space, nearby stations and schools and property features (e.g. wooden floors, sea view). Registered users are able to search by additional parameters such as price change.

As an alternative to the regular 'list view' of property results, users can also opt to see the search results plotted on [[Bing Maps]] (previously they used [[Google map]]) to allow users to look for property by location. (Some users are unimpressed with the lack of precision of the inferior Bing offering, which often manages to put the marker in a field, compared to the accuracy and ease of use of Googlemaps).  Users are able to drag and zoom the map, with relevant properties automatically placed in view. It is also possible for users to draw a catchment area directly onto the map of where they would like to search.

==Data==

Globrix data was sometimes used by the national media to illustrate stories on house prices,<ref>House prices drop 100,000 in two weeks in race to sell before Christmas - Daily Mail [http://www.dailymail.co.uk/news/article-1089563/House-prices-drop-100-000-WEEKS-race-sell-Christmas.html]</ref> the economy, area trends, consumer confidence<ref>[http://news.bbc.co.uk/1/hi/business/7737507.stm House sales rise as prices fall - BBC News]</ref> and the property market.<ref>[http://www.telegraph.co.uk/finance/personalfinance/borrowing/mortgages/3268208/Housing-market-stagnates-as-buyers-disappear.html Housing market stagnates as buyers disappear - Daily Telegraph]</ref>

==Awards==

In 2008, Globrix was awarded 'Best Property Portal UK' which is awarded by one of the group's own newspapers, the [[The Daily Mail]].<ref>[http://www.residentialpropertyawards.net/index.php/International/Winners/Winners-of-2008.html Daily Mail Property Awards 2008]</ref> Globrix also won 'Estate Agency Service Firm of the Year' at The Negotiator Awards.<ref>[http://negotiator-magazine.co.uk/events/awards/categories-and-finalists/agency-service-firm-of-the-year/ The Negotiator Awards 2008]</ref>

==Founders==

Globrix was founded by Dan Lee and Ian Parry, both ex employees of UK-based search company [[Autonomy Corporation|Autonomy]] and the Norwegian search company [[Fast Search & Transfer|FAST]].

==Merged with Zoopla==

In December 2012 Globrix merged with [[Zoopla]].<ref name="Estate Agent Today">{{cite web|title=Zoopla acquires Globrix as it steps up battle against Rightmove|url=http://www.estateagenttoday.co.uk/news_features/Zoopla-acquires-Globrix-as-it-steps-up-battle-against-Rightmove|work=Estate Agent Today|accessdate=8 September 2013}}</ref>

== References ==
<references/>

==External links==
* [http://www.globrix.com/ Globrix homepage]
* [http://www.ft.com/cms/s/0/bc401968-824e-11dc-8a8f-0000779fd2ac.html News International invests in search engine] - Financial Times
* [http://www.independent.co.uk/news/business/analysis-and-features/home-search-sites-have-a-new-kid-on-the-block-786342.html Home search sites have a new kid on the block] - The Independent

[[Category:Information retrieval]]
[[Category:Internet search engines]]
[[Category:Online real estate companies]]
>>EOP<<
44<|###|>Discounted cumulative gain
'''Discounted cumulative gain''' ('''DCG''') is a measure of ranking quality. In [[information retrieval]], it is often used to measure effectiveness of [[World Wide Web|web]] [[search engine]] [[algorithm]]s or related applications. Using a [[Relevance (information retrieval)|graded relevance]] scale of documents in a search engine result set, DCG measures the usefulness, or ''gain'', of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom with the gain of each result discounted at lower ranks.<ref>Kalervo Jarvelin, Jaana Kekalainen: Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems 20(4), 422446 (2002)</ref>

== Overview ==

Two assumptions are made in using DCG and its related measures.

# Highly relevant documents are more useful when appearing earlier in a search engine result list (have higher ranks)
# Highly relevant documents are more useful than marginally relevant documents, which are in turn more useful than irrelevant documents.

DCG originates from an earlier, more primitive, measure called Cumulative Gain.

=== Cumulative Gain ===

Cumulative Gain (CG) is the predecessor of DCG and does not include the position of a result in the consideration of the usefulness of a result set. In this way, it is the sum of the graded relevance values of all results in a search result list. The CG at a particular rank position <math>p</math> is defined as:

:<math> \mathrm{CG_{p}} = \sum_{i=1}^{p} rel_{i} </math>

Where <math>rel_{i}</math> is the graded relevance of the result at position <math>i</math>.

The value computed with the CG function is unaffected by changes in the ordering of search results. That is, moving a highly relevant document <math>d_{i}</math> above a higher ranked, less relevant, document <math>d_{j}</math> does not change the computed value for CG. Based on the two assumptions made above about the usefulness of search results, DCG is used in place of CG for a more accurate measure.

=== Discounted Cumulative Gain ===

The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result. The discounted CG accumulated at a particular rank position <math>p</math> is defined as:<ref name="stanfordireval">{{cite web|title=Introduction to Information Retrieval - Evaluation|url=http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf|publisher=Stanford University|accessdate=23 March 2014|date=21 April 2013}}</ref>

:<math> \mathrm{DCG_{p}} = rel_1 + \sum_{i=2}^{p} \frac{rel_{i}}{\log_{2}(i)} </math>

Previously there has not been shown any theoretically sound justification for using a [[logarithm]]ic reduction factor<ref>{{cite book | title=Search Engines: Information Retrieval in Practice | author=B. Croft, D. Metzler, and T. Strohman |year=2009 | publisher=''Addison Wesley"}}</ref> other than the fact that it produces a smooth reduction.

An alternative formulation of DCG<ref>Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning (ICML '05). ACM, New York, NY, USA, 89-96. DOI=10.1145/1102351.1102363 http://doi.acm.org/10.1145/1102351.1102363</ref> places stronger emphasis on retrieving relevant documents:

:<math> \mathrm{DCG_{p}} = \sum_{i=1}^{p} \frac{ 2^{rel_{i}} - 1 }{ \log_{2}(i+1)} </math>

The latter formula is commonly used in industry including major web search companies<ref name="stanfordireval"/> and data science competition platform such as Kaggle.<ref>{{cite web|title=Normalized Discounted Cumulative Gain|url=https://www.kaggle.com/wiki/NormalizedDiscountedCumulativeGain|accessdate=23 March 2014}}</ref>

In Croft, Metzler and Strohman (page 320, 2010), the authors mistakenly claim that these two formulations of DCG are the same when the relevance values of documents are [[binary function|binary]]; <math>rel_{i} \in \{0,1\}</math>.  To see that they are not the same, let there be one relevant document and that relevant document is at rank 2.  The first version of DCG equals 1 / log2(2) = 1.  The second version of DCG equals 1 / log2(2+1) = 0.631.  The way that the two formulations of DCG are the same for binary judgments is in the way gain in the numerator is calculated.  For both formulations of DCG, binary relevance produces gain at rank i of 0 or 1.  No matter the number of relevance grades, the two formulations differ in their discount of gain.

Note that Croft et al. (2010) and Burges et al. (2005) present the second DCG with a log of base e, while both versions of DCG above use a log of base 2.  When computing NDCG with the second formulation of DCG, the base of the log does not matter, but the base of the log does affect the value of NDCG for the first formulation.  Clearly, the base of the log affects the value of DCG in both formulations.

Recently, Wang et al.(2013)<ref>Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei Chen, Tie-Yan Liu. 2013. A Theoretical Analysis of NDCG Ranking Measures. In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013).</ref> give theoretical guarantee for using the logarithmic reduction factor in NDCG. Specifically, the authors prove for every pair of substantially different ranking functions, the ranking measure can decide which one is better in a consistent manner on almost all datasets.

=== Normalized DCG ===

Search result lists vary in length depending on the [[Web search query|query]]. Comparing a search engine's performance from one query to the next cannot be consistently achieved using DCG alone, so the cumulative gain at each position for a chosen value of <math>p</math> should be normalized across queries. This is done by sorting documents of a result list by relevance, producing the maximum possible DCG till position <math>p</math>, also called Ideal DCG (IDCG) till that position. For a query, the ''normalized discounted cumulative gain'', or nDCG, is computed as:

:<math> \mathrm{nDCG_{p}} = \frac{DCG_{p}}{IDCG_{p}} </math>

The nDCG values for all queries can be averaged to obtain a measure of the average performance of a search engine's ranking algorithm. Note that in a perfect ranking algorithm, the <math>DCG_p</math> will be the same as the <math>IDCG_p</math> producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.

The main difficulty encountered in using nDCG is the unavailability of an ideal ordering of results when only partial [[relevance feedback]] is available.

== Example ==

Presented with a list of documents in response to a search query, an experiment participant is asked to judge the relevance of each document to the query. Each document is to be judged on a scale of 0-3 with 0 meaning irrelevant, 3 meaning completely relevant, and 1 and 2 meaning "somewhere in between". For the documents ordered by the ranking algorithm as

:<math> D_{1}, D_{2}, D_{3}, D_{4}, D_{5}, D_{6} </math>

the user provides the following relevance scores:

:<math> 3, 2, 3, 0, 1, 2 </math>

That is: document 1 has a relevance of 3, document 2 has a relevance of 2, etc. The Cumulative Gain of this search result listing is:

:<math> \mathrm{CG_{6}} = \sum_{i=1}^{6} rel_{i} = 3 + 2 + 3 + 0 + 1 + 2 = 11</math>

Changing the order of any two documents does not affect the CG measure. If <math>D_3</math> and <math>D_4</math> are switched, the CG remains the same, 11. DCG is used to emphasize highly relevant documents appearing early in the result list. Using the logarithmic scale for reduction, the DCG for each result in order is:

{| class="wikitable" border="1"
|-
! <math>i</math>
! <math>rel_{i}</math>
! <math>\log_{2}i</math>
! <math> \frac{rel_{i}}{\log_{2}i} </math>
|-
| 1
| 3
| 0
| N/A
|-
| 2
| 2
| 1
| 2
|-
| 3
| 3
| 1.585
| 1.892
|-
| 4
| 0
| 2.0
| 0
|-
| 5
| 1
| 2.322
| 0.431
|-
| 6
| 2
| 2.584
| 0.774
|}

So the <math>DCG_{6}</math> of this ranking is:

:<math> \mathrm{DCG_{6}} = rel_{1} + \sum_{i=2}^{6} \frac{rel_{i}}{\log_{2}i} = 3 + (2 + 1.892 + 0 + 0.431 + 0.774) = 8.10</math>

Now a switch of <math>D_3</math> and <math>D_4</math> results in a reduced DCG because a less relevant document is placed higher in the ranking; that is, a more relevant document is discounted more by being placed in a lower rank.

The performance of this query to another is incomparable in this form since the other query may have more results, resulting in a larger overall DCG which may not necessarily be better. In order to compare, the DCG values must be normalized.

To normalize DCG values, an ideal ordering for the given query is needed. For this example, that ordering would be the [[Monotonic|monotonically decreasing]] sort of the relevance judgments provided by the experiment participant, which is:

:<math> 3, 3, 2, 2, 1, 0 </math>

The DCG of this ideal ordering, or ''IDCG'', is then:

:<math> \mathrm{IDCG_{6}} = 8.69 </math>

And so the nDCG for this query is given as:

:<math> \mathrm{nDCG_{6}} = \frac{DCG_{6}}{IDCG_{6}} = \frac{8.10}{8.69} = 0.932 </math>

== Limitations ==
# Normalized DCG metric does not penalize for bad documents in the result. For example, if a query returns two results with scores <math> 1,1,1 </math> and <math> 1,1,1,0 </math> respectively, both would be considered equally good even if later contains a bad result. One way to take into account this limitation is use <math>1 - 2^{rel_{i}}</math> in numerator for scores for which we want to penalize and <math>2^{rel_{i}} - 1</math> for all others. For example, for the ranking judgments <math>Excellent, Fair, Bad</math> one might use numerical scores <math>1,0,-1</math> instead of <math>2,1,0</math>.
# Normalized DCG does not penalize for missing documents in the result. For example, if a query returns two results with scores <math> 1,1,1 </math> and <math> 1,1,1,1,1 </math> respectively, both would be considered equally good. One way to take into account this limitation is to enforce fixed set size for the result set and use minimum scores for the missing documents. In previous example, we would use the scores <math> 1,1,1,0,0 </math> and <math> 1,1,1,1,1 </math> and quote nDCG as nDCG@5.
# Normalized DCG may not be suitable to measure performance of queries that may typically often have several equally good results. This is especially true when this metric is limited to only first few results as it is done in practice. For example, for queries such as "restaurants" nDCG@1 would account for only first result and hence if one result set contains only 1 restaurant from the nearby area while the other contains 5, both would end up having same score even though latter is more comprehensive.

== References ==
{{Reflist|1}}

[[Category:Information retrieval|*]]
>>EOP<<
50<|###|>Relevance feedback
'''Relevance [[feedback]]''' is a feature of some [[information retrieval]] systems.  The idea behind relevance feedback is to take the results that are initially returned from a given query and to use information about whether or not those results are relevant to perform a new query.  We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or "pseudo" feedback.

== Explicit feedback ==

Explicit feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query. This type of feedback is defined as explicit only when the assessors (or other users of a system) know that the feedback provided is interpreted as [[Relevance (information retrieval)|relevance]] judgments.

Users may indicate relevance explicitly using a ''binary'' or ''graded'' relevance system. Binary relevance feedback indicates that a document is either relevant or irrelevant for a given query. Graded relevance feedback indicates the relevance of a document to a query on a scale using numbers, letters, or descriptions (such as "not relevant", "somewhat relevant", "relevant", or "very relevant"). Graded relevance may also take the form of a cardinal ordering of documents created by an assessor; that is, the assessor places documents of a result set in order of (usually descending) relevance.  An example of this would be the [[SearchWiki]] feature implemented by [[Google]] on their search website.

The relevance feedback information needs to be interpolated with the original query to improve retrieval performance, such as the well-known [[Rocchio Classification#Algorithm|Rocchio Algorithm]].

A performance [[Metric (mathematics)|metric]] which became popular around 2005 to measure the usefulness of a ranking [[algorithm]] based on the explicit relevance feedback is [[NDCG]]. Other measures include [[Precision (information retrieval)|precision]] at ''k'' and [[Mean average precision#Mean average precision|mean average precision]].

== Implicit feedback ==

Implicit feedback is inferred from user behavior, such as noting which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions [http://www.scils.rutgers.edu/etc/mongrel/kelly-belkin-SIGIR2001.pdf].

The key differences of implicit relevance feedback from that of explicit include [http://haystack.lcs.mit.edu/papers/kelly.sigirforum03.pdf]:

# the user is not assessing relevance for the benefit of the IR system, but only satisfying their own needs and
# the user is not necessarily informed that their behavior (selected documents) will be used as relevance feedback

An example of this is the [[Surf Canyon]] [[browser extension]], which advances search results from later pages of the result set based on both user interaction (clicking an icon) and time spent viewing the page linked to in a search result.

== Blind feedback ==

Pseudo relevance feedback, also known as  blind relevance feedback, provides a method for automatic local analysis. It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction. The method is to do normal retrieval to find an initial set of most relevant documents, to then assume that the top "k" ranked documents are relevant, and finally to do relevance feedback as before under this assumption. The procedure is:

# Take the results returned by initial query as relevant results (only top k with k being between 10 to 50 in most experiments).
# Select top 20-30 (indicative number) terms from these documents using for instance [[tf-idf]] weights.
# Do Query Expansion, add these terms to query, and then match the returned documents for this query and finally return the most relevant documents.

Some experiments such as results from the Cornell SMART system published in (Buckley et al.1995), show improvement of retrieval systems performances using pseudo-relevance feedback in the context of TREC 4 experiments.

This automatic technique mostly works. Evidence suggests that it tends to work better than global analysis.<ref>Jinxi Xu and W. Bruce Croft, [http://portal.acm.org/citation.cfm?id=243202''Query expansion using local and global document analysis''], in Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 1996.</ref> Through a query expansion, some relevant documents missed in the initial round can then be retrieved to improve the overall performance. Clearly, the effect of this method strongly relies on the quality of selected expansion terms. It has been found to improve performance in the TREC ad hoc task {{Citation needed|date=March 2011}}. But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile. In addition, if the words added to the original query are unrelated to the query topic, the quality of the retrieval is likely to be degraded, especially in Web search, where web documents often cover multiple different topics. To improve the quality of expansion words in pseudo-relevance feedback, a positional relevance feedback for pseudo-relevance feedback has been proposed to select from feedback documents those words that are focused on the query topic based on positions of words in feedback documents.<ref>Yuanhua Lv and ChengXiang Zhai, [http://portal.acm.org/citation.cfm?id=1835546''Positional relevance model for pseudo-relevance feedback''], in Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 2010.</ref> 
Specifically, the positional relevance model assigns more weights to words occurring closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic.

Blind feedback automates the manual part of relevance feedback and has the advantage that assessors are not required.

== Using relevance information ==

Relevance information is utilized by using the contents of the relevant documents to either adjust the weights of terms in the original query, or by using those contents to add words to the query.  Relevance feedback is often implemented using the [[Rocchio Classification#Algorithm|Rocchio Algorithm]].

==Further reading==
*[http://www.umiacs.umd.edu/~jimmylin/LBSC796-INFM718R-2006-Spring/lecture7.ppt Relevance feedback lecture notes] - Jimmy Lin's lecture notes, adapted from Doug Oard's
*[http://www.ischool.berkeley.edu/~hearst/irbook/chapters/chap10.html] - chapter from ''Modern Information Retrieval''
*Stefan Buttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.

== References ==
{{reflist|2}}

[[Category:Information retrieval]]

[[zh:]]
>>EOP<<
56<|###|>Enterprise search
'''Definition:''' '''Enterprise search''' is the organized retrieval of '''structured''' and '''unstructured''' data within an organization. 


'''Enterprise search''' is the practice of making content from multiple enterprise-type sources, such as [[database]]s and [[intranet]]s, searchable to a defined audience.

==Enterprise search summary==
"Enterprise Search" is used to describe the software of search information within an enterprise (though the search function and its results may still be public).<ref>[http://www.aiim.org/What-is-Enterprise-Search What is Enterprise Search?]</ref> Enterprise search can be contrasted with [[web search]], which applies search technology to documents on the open web, and [[desktop search]], which applies search technology to the content on a single computer.

Enterprise search systems index data and documents from a variety of sources such as: [[file systems]], [[intranets]], [[document management system]]s, [[e-mail]], and [[databases]]. Many enterprise search systems integrate structured and unstructured data in their collections.<ref>[http://www.arma.org/bookstore/files/Delgado.pdf The New Face of Enterprise Search: Bridging Structured and Unstructured Information]</ref> Enterprise search systems also use access controls to enforce a security policy on their users.<ref>[http://www.ideaeng.com/tabId/98/itemId/118/Mapping-Security-Requirements-to-Enterprise-Search.aspx Mapping Security Requirements to Enterprise Search - Part 1: Defining Specific Security Requirements]</ref>

Enterprise search can be seen as a type of [[vertical search]] of an enterprise.

==Components of an enterprise search system==
In an enterprise search system, content goes through various phases from source repository to search results:

=== Content awareness ===
Content awareness (or "content collection") is usually either a push or pull model. In the push model, a source system is integrated with the search engine in such a way that it connects to it and pushes new content directly to its [[API]]s. This model is used when realtime indexing is important. In the pull model, the software gathers content from sources using a connector such as a [[web crawler]] or a [[database]] connector. The connector typically polls the source with certain intervals to look for new, updated or deleted content.<ref>[http://www.information-management.com/issues/20_7/content_management_data_integration_indexing_metadata-10019105-1.html Understanding Content Collection and Indexing]</ref>

=== Content processing and analysis ===
Content from different sources may have many different formats or document types, such as XML, HTML, Office document formats or plain text. The content processing phase processes the incoming documents to plain text using document filters. It is also often necessary to normalize content in various ways to improve [[Recall (information retrieval)|recall]] or [[Precision (information retrieval)|precision]]. These may include [[stemming]], [[lemmatization]], [[synonym]] expansion, [[entity extraction]], [[part of speech]] tagging.

As part of processing and analysis, [[tokenization (lexical analysis)|tokenization]] is applied to split the content into [[Lexical analysis#Token|tokens]] which is the basic matching unit. It is also common to normalize tokens to lower case to provide case-insensitive search, as well as to normalize accents to provide better recall.<ref>[http://packages.python.org/Whoosh/stemming.html Stemming, Variations, and Accent Folding]</ref>

=== Indexing ===
The resulting text is stored in an [[Index (search engine)|index]], which is optimized for quick lookups without storing the full text of the document. The index may contain the dictionary of all unique words in the corpus as well as information about ranking and [[term frequency]].

=== Query Processing ===
Using a web page, the user issues a [[Web search query|query]] to the system. The query consists of any terms the user enters as well as navigational actions such as [[faceted search|faceting]] and paging information.

=== Matching ===
The processed query is then compared to the stored index, and the search system returns results (or "hits") referencing source documents that match. Some systems are able to present the document as it was indexed.

==Differences from web search==
Beyond the difference in the kinds of materials being indexed, enterprise search systems also typically include functionality that is not associated with the mainstream [[web search engine]]s. These include:
*Adapters to index content from a variety of repositories, such as [[databases]] and [[content management systems]].
*[[Federated search]], which consists of
# transforming a query and broadcasting it to a group of disparate databases or external content sources with the appropriate syntax,
# merging the results collected from the databases,
# presenting them in a succinct and unified format with minimal duplication, and
# providing a means, performed either automatically or by the portal user, to sort the merged result set.
*[[Enterprise bookmarking]], collaborative [[tag (metadata)|tagging]] systems for capturing knowledge about structured and semi-structured enterprise data.
*[[Entity extraction]] that seeks to locate and classify elements in text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.
*[[Faceted search]], a technique for accessing a collection of information represented using a [[faceted classification]], allowing users to explore by filtering available information.
*Access control, usually in the form of an [[Access control list]] (ACL), is often required to restrict access to documents based on individual user identities. There are many types of access control mechanisms for different content sources making this a complex task to address comprehensively in an enterprise search environment (see below).
*[[Text clustering]], which groups the top several hundred search results into topics that are computed on the fly from the search-results descriptions, typically titles, excerpts (snippets), and meta-data.  This technique lets users navigate the content by topic rather than by the meta-data that is used in faceting. Clustering compensates for the problem of incompatible meta-data across multiple enterprise repositories, which hinders the usefulness of faceting.
*[[User interfaces]], which in web search are deliberately kept simple in order not to distract the user from clicking on ads, which generates the revenue.  Although the business model for enterprise search could include showing ads, in practice this is not done.  To enhance end user productivity, enterprise vendors continually experiment with rich UI functionality which occupies significant screen space, which would be problematic for web search.

==Relevance factors for enterprise search==
The factors that determine the relevance of search results within the context of an enterprise overlap with but are different from those that apply to web search. In general, enterprise search engines cannot take advantage of the rich [[hyperlink|link structure]] as is found on the web's [[hypertext]] content, however, a new breed of Enterprise search engines based on a bottom-up [[Web 2.0]] technology are providing both a contributory approach and [[hyperlink]]ing within the enterprise. Algorithms like [[PageRank]] exploit hyperlink structure to assign authority to documents, and then use that authority as a query-independent relevance factor. In contrast, enterprises typically have to use other query-independent factors, such as a document's recency or popularity, along with query-dependent factors traditionally associated with [[information retrieval]] algorithms.  Also, the rich functionality of enterprise search UIs, such as clustering and faceting, diminish reliance on ranking as the means to direct the user's attention.

==Access Control - early binding vs late binding==
Security and restricted access to documents is an important matter in Enteprise Search. There are two main approaches to apply restricted access: early binding vs late binding.<ref>[http://enterprisesearch.co/enterprise-search-document-access-control/ Enterprise Search: document access control]</ref>

===Late binding===
Permissions are analyzed and assigned to documents at query stage. Query engine generates a document set and before returning it to a user this set is filtered based on user access rights. It is costly process but accurate (based on user permissions at the moment of query).

===Early binding===
Permissions are analyzed and assigned to documents at indexing stage. It is much more effective than late binding, but could be inaccurate (user might be granted or revoked permissions between in the period between indexing and querying).

==Search Relevance Testing options==
Search application relevance can be determined by following relevance testing options like<ref>[http://searchhub.org/2009/09/02/debugging-search-application-relevance-issues/  Debugging Search Application Relevance Issues]</ref>
*Focus groups
*Reference evaluation protocol (based on relevance judgements of results from agreed-upon queries performed against common document corpuses)
*Empirical testing
*[[A/B testing]]
*Log analysis on a Beta production site
*Online ratings

==See also==
*[[Comparison of enterprise search software]]
*[[List of enterprise search vendors]]
*[[List of Search Engines]]
*[[Collaborative search engine]]
*[[Data Defined Storage]] 
*[[Enterprise bookmarking]]
*[[Enterprise information access]]
*[[Knowledge management]]
*[[Text mining]]
*[[Faceted search]]
*[[Information Extraction]]
*[[Vertical search|Vertical Search]]

==References==
{{Reflist}}

{{DEFAULTSORT:Enterprise Search}}
[[Category:Information retrieval]]
[[Category:Searching]]
>>EOP<<
62<|###|>Isearch
{{for|the adware|Isearch (malware)}}

'''Isearch''' is [[open-source software|open-source]] [[text retrieval]] software first developed in 1994 by Nassib Nassar as part of the Isite [[Z39.50]] information framework. The project started at the Clearinghouse for Networked Information Discovery and Retrieval (CNIDR) of the North Carolina supercomputing center MCNC and funded by the [[National Science Foundation]] to follow in the track of [[Wide Area Information Server|WAIS]] and develop prototype systems for distributed information networks encompassing Internet applications, library catalogs and other information resources.

The main features of Isearch include full text and field searching, relevance ranking, Boolean queries, and support for many document types such as HTML, mail folders, list digests, MEDLINE, BibTeX, SGML/XML, FGDC Metadata, NASA DIF, ANZLIC metadata, ISO 19115 metadata and many other resource types and document formats.

It was the first search engine to be designed from the ground up to support [[SGML]] and ISO [[Z39.50]] search and retrieval. It included many innovations including the "document type" modelwhich is simply a (object oriented) method of associating each document with a class of functions providing a standard interface for accessing the document. It was one of the first engines (if not the first) to ever support XML.

The Isearch search/indexing text algorithms were based on [[Gaston Gonnet]]'s seminal work into PAT arrays and trees for text retrieval--- ideas that were developed for the New Oxford English Dictionary Project at the Univ. of Waterloo, and provided the seeds for [[Tim Bray]]'s PAT SGML engine that formed the basis of [[Open Text]]. One of the limiting factors, however, of the  Isearch design was that it was not well suited to handle the extremely large data sets that became popular in the mid to late 1990s. In many cases Isearch was adapted or modified to use different algorithms but usually retained the document type model and the architectural relationship with Isite.

Isearch was widely adopted and used in hundreds of public search sites, including  many high profile projects such as the [http://patft1.uspto.gov/ U.S. Patent and Trademark Office (USPTO) patent search],[http://clearinghouse3.fgdc.gov/  the Federal Geographic Data Clearinghouse (FGDC)], the NASA Global Change Master Directory, the NASA EOS Guide System, the NASA Catalog Interoperability Project, the Astronomical pre-print service based at the Space Telescope Science Institute, The PCT Electronic Gazette at the World Intellectual Property Organization (WIPO), Linsearch (a search engine for Open Source Software designed by Miles Efron), the SAGE Project of the Special Collections Department at Emory University, Eco Companion Australasia (an environmental geospatial resources catalog), Australian National Genomic Information Service (ANGIS), the [[Open Directory Project]] and numerous governmental portals in the context of the Government Information Locator Service (GILS) [[United States Government Printing Office|GPO]] mandate (ended in 2005?).

From 1994 to 1998 most of the development was centered around the Clearinghouse for Networked Information Discovery and Retrieval (CNIDR) in North Carolina (Engine core) and BSn in Germany (Doctypes). By 1998 much of the open-source Isearch core developers re-focused development into several spin-offs. In 1998 it became part of the Advanced Search Facility reference software platform funded by the U.S. Department of Commerce.

A/WWW Enterprises now maintains the open source version for public usage, supported by paying government clients, such as the U.S. Patent and Trademark Office, NASA, and the FGDC who have provided support to enhance the functionality and reliability of the software. The software suite is considered a reference implementation of catalog service software.

As of 2010, the open source version of Isearch is still used on 250+ nodes of FGDC, and by ANZLIC in Australia and selected Geospatial OneStop contributors to facilitate harvesting by GOS, including NOAA, Census Bureau and the Tenn. Field Office of the US Fish and Wildlife Service, among others.

==References==
*[http://www.springerlink.com/content/g5e2wfd0lekygvut/ Application of Metadata Concepts to Discovery of Internet Resources]
*[http://www.springerlink.com/content/b5chmkgx8akg4m2h/ An Operational Metadata Framework for Searching, Indexing, and Retrieving Distributed Geographic Information Services on the Internet]
* The UNIX Web Server Book, Second Edition, by R. Douglas Matthews et al. (Ventana Press, 1997).
* [http://www.webtechniques.com/archives/1997/05/nassar/  "Searching With Isearch". May 1997, Web Techniques]
* [http://www.itl.nist.gov/fipspubs/fip192.htm FIPS-192: APPLICATION PROFILE FOR THE GOVERNMENT INFORMATION LOCATOR SERVICE (GILS)]
* [http://www.uneca.org/awich/AWICH%20Workshop/YaoundeWorkshop/Clearinghouse%20Yaounde.pdf Clearinghouse and Metadata Concepts, Danel Behanu, U.N. Economic Commission for Africa,  2004]
* [http://web.archive.org/web/19991006225226/http://www.whitehouse.gov/OMB/memoranda/m9805.html M-98-05 Guidance on the Government Information Locator Service] published by the [[Office of Management and Budget|OMB]]
* [http://www.hpcwire.com/archives/3149.html 01/1995 Press Release: Patent Office Launch Internet AIDS Patent Library]

==External links==
*[http://www.fgdc.gov/dataandservices/isite U.S. Federal Geographic Data Committee Isite]
*[http://isite.awcubed.com/ Isite/Isearch2 Documentation Site]
*[ftp://ftp.awcubed.com/pub/Software Current Isearch download site]
*[http://www.etymon.com/tr.html Etymon: Isearch]
*[http://www.ibu.de/node/52 BSn/NONMONOTONIC Lab: IB Search Engine], embeddable search engine. A commercial spin-off from the Isearch project.

===Comparisons===
* [http://www.ukoln.ac.uk/metadata/roads/product-comparison/  Product Comparison: Information Gateway Software]
* [http://wrg.upf.edu/WRG/dctos/Middleton-Baeza.pdf  A Comparison of Open Source Search Engines, Christian Middleton, Ricardo Baeza-Yates]
* [http://www.infomotions.com/musings/opensource-indexers/ Comparing Open Source Indexers]

[[Category:Information retrieval]]
[[Category:Free search engine software]]
>>EOP<<
68<|###|>Faceted search
{{mergeto|Faceted classification|date=January 2015}}
'''Faceted search''', also called '''faceted navigation''' or '''faceted browsing''', is a technique for accessing information organized according to a [[faceted classification]] system, allowing users to explore a collection of information by applying multiple filters. A faceted classification system classifies each information element along multiple explicit dimensions, called facets, enabling the classifications to be accessed and ordered in multiple ways rather than in a single, pre-determined, [[taxonomy (general)|taxonomic]] order.<ref name="Faceted Search">[http://www.morganclaypool.com/doi/abs/10.2200/S00190ED1V01Y200904ICR005 Faceted Search], Morgan & Claypool, 2009</ref>

Facets correspond to properties of the information elements. They are often derived by analysis of the text of an item using [[entity extraction]] techniques or from pre-existing fields in a database such as author, descriptor, language, and format. Thus, existing web-pages, product descriptions or online collections of articles can be augmented with navigational facets.

Within the academic community, faceted search has attracted interest primarily among [[library and information science]] researchers, and to some extent among [[computer science]] researchers specializing in [[information retrieval]].{{fact|date=May 2014}}

==Development==

The [[Association for Computing Machinery]]'s [[Special Interest Group on Information Retrieval]] provided the following description of the role of faceted search for a 2006 workshop:
<blockquote>
The web search world, since its very beginning, has offered two paradigms:
*Navigational search uses a hierarchy structure (taxonomy) to enable users to browse the information space by iteratively narrowing the scope of their quest in a predetermined order, as exemplified by [[Yahoo! Directory]], [[Open Directory Project|DMOZ]], etc.
*Direct search allows users to simply write their queries as a bag of words in a text box. This approach has been made enormously popular by [[Web search engine]]s. 
Over the last few years, the direct search paradigm has gained dominance and the navigational approach became less and less popular. Recently, a new approach has emerged, combining both paradigms, namely the faceted search approach. Faceted search enables users to navigate a multi-dimensional information space by combining text search with a progressive narrowing of choices in each dimension. It has become the prevailing user interaction mechanism in e-commerce sites and is being extended to deal with [[semi-structured data]], continuous dimensions, and [[Folksonomy | folksonomies]].<ref name="sigir06">[http://facetedsearch.googlepages.com SIGIR'2006 Workshop on Faceted Search - Call for Participation]</ref>
</blockquote>

==Technology==

Various search engine software supports faceted classification.

* [[Apache Lucene]] and derived software:
**  [[Apache Solr]]
** [[Swiftype]]
** [[Elasticsearch]]
* A number of major vendors listed at [[Comparison of enterprise search software#Faceted_Navigation]]
* [[Dieselpoint]]
* [[Endeca]]
* iSeek, search engine for general web and education<ref>[http://www.iseek.com iSeek]</ref>
* [[SpeedTrack]]<ref>http://www.speedtrack.com/technology</ref>
* XSEARCH<ref>[http://www.weitkamper.com]</ref>

==Mass market use==

Faceted search has become a popular technique in commercial search applications, particularly for online retailers and libraries. An increasing number of [[List of Enterprise Search Vendors|enterprise search vendors]] such as [[Swiftype]] provide software for implementing faceted search applications.

Online retail catalogs pioneered the earliest applications of faceted search, reflecting both the faceted nature of product data (most products have a type, brand, price, etc.) and the ready availability of the data in retailers' existing information-systems. In the early 2000s retailers started using faceted search. A 2014 benchmark of 50 of the largest US based online retailers reveals that despite the benefits of faceted search, only 40% of the sites have implemented it. <ref name="Smashing Magazine: The Current State of E-Commerce Search (2014)">[http://www.smashingmagazine.com/2014/08/18/the-current-state-of-e-commerce-search/ Smashing Magazine: The Current State of E-Commerce Search] Retrieved on 2014-08-27.</ref> Examples include the filtering options that appear in the left column on [[amazon.com]] or [[Google Shopping]] after a keyword search has been performed.

==Libraries and information science==




In 1933, the noted librarian [[S. R. Ranganathan|Ranganathan]] proposed a [[faceted classification]] system for library materials, known as [[colon classification]]. In the pre-computer era, he did not succeed in replacing the pre-coordinated [[Dewey Decimal Classification]] system.<ref name="Major classification systems : the Dewey Centennial">[http://archive.org/details/majorclassificat00alle Major classification systems : the Dewey Centennial]</ref>

Modern online library catalogs, also known as [[OPAC]]s, have increasingly adopted faceted search interfaces. Noted examples include the [[North Carolina State University]] library catalog (part of the Triangle Research Libraries Network) and the [[Online Computer Library Center|OCLC]] Open [[WorldCat]] system.

InfoHarness<ref>{{cite journal|last1=Shklar|first1=Leon|last2=Thatte|first2=Satish|last3=Marcus|first3=Howard|last4=Sheth|first4=Amit|title=The "InfoHarness" Information Integration Platform|journal=Proceedings of the Second International Conference on the World Wide Web|date=1994|url=http://citeseer.uark.edu:8080/citeseerx/viewdoc/summary?doi=10.1.1.43.4042|accessdate=5 January 2015}}</ref> <ref>{{cite journal|last1=Shklar|first1=Leon|last2=Sheth|first2=Amit|last3=Kashyap|first3=Vipul|last4=Shah|first4=Kshitij|title=InfoHarness: Use of automatically generated metadata for search and retrieval of heterogeneous information|journal=Advanced Information Systems Engineering|date=20 July 2005|doi=10.1007/3-540-59498-1_248|url=http://link.springer.com/chapter/10.1007/3-540-59498-1_248|accessdate=5 January 2015|ref=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.7442}}</ref> is one of the first Web System (developed in 1994) that provided faceted search over heterogeneous information artifacts such as Web pages, images, videos and documents. The [[CiteSeerX]] project<ref>[http://citeseerx.ist.psu.edu/ CiteSeerX]. Citeseerx.ist.psu.edu. Retrieved on 2013-07-21.</ref> at the [[Pennsylvania State University]] allows faceted search for academic documents and continues to expand into other facets such as table search.

==See also==
* [[Enterprise Search]]
* [[Exploratory search]]
* [[Faceted classification]]
* [[Humancomputer information retrieval]]
* [[Information Extraction]]
* [[NoSQL]]
* [[Trove (website)]]

==References==
<References/>

{{DEFAULTSORT:Faceted Search}}
[[Category:Information retrieval]]
>>EOP<<
74<|###|>MAREC
{{other uses}}
The '''MA'''trixware '''RE'''search '''C'''ollection ('''MAREC''') is a standardised patent data corpus available for research purposes. MAREC seeks to represent patent documents of several languages in order to answer specific research questions.<ref>Merz C., (2003) A Corpus Query Tool For Syntactically Annotated Corpora Licentiate Thesis, The University of Zurich, Department of Computation linguistic, Switzerland</ref><ref>Biber D., Conrad S., and Reppen R. (2000) Corpus Linguistics: Investigating Language Structure and Use. Cambridge University Press, 2nd edition</ref> It consists of 19 million patent documents in different languages, normalised to a highly specific [[XML]] schema.

MAREC is intended as raw material for research in areas such as [[information retrieval]], [[natural language processing]] or [[machine translation]], which require large amounts of complex documents.<ref>Manning, C. D. and Schutze, H. (2002) Foundations of statistical natural language processing Cambridge, MA, Massachusetts Institute of Technology (MIT)  ISBN 0-262-13360-1.</ref> The collection contains documents in 19 languages, the majority being English, German and French, and about half of the documents include full text.

In MAREC, the documents from different countries and sources are normalised to a common XML format with a uniform patent numbering scheme and citation format. The standardised fields include dates, countries, languages, references, person names, and companies as well as subject classifications such as [[International Patent Classification|IPC]] codes.<ref>European Patent Office (2009) [http://documents.epo.org/projects/babylon/eponet.nsf/0/1AFC30805E91D074C125758A0051718A/$File/guidelines_2009_complete_en.pdf Guidelines for examination in the European Patent Office], Published by European Patent Office, Germany (April 2009)</ref>

MAREC is a comparable corpus, where many documents are available in similar versions in other languages. A comparable corpus can be defined as consisting of texts that share similar topics  news text from the same time period in different countries, while a parallel corpus is defined as a collection of documents with aligned translations from the source to the target language.<ref>Jarvelin A. , Talvensaari T. , Jarvelin Anni, (2008) Data driven methods for improving mono- and cross-lingual IR performance in noisy environments, Proceedings of the second workshop on Analytics for noisy unstructured text data, (Singapore)</ref> Since the patent document refers to the same invention or concept of idea the text is a translation of the invention, but it does not have to be a direct translation of the text itself  text parts could have been removed or added for clarification reasons.

The 19,386,697 XML files measure a total of 621 GB and are hosted by the [[Information Retrieval Facility]]. Access and support are free of charge for research purposes.

== Use Cases ==
* MAREC is used in the [[Patent Language Translations Online (PLuTO)]] project.

== References ==
{{Reflist}}

== External links ==
* [http://www.ir-facility.org/prototypes/marec User guide and statistics]
* [http://ir-facility.org Information Retrieval Facility]

[[Category:Corpora]]
[[Category:Information retrieval]]
[[Category:Machine translation]]
[[Category:Natural language processing]]
[[Category:XML]]
>>EOP<<
80<|###|>Binary Independence Model
{{context|date=June 2012}}
The '''Binary Independence Model''' (BIM)<ref name="cyu76" /><ref name="jones77"/> is a probabilistic [[information retrieval]] technique that makes some simple assumptions to make the estimation of document/query similarity probability feasible.

==Definitions==
The Binary Independence Assumption is that documents are [[bit array|binary vector]]s. That is, only the presence or absence of terms in documents are recorded. Terms are [[independence (probability theory)|independently]] distributed in the set of relevant documents and they are also independently distributed in the set of irrelevant documents.
The representation is an ordered set of [[Boolean data type|Boolean]] variables. That is, the representation of a document or query is a vector with one Boolean element for each term under consideration. More specifically, a document is represented by a vector ''d = (x<sub>1</sub>, ..., x<sub>m</sub>)'' where ''x<sub>t</sub>=1'' if term ''t'' is present in the document ''d'' and ''x<sub>t</sub>=0'' if it's not. Many documents can have the same vector representation with this simplification. Queries are represented in a similar way.
"Independence" signifies that terms in the document are considered independently from each other and  no association between terms is modeled. This assumption is very limiting, but it has been shown that it gives good enough results for many situations. This independence is the "naive" assumption of a [[Naive Bayes classifier]], where properties that imply each other are nonetheless treated as independent for the sake of simplicity. This assumption allows the representation to be treated as an instance of a [[Vector space model]] by considering each term as a value of 0 or 1 along a dimension orthogonal to the dimensions used for the other terms.

The probability ''P(R|d,q)'' that a document is relevant derives from the probability of relevance of the terms vector of that document ''P(R|x,q)''. By using the [[Bayes rule]] we get:

<math>P(R|x,q) = \frac{P(x|R,q)*P(R|q)}{P(x|q)}</math>

where ''P(x|R=1,q)'' and ''P(x|R=0,q)'' are the probabilities of retrieving a relevant or nonrelevant document, respectively. If so, then that document's representation is ''x''.
The exact probabilities can not be known beforehand, so use estimates from statistics about the collection of documents must be used.

''P(R=1|q)'' and ''P(R=0|q)'' indicate the previous probability of retrieving a relevant or nonrelevant document respectively for a query ''q''. If, for instance, we knew the percentage of relevant documents in the collection, then we could use it to estimate these probabilities.
Since a document is either relevant or nonrelevant to a query we have that:

<math>P(R=1|x,q) + P(R=0|x,q) = 1</math>

=== Query Terms Weighting ===
Given a binary query and the [[dot product]] as the similarity function between a document and a query, the problem is to assign weights to the
terms in the query such that the retrieval effectiveness will be high. Let <math>p_i</math> and <math>q_i</math> be the probability that a relevant document and an irrelevant document has the <math>i^{th}</math> term respectively. Yu and [[Gerard Salton|Salton]],<ref name="cyu76" /> who first introduce BIM, propose that the weight of the <math>i^{th}</math> term is an increasing function of <math>Y_i =  \frac{p_i *(1-q_i)}{(1-p_i)*q_i}</math>. Thus, if <math>Y_i</math> is higher than <math>Y_j</math>, the weight
of term <math>i</math> will be higher than that of term <math>j</math>. Yu and Salton<ref name="cyu76" /> showed that such a weight assignment to query terms yields better retrieval effectiveness than if query terms are equally weighted. [[Stephen Robertson (computer scientist)|Robertson]] and [[Karen Sparck Jones|Sparck Jones]]<ref name="jones77"/> later showed that if the <math>i^{th}</math> term is assigned the weight of <math>log Y_i</math>, then optimal retrieval effectiveness is obtained under the Binary Independence Assumption.

The Binary Independence Model was introduced by Yu and Salton.<ref name="cyu76" /> The name Binary Independence Model was coined by Robertson and Sparck Jones.<ref name="jones77"/>

== See also ==

* [[Bag of words model]]

==Further reading==
* {{citation | url=http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html | title=Introduction to Information Retrieval | author=Christopher D. Manning | coauthors=Prabhakar Raghavan & Hinrich Schutze | publisher=Cambridge University Press | year=2008}}
* {{citation | url=http://www.ir.uwaterloo.ca/book/ | title=Information Retrieval: Implementing and Evaluating Search Engines | author=Stefan B&uuml;ttcher | coauthors=Charles L. A. Clarke & Gordon V. Cormack | publisher=MIT Press | year=2010}}

==References==
{{Reflist|refs=
<ref name="cyu76">{{cite doi | 10.1145/321921.321930 }}</ref>
<ref name="jones77">{{cite doi | 10.1002/asi.4630270302 }}</ref> 
}}

[[Category:Information retrieval]]
[[Category:Probabilistic models]]
>>EOP<<
86<|###|>Greenpilot
{{COI|date=April 2010}}
The online portal '''Greenpilot''' is a service provided by the German National Library of Medicine, ZB MED.

The project is funded by the German Research Foundation ([[Deutsche Forschungsgemeinschaft]]) and gets its technical support from  [[Averbis]] Ltd. The portal first went online May 29, 2009 and currently runs in the updated beta version. In the context of the 'Germany - Land of Ideas' (Deutschland - Land der Ideen) initiative under the patronage of the [[President of Germany]] [[Horst Kohler]] the ZB MED was awarded the distinction 'Selected Landmark 2009' (Ausgewahlter Ort 2009).<ref>[http://idw-online.de/pages/de/news315583 Pressemitteilung im Informationsdienst Wissenschaft vom 15. Mai 2009 ]</ref>

==Objective==
The Greenpilot portal is a [[digital library]] specialised in the fields of Nutritional, Agricultural and Environmental Sciences. It aims to provide researchers in the three fields with a collection of scientific literature which is easy to access and of high quality. Especially the [[gray literature]] is often difficult to find and retrieve for the average user so Greenpilot also aims to make access to these sources easier. The service addresses itself not only to scientists and students but also to the broadly interested public. Greenpilot has been modelled after the corresponding digital library for Medicine, Medpilot,<ref>[http://www.medpilot.de/ Medpilot portal]</ref> also a project of the German National Library of Medicine. The ZB MED has chosen the slogan 'Greenpilot - all about life and science' as a motto. In Greenpilot scientifically relevant databases, library catalogues and websites can be searched by entering a search term and the results are presented in a standardised web interface.

==Technical Background==
Greenpilot is a search engine based on intuitive search engine technology. The portal's software was developed in the programming language [[Perl]]. The search engine technology is based upon the 'Averbis Search Platform' software developed by the Averbis Ltd. and uses the [[open source]] software [[Lucene]]. Functionally this is an expert search engine which centres around the intelligent semantic connection of search terms by means of a standardised vocabulary. This is made possible by Averbis's MSI software which provides:

* semantic search optimised for the fields of Medicine and Life Sciences
* a contextual analysis of texts taking synonyms and compounds into account
* multilingual and cross-language search
* linking of lay and expert vocabulary
The search results are generated from a search index.

Additionally a [[metasearch]] can be conducted in order to search other databases not contained in the index. This search is based upon individual results from the specific database searched.

==Contents==
The Greenpilot portal integrates various scientifically relevant information resources under a uniform search interface. These resources are diverse and encompass national and international expert databases, library catalogues of national libraries with a focus on specific topics, full text documents from [[open access (publishing)|open access]] journals as well as information contained on about one thousand scientifically relevant websites selected for Greenpilot.
The following is a list of sources from November 2009:<ref>[http://www.greenpilot.de/beta2/app/misc/help/8cafcf93601eb861aaef86b5ce99ecdc/Datenbanken List of databases in Greenpilot]</ref>

===Library Catalogues===
* Catalogue of the German National Library of Medicine (ZB MED Nutrition. Environment. Agriculture)
* Catalogue of the German National Library of Medicine (ZB MED Medicine. Health)
* Catalogue of the Bonn University Library
* Library catalogues of scientifically relevant departments within the collective library network (GBV)
* Catalogue of the Federal Ministry of Food, Agriculture and Consumer Protection (BMELV)
* Catalogue of the Johann Heinrich von Thunen-Institut (vTI), Federal Research Institute for Rural Areas, Forestry and Fisheries
* Catalogue of the Julius Kuhn-Institut, Federal Research Centre for Cultivated Plants
* Catalogue of the Friedrich Loffler-Institut, Federal Research Institute for Animal Health
* Catalogue of the Max Rubner-Institut, Federal Research Institute for Nutrition and Food
* Catalogue of the Federal Institute for Risk Assessment
* Catalogue of the Leibniz Institute for Marine Science (IFM-GEOMAR)
* Catalogue of the Leibniz Institute for Plant Genetics and Crop Plant Research (IPK-Plant Genetics and Crop Plant)
* Catalogue of the Leibniz Institute for Plant Biochemistry (IPB-Plant Chemistry)
* Catalogue of the special collection inshore and deep-sea fishery
* Catalogue of the University of Veterinary Medicine Hannover (TiHo-Veterinary Sciences)
* Catalogue of the German National Library of Economics (ZBW)

===Bibliographic databases===
* AGRIS (19752008), FAO ( Food and Agriculture Organization of the United Nations)
* VITIS-VEA, Viticulture and Enology Abstracts
* Medline (20042009)
* UFORDAT, Environmental Research Database (UBA)
* ULIDAT, Environmental Literature Database (UBA)
* ELFIS, International Information System for the Agricultural Sciences and Technology

===Relevant Internet Sources===
* Reviewed list of [[URL]]s selected by the ZB MED Nutrition. Environment. Agriculture
* Open Access journals with full text documents

===Metasearch===
* GetInfo, the knowledge portal for Technical Science provided by the Library for Technical Sciences (TIB) and the professional information centres FIZ Technik Frankfurt, FIZ Karlsruhe and FIZ CHEMIE Berlin.
* ECONIS, Catalogue of the German National Library of Economics (ZBW).

==Other Features==

===Search and results page===
* Search and advanced search
* Context sensitive help function
* [[Truncation]] and [[Boolean function]]s
* Personalised refining of search results by filtering for a specific document type, language or database
* [[Bookmark]]s

===Document ordering===
* Ordering directly from the results page is made possible by using the document delivery service of the ZB MED or the Electronic Journals Library ([[Elektronische Zeitschriftenbibliothek]]).

===Personalisation===
* My Greenpilot: a feature requiring the user to sign up for an account. The service is free of charge and offers an overview of ordered documents as well as enabling individual managing of customer data.

==See also==
*[[List of digital library projects]]
*[[vascoda]]

==References==
<references />

==External links==
* [http://www.greenpilot.de Greenpilot website]
* [http://www.zbmed.de/home.html?lang=en Website of the German National Library of Medicine, ZB MED]
* [http://www.land-of-ideas.org Germany - Land of Ideas website]

{{coord missing|Germany}}

[[Category:Libraries in Germany]]
[[Category:Information retrieval]]
[[Category:Internet search engines]]
>>EOP<<
92<|###|>EXCLAIM
{{For|the Canadian magazine|Exclaim!}}
The '''EXtensible Cross-Linguistic Automatic Information Machine (EXCLAIM)''' is an integrated tool for [[cross-language information retrieval]] (CLIR), created at the [[University of California, Santa Cruz]] in early 2006. It is currently in a beta stage of development, with some support for more than a dozen languages. The lead developers are Justin Nuger and Jesse Saba Kirchner.

Early work on CLIR depended on manually constructed parallel corpora for each pair of languages. This method is labor-intensive compared to parallel corpora created automatically. A more efficient way of finding data to train a CLIR system is to use matching pages on the [[World Wide Web|web]] which are written in different languages.<ref>
{{cite web
|title=Cross-Language Information Retrieval based on Parallel Texts and Automatic Mining of Parallel Texts in the Web
|url=http://www.iro.umontreal.ca/%7Enie/Publication/nie-sigir99.pdf
|format=PDF|publisher=ACM-SIGIR 1999
|accessdate=2006-12-02
}}
</ref>

EXCLAIM capitalizes on the idea of latent parallel corpora on the [[World Wide Web|web]] by automating the alignment of such corpora in various domains. The most significant of these is [[Wikipedia]] itself, which includes articles in [http://meta.wikimedia.org/wiki/Complete_list_of_language_Wikipedias_available 250 languages]. The role of EXCLAIM is to use [[semantics]] and [[linguistics|linguistic]] analytic tools to align the information in these Wikipedias so that they can be treated as parallel corpora. EXCLAIM is also extensible to incorporate information from many other sources, such as the [[Chinese Community Health Resource Center]] (CCHRC).

One of the main goals of the EXCLAIM project is to provide the kind of computational tools and CLIR tools for [[minority languages]] and [[endangered languages]] which are often available only for powerful or prosperous majority languages.

==Current status==

EXCLAIM is in a beta state, with varying degrees of functionality for different languages. Support for CLIR using the Wikipedia dataset and the most current version of EXCLAIM (v.0.5), including full UTF-8 support and Porter stemming for the English component, is available for the following twenty-three languages:

{| class="wikitable"
| [[Albanian language|Albanian]]
|-
| [[Amharic]]
|-
| [[Bengali language|Bengali]]
|-
| [[Gothic language|Gothic]]
|-
| [[Greek language|Greek]]
|-
| [[Icelandic language|Icelandic]]
|-
| [[Indonesian language|Indonesian]]
|-
| [[Irish language|Irish]]
|-
| [[Javanese language|Javanese]]
|-
| [[Latvian language|Latvian]]
|-
| [[Malagasy language|Malagasy]]
|-
| [[Mandarin Chinese]]
|-
| [[Nahuatl]]
|-
| [[Navajo language|Navajo]]
|-
| [[Quechua languages|Quechua]]
|-
| [[Sardinian language|Sardinian]]
|-
| [[Swahili language|Swahili]]
|-
| [[Tagalog language|Tagalog]]
|-
| [[Standard Tibetan|Tibetan]]
|-
| [[Turkish language|Turkish]]
|-
| [[Welsh language|Welsh]]
|-
| [[Wolof language|Wolof]]
|-
| [[Yiddish]]
|}

Support using the Wikipedia dataset and an earlier version of EXCLAIM (v.0.3) is available for the following languages:

{| class="wikitable"
|-
| [[Dutch language|Dutch]]
|-
| [[Spanish language|Spanish]]
|}

Significant developments in the most recent version of EXCLAIM include support for Mandarin Chinese. By developing support for this language, EXCLAIM has added solutions to [[text segmentation|segmentation]] and [[character encoding|encoding]] problems which will allow the system to be extended to many other languages written with non-European orthographic conventions. This support is supplied through the Trimming And Reformatting Modular System ([[TARMS]]) toolkit.

Future versions of EXCLAIM will extend the system to additional languages. Other goals include incorporation of available latent datasets in addition to the Wikipedia dataset.

The EXCLAIM development plan calls for an integrated CLIR instrument usable searching from English for information in any of the supported languages, or searching from any of the supported languages for information in English when EXCLAIM 1.0 is released. Future versions will allow searching from any supported language into any other, and searching from and into multiple languages.

==Further applications==

EXCLAIM has been incorporated into several projects which rely on cross-language [[query expansion]] as part of their [[backend]]s. One such project is a cross-linguistic [[readability]] software generation framework, detailed in work presented at [[Association for Computational Linguistics|ACL 2009]].<ref>{{cite web
|title=A crosslinguistic readability framework
|url=http://www.aclweb.org/anthology/W/W09/W09-3103.pdf
|format=PDF|publisher=ACL-IJNLP 2009
|accessdate=2009-09-04
}}
</ref>

==Notes and references==

{{reflist}}

==External links==
*[http://www.soe.ucsc.edu/~jnuger/cgi-bin/exclaim.cgi EXCLAIM Website]
*[http://www.w3.org/DesignIssues/Semantic.html Semantic Web Roadmap]
*[http://www.cchphmo.com/cchrchealth/index_E.html Chinese Cultural Health Resource Center]
*[http://ju-st.in/ Justin Nuger's professional webpage]
*[http://people.ucsc.edu/~kirchner/ Jesse Saba Kirchner's professional webpage]

{{DEFAULTSORT:Exclaim}}
[[Category:Information retrieval]]
>>EOP<<
98<|###|>Cluster labeling
In [[natural language processing]] and [[information retrieval]], '''cluster labeling''' is the problem of picking descriptive, human-readable labels for the clusters produced by a [[document clustering]] algorithm; standard clustering algorithms do not typically produce any such labels. Cluster labeling algorithms examine the contents of the documents per cluster to find labeling that summarize the topic of each cluster and distinguish the clusters from each other.

==Differential cluster labeling==
Differential cluster labeling labels a cluster by comparing term [[probability distribution|distributions]] across clusters, using techniques also used for [[feature selection]] in [[document classification]], such as [[mutual information]] and [[Pearson's chi-squared test|chi-squared feature selection]].  Terms having very low frequency are not the best in representing the whole cluster and can be omitted in labeling a cluster.  By omitting those rare terms and using a differential test, one can achieve the best results with differential cluster labeling.<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Cluster Labeling''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/cluster-labeling-1.html>.</ref>

===Pointwise mutual information===

{{Main|Pointwise mutual information}}

In the fields of [[probability theory]] and [[information theory]], mutual information measures the degree of dependence of two [[random variables]].  The mutual information of two variables {{mvar|X}} and {{mvar|Y}} is defined as:

<math>I(X, Y) = \sum_{x\in X}{ \sum_{y\in Y} {p(x, y)log_2\left(\frac{p(x, y)}{p_1(x)p_2(y)}\right)}}</math>

where ''p(x, y)'' is the [[joint probability|joint probability distribution]] of the two variables, ''p<sub>1</sub>(x)'' is the probability distribution of X, and ''p<sub>2</sub>(y)'' is the probability distribution of Y.

In the case of cluster labeling, the variable X is associated with membership in a cluster, and the variable Y is associated with the presence of a term.<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Mutual Information''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html>.</ref>  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:

<math>I(C, T) = \sum_{c\in {0, 1}}{ \sum_{t\in {0, 1}} {p(C = c, T = t)log_2\left(\frac{p(C = c, T = t)}{p(C = c)p(T = t)}\right)}}</math>

In this case, ''p(C = 1)'' represents the probability that a randomly selected document is a member of a particular cluster, and ''p(C = 0)'' represents the probability that it isn't.  Similarly, ''p(T = 1)'' represents the probability that a randomly selected document contains a given term, and ''p(T = 0)'' represents the probability that it doesn't.  The [[joint probability|joint probability distribution function]] ''p(C, T)'' represents the probability that two events occur simultaneously.  For example, ''p(0, 0)'' is the probability that a document isn't a member of cluster ''c'' and doesn't contain term ''t''; ''p(0, 1)'' is the probability that a document isn't a member of cluster ''c'' and does contain term ''t''; and so on.

===Chi-Squared Selection===
{{Main|Pearson's chi-squared test}}
The Pearson's chi-squared test can be used to calculate the probability that the occurrence of an event matches the initial expectations.  In particular, it can be used to determine whether two events, A and B, are [[statistically independent]].  The value of the chi-squared statistic is:

<math>X^2 = \sum_{a \in A}{\sum_{b \in B}{\frac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}</math>

where ''O<sub>a,b</sub>'' is the ''observed'' frequency of a and b co-occurring, and ''E<sub>a,b</sub>'' is the ''expected'' frequency of co-occurrence.

In the case of cluster labeling, the variable A is associated with membership in a cluster, and the variable B is associated with the presence of a term.  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:

<math>X^2 = \sum_{a \in {0,1}}{\sum_{b \in {0,1}}{\frac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}</math>

For example, ''O<sub>1,0</sub>'' is the observed number of documents that are in a particular cluster but don't contain a certain term, and ''E<sub>1,0</sub>'' is the expected number of documents that are in a particular cluster but don't contain a certain term.
Our initial assumption is that the two events are independent, so the expected probabilities of co-occurrence can be calculated by multiplying individual probabilities:<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Chi2 Feature Selection''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html>.</ref>

''E<sub>1,0</sub> = N * P(C = 1) * P(T = 0)''

where N is the total number of documents in the collection.

==Cluster-Internal Labeling==
Cluster-internal labeling selects labels that only depend on the contents of the cluster of interest. No comparison is made with the other clusters.
Cluster-internal labeling can use a variety of methods, such as finding terms that occur frequently in the centroid or finding the document that lies closest to the centroid.

===Centroid Labels===
{{Main|Vector space model}}
A frequently used model in the field of [[information retrieval]] is the vector space model, which represents documents as vectors.  The entries in the vector correspond to terms in the [[vocabulary]]. Binary vectors have a value of 1 if the term is present within a particular document and 0 if it is absent. Many vectors make use of weights that reflect the importance of a term in a document, and/or the importance of the term in a document collection. For a particular cluster of documents, we can calculate the [[centroid]] by finding the [[arithmetic mean]] of all the document vectors.  If an entry in the centroid vector has a high value, then the corresponding term occurs frequently within the cluster.  These terms can be used as a label for the cluster.
One downside to using centroid labeling is that it can pick up words like "place" and "word" that have a high frequency in written text, but have little relevance to the contents of the particular cluster.

===Contextualized centroid labels===
A simple, cost-effective way of overcoming the above limitation is to embed the centroid terms with the highest weight in a graph structure that provides a context for their interpretation and selection. <ref>Francois Role, Moahmed Nadif. [http://dl.acm.org/citation.cfm?id=2574675 Beyond cluster labeling: Semantic interpretation of clusters contents using a graph representation.] Knowledge-Based Systems, Volume 56, January, 2014: 141-155</ref>
In this approach, a term-term co-occurrence matrix referred as <math>T_k</math> is first built for each cluster <math>S_k</math>. Each cell represents the number of times term <math>i</math> co-occurs with term <math>j</math> within a certain window of text (a sentence, a paragraph, etc.)
In a second stage, a similarity matrix <math>T_k^{sim}</math> is obtained by multiplying <math>T_k</math> with its transpose. We have <math>T_k^{sim}=T_k' T_k=(t_{{sim}_{ij}})</math>. Being the dot product of two normalized vectors <math>\tilde{t}_{i}</math> and <math>\tilde{t}_{j}</math>, <math>t_{{sim}_{ij}}</math> denotes the cosine similarity between terms <math>i</math> and <math>j</math>. The so obtained <math>T_k^{sim}</math> can then be used as the weighted adjacency matrix of a term similarity graph. The centroid terms are part of this graph, and they thus can be interpreted and scored by inspecting the terms that surround them in the graph.

===Title labels===
An alternative to centroid labeling is title labeling.  Here, we find the document within the cluster that has the smallest [[Euclidean distance]] to the centroid, and use its title as a label for the cluster.  One advantage to using document titles is that they provide additional information that would not be present in a list of terms.  However, they also have the potential to mislead the user, since one document might not be representative of the entire cluster.

===External knowledge labels===
Cluster labeling can be done indirectly using external knowledge such as pre-categorized knowledge such as the one of Wikipedia.<ref>David Carmel, Haggai Roitman, Naama Zwerdling. [http://portal.acm.org/citation.cfm?doid=1571941.1571967 Enhancing cluster labeling using wikipedia.] SIGIR 2009: 139-146</ref> In such methods, a set of important cluster text features are first extracted from the cluster documents. These features then can be used to retrieve the (weighted) K-nearest categorized documents from which candidates for cluster labels can be extracted. The final step involves the ranking of such candidates. Suitable methods are such that are based on a voting or a fusion process which is determined using the set of categorized documents and the original cluster features.

==External links==
* [http://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-clustering-1.html Hierarchical Clustering]
* [http://erulemaking.ucsur.pitt.edu/doc/papers/dgo06-labeling.pdf Automatically Labeling Hierarchical Clusters]

==References==
<references/>

{{DEFAULTSORT:Cluster Labeling}}
[[Category:Information retrieval]]
>>EOP<<
104<|###|>Extended Boolean model
The '''Extended Boolean model''' was described in a Communications of the ACM article appearing in 1983, by Gerard Salton, Edward A. Fox, and Harry Wu. The goal of the Extended Boolean model is to overcome the drawbacks of the Boolean model that has been used in [[information retrieval]]. The Boolean model doesn't consider term weights in queries, and the result set of a Boolean query is often either too small or too big. The idea of the extended model is to make use of partial matching and term weights as in the vector space model. It combines the characteristics of the [[Vector Space Model]] with the properties of [[Boolean algebra (logic)|Boolean algebra]] and ranks the similarity between queries and documents. This way a document may be somewhat relevant if it matches some of the queried terms and will be returned as a result, whereas in the [[Standard Boolean model]] it wasn't.<ref>	
{{citation | url=http://portal.acm.org/citation.cfm?id=358466 | last=Salton | first=Gerard | coauthors=Edward A. Fox, Harry Wu | title=Extended Boolean information retrieval | publisher=Communications of the ACM, Volume 26,  Issue 11 | year=1983 }}</ref>

Thus, the extended Boolean model can be considered as a generalization of both the Boolean and vector space models; those two are special cases if suitable settings and definitions are employed. Further, research has shown effectiveness improves relative to that for Boolean query processing.  Other research has shown that [[relevance feedback]] and [[query expansion]] can be integrated with extended Boolean query processing.

==Definitions==
In the '''Extended Boolean model''', a document is represented as a vector (similarly to in the vector model). Each ''i'' [[Dimension (vector space)|dimension]] corresponds to a separate term associated with the document.

The weight of term {{math|''K<sub>x</sub>''}} associated with document {{math|''d<sub>j</sub>''}} is measured by its normalized [[Term frequency]] and can be defined as:

<math>
w_{x,j}=f_{x,j}*\frac{Idf_{x}}{max_{i}Idf_{i}}
</math>

where {{math|''Idf<sub>x</sub>''}} is [[inverse document frequency]].

The weight vector associated with document {{math|''d<sub>j</sub>''}} can be represented as:

<math>\mathbf{v}_{d_j} = [w_{1,j}, w_{2,j}, \ldots, w_{i,j}]</math>

==The 2 Dimensions Example==
{{multiple image
 | width     = 150
 | image1    = 2D_Extended_Boolean_model_OR_example.png
 | alt1      = Figure 1
 | caption1  = '''Figure 1:''' The similarities of {{math|''q'' {{=}} (''K<sub>x</sub>'' &or; ''K<sub>y</sub>'')}} with documents {{math|''d<sub>j</sub>''}} and {{math|''d''<sub>''j''+1</sub>}}.
 | image2    = 2D_Extended_Boolean_model_AND_example.png
 | alt2      = Figure 2
 | caption2  = '''Figure 2:''' The similarities of {{math|''q'' {{=}} (''K<sub>x</sub>'' &and; ''K<sub>y</sub>'')}} with documents {{math|''d<sub>j</sub>''}} and {{math|''d''<sub>''j''+1</sub>}}.
}}

Considering the space composed of two terms {{math|''K<sub>x</sub>''}} and {{math|''K<sub>y</sub>''}} only, the corresponding term weights are {{math|''w''<sub>1</sub>}} and {{math|''w''<sub>2</sub>}}.<ref>[http://www.cs.cityu.edu.hk/~cs5286/Lectures/Lwang.ppt Lusheng Wang]</ref>  Thus, for query {{math|''q<sub>or</sub>'' {{=}} (''K<sub>x</sub>'' &or; ''K<sub>y</sub>'')}}, we can calculate the similarity with the following formula:
 
<math>sim(q_{or},d)=\sqrt{\frac{w_1^2+w_2^2}{2}}</math>

For query {{math|''q<sub>and</sub>'' {{=}} (''K<sub>x</sub>'' &and; ''K<sub>y</sub>'')}}, we can use:

<math>sim(q_{and},d)=1-\sqrt{\frac{(1-w_1)^2+(1-w_2)^2}{2}}</math>

==Generalizing the idea and P-norms==
We can generalize the previous 2D extended Boolean model example to higher t-dimensional space using Euclidean distances.

This can be done using [[P-norm]]s which extends the notion of distance to include p-distances, where {{math|1 &le; ''p'' &le; &infin;}} is a new parameter.<ref>{{ citation | last=Garcia | first= Dr. E. | url=http://www.miislita.com/term-vector/term-vector-6-boolean-model.html | title=The Extended Boolean Model - Weighted Queries: Term Weights, p-Norm Queries and Multiconcept Types. Boolean OR Extended? AND that is the Query }}</ref>

*A generalized conjunctive query is given by:
:<math>q_{or}=k_1 \lor^p k_2 \lor^p .... \lor^p k_t  </math>

*The similarity of <math>q_{or}</math> and <math>d_j</math> can be defined as:
''':<math>sim(q_{or},d_j)=\sqrt[p]{\frac{w_1^p+w_2^p+....+w_t^p}{t}}</math>'''

*A generalized disjunctive query is given by:
:<math>q_{and}=k_1 \land^p k_2 \land^p .... \land^p k_t  </math>

*The similarity of <math>q_{and}</math> and <math>d_j</math> can be defined as:
:<math>sim(q_{and},d_j)=1-\sqrt[p]{\frac{(1-w_1)^p+(1-w_2)^p+....+(1-w_t)^p}{t}}</math>

==Examples==
Consider the query {{math|''q'' {{=}} (''K''<sub>1</sub> &and; ''K''<sub>2</sub>) &or; ''K''<sub>3</sub>}}. The similarity between query {{math|''q''}} and document {{math|''d''}} can be computed using the formula:

<math>sim(q,d)=\sqrt[p]{\frac{(1-\sqrt[p]{(\frac{(1-w_1)^p+(1-w_2)^p}{2}}))^p+w_3^p}{2}}</math>

==Improvements over the Standard Boolean Model==

Lee and Fox<ref>{{citation | last=Lee | first=W. C. | coauthors=E. A. Fox | year=1988 | title=Experimental Comparison of Schemes for Interpreting Boolean Queries}}</ref> compared the Standard and Extended Boolean models with three test collections, CISI, CACM and INSPEC.
Using P-norms they obtained an average precision improvement of 79%, 106% and 210% over the Standard model, for the CISI, CACM and INSPEC collections, respectively.<br>
The P-norm model is computationally expensive because of the number of exponentiation operations that it requires but it achieves much better results than the Standard model and even [[Fuzzy retrieval]] techniques. The [[Standard Boolean model]] is still the most efficient.

==Further reading==
* [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.1997 Adaptive Feedback Methods in an Extended Boolean Model  by Dr.Jongpill Choi]
* [http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6VC8-454T5MS-2&_user=513551&_rdoc=1&_fmt=&_orig=search&_sort=d&_docanchor=&view=c&_searchStrId=1117914301&_rerunOrigin=google&_acct=C000025338&_version=1&_urlVersion=0&_userid=513551&md5=4eab0da46bfe361afa883e48f2060feb Interpolation of the extended Boolean retrieval model ]
* {{citation | title=Information Retrieval: Algorithms and Data structures; Extended Boolean model | last=Fox | first=E. | coauthors=S. Betrabet , M. Koushik , W. Lee | year=1992 | publisher=Prentice-Hall, Inc. | url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes}}
* {{citation | title=Experiments with Automatic Query Formulation in the Extended Boolean Model | url=http://www.springerlink.com/content/tk1t141253257613/ | first= Lucie | last= Skorkovska | coauthors=Pavel Ircing | year=2009 | publisher= Springer Berlin / Heidelberg}}

==See also==
*[[Information retrieval]]

==References==
{{reflist}}

{{DEFAULTSORT:Extended Boolean Model}}
[[Category:Information retrieval]]
>>EOP<<
110<|###|>International Society for Music Information Retrieval
The '''International Society for Music Information Retrieval''' ('''ISMIR''') is an international forum for research on the organization of music-related data. It started as an informal group steered by an ''ad hoc'' committee in 2000<ref>[http://www.ismir.net/texts/Byrd02.html Donald Byrd and Michael Fingerhut: ''The History of ISMIR - A Short Happy Tale''. D-Lib Magazine, Vol. 8 No. 11 ISSN: 1082-9873.]</ref> which established a yearly symposium - whence "ISMIR", which meant '''International Symposium on Music Information Retrieval'''. It was turned into a conference in 2002 while retaining the acronym. ISMIR was incorporated in Canada on July 4, 2008.<ref>[http://www.ismir.net/ISMIR-Letters-Patent.pdf ISMIR Letters Patent. Canada, July 4, 2008.]</ref>

==Purpose==
Given the tremendous growth of digital music and music metadata in recent years, methods for effectively extracting, searching, and organizing music information have received widespread interest from academia and the information and entertainment industries. The purpose of ISMIR is to provide a venue for the exchange of news, ideas, and results through the presentation of original theoretical or practical work. By bringing together researchers and developers, educators and librarians, students and professional users, all working in fields that contribute to this multidisciplinary domain, the conference also serves as a discussion forum, provides introductory and in-depth information on specific domains, and showcases current products.

As the term Music Information Retrieval (MIR) indicates, this research is motivated by the desire to provide music lovers, music professionals and music industry with robust, effective and usable methods and tools to help them locate, retrieve and experience the music they wish to have access to. MIR is a truly interdisciplinary area, involving researchers from the disciplines of musicology, cognitive science, library and information science, computer science and many others.

==Annual Conference==
Since its inception in 2000, ISMIR has been the worlds leading forum for research on the modelling, creation, searching, processing and use of musical data. Researchers across the globe meet at the annual conference conducted by the society. It is known by the same acronym as the society, ISMIR. Following is the list of previous conferences held by the society.
* [http://ismir2012.ismir.net ISMIR 2012], 812 October 2012, Porto (Portugal) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2012'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2011.ismir.net ISMIR 2011], 2428 October 2011, Miami (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2011'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2010.ismir.net ISMIR 2010], 913 August 2010, Utrecht (Netherlands) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2010'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2009.ismir.net ISMIR 2009], 2630 October 2009, Kobe (Japan) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2009'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2008.ismir.net ISMIR 2008], 1418 September 2008, Philadelphia (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2008'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2007.ismir.net ISMIR 2007], 2330 September 2007, Vienna (Austria) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2007'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2006.ismir.net ISMIR 2006], 812 October 2006, Victoria, BC (Canada) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2006'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2005.ismir.net ISMIR 2005], 1115 September 2005, London (UK) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2005'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2004.ismir.net ISMIR 2004], 1015 October 2004, Barcelona (Spain) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2004'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2003.ismir.net ISMIR 2003], 2630 October 2003, Baltimore, Maryland (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2003'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2002.ismir.net ISMIR 2002], 1317 October 2002, Paris (France) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2002'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2001.ismir.net ISMIR 2001], 1517 October 2001, Bloomington, Indiana (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2001'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2000.ismir.net ISMIR 2000], 2325 October 2000, Plymouth, Massachusetts (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='200'&page=0&order=Authors&order_type=ASC proceedings]

The [http://www.ismir.net/ official webpage] provides a more up-to-date information on past and future conferences and provides access to all past websites and to the [http://www.ismir.net/proceedings cumulative database] of all papers, posters and tutorials presented at these conferences.

==MIREX==
The Music Information Retrieval Evaluation eXchange (MIREX)<ref>[http://www.music-ir.org/mirex MIREX Wiki]</ref> is an annual evaluation campaign for Music Information Retrieval (MIR) algorithms, coupled to the ISMIR conference.

MIR tasks evaluated at past MIREXs include:
*[http://www.music-ir.org/mirex/wiki/Audio_Train/Test_Tasks Audio Train/Test Tasks]
**Audio Artist Identification
**Audio Genre Classification
**Audio Music Mood Classification
**Audio Classical Composer Identification
*[http://www.music-ir.org/mirex/wiki/Symbolic_Genre_Classification Symbolic Genre Classification]
*[http://www.music-ir.org/mirex/wiki/Audio_Onset_Detection Audio Onset Detection]
*[http://www.music-ir.org/mirex/wiki/Audio_Key_Detection Audio Key Detection]
*[http://www.music-ir.org/mirex/wiki/Symbolic_Key_Detection Symbolic Key Detection]
*[http://www.music-ir.org/mirex/wiki/Audio_Tag_Classification Audio Tag Classification]
*[http://www.music-ir.org/mirex/wiki/Audio_Cover_Song_Identification Audio Cover Song Identification]
*[http://www.music-ir.org/mirex/wiki/Real-time_Audio_to_Score_Alignment_(a.k.a_Score_Following) Real-time Audio to Score Alignment (a.k.a Score Following)]
*[http://www.music-ir.org/mirex/wiki/Query_by_Singing/Humming Query by Singing/Humming]
*[http://www.music-ir.org/mirex/wiki/Multiple_Fundamental_Frequency_Estimation_&_Tracking Multiple Fundamental Frequency Estimation & Tracking]
*[http://www.music-ir.org/mirex/wiki/Audio_Chord_Estimation Audio Chord Estimation]
*[http://www.music-ir.org/mirex/wiki/Audio_Melody_Extraction Audio Melody Extraction]
*[http://www.music-ir.org/mirex/wiki/Query_by_Tapping Query by Tapping]
*[http://www.music-ir.org/mirex/wiki/Audio_Beat_Tracking Audio Beat Tracking]
*[http://www.music-ir.org/mirex/wiki/Audio_Music_Similarity_and_Retrieval Audio Music Similarity and Retrieval]
*[http://www.music-ir.org/mirex/wiki/Symbolic_Melodic_Similarity Symbolic Melodic Similarity]
*[http://www.music-ir.org/mirex/wiki/Structural_Segmentation Structural Segmentation]
*[http://www.music-ir.org/mirex/wiki/Audio_Drum_Detection Audio Drum Detection]
*[http://www.music-ir.org/mirex/wiki/Audio_Tempo_Extraction Audio Tempo Extraction]

==See also==
* [[International Conference on Digital Audio Effects]]
* [[Music Technology]]
* [[Sound and music computing|Sound and Music Computing]]
* [[Sound and Music Computing Conference]]

==Notes==
{{Reflist}}

[[Category:Music technology]]
[[Category:Multimedia]]
[[Category:Information retrieval]]
>>EOP<<
116<|###|>Online search
'''Online search''' is the process of interactively searching for and retrieving requested information via a computer from [[database]]s that are [[online]].<ref name="whatis?">{{cite journal|last1=Hawkins|first1= Donald T.|last2= Brown|first2= Carolyn P.|date=Jan 1980|title=What Is an Online Search?|journal=Online|volume=4|issue=1|pages=1218|id=Eric:EJ214713| accessdate=2011-04-04}}</ref> Interactive searches became possible in the 1980s with the advent of faster databases and [[smart terminal]]s.<ref name="whatis?"/> In contrast, [[computerized batch searching]] was prevalent in the 1960s and 1970s.<ref name="whatis?"/> Today, searches through [[web search engine]]s constitute the majority of online searches.

Online searches often supplement reference transactions.

==References==
{{reflist}}

{{Internet search}}

[[Category:Internet terminology]]
[[Category:Information retrieval]]


{{web-stub}}
>>EOP<<
122<|###|>Search engine indexing
{{Too many see alsos|date=December 2012}}
'''Search engine indexing''' collects, parses, and stores [[data (computing)|data]] to facilitate fast and accurate [[information retrieval]]. Index design incorporates interdisciplinary concepts from linguistics, cognitive psychology, mathematics, [[Information technology|informatics]], and computer science.  An alternate name for the process in the context of [[search engine]]s designed to find web pages on the Internet is ''[[web indexing]]''.

Popular engines focus on the full-text indexing of online, natural language documents.<ref>Clarke, C., Cormack, G.: Dynamic Inverted Indexes for a Distributed Full-Text Retrieval System. TechRep MT-95-01, University of Waterloo, February 1995.</ref> [[Multimedia|Media types]] such as video and audio<ref>http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf</ref> and graphics<ref>Charles E. Jacobs, Adam Finkelstein, David H. Salesin. [http://grail.cs.washington.edu/projects/query/mrquery.pdf Fast Multiresolution Image Querying]. Department of Computer Science and Engineering, University of Washington. 1995. Verified Dec 2006</ref> are also searchable.

[[Metasearch engine|Meta search engines]] reuse the indices of other services and do not store a local index, whereas cache-based search engines permanently store the index along with the  [[text corpus|corpus]]. Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while [[Intelligent agent|agent]]-based search engines index in [[Real time business intelligence|real time]].

==Indexing==
The purpose of storing an index is to optimize speed and performance in finding relevant documents for a search query. Without an index, the search engine would [[Lexical analysis|scan]] every document in the corpus, which would require considerable time and computing power.  For example, while an index of 10,000 documents can be queried within milliseconds, a sequential scan of every word in 10,000 large documents could take hours. The additional computer storage required to store the index, as well as the considerable increase in the time required for an update to take place, are traded off for the time saved during information retrieval.

===Index design factors===
Major factors in designing a search engine's architecture include:

; Merge factors : How data enters the index, or how words or subject features are added to the index during text corpus traversal, and whether multiple indexers can work asynchronously. The indexer must first check whether it is updating old content or adding new content. Traversal typically correlates to the [[Web crawling|data collection]] policy. Search engine index merging is similar in concept to the [[Merge (SQL)|SQL Merge]] command and other merge algorithms.<ref>Brown, E.W.: Execution Performance Issues in Full-Text Information Retrieval. Computer Science Department, University of Massachusetts Amherst, Technical Report 95-81, October 1995.</ref>
; Storage techniques : How to store the index [[data]], that is, whether information should be data compressed or filtered.
; Index size : How much computer storage is required to support the index.
; Lookup speed : How quickly a word can be found in the inverted index. The speed of finding an entry in a data structure, compared with how quickly it can be updated or removed, is a central focus of computer science.
; Maintenance : How the index is maintained over time.<ref>Cutting, D., Pedersen, J.: Optimizations for dynamic inverted index maintenance. Proceedings of SIGIR, 405-411, 1990.</ref>
;Fault tolerance : How important it is for the service to be reliable. Issues include dealing with index corruption, determining whether bad data can be treated in isolation, dealing with bad hardware, [[partition (database)|partitioning]], and schemes such as [[hash function|hash-based]] or composite partitioning,<ref>[http://dev.mysql.com/doc/refman/5.1/en/partitioning-linear-hash.html Linear Hash Partitioning]. MySQL 5.1 Reference Manual. Verified Dec 2006</ref> as well as [[Replication (computer science)|replication]].

===Index data structures===
Search engine architectures vary in the way indexing is performed and in methods of index storage to meet the various design factors.

;[[Suffix tree]] : Figuratively structured like a tree, supports linear time lookup. Built by storing the suffixes of words. The suffix tree is a type of [[trie]]. Tries support extendable hashing, which is important for search engine indexing.<ref>[http://www.nist.gov/dads/HTML/trie.html trie], [http://www.nist.gov/dads Dictionary of Algorithms and Data Structures], [http://www.nist.gov U.S. National Institute of Standards and Technology].</ref> Used for searching for patterns in [[DNA]] sequences and clustering. A major drawback is that storing a word in the tree may require space beyond that required to store the word itself.<ref name="Gus97">{{cite book
 | last = Gusfield
 | first = Dan
 | origyear = 1997
 | year = 1999
 | title = Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology
 | publisher = Cambridge University Press
 | location = USA
 | isbn = 0-521-58519-8}}.
</ref> An alternate representation is a [[suffix array]], which is considered to require less virtual memory and supports data compression such as the [[Burrows-Wheeler transform|BWT]] algorithm.

;[[Inverted index]] : Stores a list of occurrences of each atomic search criterion,<ref>Black, Paul E., [http://www.nist.gov/dads/HTML/invertedIndex.html inverted index], [http://www.nist.gov/dads Dictionary of Algorithms and Data Structures], [http://www.nist.gov U.S. National Institute of Standards and Technology] Oct 2006. Verified Dec 2006.</ref> typically in the form of a [[hash table]] or [[binary tree]].<ref>C. C. Foster, Information retrieval: information storage and retrieval using AVL trees, Proceedings of the 1965 20th national conference, p.192-205, August 2426, 1965, Cleveland, Ohio, United States</ref><ref>Landauer, W. I.: The balanced tree and its utilization in information retrieval. IEEE Trans. on Electronic Computers, Vol. EC-12, No. 6, December 1963.</ref>

;[[Citation index]] : Stores citations or hyperlinks between documents to support citation analysis, a subject of [[Bibliometrics]].
;[[N-gram|Ngram index]] : Stores sequences of length of data to support other types of retrieval or text mining.<ref>[http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13 Google Ngram Datasets] for sale at [http://www.ldc.upenn.edu/ LDC] Catalog</ref>
;[[Document-term matrix]] : Used in latent semantic analysis, stores the occurrences of words in documents in a two-dimensional [[sparse matrix]].

===Challenges in parallelism===
A major challenge in the design of search engines is the management of serial computing processes. There are many opportunities for [[race conditions]] and coherent faults. For example, a new document is added to the corpus and the index must be updated, but the index simultaneously needs to continue responding to search queries. This is a collision between two competing tasks. Consider that authors are producers of information, and a web crawler is the consumer of this information, grabbing the text and storing it in a cache (or [[Text corpus|corpus]]). The forward index is the consumer of the information produced by the corpus, and the inverted index is the consumer of information produced by the forward index. This is commonly referred to as a '''producer-consumer model'''. The indexer is the producer of searchable information and users are the consumers that need to search.  The challenge is magnified when working with distributed storage and distributed processing. In an effort to scale with larger amounts of indexed information, the search engine's architecture may involve [[distributed computing]], where the search engine consists of several machines operating in unison. This increases the possibilities for incoherency and makes it more difficult to maintain a fully synchronized, distributed, parallel architecture.<ref>Jeffrey Dean and Sanjay Ghemawat. MapReduce: Simplified Data Processing on Large Clusters. Google, Inc. OSDI. 2004.</ref>

===Inverted indices===
Many search engines incorporate an [[inverted index]] when evaluating a [[search query]] to quickly locate documents containing the words in a query and then rank these documents by relevance. Because the inverted index stores a list of the documents containing each word, the search engine can use direct [[random access|access]] to find the documents associated with each word in the query in order to retrieve the matching documents quickly. The following is a simplified illustration of an inverted index:

{| align="center" class="wikitable"
|+ Inverted Index
|-
! Word !! Documents
|-
| the || Document 1, Document 3, Document 4, Document 5, Document 7
|-
| cow || Document 2, Document 3, Document 4
|-
| says || Document 5
|-
| moo || Document 7
|}

This index can only determine whether a word exists within a particular document, since it stores no information regarding the frequency and position of the word; it is therefore considered to be a [[boolean datatype|boolean]] index. Such an index determines which documents match a query but does not rank matched documents. In some designs the index includes additional information such as the frequency of each word in each document or the positions of a word in each document.<ref>Grossman, Frieder, Goharian. [http://www.cs.clemson.edu/~juan/CPSC862/Concept-50/IR-Basics-of-Inverted-Index.pdf IR Basics of Inverted Index]. 2002. Verified Aug 2011.</ref> Position information enables the search algorithm to identify word proximity to support searching for phrases; frequency can be used to help in ranking the relevance of documents to the query. Such topics are the central research focus of [[information retrieval]].

The inverted index is a [[sparse matrix]], since not all words are present in each document. To reduce computer storage memory requirements, it is stored differently from a two dimensional [[Array data structure|array]]. The index is similar to the [[document-term matrix|term document matrices]] employed by [[latent semantic analysis]]. The inverted index can be considered a form of a hash table. In some cases the index is a form of a [[binary tree]], which requires additional storage but may reduce the lookup time. In larger indices the architecture is typically a [[distributed hash table]].<ref>Tang, Hunqiang. [[Sandhya Dwarkadas|Dwarkadas, Sandhya]]. "Hybrid Global Local Indexing for Efficient
Peer to Peer Information Retrieval". University of Rochester. Pg 1. http://www.cs.rochester.edu/u/sandhya/papers/nsdi04.ps</ref>

===Index merging===
The inverted index is filled via a merge or rebuild. A rebuild is similar to a merge but first deletes the contents of the inverted index. The architecture may be designed to support incremental indexing,<ref>Tomasic, A., et al.: Incremental Updates of Inverted Lists for Text Document Retrieval. Short Version of Stanford University Computer Science Technical Note STAN-CS-TN-93-1, December, 1993.</ref> where a merge identifies the document or documents to be added or updated and then parses each document into words. For technical accuracy, a merge conflates newly indexed documents, typically residing in virtual memory, with the index cache residing on one or more computer hard drives.

After parsing, the indexer adds the referenced document to the document list for the appropriate words. In a larger search engine, the process of finding each word in the inverted index (in order to report that it occurred within a document) may be too time consuming, and so this process is commonly split up into two parts, the development of a forward index and a process which sorts the contents of the forward index into the inverted index. The inverted index is  so named because it is an inversion of the forward index.

===The forward index===
The forward index stores a list of words for each document. The following is a simplified form of the forward index:

{| align="center" class="wikitable"
|+ Forward Index
|-
! Document !! Words
|-
| Document 1 || the,cow,says,moo
|-
| Document 2 || the,cat,and,the,hat
|-
| Document 3 || the,dish,ran,away,with,the,spoon
|}

The rationale behind developing a forward index is that as documents are parsing, it is better to immediately store the words per document.  The delineation enables Asynchronous system processing, which partially circumvents the inverted index update [[wikt:bottleneck|bottleneck]].<ref>Sergey Brin and Lawrence Page. [http://infolab.stanford.edu/~backrub/google.html The Anatomy of a Large-Scale Hypertextual Web Search Engine]. [[Stanford University]]. 1998. Verified Dec 2006.</ref> The forward index is [[Sorting algorithm|sorted]] to transform it to an inverted index. The forward index is essentially a list of pairs consisting of a document and a word, collated by the document. Converting the forward index to an inverted index is only a matter of sorting the pairs by the words. In this regard, the inverted index is a word-sorted forward index.

===Compression===
Generating or maintaining a large-scale search engine index represents a significant storage and processing challenge. Many search engines utilize a form of compression to reduce the size of the indices on [[computer storage|disk]].<ref>H.S. Heaps. Storage analysis of a compression coding for a document database. 1NFOR, I0(i):47-61, February 1972.</ref> Consider the following scenario for a full text, Internet search engine.

* It takes 8 bits (or 1 [[byte]]) to store a single character. Some [[character encoding|encodings]] use 2 bytes per character<ref>[http://www.unicode.org/faq/basic_q.html#15 The Unicode Standard - Frequently Asked Questions]. Verified Dec 2006.</ref><ref>[http://www.uplink.freeuk.com/data.html Storage estimates]. Verified Dec 2006.</ref>
* The average number of characters in any given word on a page may be estimated at 5 ([[Wikipedia:Size comparisons]])

Given this scenario, an uncompressed index (assuming a non-[[conflation|conflated]], simple, index) for 2 billion web pages would need to store 500 billion word entries. At 1 byte per character, or 5 bytes per word, this would require 2500 gigabytes of storage space alone. This space requirement may be even larger for a fault-tolerant distributed storage architecture. Depending on the compression technique chosen, the index can be reduced to a fraction of this size. The tradeoff is the time and processing power required to perform compression and decompression.

Notably, large scale search engine designs incorporate the cost of storage as well as the costs of electricity to power the storage. Thus compression is a measure of cost.

==Document parsing==
Document parsing breaks apart the components (words) of a document or other form of media for insertion into the forward and inverted indices. The words found are called ''tokens'', and so, in the context of search engine indexing and [[natural language processing]], parsing is more commonly referred to as [[Tokenization (lexical analysis)|tokenization]]. It is also sometimes called [[word boundary disambiguation]], [[Part-of-speech tagging|tagging]], [[text segmentation]], [[content analysis]], text analysis, [[text mining]], [[Agreement (linguistics)|concordance]] generation, [[speech segmentation]], [[Lexical analysis|lexing]], or [[lexical analysis]]. The terms 'indexing', 'parsing', and 'tokenization' are used interchangeably in corporate slang.

Natural language processing, as of 2006, is the subject of continuous research and technological improvement. Tokenization presents many challenges in extracting the necessary information from documents for indexing to support quality searching. Tokenization for indexing involves multiple technologies, the implementation of which are commonly kept as corporate secrets.

=== Challenges in natural language processing ===
; Word Boundary Ambiguity : Native [[English language|English]] speakers may at first consider tokenization to be a straightforward task, but this is not the case with designing a [[multilingual]] indexer.  In digital form, the texts of other languages such as [[Chinese language|Chinese]], [[Japanese language|Japanese]] or [[Arabic language|Arabic]] represent a greater challenge, as words are not clearly delineated by [[Whitespace (computer science)|whitespace]]. The goal during tokenization is to identify words for which users will search. Language-specific logic is employed to properly identify the boundaries of words, which is often the rationale for designing a parser for each language supported (or for groups of languages with similar boundary markers and syntax).

; Language Ambiguity : To assist with properly ranking matching documents, many search engines collect additional information about each word, such as its [[language]] or [[lexical category]] ([[part of speech]]). These techniques are language-dependent, as the syntax varies among languages. Documents do not always clearly identify the language of the document or represent it accurately. In tokenizing the document, some search engines attempt to automatically identify the language of the document.

; Diverse File Formats : In order to correctly identify which bytes of a document represent characters, the file format must be correctly handled. Search engines which support multiple file formats must be able to correctly open and access the document and be able to tokenize the characters of the document.

; Faulty Storage : The quality of the natural language data may not always be perfect.  An unspecified number of documents, particular on the Internet, do not closely obey proper file protocol.  [[Binary data|Binary]] characters may be mistakenly encoded into various parts of a document. Without recognition of these characters and appropriate handling, the index quality or indexer performance could degrade.

=== Tokenization ===
Unlike [[literacy|literate]] humans, computers do not understand the structure of a natural language document and cannot automatically recognize words and sentences. To a computer, a document is only a sequence of bytes. Computers do not 'know' that a space character separates words in a document. Instead, humans must program the computer to identify what constitutes an individual or distinct word, referred to as a token. Such a program is commonly called a [[tokenizer]] or [[parser]] or [[Lexical analysis|lexer]]. Many search engines, as well as other natural language processing software, incorporate [[Comparison of parser generators|specialized programs]] for parsing, such as [[YACC]] or [[Lex programming tool|Lex]].

During tokenization, the parser identifies sequences of characters which represent words and other elements, such as punctuation, which are represented by numeric codes, some of which are non-printing control characters. The parser can also identify [[Entity extraction|entities]] such as [[email]] addresses, phone numbers, and [[Uniform Resource Locator|URL]]s. When identifying each token, several characteristics may be stored, such as the token's case (upper, lower, mixed, proper), language or encoding, lexical category (part of speech, like 'noun' or 'verb'), position, sentence number, sentence position, length, and line number.

=== Language recognition ===
If the search engine supports multiple languages, a common initial step during tokenization is to identify each document's language; many of the subsequent steps are language dependent (such as [[stemming]] and [[part of speech]] tagging). [[Language identification|Language recognition]] is the process by which a computer program attempts to automatically identify, or categorize, the [[language]] of a document. Other names for language recognition include language classification, language analysis, language identification, and language tagging. Automated language recognition is the subject of ongoing research in [[natural language processing]]. Finding which language the words belongs to may involve the use of a [[language recognition chart]].

=== Format analysis ===
If the search engine supports multiple [[File format|document formats]], documents must be prepared for tokenization. The challenge is that many document formats contain formatting information in addition to textual content.  For example, [[HTML]] documents contain HTML tags, which specify formatting information such as new line starts, '''bold''' emphasis, and [[font]] size or [[Font family|style]].  If the search engine were to ignore the difference between content and 'markup', extraneous information would be included in the index, leading to poor search results. Format analysis is the identification and handling of the formatting content embedded within documents which controls the way the document is rendered on a computer screen or interpreted by a software program. Format analysis is also referred to as structure analysis, format parsing, tag stripping, format stripping, text normalization, text cleaning, and text preparation. The challenge of format analysis is further complicated by the intricacies of various file formats. Certain file formats are proprietary with very little information disclosed, while others are well documented. Common, well-documented file formats that many search engines support include:

* [[HTML]]
* [[ASCII]] text files (a text document without specific computer readable formatting)
* [[Adobe Systems|Adobe]]'s Portable Document Format ([[PDF]])
* [[PostScript]] (PS)
* [[LaTeX]]
* [[UseNet]] netnews server formats
* [[XML]] and derivatives like [[RSS]]
* [[SGML]]
* [[Multimedia]] [[meta data]] formats like [[ID3]]
* [[Microsoft Word]]
* [[Microsoft Excel]]
* [[Microsoft PowerPoint]]
* IBM [[Lotus Notes]]
Options for dealing with various formats include using a publicly available commercial parsing tool that is offered by the organization which developed, maintains, or owns the format, and writing a custom [[parser]].

Some search engines support inspection of files that are stored in a [[Compressor (software)|compressed]] or encrypted file format.  When working with a compressed format, the indexer first decompresses the document; this step may result in one or more files, each of which must be indexed separately. Commonly supported [[list of archive formats|compressed file format]]s include:

* [[ZIP (file format)|ZIP]] - Zip archive file
* [[RAR]] - Roshal ARchive file
* [[Cabinet (file format)|CAB]] - [[Microsoft Windows]] Cabinet File
* [[Gzip]] - File compressed with gzip
* [[Bzip2|BZIP]] - File compressed using bzip2
* [[Tar (file format)|Tape ARchive (TAR)]], [[Unix]] archive file, not (itself) compressed
* TAR.Z, TAR.GZ or TAR.BZ2 - [[Unix]] archive files compressed with Compress, GZIP or BZIP2

Format analysis can involve quality improvement methods to avoid including 'bad information' in the index.  Content can manipulate the formatting information to include additional content. Examples of abusing document formatting for [[spamdexing]]:

* Including hundreds or thousands of words in a section which is hidden from view on the computer screen, but visible to the indexer, by use of formatting (e.g. hidden [[Span and div|"div" tag]] in [[HTML]], which may incorporate the use of [[CSS]] or [[JavaScript]] to do so).
* Setting the foreground font color of words to the same as the background color, making words hidden on the computer screen to a person viewing the document, but not hidden to the indexer.

=== Section recognition ===
Some search engines incorporate section recognition, the identification of major parts of a document, prior to tokenization. Not all the documents in a corpus read like a well-written book, divided into organized chapters and pages.  Many documents on the [[Internet|web]], such as newsletters and corporate reports, contain erroneous content and side-sections which do not contain primary material (that which the document is about). For example, this article displays a side menu with links to other web pages. Some file formats, like HTML or PDF, allow for content to be displayed in columns. Even though the content is displayed, or rendered, in different areas of the view, the raw markup content may store this information sequentially. Words that appear sequentially in the raw source content are indexed sequentially, even though these sentences and paragraphs are rendered in different parts of the computer screen. If search engines index this content as if it were normal content, the quality of the index and search quality may be degraded due to the mixed content and improper word proximity. Two primary problems are noted:

* Content in different sections is treated as related in the index, when in reality it is not
* Organizational 'side bar' content is included in the index, but the side bar content does not contribute to the meaning of the document, and the index is filled with a poor representation of its documents.

Section analysis may require the search engine to implement the rendering logic of each document, essentially an abstract representation of the actual document, and then index the representation instead. For example, some content on the Internet is rendered via JavaScript. If the search engine does not render the page and evaluate the JavaScript within the page, it would not 'see' this content in the same way and would index the document incorrectly. Given that some search engines do not bother with rendering issues, many web page designers avoid displaying content via JavaScript or use the [[Noscript tag]] to ensure that the web page is indexed properly.  At the same time, this fact can also be [[spamdexing|exploited]] to cause the search engine indexer to 'see' different content than the viewer.

=== HTML Priority System ===
{{Section OR|date=November 2013}}
Indexing often has to recognize the [[HTML]] tags to organize priority. Indexing low priority to high margin to labels like ''strong'' and ''link'' to optimize the order of priority if those labels are at the beginning of the text could not prove to be relevant. Some indexers like [[Google]] and [[Bing]] ensure that the [[search engine]] does not take the large texts as relevant source due to[[ strong type system]] compatibility.<ref>Google Webmaster Tools, "Hypertext Markup Language 5", Conference for SEO January 2012.</ref>

=== Meta tag indexing ===
Specific documents often contain embedded meta information such as author, keywords, description, and language. For HTML pages, the [[meta tag]] contains keywords which are also included in the index. Earlier Internet [[search engine technology]] would only index the keywords in the meta tags for the forward index; the full document would not be parsed. At that time full-text indexing was not as well established, nor was [[computer hardware]] able to support such technology.  The design of the HTML markup language initially included support for meta tags for the very purpose of being properly and easily indexed, without requiring tokenization.<ref>Berners-Lee, T., "Hypertext Markup Language - 2.0", RFC 1866, Network Working Group, November 1995.</ref>

As the Internet grew through the 1990s, many [[brick and mortar business|brick-and-mortar corporations]] went 'online' and established corporate websites. The keywords used to describe webpages (many of which were corporate-oriented webpages similar to product brochures) changed from descriptive to marketing-oriented keywords designed to drive sales by placing the webpage high in the search results for specific search queries. The fact that these keywords were subjectively specified was leading to [[spamdexing]], which drove many search engines to adopt full-text indexing technologies in the 1990s. Search engine designers and companies could only place so many 'marketing keywords' into the content of a webpage before draining it of all interesting and useful information.  Given that conflict of interest with the business goal of designing user-oriented websites which were 'sticky', the [[customer lifetime value]] equation was changed to incorporate more useful content into the website in hopes of retaining the visitor. In this sense, full-text indexing was more objective and increased the quality of search engine results, as it was one more step away from subjective control of search engine result placement, which in turn furthered research of full-text indexing technologies.

In [[Desktop search]], many solutions incorporate meta tags to provide a way for authors to further customize how the search engine will index content from various files that is not evident from the file content. Desktop search is more under the control of the user, while Internet search engines must focus more on the full text index.

== See also ==
{{div col|colwidth=25em}}
* [[Compound term processing]]
* [[Concordance (publishing)|Concordance]]
* [[Content analysis]]
* [[Controlled vocabulary]]
* [[Desktop search]]
* [[Documentation]]
* [[Document retrieval|Document Retrieval]]
* [[Full text search]]
* [[Index (database)]]
* [[Information extraction]]
* [[Information retrieval]]
* [[Key Word in Context|Keyword In Context Indexing]]
* [[Latent semantic indexing]]
* [[List of search engines]]
* [[Natural language processing]]
* [[Search engine]]
* [[Selection-based search]]
* [[Semantic Web]]
* [[Site map]]
* [[Text mining]]
* [[Text retrieval|Text Retrieval]]
* [[Vertical search]]
* [[Web crawler]]
* [[Web indexing]]
* [[Website Parse Template]]
*[[Windows indexing service]]<ref>Krishna Nareddy. [http://msdn2.microsoft.com/en-us/library/ms951558.aspx Indexing with Microsoft Index Server]. MSDN Library. Microsoft Corporation. January 30, 1998. Verified Dec 2006. Note that this is a commercial, external link.</ref>
{{div col end}}

== References ==
<references/>

==Further reading==
*R. Bayer and E. McCreight. Organization and maintenance of large ordered indices. Acta Informatica, 173-189, 1972.
*[[Donald E. Knuth]]. The art of computer programming, volume 1 (3rd ed.): fundamental algorithms, Addison Wesley Longman Publishing Co. Redwood City, CA, 1997.
*[[Donald E. Knuth]]. The art of computer programming, volume 3: (2nd ed.) sorting and searching, Addison Wesley Longman Publishing Co. Redwood City, CA, 1998.
*[[Gerald Salton]]. Automatic text processing, Addison-Wesley Longman Publishing Co., Inc., Boston, MA, 1988.
*[[Gerard Salton]]. Michael J. McGill, Introduction to Modern Information Retrieval, McGraw-Hill, Inc., New York, NY, 1986.
*[[Gerard Salton]]. Lesk, M.E.: Computer evaluation of indexing and text processing. Journal of the ACM. January 1968.
*[[Gerard Salton]]. The SMART Retrieval System - Experiments in Automatic Document Processing. Prentice Hall Inc., Englewood Cliffs, 1971.
*[[Gerard Salton]]. The Transformation, Analysis, and Retrieval of Information by Computer, Addison-Wesley, Reading, Mass., 1989.
*Baeza-Yates, R., Ribeiro-Neto, B.: Modern Information Retrieval. Chapter 8. ACM Press 1999.
*G. K. Zipf. Human Behavior and the Principle of Least Effort. Addison-Wesley, 1949.
*Adelson-Velskii, G.M., Landis, E. M.: An information organization algorithm. DANSSSR, 146, 263-266 (1962).
*[[Edward H. Sussenguth Jr.]], Use of tree structures for processing files, Communications of the ACM, v.6 n.5, p.&nbsp;272-279, May 1963
*Harman, D.K., et al.: Inverted files. In Information Retrieval: Data Structures and Algorithms, Prentice-Hall, pp 2843, 1992.
*Lim, L., et al.: Characterizing Web Document Change, LNCS 2118, 133146, 2001.
*Lim, L., et al.: Dynamic Maintenance of Web Indexes Using Landmarks. Proc. of the 12th W3 Conference, 2003.
*Moffat, A., Zobel, J.: Self-Indexing Inverted Files for Fast Text Retrieval. ACM TIS, 349379, October 1996, Volume 14, Number 4.
*[[Kurt Mehlhorn|Mehlhorn, K.]]: Data Structures and Efficient Algorithms, Springer Verlag, EATCS Monographs, 1984.
*[[Kurt Mehlhorn|Mehlhorn, K.]], [[Mark Overmars|Overmars, M.H.]]: Optimal Dynamization of Decomposable Searching Problems. IPL 12, 9398, 1981.
*[[Kurt Mehlhorn|Mehlhorn, K.]]: Lower Bounds on the Efficiency of Transforming Static Data Structures into Dynamic Data Structures. Math. Systems Theory 15, 116, 1981.
*Koster, M.: ALIWEB: Archie-Like indexing in the Web. Computer Networks and ISDN Systems, Vol. 27, No. 2 (1994) 175-182 (also see Proc. First Int'l World Wide Web Conf., Elsevier Science, Amsterdam, 1994, pp.&nbsp;175182)
*[[Serge Abiteboul]] and [[Victor Vianu]]. [http://dbpubs.stanford.edu:8090/pub/showDoc.Fulltext?lang=en&doc=1996-20&format=text&compression=&name=1996-20.text Queries and Computation on the Web]. Proceedings of the International Conference on Database Theory. Delphi, Greece 1997.
*Ian H Witten, Alistair Moffat, and Timothy C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images. New York: Van Nostrand Reinhold, 1994.
*A. Emtage and P. Deutsch, "Archie--An Electronic Directory Service for the Internet." Proc. Usenix Winter 1992 Tech. Conf., Usenix Assoc., Berkeley, Calif., 1992, pp.&nbsp;93110.
*M. Gray, [http://www.mit.edu/people/mkgray/net/ World Wide Web Wanderer].
*D. Cutting and J. Pedersen. "Optimizations for Dynamic Inverted Index Maintenance." Proceedings of the 13th International Conference on Research and Development in Information Retrieval, pp.&nbsp;405411, September 1990.
*Stefan Buttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.

{{Internet search}}

{{DEFAULTSORT:Index (Search Engine)}}
[[Category:Information retrieval]]
[[Category:Searching]]
[[Category:Indexing]]
[[Category:Internet search algorithms]]
>>EOP<<
128<|###|>Semantic compression
In [[natural language processing]], '''semantic compression''' is a process of compacting a lexicon used to build 
a textual document (or a set of documents) by reducing language heterogeneity, while maintaining text [[semantics]]. 
As a result, the same ideas can be represented using a smaller set of words.

Semantic compression is a [[lossy compression]], that is, some data is being discarded, and an original document 
cannot be reconstructed in a reverse process.

==Semantic compression by generalization==
Semantic compression is basically achieved in two steps, using [[frequency list|frequency dictionaries]] and [[semantic network]]:
#	determining cumulated term frequencies to identify target lexicon,
#	replacing less frequent terms with their hypernyms ([[generalization]]) from target lexicon.<ref>[http://dx.doi.org/10.1007/978-3-642-12090-9_10 D. Ceglarek, K. Haniewicz, W. Rutkowski, Semantic Compression for Specialised Information Retrieval Systems], Advances in Intelligent Information and Database Systems, vol. 283, p. 111-121, 2010</ref>

Step 1 requires assembling word frequencies and 
information on semantic relationships, specifically [[hyponymy]]. Moving upwards in word hierarchy, 
a cumulative concept frequency is calculating by adding a sum of hyponyms' frequencies to frequency of their hypernym:
<math>cum f(k_{i}) = f(k_{i}) + \sum_{j} cum f(k_{j})</math> where <math>k_{i}</math> is a hypernym of <math>k_{j}</math>.
Then, a desired number of words with top cumulated frequencies are chosen to build a targed lexicon.

In the second step, compression mapping rules are defined for the remaining words, in order to handle every occurrence 
of a less frequent hyponym as its hypernym in output text.

;Example

The below fragment of text has been processed by the semantic compression. Words in bold have been replaced by their hypernyms.

<blockquote>They are both '''nest''' building '''social insects''', but '''paper wasps''' and honey '''bees''' '''organize''' their '''colonies''' 
in very different '''ways'''. In a new study, researchers report that despite their '''differences''', these insects 
'''rely on''' the same network of genes to guide their '''social behavior'''.The study appears in the Proceedings of the 
'''Royal Society B''': Biological Sciences. Honey '''bees''' and '''paper wasps''' are separated by more than 100 million years of 
'''evolution''', and there are '''striking differences''' in how they divvy up the work of '''maintaining''' a '''colony'''.</blockquote>

The procedure outputs the following text:

<blockquote>They are both '''facility''' building '''insect''', but '''insect''' and honey '''insects''' '''arrange''' their '''biological groups''' 
in very different '''structure'''. In a new study, researchers report that despite their '''difference of opinions''', these insects 
'''act''' the same network of genes to '''steer''' their '''party demeanor'''. The study appears in the proceeding of the 
'''institution bacteria''' Biological Sciences. Honey '''insects''' and '''insect''' are separated by more than hundred million years of 
'''organic process''', and there are '''impinging difference of opinions''' in how they divvy up the work of '''affirming''' a '''biological group'''.</blockquote>

==Implicit semantic compression==
A natural tendency to keep natural language expressions concise can be perceived as a form of implicit semantic compression, by omitting unmeaningful words or redundant meaningful words (especially to avoid [[pleonasm]]s)
.<ref>[http://dx.doi.org/10.3115/990100.990155 N. N. Percova, On the types of semantic compression of text],
COLING '82 Proceedings of the 9th Conference on Computational Linguistics, vol. 2, p. 229-231, 1982</ref>

==Applications and advantages==
In the [[vector space model]], compacting a lexicon leads to a reduction of [[curse of dimensionality|dimensionality]], which results in less 
[[computational complexity]] and a positive influence on efficiency. 

Semantic compression is advantageous in information retrieval tasks, improving their effectiveness (in terms of both precision and recall).<ref>[http://dl.acm.org/citation.cfm?id=1947662.1947683 D. Ceglarek, K. Haniewicz, W. Rutkowski, Quality of semantic compression in classification] Proceedings of the 2nd International Conference on Computational Collective Intelligence: Technologies and Applications, vol. 1, p. 162-171, 2010</ref> This is due to more precise descriptors (reduced effect of language diversity  limited language redundancy, a step towards a controlled dictionary).

As in the example above, it is possible to display the output as natural text (re-applying inflexion, adding stop words).

==See also==
* [[Text simplification]]
* [[Lexical substitution]]
* [[Information theory]]
* [[Quantities of information]]

==References==
<references/>

==External links==
* [http://semantic.net.pl/semantic_compression.php Semantic compression on Project SENECA (Semantic Networks and Categorization) website]

[[Category:Information retrieval]]
[[Category:Natural language processing]]
[[Category:Quantitative linguistics]]
[[Category:Computational linguistics]]
>>EOP<<
134<|###|>Category:Vector space model
[[Category:Information retrieval]]
>>EOP<<
140<|###|>SrensenDice coefficient
The '''SrensenDice index''', also known by other names (see Names, below), is a [[statistic]] used for comparing the similarity of two [[Sample (statistics)|samples]]. It was independently developed by the [[botanist]]s [[Thorvald Srensen]]<ref>{{cite journal |last=Srensen |first=T. |year=1948 |title=A method of establishing groups of equal amplitude in [[plant sociology]] based on similarity of species and its application to analyses of the vegetation on Danish commons |journal=[[Kongelige Danske Videnskabernes Selskab]] |volume=5 |issue=4 |pages=134 |doi= }}</ref> and [[Lee Raymond Dice]],<ref>{{cite journal |last=Dice |first=Lee R. |title=Measures of the Amount of Ecologic Association Between Species |jstor=1932409 |journal=Ecology |volume=26 |issue=3 |year=1945 |pages=297302 |doi=10.2307/1932409 }}</ref> who published in 1948 and 1945 respectively.

==Name==
The index is known by several other names, usually '''Srensen index''' or '''Dice's coefficient'''. Both names also see "similarity coefficient", "index", and other such variations. Common alternate spellings for Srensen are Sorenson, Soerenson index and Sorenson index, and all three can also be seen with the sen ending.

Other names include:
*[[Jan Czekanowski|Czekanowski]]'s binary (non-quantitative) index<ref name ="gallagher"/>

==Quantitative version==
The expression is easily extended to [[Abundance (ecology)|abundance]] instead of presence/absence of species. This quantitative version is known by several names:
* Quantitative SrensenDice index<ref name ="gallagher"/>
* Quantitative Srensen index<ref name ="gallagher"/>
* Quantitative Dice index<ref name ="gallagher"/>
* [[Bray Curtis dissimilarity|Bray-Curtis similarity]] (1 minus the ''Bray-Curtis dissimilarity'')<ref name ="gallagher"/>
* [[Jan Czekanowski|Czekanowski]]'s quantitative index<ref name ="gallagher"/>
* Steinhaus index<ref name ="gallagher"/>
* [[E. C. Pielou|Pielou]]'s percentage similarity<ref name ="gallagher"/>
* 1 minus the [[Hellinger distance]]<ref>{{cite journal |first=J. Roger |last=Bray |first2=J. T. |last2=Curtis |year=1948 |title=An Ordination of the Upland Forest Communities of Southern Wisconsin |journal=Ecological Monographs |volume=27 |issue=4 |pages=326349 |doi=10.2307/1942268 }}</ref>

==Formula==
Srensen's original formula was intended to be applied to presence/absence data, and is

:<math> QS = \frac{2C}{A + B} = \frac{2 |A \cap B|}{|A| + |B|}</math>

where ''A'' and ''B'' are the number of species in samples A and B, respectively, and ''C'' is the number of species shared by the two samples; QS is the quotient of similarity and ranges between 0 and 1.<ref>http://www.sekj.org/PDF/anbf40/anbf40-415.pdf</ref>

It can be viewed as a similarity measure over sets:

:<math>s = \frac{2 | X \cap Y |}{| X | + | Y |} </math>

Similarly to Jaccard, the set operations can be expressed in terms of vector operations over binary vectors ''A'' and ''B'':

<math>s_v = \frac{2 | A \cdot B |}{| A |^2 + | B |^2} </math>

which gives the same outcome over binary vectors and also gives a more general similarity metric over vectors in general terms.

For sets ''X'' and ''Y'' of keywords used in [[information retrieval]], the coefficient may be defined as twice the shared information (intersection) over the sum of cardinalities :<ref>{{cite book |last=van Rijsbergen |first=Cornelis Joost |year=1979
|title=Information Retrieval
|url=http://www.dcs.gla.ac.uk/Keith/Preface.html |publisher=Butterworths |location=London |isbn=3-642-12274-4 }}</ref>

When taken as a string similarity measure, the coefficient may be calculated for two strings, ''x'' and ''y'' using [[bigram]]s as follows:<ref>{{cite conference |last=Kondrak |first=Grzegorz |author2=Marcu, Daniel |author3= Knight, Kevin  |year=2003
|title=Cognates Can Improve Statistical Translation Models
|booktitle=Proceedings of HLT-NAACL 2003: Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics
|pages=4648 |url=http://aclweb.org/anthology/N/N03/N03-2016.pdf}}</ref>

:<math>s = \frac{2 n_t}{n_x + n_y}</math>

where ''n''<sub>''t''</sub> is the number of character bigrams found in both strings, ''n''<sub>''x''</sub> is the number of bigrams in string ''x'' and ''n''<sub>''y''</sub> is the number of bigrams in string ''y''. For example, to calculate the similarity between:

:<code>night</code>
:<code>nacht</code>

We would find the set of bigrams in each word:
:{<code>ni</code>,<code>ig</code>,<code>gh</code>,<code>ht</code>}
:{<code>na</code>,<code>ac</code>,<code>ch</code>,<code>ht</code>}

Each set has four elements, and the intersection of these two sets has only one element: <code>ht</code>.

Inserting these numbers into the formula, we calculate, ''s''&nbsp;=&nbsp;(2&nbsp;&nbsp;1)&nbsp;/&nbsp;(4&nbsp;+&nbsp;4)&nbsp;=&nbsp;0.25.

==Difference from Jaccard ==
This coefficient is not very different in form from the [[Jaccard index]].  However, since it doesn't satisfy the triangle inequality, it can be considered a [[Metric (mathematics)#Generalized metrics|semimetric]] version of the Jaccard index.<ref name ="gallagher"/>

The function ranges between zero and one, like Jaccard. Unlike Jaccard, the corresponding difference function

:<math>d = 1 -  \frac{2 | X \cap Y |}{| X | + | Y |} </math>

is not a proper distance metric as it does not possess the property of [[triangle inequality]].<ref name ="gallagher">Gallagher, E.D., 1999. [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.1334&rep=rep1&type=pdf COMPAH Documentation], University of Massachusetts, Boston</ref> The simplest counterexample of this is given by the three sets {a}, {b}, and {a,b}, the distance between the first two being 1, and the difference between the third and each of the others being one-third. To satisfy the triangle inequality, the sum of ''any'' two of these three sides must be greater than or equal to the remaining side. However, the distance between {a} and {a,b} plus the distance between {b} and {a,b} equals 2/3 and is therefore less than the distance between {a} and {b} which is 1.

==Applications==
The SrensenDice coefficient is mainly useful for ecological community data (e.g. Looman & Campbell, 1960<ref>[http://links.jstor.org/sici?sici=0012-9658%28196007%2941%3A3%3C409%3AAOSK%28F%3E2.0.CO%3B2-1 Looman, J. and Campbell, J.B. (1960) Adaptation of Sorensen's K (1948) for estimating unit affinities in prairie vegetation. Ecology 41 (3): 409416.]</ref>). Justification for its use is primarily  empirical rather than theoretical (although it can be justified  theoretically as the intersection of two [[fuzzy set]]s<ref>[http://dx.doi.org/10.1007/BF00039905 Roberts, D.W. (1986) Ordination on the basis of fuzzy set theory. Vegetatio 66 (3): 123131.]</ref>). As compared to [[Euclidean distance]], Srensen distance retains sensitivity in more heterogeneous data sets and gives less weight to outliers.<ref>McCune, Bruce & Grace, James (2002) Analysis of Ecological Communities. Mjm Software Design; ISBN 0-9721290-0-6.</ref>

==See also==
* [[Correlation]]
* [[Czekanowski similarity index]]
* [[Jaccard index]]
* [[Hamming distance]]
* [[Horns index]]
* [[Hurlberts index]]
* [[Kulczynski similarity index]]
* [[Pianka's index]]
* [[MacArthur and Levin's index]]
* [[Mantel test]]
* [[Morisita's overlap index]]
* [[Most frequent k characters]]
* [[Overlap coefficient]]
* [[Renkonen similarity index]] (due to [[Olavi Renkonen]])
* [[Simplified Morisitas index]]
* [[Tversky index]]
* [[Universal adaptive strategy theory (UAST)]]

==References==
{{reflist}}

==External links==
{{Wikibooks|Algorithm implementation|Strings/Dice's coefficient|Dice's coefficient}}
* Open Source [https://github.com/rockymadden/stringmetric/blob/master/core/src/main/scala/com/rockymadden/stringmetric/similarity/DiceSorensenMetric.scala Dice / Sorensen] [[Scala programming language|Scala]] implementation as part of the larger [http://rockymadden.com/stringmetric/ stringmetric project]

{{DEFAULTSORT:Sorensen-Dice coefficient}}
[[Category:Information retrieval]]
[[Category:String similarity measures]]
[[Category:Measure theory]]
>>EOP<<
146<|###|>Tag (metadata)
{{hatnote|Not to be confused with [[Markup language]] or [[HTML element]] tags.}}
[[File:Web 2.0 Map.svg|thumb|right|250px|A [[tag cloud]] with terms related to [[Web 2.0]]]]

In [[information system]]s, a '''tag''' is a non-hierarchical [[index term|keyword or term]] assigned to a piece of information (such as an [[Bookmark (World Wide Web)|Internet bookmark]], digital image, or [[computer file]]). This kind of [[metadata]] helps describe an item and allows it to be found again by browsing or searching. Tags are generally chosen informally and personally by the item's creator or by its viewer, depending on the system.

Tagging was popularized by websites associated with [[Web 2.0]] and is an important feature of many Web 2.0 services. It is now also part of some desktop software.

==History==

Labeling and tagging are carried out to perform functions such as aiding in [[Classification (machine learning)|classification]], marking ownership, noting boundaries, and indicating [[online identity]]. They may take the form of words, images, or other identifying marks. An analogous example of tags in the physical world is museum object tagging. In the organization of information and objects, the use of textual keywords as part of identification and classification long  predates computers. However, computer based searching made the use of keywords a rapid way of exploring records.

[[File:A Description of the Equator and Some Otherlands, collaborative hypercinema portal Upload page.jpg|thumb|A Description of the Equator and Some Otherlands, collaborative hypercinema portal, produced by documenta X, 1997. User upload page associating user contributed media with the term ''Tag''.]] Online and Internet databases and early websites deployed them as a way for publishers to help users find content. In 1997, the collaborative portal "A Description of the Equator and Some Other Lands" produced by [[documenta]] X, Germany, coined the folksonomic term ''Tag'' for its co-authors and guest authors on its Upload page. In "The Equator" the term ''Tag'' for user-input was described as an ''abstract literal or keyword'' to aid the user. Turned out in Web 1.0 days, all "Otherlands" users defined singular ''Tags'', and did not share ''Tags'' at that point.

In 2003, the [[social bookmarking]] website [[Delicious (website)|Delicious]] provided a way for its users to add "tags" to their bookmarks (as a way to help find them later); Delicious also provided browseable aggregated views of the bookmarks of all users featuring a particular tag.<ref>[http://flickr.com/photos/joshu/765809051/in/set-72157600740166824/ Screenshot of tags on del.icio.us] in 2004 and [http://flickr.com/photos/joshu/765817375/in/set-72157600740166824/ Screenshot of a tag page on del.icio.us], also in 2004, both published by [[Joshua Schachter]] on July 9, 2007.</ref> [[Flickr]] allowed its users to add their own text tags to each of their pictures, constructing flexible and easy metadata that made the pictures highly searchable.<ref>[http://www.adaptivepath.com/ideas/essays/archives/000519.php "An Interview with Flickr's Eric Costello"] by Jesse James Garrett, published on August 4, 2005. Quote: "Tags were not in the initial version of Flickr. Stewart Butterfield...liked the way they worked on del.icio.us, the social bookmarking application. We added very simple tagging functionality, so you could tag your photos, and then look at all your photos with a particular tag, or any one persons photos with a particular tag."</ref> The success of Flickr and the influence of Delicious popularized the concept,<ref>An example is [http://www.adammathes.com/academic/computer-mediated-communication/folksonomies.html "Folksonomies - Cooperative Classification and Communication Through Shared Metadata"] by Adam Mathes, December 2004. It focuses on tagging in Delicious and Flickr.</ref> and other [[social software]] websites&nbsp; such as [[YouTube]], [[Technorati]], and [[Last.fm]]&nbsp; also implemented tagging. Other traditional and web applications have incorporated the concept such as "Labels" in [[Gmail]] and the ability to add and edit tags in [[iTunes]] or [[Winamp]].

Tagging has gained wide popularity due to the growth of social networking, photography sharing and bookmarking sites. These sites allow users to create and manage labels (or tags) that categorize content using simple keywords. The use of keywords as part of an identification and classification system long predates computers. In the early days of the web keywords meta tags were used by web page designers to tell search engines what the web page was about. Today's tagging takes the meta keywords concept and re-uses it. The users add the tags. The tags are clearly visible, and are themselves links to other items that share that keyword tag.

Knowledge tags are an extension of [[Index term|keyword]] tags. They were first used by [[Jumper 2.0]], an [[open source]] [[Web 2.0]] software platform released by Jumper Networks on 29 September 2008.<ref>{{Citation|url=http://www.jumpernetworks.com/ NEWS-Jumper_Networks_Releases_Jumper_2.0_Platform.pdf|title=Jumper Networks Press Release for Jumper 2.0|publisher=Jumper Networks, Inc.|date=29 September 2008}}</ref> Jumper 2.0 was the first [[collaborative search engine]] platform to use a method of expanded tagging for [[knowledge capture]].

Websites that include tags often display collections of tags as [[tag cloud]]s. A user's tags are useful both to them and to the larger community of the website's users.

Tags may be a "bottom-up" type of classification, compared to [[hierarchy|hierarchies]], which are "top-down". In a traditional hierarchical system ([[Taxonomy (general)|taxonomy]]), the designer sets out a limited number of terms to use for classification, and there is one correct way to classify each item. In a tagging system, there are an unlimited number of ways to classify an item, and there is no "wrong" choice. Instead of belonging to one category, an item may have several different tags. Some researchers and applications have experimented with combining structured hierarchy and "flat" tagging to aid in information retrieval.<ref>[http://infolab.stanford.edu/~heymann/taghierarchy.html Tag Hierarchies], research notes by Paul Heymann.</ref>

==Examples==
=== Within a Blog ===
Many [[blog]] systems allow authors to add free-form tags to a post, along with (or instead of) placing the post into categories. For example, a post may display that it has been tagged with ''baseball'' and ''tickets''. Each of those tags is usually a [[web link]] leading to an index page listing all of the posts associated with that tag. The blog may have a sidebar listing all the tags in use on that blog, with each tag leading to an index page. To reclassify a post, an author edits its list of tags. All connections between posts are automatically tracked and updated by the blog software; there is no need to relocate the page within a complex hierarchy of categories.

===For an event===
An official tag is a keyword adopted by events and conferences for participants to use in their web publications, such as blog entries, photos of the event, and presentation slides. Search engines can then index them to make relevant materials related to the event searchable in a uniform way. In this case, the tag is part of a [[controlled vocabulary]].

===In research===
A researcher may work with a large collection of items (e.g. press quotes, a bibliography, images) in digital form. If he/she wishes to associate each with a small number of themes (e.g. to chapters of a book, or to sub-themes of the overall subject), then a group of tags for these themes can be attached to each of the items in the larger collection. In this way, free form [[categorization|classification]] allows the author to manage what would otherwise be unwieldy amounts of information. Commercial, as well as some free computer applications are readily available to do this.

==Special types==
===Triple tags===
{{see also|Microformat}}
A '''triple tag''' or '''machine tag''' uses a special [[syntax]] to define extra [[semantic]] information about the tag, making it easier or more meaningful for interpretation by a computer program. Triple tags comprise three parts: a [[namespace]], a [[wikt:predicate|predicate]], and a value. For example, "geo:long=50.123456" is a tag for the geographical [[longitude]] coordinate whose value is 50.123456. This triple structure is similar to the [[Resource Description Framework]] model for information.

The triple tag format was first devised for geolicious<ref>[http://brainoff.com/weblog/2004/11/05/124 geo.lici.us : geotagging hosted services] by Mikel Maron, November 5, 2004.</ref> in November 2004, to map [[Delicious (website)|Delicious]] bookmarks, and gained wider acceptance after its adoption by [http://stamen.com/projects/mappr Mappr] and GeoBloggers<ref>[http://web.archive.org/web/20071011024028/http://geobloggers.com/archives/2006/01/11/advanced-tagging-and-tripletags/ Advanced Tagging and TripleTags] by Reverend Dan Catt, ''Geobloggers'', January 11, 2006.</ref> to map [[Flickr]] photos. In January 2007, [[Aaron Straup Cope]] at [[Flickr]] introduced the term ''machine tag'' as an alternative name for the triple tag, adding some questions and answers on purpose, syntax, and use.<ref>[http://www.flickr.com/groups/api/discuss/72157594497877875/ Machine tags], a post by Aaron Straup Cope in the Flickr API group, January 24, 2007.</ref>

Specialized metadata for geographical identification is known as ''[[geotagging]]''; machine tags are also used for other purposes, such as identifying photos taken at a specific event or naming species using [[binomial nomenclature]].<ref>[http://www.flickr.com/groups/encyclopedia_of_life/rules/ Encyclopedia of Life use of machine tag], The Encyclopedia of Life project rules including the required use of a taxonomy machine tag, September 19, 2009.</ref>

===Hashtags===
{{main|Hashtag}}
A hashtag is a kind of metadata tag marked by the prefix <code>#</code>, sometimes known as a "hash" symbol. This form of tagging is used on [[microblogging]] and [[social networking service]]s such as [[Twitter]], [[Facebook]], [[Google+]], [[VK (social network)|VK]] and [[Instagram]].

===Knowledge tags===
A knowledge tag is a type of [[metadata|meta-information]] that describes or defines some aspect of an information resource (such as a [[document]], [[digital image]], [[database table|relational table]], or [[web page]]). Knowledge tags are more than traditional non-hierarchical [[index term|keywords or terms]]. They are a type of [[metadata]] that captures knowledge in the form of descriptions, categorizations, classifications, [[semantics]], comments, notes, annotations, [[hyperdata]], [[hyperlinks]], or references that are collected in tag profiles. These tag profiles reference an information resource that resides in a distributed, and often heterogeneous, storage repository. Knowledge tags are a [[knowledge management]] discipline that leverages [[Enterprise 2.0]] methodologies for users to capture insights, expertise, attributes, dependencies, or relationships associated with a data resource. It generally allows greater flexibility than other [[knowledge management]] classification systems.

Capturing knowledge in tags takes many different forms, there is factual knowledge (that found in books and data), conceptual knowledge (found in perspectives and concepts), expectational knowledge (needed to make judgments and hypothesis), and methodological knowledge (derived from reasoning and strategies).<ref>
{{Citation
 | last=Wiig | first=K. M.
 | year= 1997
 | title=Knowledge Management: An Introduction and Perspective
 | journal=Journal of Knowledge Management
 | volume=1 | issue=1
 | pages=614
 | url=http://www.mendeley.com/c/67997727/Wiig-1997-Knowledge-Management-An-Introduction-and-Perspective/
 | doi=10.1108/13673279710800682
}}
</ref> These forms of [[knowledge]] often exist outside the data itself and are derived from personal experience, insight, or expertise. 

Knowledge tags, in fact, manifest themselves in any number of ways  conceptual knowledge tags describe procedures, lessons learned, and facts that are related to the information resource. [[Tacit knowledge]] tags, manifests itself through skills, habits or learning by doing and represent experience or organizational intelligence. Anecdotal knowledge, is a memory of a particular case or event that may not surface without context.<ref>
{{citation
 | last=Getting | first=Brian
 | year= 2007
 | title=What Are Tags And What Is Tagging?
 | publisher=Practical eCommerce
 | url=http://www.practicalecommerce.com/articles/589
}}
</ref> 

Knowledge can best be defined as information possessed in the mind of an individual: it is personalized or subjective information related to facts, procedures, concepts, interpretations, ideas, observations and judgments (which may or may not be unique, useful, accurate, or structurable). Knowledge tags are considered an expansion of the information itself that adds additional value, context, and meaning to the information.<ref>{{citation
| author=Cambria, Erik and Hussain, Amir | title=Sentic album: Content-, concept-, and context-based online personal photo management system | journal=Cognitive Computation | volume=4 | issue=4 | pages=477-496 | year=2012 | doi=10.1007/s12559-012-9145-4}}</ref> Knowledge tags are valuable for preserving organizational intelligence that is often lost due to turn-over, for sharing knowledge stored in the minds of individuals that is typically isolated and unharnessed by the organization, and for connecting knowledge that is often lost or disconnected from an information resource.<ref>
{{Citation
 | last=Alavi | first=Maryam
 | last2=Leidner
 | year= 1999
 | title=Knowledge Management Systems: Issues, Challenges, and Benefits
 | journal=Communications of the Association for Information Systems
 | volume=1 | issue=7
 | url=http://www.belkcollege.uncc.edu/jpfoley/Readings/artic07.pdf
}}
</ref>

== Advantages and disadvantages ==
{{procon|date=November 2012}}

In a typical tagging system, there is no explicit information about the meaning or [[semantics]] of each tag, and a user can apply new tags to an item as easily as applying older tags. Hierarchical classification systems can be slow to change, and are rooted in the culture and era that created them.<ref name="Smith2008">Smith, Gene (2008). Tagging: People-Powered Metadata for the Social Web. Berkeley, CA: New Riders. ISBN 0-321-52917-0</ref> The flexibility of tagging allows users to classify their collections of items in the ways that they find useful, but the personalized variety of terms can present challenges when searching and browsing.

When users can freely choose tags (creating a [[folksonomy]], as opposed to selecting terms from a [[controlled vocabulary]]), the resulting metadata can include [[homonym]]s (the same tags used with different meanings) and [[synonym]]s (multiple tags for the same concept), which may lead to inappropriate connections between items and inefficient searches for information about a subject.<ref>Golder, Scott A. Huberman, Bernardo A. (2005).
"[http://arxiv.org/abs/cs.DL/0508082 The Structure of Collaborative Tagging Systems]." Information Dynamics Lab, HP Labs. Visited November 24, 2005.</ref> For example, the tag "orange" may refer to the [[Orange (fruit)|fruit]] or the [[Orange (colour)|color]], and items related to a version of the [[Linux kernel]] may be tagged "Linux", "kernel", "Penguin", "software", or a variety of other terms. Users can also choose tags that are different [[inflection]]s of words (such as singular and plural),<ref>[http://keithdevens.com/weblog/archive/2004/Dec/24/SvP.tags Singular vs. plural tags in a tag-based categorization system] by Keith Devens, December 24, 2004.</ref> which can contribute to navigation difficulties if the system does not include [[stemming]] of tags when searching or browsing. Larger-scale folksonomies address some of the problems of tagging, in that users of tagging systems tend to notice the current use of "tag terms" within these systems, and thus use existing tags in order to easily form connections to related items. In this way, folksonomies collectively develop a partial set of tagging conventions.

===Complex system dynamics===

Despite the apparent lack of control, research has shown that a simple form of shared vocabularies emerges in social bookmarking systems. Collaborative tagging exhibits a form of [[complex system]]s dynamics,<ref name="WWW07-ref">Harry Halpin, Valentin Robu, Hana Shepherd [http://portal.acm.org/citation.cfm?id=1242572.1242602 The Complex Dynamics of Collaborative Tagging], Proceedings of the 16th International Conference on the World Wide Web (WWW'07), Banff, Canada, pp. 211-220, ACM Press, 2007. Downloadable on [http://www2007.org/papers/paper635.pdf the conference's website]</ref> (or [[Self-organization|self organizing]] dynamics). Thus, even if no central controlled vocabulary constrains the actions of individual users, the distribution of tags that describe different resources (e.g., websites) converges over time to stable [[power law]] distributions.<ref name="WWW07-ref"/> Once such stable distributions form, simple vocabularies can be extracted by examining the [[correlation]]s that form between different tags.  This informal collaborative system of tag creation and management has been called a [[folksonomy]].

===Spamming===

Tagging systems open to the public are also open to tag spam, in which people apply an excessive number of tags or unrelated tags to an item (such as a [[YouTube]] video) in order to attract viewers. This abuse can be mitigated using human or statistical identification of spam items.<ref>[http://heymann.stanford.edu/tagspam.html Tag Spam], research notes by Paul Heymann.</ref> The number of tags allowed may also be limited to reduce spam.

==Syntax==
Some tagging systems provide a single [[text box]] to enter tags, so to be able to [[tokenize]] the string, a [[Wiktionary:separator|separator]] must be used. Two popular separators are the [[Space (punctuation)|space character]] and the [[comma]]. To enable the use of separators in the tags, a system may allow for higher-level separators (such as [[quotation mark]]s) or [[escape character]]s. Systems can avoid the use of separators by allowing only one tag to be added to each input [[Web widget|widget]] at a time, although this makes adding multiple tags more time-consuming.

A syntax for use within [[HTML]] is to use the '''rel-tag''' [[microformat]] which uses the [[Rel attribute|''rel'' attribute]] with value "tag" (i.e., <code>rel="tag"</code>) to indicate that the linked-to page acts as a tag for the current context.<ref>[http://microformats.org/wiki/rel-tag rel tag microformat specification], Microformats Wiki, January 10, 2005.</ref>

==See also==
{{colbegin||27em}}
* [[Collective intelligence]]
* [[Concept map]]
* [[Enterprise 2.0]]
* [[Enterprise bookmarking]]
* [[Explicit knowledge]]
* [[Faceted classification]]
* [[Folksonomy]]
* [[Information ecology]]
* [[Knowledge representation]]
* [[Knowledge transfer]]
* [[Metaknowledge]]
* [[Ontology (information science)]]
* [[Organisational memory]]
* [[Semantic web]]
* [[Tag cloud]]
* [[Web 2.0]]
{{colend}}
'''Others'''
{{colbegin||27em}}
* [[Collective unconscious]]
* [[Human-computer interaction]]
* [[Social network aggregation]]
* [[Enterprise social software]]
* [[Expert system]]
* [[Knowledge]]
* [[Knowledge base]]
* [[Knowledge worker]]
* [[Management information system]]
* [[Microformats]]
* [[Social network]]
* [[Social software]]
* [[Sociology of knowledge]]
* [[Tacit Knowledge]]
{{colend}}

==References==
{{reflist|30em}}

'''General'''
{{refbegin}}
*{{Citation
 | surname1=Nonaka | given1=Ikujiro
 | year=1994
 | title= A dynamic theory of organizational knowledge creation
 | journal= ORGANIZATION SCIENCE/ Vol. 5, No. 1, February 1994
 | pages=1437
 | url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=889992
}}
*{{Citation
 | surname1=Wigg | given1=Karl M  
 | year=1993  
 | title= Knowledge Management Foundations: Thinking About Thinking: How People and Organizations Create, Represent and Use Knowledge 
 | journal= Arlington: Schema Press  
 | pages=153
 | url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=889992 
}} 
*{{Citation
 | surname1=Alavi | given1=Maryam
 | surname2=Leidner | given2=Dorothy E.
 | year=1999
 | title=Knowledge management systems: issues, challenges, and benefits
 | journal=Communications of the AIS
 | volume=1| issue=2 | url=http://portal.acm.org/citation.cfm?id=374117
}}
*{{Citation
 | surname1=Sandy | given1=Kemsley
 | year=2009
 | title=Models, Social Tagging and Knowledge Management #BPM2009 #BPMS209
 | journal=BPM, Enterprise 2.0 and technology trends in business
 | url=http://www.column2.com/2009/09/models-social-tagging-and-knowledge-management-bpm2009-bpms209/
}}
{{refend}}

==External links==
* [http://www.inc.com/tech-blog/twitter-hashtag-techniques-for-businesses.html Hashtag Techniques for Businesses], Curt Finch. Inc Magazine. May 26, 2011.
* [http://www.tbray.org/tmp/tag-urn.html A Uniform Resource Name (URN) Namespace for Tag Metadata].  Tim Bray.  Internet draft, expired August 5, 2007.

{{Web syndication}}

{{DEFAULTSORT:Tag (Metadata)}}
[[Category:Collective intelligence]]
[[Category:Computer jargon]]
[[Category:Information retrieval]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Reference]]
[[Category:Web 2.0]]
>>EOP<<
152<|###|>URL redirection
{{Selfref|For redirection on Wikipedia, see [[Wikipedia:Redirect]].}}

{{refimprove|date=July 2014}}
'''URL redirection''', also called '''URL forwarding''', is a [[World Wide Web]] technique for making a [[web page]] available under more than one [[Uniform Resource Locator|URL]] address. When a [[web browser]] attempts to open a URL that has been redirected, a page with a different URL is opened. Similarly, '''domain redirection''' or '''domain forwarding''' is when all pages in a URL [[Domain name|domain]] are redirected to a different domain, as when [http://www.wikipedia.com wikipedia.com] and [http://www.wikipedia.net wikipedia.net] are automatically redirected to [http://www.wikipedia.org wikipedia.org].
URL redirection can be used for [[URL shortening]], to prevent [[link rot|broken links]] when web pages are moved, to allow multiple domain names belonging to the same owner to refer to a single [[website|web site]], to guide navigation into and out of a website, for privacy protection, and for less innocuous purposes such as [[phishing]] attacks.

== Purposes ==
There are several reasons to use URL redirection :

=== Similar domain names ===
A user might mis-type a URLfor example, "example.com" and "exmaple.com". Organizations often register these "mis-spelled" domains and re-direct them to the "correct" location: example.com. The addresses example.com and example.net could both redirect to a single domain, or web page, such as example.org. This technique is often used to "reserve" other [[top-level domain]]s (TLD) with the same name, or make it easier for a true ".edu" or ".net" to redirect to a more recognizable ".com" domain.

=== Moving pages to a new domain ===
Web pages may be redirected to a new domain for three reasons:
* a site might desire, or need, to change its domain name;
* an author might move his or her individual pages to a new domain;
* two web sites might merge.

With URL redirects, incoming links to an outdated URL can be sent to the correct location. These links might be from other sites that have not realized that there is a change or from bookmarks/favorites that users have saved in their browsers.

The same applies to [[search engine]]s. They often have the older/outdated domain names and links in their database and will send search users to these old URLs. By using a "moved permanently" redirect to the new URL, visitors will still end up at the correct page. Also, in the next search engine pass, the search engine should detect and use the newer URL.

=== Logging outgoing links ===
The access logs of most web servers keep detailed information about where visitors came from and how they browsed the hosted site.  They do not, however, log which links visitors left by.  This is because the visitor's browser has no need to communicate with the original server when the visitor clicks on an outgoing link.

This information can be captured in several ways.  One way involves URL redirection.  Instead of sending the visitor straight to the other site, links on the site can direct to a URL on the original website's domain that automatically redirects to the real target. This technique bears the downside of the delay caused by the additional request to the original website's server. As this added request will leave a trace in the server log, revealing exactly which link was followed, it can also be a privacy issue.<ref>
{{cite journal
  | title = Google revives redirect snoopery
  | journal = blog.anta.net
  | date = 2009-01-29
  | url = http://blog.anta.net/2009/01/29/509/
  | issn = 1797-1993
  | archiveurl=http://web.archive.org/web/20110817024348/http://blog.anta.net/2009/01/29/509/
  | archivedate=2011-08-17
}}</ref>

The same technique is also used by some corporate websites to implement a statement that the subsequent content is at another site, and therefore not necessarily affiliated with the corporation. In such scenarios, displaying the warning causes an additional delay.

=== Short aliases for long URLs ===
{{Main|URL shortening}}

Web applications often include lengthy descriptive attributes in their URLs which represent data hierarchies, command structures, transaction paths and session information. This practice results in a URL that is aesthetically unpleasant and difficult to remember, and which may not fit within the size limitations of [[microblogging]] sites. [[URL shortening]] services provide a solution to this problem by redirecting a user to a longer URL from a shorter one.

=== Meaningful, persistent aliases for long or changing URLs ===
{{See also|Permalink|PURL|Link rot}}

Sometimes the URL of a page changes even though the content stays the same. Therefore URL redirection can help users who have bookmarks. This is routinely done on Wikipedia whenever a page is renamed.

=== Post/Redirect/Get ===
{{Main|Post/Redirect/Get}}

Post/Redirect/Get (PRG) is a [[web development]] [[design pattern]] that prevents some duplicate [[form (web)|form]] submissions, creating a more intuitive interface for [[user agent]]s (users).

=== Manipulating search engines ===
Redirect techniques are used to fool search engines.  For example, one page could show popular search terms to search engines but redirect the visitors to a different target page.  There are also cases where redirects have been used to "steal" the page rank of one popular page and use it for a different page, They will also redirect using searches with search engines as searches, usually involving the 302 [[List of HTTP status codes|HTTP status code]] of "moved temporarily."<ref>{{cite web|url=http://www.pandia.com/sw-2004/40-hijack.html |title=Google's serious hijack problem  Spammers hijack web site listings in Google |date=September 13, 2004 |publisher=Pandia.com |archiveurl=http://web.archive.org/web/20130605153457/http://www.pandia.com/sw-2004/40-hijack.html |archivedate=2013-06-05}}</ref><ref>[http://www.loriswebs.com/hijacking_web_pages.html "Stop Scrapers From Hijacking your Web Pages"]. Lori's Web Design.com. Retrieved 2013-12-18.</ref>

Search engine providers have noticed the problem and are working on appropriate actions.{{Citation needed|date=August 2009}}

As a result, today, such manipulations usually result in less rather than more site exposure.

=== Manipulating visitors ===
URL redirection is sometimes used as a part of [[phishing]] attacks that confuse visitors about which web site they are visiting.{{Citation needed|date=January 2010}} Because modern browsers always show the real URL in the address bar, the threat is lessened. However, redirects can also take you to sites that will otherwise attempt to attack in other ways. For example, a redirect might take a user to a site that would attempt to trick them into downloading antivirus software and, ironically, installing a [[trojan horse (computing)|trojan]] of some sort instead.

=== Removing <code>referer</code> information ===
When a link is clicked, the browser sends along in the [[HTTP request]] a field called [[HTTP referer|referer]] which indicates the source of the link. This field is populated with the URL of the current web page, and will end up in the [[server log|logs]] of the server serving the external link. Since sensitive pages may have sensitive URLs (for example, <code><nowiki>http://company.com/plans-for-the-next-release-of-our-product</nowiki></code>), it is not desirable for the <code>referer</code> URL to leave the organization. A redirection page that performs [[Referer#Referrer hiding|referrer hiding]] could be embedded in all external URLs, transforming for example <code><nowiki>http://externalsite.com/page</nowiki></code> into <code><nowiki>http://redirect.company.com/http://externalsite.com/page</nowiki></code>. This technique also eliminates other potentially sensitive information from the referer URL, such as the [[session ID]], and can reduce the chance of [[phishing]] by indicating to the end user that they passed a clear gateway to another site.

== Techniques ==
Several different kinds of response to the browser will result in a redirection.  These vary in whether they affect [[HTTP headers]] or HTML content.  The techniques used typically depend on the role of the person implementing it and their access to different parts of the system.  For example, a web author with no control over the headers might use a [[meta refresh|Refresh meta tag]] whereas a web server administrator redirecting all pages on a site is more likely to use server configuration.

=== Manual redirect ===
The simplest technique is to ask the visitor to follow a link to the new page, usually using an HTML anchor like:

<source lang="html4strict">
Please follow <a href="http://www.example.com/">this link</a>.
</source>

This method is often used as a fall-back&nbsp; if the browser does not support the automatic redirect, the visitor can still reach the target document by following the link.

=== HTTP status codes 3xx ===
In the [[HTTP]] [[Protocol (computing)|protocol]] used by the [[World Wide Web]], a '''redirect''' is a response with a [[List of HTTP status codes|status code]] beginning with ''3'' that causes a browser to display a different page.  The different codes describe the reason for the redirect, which allows for the correct subsequent action (such as changing links in the case of code 301, a permanent change of address).

HTTP/1.1 defines [http://tools.ietf.org/html/rfc7231#section-6.4 several status codes] for redirection:
* [[HTTP 300|300 multiple choices]] (e.g. offer different languages)
* [[HTTP 301|301 moved permanently]]
* [[HTTP 302|302 found]] (originally "temporary redirect" in HTTP/1.0 and popularly used for CGI scripts; superseded by 303 and 307 in HTTP/1.1 but preserved for backward compatibility)
* [[HTTP 303|303 see other]] (forces a GET request to the new URL even if original request was POST)
* [[HTTP 307|307 temporary redirect]] (provides a new URL for the browser to resubmit a GET or POST request)

All of these status codes require that the URL of the redirect target be given in the Location: header of the HTTP response.  The 300 multiple choices will usually list all choices in the body of the message and show the default choice in the Location: header.

(Status codes [[HTTP 304|304 not modified]] and [[HTTP 305|305 use proxy]] are not redirects).

An [[HTTP]] response with the 301 "moved permanently" redirect looks like this:

<source lang="html4strict">
HTTP/1.1 301 Moved Permanently
Location: http://www.example.org/
Content-Type: text/html
Content-Length: 174

<html>
<head>
<title>Moved</title>
</head>
<body>
<h1>Moved</h1>
<p>This page has moved to <a href="http://www.example.org/">http://www.example.org/</a>.</p>
</body>
</html>
</source>

==== Using server-side scripting for redirection ====
Web authors producing HTML content can't usually create redirects using HTTP headers as these are generated automatically by the web server program when serving an HTML file.  The same is usually true even for programmers writing CGI scripts, though some servers allow scripts to add custom headers (e.g. by enabling "non-parsed-headers").  Many web servers will generate a 3xx status code if a script outputs a "Location:" header line.  For example, in [[PHP]], one can use the "header" function:

<source lang="php">
header('HTTP/1.1 301 Moved Permanently');
header('Location: http://www.example.com/');
exit();
</source>

(More headers may be required to prevent caching<ref name="php-301-robust-solution">{{cite web|url=http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution/ |title=PHP Redirects: 302 to 301 Rock Solid Robust Solution |publisher=WebSiteFactors.co.uk |archiveurl=http://web.archive.org/web/20121012042703/http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution |archivedate=2012-10-12}}</ref>).

The programmer must ensure that the headers are output before the body.  This may not fit easily with the natural flow of control through the code.  To help with this, some frameworks for server-side content generation can buffer the body data.  In the [[Active Server Pages|ASP scripting]] language, this can also be accomplished using <code>response.buffer=true</code> and <code>response.redirect <nowiki>"http://www.example.com/"</nowiki></code>

HTTP/1.1 [http://tools.ietf.org/html/rfc7231#section-7.1.2 allows for] either a relative URI reference or an absolute URI reference. If the URI reference is relative the client computes the required absolute URI reference according to [http://tools.ietf.org/html/rfc3986#section-5 the rules defined in RFC 3986].

==== Apache mod_rewrite ====
The [[Apache HTTP Server]]'s [http://httpd.apache.org/docs/current/mod/mod_alias.html mod_alias] extension can be used to redirect certain requests.  Typical configuration directives look like:

<source lang="apache">
Redirect permanent /oldpage.html http://www.example.com/newpage.html
Redirect 301 /oldpage.html http://www.example.com/newpage.html
</source>
</blockquote>

For more flexible URL rewriting and redirection, Apache [http://httpd.apache.org/docs/current/mod/mod_rewrite.html mod_rewrite] can be used.  E.g. to redirect a requests to a canonical domain name:
<source lang="apache">
RewriteEngine on
RewriteCond %{HTTP_HOST} ^([^.:]+\.)*oldsite\.example\.com\.?(:[0-9]*)?$ [NC]
RewriteRule ^(.*)$ http://newsite.example.net/$1 [R=301,L]
</source>

Such configuration can be applied to one or all sites on the server through the server configuration files or to a single content directory through a <code>.htaccess</code> file.

==== nginx rewrite ====
[[Nginx]] has an integrated http rewrite module,<ref>{{cite web|url=http://nginx.org/r/rewrite |title=Module ngx_http_rewrite_module - rewrite |publisher=nginx.org |date= |accessdate=24 December 2014}}</ref> which can be used to perform advanced URL processing and even web-page generation (with the <tt>return</tt> directive).  A showing example of such advanced use of the rewrite module is [http://mdoc.su/ mdoc.su], which implements a deterministic [[URL shortening]] service entirely with the help of nginx configuration language alone.<ref>{{cite mailing list |date=18 February 2013 |url=http://mailman.nginx.org/pipermail/nginx/2013-February/037592.html |mailinglist=nginx@nginx.org |title=A dynamic web-site written wholly in nginx.conf? Introducing mdoc.su! |first=Constantine A. |last=Murenin |accessdate=24 December 2014}}</ref><ref>{{cite web |url=http://mdoc.su/ |title=mdoc.su  Short manual page URLs for FreeBSD, OpenBSD, NetBSD and DragonFly BSD |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}</ref>

For example, if a request for [http://mdoc.su/DragonFlyBSD/HAMMER.5 <tt>/DragonFlyBSD/HAMMER.5</tt>] were to come along, it would first be redirected internally to <tt>/d/HAMMER.5</tt> with the first rewrite directive below (only affecting the internal state, without any HTTP replies issued to the client just yet), and then with the second rewrite directive, an [[HTTP response]] with a [[HTTP 302|302 Found status code]] would be issued to the client to actually redirect to the external [[Common Gateway Interface|cgi script]] of web-[[man page|man]]:<ref>{{cite web |url=http://nginx.conf.mdoc.su/mdoc.su.nginx.conf |title=mdoc.su.nginx.conf |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}</ref>
<source lang="pcre">
	location /DragonFly {
		rewrite	^/DragonFly(BSD)?([,/].*)?$	/d$2	last;
	}
	location /d {
		set	$db	"http://leaf.dragonflybsd.org/cgi/web-man?command=";
		set	$ds	"&section=";
		rewrite	^/./([^/]+)\.([1-9])$		$db$1$ds$2	redirect;
	}
</source>

=== Refresh Meta tag and HTTP refresh header ===
[[Netscape]] introduced the [[meta refresh]] feature which refreshes a page after a certain amount of time.  This can specify a new URL to replace one page with another.  This is supported by most web browsers.  See
* [http://www.w3schools.com/tags/tag_meta.asp HTML <meta> tag]
* [http://web.archive.org/web/20020802170847/http://wp.netscape.com/assist/net_sites/pushpull.html An exploration of dynamic documents]

A timeout of zero seconds effects an immediate redirect. This is treated like a 301 permanent redirect by Google, allowing transfer of PageRank to the target page.<ref>[http://sebastians-pamphlets.com/google-and-yahoo-treat-undelayed-meta-refresh-as-301-redirect/ "Google and Yahoo accept undelayed meta refreshs as 301 redirects"]. Sebastian's Pamphlets. 3 September 2007.</ref>

This is an example of a simple HTML document that uses this technique:
<source lang="html4strict">
<html>
<head>
<meta http-equiv="Refresh" content="0; url=http://www.example.com/" />
</head>
<body>
<p>Please follow <a href="http://www.example.com/">this link</a>.</p>
</body>
</html>
</source>

This technique can be used by [[Web designer|web authors]] because the meta tag is contained inside the document itself.  The meta tag must be placed in the "head" section of the HTML file.  The number "0" in this example may be replaced by another number to achieve a delay of that many seconds.  The anchor in the "body" section is for users whose browsers do not support this feature.

The same effect can be achieved with an HTTP <code>refresh</code> header:
<source lang="html4strict">
HTTP/1.1 200 ok
Refresh: 0; url=http://www.example.com/
Content-type: text/html
Content-length: 78

Please follow <a href="http://www.example.com/">this link</a>.
</source>

This response is easier to generate by CGI programs because one does not need to change the default status code.

Here is a simple CGI program that effects this redirect:
<source lang="perl">
#!/usr/bin/perl
print "Refresh: 0; url=http://www.example.com/\r\n";
print "Content-type: text/html\r\n";
print "\r\n";
print "Please follow <a href=\"http://www.example.com/\">this link</a>!"
</source>

Note: Usually, the HTTP server adds the status line and the Content-length header automatically.

The [[World Wide Web Consortium|W3C]] discourage the use of meta refresh, since it does not communicate any information about either the original or new resource, to the browser (or [[search engine]]). The W3C's [http://www.w3.org/TR/WAI-WEBCONTENT/#tech-no-periodic-refresh Web Content Accessibility Guidelines (7.4)] discourage the creation of auto-refreshing pages, since most web browsers do not allow the user to disable or control the refresh rate.  Some articles that they have written on the issue include [http://www.w3.org/TR/WAI-WEBCONTENT/#gl-movement W3C Web Content Accessibility Guidelines (1.0): Ensure user control of time-sensitive content changes], [http://www.w3.org/QA/Tips/reback Use standard redirects: don't break the back button!] and [http://www.w3.org/TR/WCAG10-CORE-TECHS/#auto-page-refresh Core Techniques for Web Content Accessibility Guidelines 1.0 section 7].

=== JavaScript redirects ===
[[JavaScript]] can cause a redirect by setting the <code>window.location</code> attribute, e.g.:
<syntaxhighlight lang="ecmascript">
window.location='http://www.example.com/'
</syntaxhighlight>
Normally JavaScript pushes the redirector site's [[URL]] to the browser's history. It can cause redirect loops when users hit the back button. With the following command you can prevent this type of behaviour.<ref>{{cite web|url=http://online-marketing-technologies.com/tools/javascript-redirection-generator.html|title=Advanced JavaScript Redirections|publisher=Online Marketing Technologies}}</ref>
<syntaxhighlight lang="ecmascript">
window.location.replace('http://www.example.com/')
</syntaxhighlight>
However, HTTP headers or the refresh meta tag may be preferred for security reasons and because JavaScript will not be executed by some browsers and many [[web crawler]]s.

=== Frame redirects ===
A slightly different effect can be achieved by creating a single HTML [[Iframe|frame]] that contains the target page:
<source lang="html4strict">
<frameset rows="100%">
  <frame src="http://www.example.com/">
  <noframes>
    <body>Please follow <a href="http://www.example.com/">link</a>.</body>
  </noframes>
</frameset>
</source>

One main difference to the above redirect methods is that for a frame redirect, the browser displays the URL of the frame document and not the URL of the target page in the URL bar.

This ''cloaking'' technique may be used so that the reader sees a more memorable URL or to fraudulently conceal a [[phishing]] site as part of [[website spoofing]].<ref>Aaron Emigh (19 January 2005). [http://www.sfbay-infragard.org/Documents/phishing-sfectf-report.pdf "Anti-Phishing Technology"] (PDF). Radix Labs.</ref>

The same effect can be done with an inline frame:
<source lang="html4strict">
<iframe height="100%" width="100%" src="http://www.example.com/">
Please follow <a href="http://www.example.com/">link</a>.
</iframe>
</source>

=== Redirect chains ===
One redirect may lead to another. For example, the URL [http://www.wikipedia.com/wiki/URL_redirection http://www.wikipedia'''.com'''/wiki/URL_redirection] (note the domain name) is first redirected to [[:www:URL redirection|http://www.wikipedia'''.org'''/wiki/URL redirection]] and then to the correct URL: http://en.wikipedia.org/wiki/URL_redirection. This is unavoidable if the different links in the chain are served by different servers though it should be minimised by ''rewriting'' the URL as much as possible on the server before returning it to the browser as a redirect.

=== Redirect loops ===
Sometimes a mistake can cause a page to end up redirecting back to itself, possibly via other pages, leading to an infinite sequence of redirects. Browsers should stop redirecting after a certain number of hops and display an error message.

[http://tools.ietf.org/html/rfc7231#section-6.4 HTTP/1.1] states:
<blockquote>
A client ''SHOULD'' detect and intervene in cyclical redirections (i.e., "infinite" redirection loops).

Note: An earlier version of this specification recommended a maximum of five redirections ([RFC2068], Section 10.3).  Content developers need to be aware that some clients might implement such a fixed limitation.
</blockquote>
Note that the URLs in the sequence might not repeat, e.g.: http://www.example.com/1 -> http://www.example.com/2 -> http://www.example.com/3 ...

== Services ==
There exist services that can perform URL redirection on demand, with no need for technical work or access to the web server your site is hosted on.

=== URL redirection services ===
A '''redirect service''' is an information management system, which provides an internet link that redirects users to the desired content. The typical benefit to the user is the use of a memorable domain name, and a reduction in the length of the URL or web address. A redirecting link can also be used as a permanent address for content that frequently changes hosts, similarly to the [[Domain Name System]].

Hyperlinks involving URL redirection services are frequently used in spam messages directed at blogs and wikis.  Thus, one way to reduce spam is to reject all edits and comments containing hyperlinks to known URL redirection services; however, this will also remove legitimate edits and comments and may not be an effective method to reduce spam.

Recently, URL redirection services have taken to using [[AJAX]] as an efficient, user friendly method for creating shortened URLs.

A major drawback of some URL redirection services is the use of delay pages, or frame based advertising, to generate revenue.

==== History ====
The first redirect services took advantage of [[top-level domains]] (TLD) such as "[[.to]]" (Tonga), "[[.at]]" (Austria) and "[[.is]]" (Iceland). Their goal was to make memorable URLs. The first mainstream redirect service was V3.com that boasted 4 million users at its peak in 2000.  V3.com success was attributed to having a wide variety of short memorable domains including "r.im", "go.to", "i.am", "come.to" and "start.at".  V3.com was acquired by FortuneCity.com, a large free web hosting company, in early 1999.<ref>{{cite news| url=http://news.bbc.co.uk/2/hi/technology/6991719.stm | work=BBC News | title=Net gains for tiny Pacific nation | date=2007-09-14 | accessdate=2010-05-27}}</ref> As the sales price of top level domains started falling from $70.00 per year to less than $10.00, use of redirection services declined.

With the launch of [[TinyURL]] in 2002 a new kind of redirecting service was born, namely [[URL shortening]]. Their goal was to make long URLs short, to be able to post them on internet forums. Since 2006, with the 140 character limit on the extremely popular [[Twitter]] service, these short URL services have been heavily used.

=== Referrer masking ===
Redirection services can hide the [[referrer]] by placing an intermediate page between the page the link is on and its destination. Although these are conceptually similar to other URL redirection services, they serve a different purpose, and they rarely attempt to shorten or obfuscate the destination URL (as their only intended side-effect is to hide referrer information and provide a clear gateway between other websites.)

This type of redirection is often used to prevent potentially-malicious links from gaining information using the referrer, for example a [[session ID]] in the query string. Many large community websites use link redirection on external links to lessen the chance of an exploit that could be used to steal account information, as well as make it clear when a user is leaving a service, to lessen the chance of effective [[phishing]]  .

Here is a simplistic example of such a service, written in [[PHP]].
<source lang="php">
<?php
$url = htmlspecialchars($_GET['url']);
header( 'Refresh: 0; url=http://'.$url );
?>
<!-- Fallback using meta refresh. -->
<html>
 <head>
  <title>Redirecting...</title>
  <meta http-equiv="refresh" content="0;url=http://<?php echo $url; ?>">
 </head>
 <body>
 Attempting to redirect to <a href="http://<?php echo $url; ?>">http://<?php echo $url; ?></a>.
 </body>
</html>
</source>

The above example does not check who called it (e.g. by referrer, although that could be spoofed).  Also, it does not check the url provided.  This means that a malicious person could link to the redirection page using a url parameter of his/her own selection, from any page, which uses the web server's resources.

==Security Issues==
URL redirection can be abused by attackers for [[Phishing]] attacks, such as [[Open Redirect]] and [[Covert Redirect]].

"An open redirect is an application that takes a parameter and redirects a user to the parameter value without any validation."<ref name="Open_Redirect">{{cite web | url=https://www.owasp.org/index.php/Open_redirect | title=Open Redirect |publisher= OWASP |date=16 March 2014 | accessdate=21 December 2014}}</ref>

"Covert Redirect is an application that takes a parameter and redirects a user to the parameter value WITHOUT SUFFICIENT validation."<ref name="Covert_Redirect">{{cite web | url=http://tetraph.com/covert_redirect/ | title=Covert Redirect |publisher= Tetraph |date=1 May 2014 | accessdate=21 December 2014}}</ref> It is disclosed in May 2014 by a mathematical doctoral student Wang Jing from Nanyang Technological University, Singapore.<ref name="CNET">{{cite web | url=http://www.cnet.com/news/serious-security-flaw-in-oauth-and-openid-discovered/ | title=Serious security flaw in OAuth, OpenID discovered |publisher= CNET |date=2 May 2014 | accessdate=21 December 2014}}</ref>

== See also ==
* [[Link rot]]
* [[Canonical meta tag]]
* [[Domain masking]]

== References ==
{{Reflist}}

== External links ==
* [http://httpd.apache.org/docs/1.3/urlmapping.html Mapping URLs to Filesystem Locations]
* [http://www.cs.ucdavis.edu/~hchen/paper/www07.pdf Paper on redirection spam (UC Davis)] (403 Forbidden link)
* [http://projects.webappsec.org/URL-Redirector-Abuse Security vulnerabilities in URL Redirectors] The Web Application Security Consortium Threat Classification
* [http://www.dancatts.com/articles/htaccess-301-redirects-for-moved-pages.php 301 Redirects for moved pages using .htaccess]
* [http://911-need-code-help.blogspot.com/2011/03/redirecting-visitors-to-preferred.html Redirecting your visitors to your preferred domain] using 301 permanent redirects&nbsp; rationale and mod_rewrite/PHP/ASP.NET implementations

{{Spamming}}

{{Use dmy dates|date=November 2010}}

{{DEFAULTSORT:Url Redirection}}
[[Category:Uniform resource locator]]
[[Category:Black hat search engine optimization]]
[[Category:Information retrieval]]
[[Category:Internet terminology]]
>>EOP<<
158<|###|>Category:Deep Web
{{cat main|Deep Web}}


[[Category:Information retrieval]]
[[Category:World Wide Web]]
>>EOP<<
164<|###|>Topic-based vector space model
The '''Topic-based Vector Space Model (TVSM)'''<ref>{{cite | url=http://www.kuropka.net/files/TVSM.pdf | title=Topic-based Vector Space Model | author=Dominik Kuropka | coauthors=Jorg Becker | year=2003}}</ref> (literature: [http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&lng=eng&id=]) extends the [[vector space model]] of [[information retrieval]] by removing the constraint that the term-vectors be orthogonal. The assumption of orthogonal terms is incorrect regarding natural languages which causes problems with synonyms and strong related terms. This facilitates the use of stopword lists, stemming and thesaurus in TVSM.
In contrast to the [[generalized vector space model]] the TVSM does not depend on concurrence-based similarities between terms. 

==Definitions==
The basic premise of TVSM is the existence of a ''d'' dimensional space ''R'' with only positive axis intercepts, i.e. ''R in R<sup>+</sup>'' and ''d in N<sup>+</sup>''. Each dimension of ''R'' represents a fundamental topic. A term vector ''t'' has a specific weight for a certain ''R''. To calculate these weights assumptions are made taking into account the document contents. Ideally important terms will have a high weight and stopwords and irrelevants terms to the topic will have a low weight. The TVSM document model is obtained as a sum of term vectors representing terms in the document. The similarity between two documents ''Di'' and ''Dj'' is defined as the scalar product of document vectors.

==Enhanced Topic-based Vector Space Model==
The enhancement of the Enhanced Topic-based Vector Space Model (eTVSM)<ref>{{cite | url= http://kuropka.net/files/HPI_Evaluation_of_eTVSM.pdf | author=Dominik Kuropka | coauthors=Artem Polyvyanyy | title=A Quantitative Evaluation of the Enhanced Topic-Based Vector Space Model | year=2007}}</ref> (literature: [http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&lng=eng&id=]) is a proposal on how to derive term vectors from an [[Ontology_(information_science) | Ontology]]. Using a synonym Ontology created from [[WordNet]] Kuropka shows good results for document similarity. If a trivial Ontology is used the results are similar to Vector Space model.

==Implementations==
* [http://sourceforge.net/projects/etvsm/ Implementation of eTVSM in python]

== References ==
{{reflist}}

[[Category:Vector space model]]
>>EOP<<
170<|###|>Reverse DNS lookup
'''{{Redirect|Reverse DNS}}

In [[computer networking]], '''reverse DNS lookup''' or '''reverse DNS resolution''' (rDNS) is the determination of a [[domain name]] that is associated with a given  [[IP address]] using the [[Domain Name System]] (DNS) of the [[Internet]].

Computer networks use the Domain Name System to determine the IP address associated with a domain name. This process is also known as ''forward'' DNS resolution.  ''Reverse'' DNS lookup is the inverse process, the resolution of an IP address to its designated domain name.

The reverse DNS database of the Internet is rooted in the ''Address and Routing Parameter Area'' (<tt>[[.arpa|arpa]]</tt>) [[top-level domain]] of the Internet. [[IPv4]] uses the <tt>in-addr.arpa</tt> domain and the <tt>ip6.arpa</tt> domain is delegated for [[IPv6]]. The process of reverse resolving an IP address uses the ''pointer'' DNS record type ([[List of DNS record types#Resource records|PTR record]]).

Informational RFCs (RFC 1033, RFC 1912 Section 2.1) specify that ''"Every Internet-reachable host should have a name"'' and that such names match with a reverse pointer record, but it is not a requirement of standards governing operation of the DNS itself.

==IPv4 reverse resolution==
Reverse DNS lookups for [[IPv4]] addresses use a ''reverse IN-ADDR entry'' in the special domain <tt>in-addr.arpa</tt>. In this domain, an IPv4 address is represented as a concatenated sequence of ''four decimal numbers'', separated by dots, to which is appended the second level domain suffix <tt>.in-addr.arpa</tt>. The four decimal numbers are obtained by splitting the 32-bit IPv4 address into four 8-bit portions and converting each 8-bit portion into a decimal number.  These decimal numbers are then concatenated in the order: least significant 8-bit portion first (leftmost), most significant 8-bit portion last (rightmost). It is important to note that ''this is the reverse order to the usual dotted-decimal convention for writing IPv4 addresses'' in textual form.
For example, an address (A) record for <tt>mail.example.com</tt> points to the IP address 192.0.2.5.
In pointer records of the reverse database, this IP address is stored as the domain name <tt>5.2.0.192.in-addr.arpa</tt> pointing back to its designated host name <tt>mail.example.com</tt>. 
This allows it to pass the [[Forward Confirmed reverse DNS]] process.

===Classless reverse DNS method===
Historically, Internet registries and Internet service providers allocated IP addresses in blocks of 256 (for Class C) or larger octet-based blocks for classes B and A.  By definition, each block fell upon an octet boundary. The structure of the reverse DNS domain was based on this definition. However, with the introduction of [[Classless Inter-Domain Routing]], IP addresses were allocated in much smaller blocks, and hence the original design of pointer records was impractical, since autonomy of administration of smaller blocks could not be granted. RFC 2317 devised a methodology to address this problem by using ''canonical name'' ([[CNAME]]) DNS records.

==IPv6 reverse resolution==
Reverse DNS lookups for [[IPv6]] addresses use the special domain <code>ip6.arpa</code>. An IPv6 address appears as a name in this domain as a sequence of [[nibble]]s in reverse order, represented as hexadecimal digits as subdomains. For example, the pointer domain name corresponding to the IPv6 address <code>2001:db8::567:89ab</code> is <code>b.a.9.8.7.6.5.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.8.b.d.0.1.0.0.2.ip6.arpa</code>.

==Multiple pointer records==
While most rDNS entries only have one PTR record, DNS does not restrict the number. However, having multiple PTR records for the same IP address is generally not recommended, unless there is a specific need.  For example, if a web server supports many [[virtual host]]s, there may be one PTR record for each host and some versions of name server software will allocate this automatically.  Multiple PTR records can cause problems, however, including triggering bugs in programs that only expect single PTR records.<ref>[http://sources.redhat.com/bugzilla/show_bug.cgi?id=5790 glibc bug #5790]</ref> In the case of a large web server, having hundreds of PTR records can cause the DNS packets to be much larger than normal, which can cause responses to be truncated if they exceed the DNS 512 byte UDP message limit.

==Records other than PTR records==
Record types other than PTR records may also appear in the reverse DNS tree. For example, encryption keys may be placed there for [[IPsec]] (RFC 4025), [[Secure Shell|SSH]] (RFC 4255) and [[Internet Key Exchange|IKE]] (RFC 4322).
[[Zero-configuration networking#DNS-SD|DNS-Based Service Discovery]] (RFC 6763) uses specially-named records in the reverse DNS tree to provide hints to clients about subnet-specific service discovery domains.<ref>{{Citation | publisher = IETF | title = RFC 6763 | url = http://tools.ietf.org/html/rfc6763#section-11}}</ref>
Less standardized usages include comments placed in [[TXT record]]s and [[LOC record]]s to identify the geophysical location of an IP address.

==Uses==
The most common uses of the reverse DNS include:
* The original use of the rDNS: network troubleshooting via tools such as [[traceroute]], [[Ping (networking utility)|ping]], and the "Received:" trace header field for [[SMTP]] e-mail, web sites tracking users (especially on [[Internet forum]]s), etc.
* One [[anti-spam techniques (e-mail)#PTR.2Freverse DNS checks|e-mail anti-spam technique]]: checking the domain names in the rDNS to see if they are likely from dialup users, dynamically assigned addresses, or other inexpensive Internet services.  Owners of such IP addresses typically assign them generic rDNS names such as "1-2-3-4-dynamic-ip.example.com."  Some corporate anti-spam services take the view that the vast majority, but by no means all, of e-mail that originates from these computers is spam with spam filters refusing e-mail with such rDNS names.<ref>[http://www.spamhaus.org/faq/answers.lasso?section=ISP%20Spam%20Issues#131 spamhaus's FAQ]</ref><ref>[http://postmaster.aol.com/info/rdns.html reference page from AOL]</ref> However data has shown that just as much if not more spam has originated from unpatched machines within corporate networks that are more likely to use out of date browsers than cheaper services such as DSL networks not to mention the difficulty of blocking spam from major providers like Yahoo and Hotmail. A recent shift has shown that spamming has switched to mainly coming from hosting companies making using rDNS even less useful.<ref>http://www.mailchannels.com/blog/2013/03/worlds-largest-spam-sources-are-all-hosting-companies/</ref> All of this adds to the argument that the few services that choose to block email servers purely on the basis of rDNS are simply discriminating without merit and often miss out more pro-active and useful indiscriminate anti spam measures.<ref>http://ask.slashdot.org/story/11/10/13/1643202/ask-slashdot-is-reverse-dns-a-worthy-standard-for-fighting-spam</ref>
* A [[forward-confirmed reverse DNS]] (FCrDNS) verification can create a form of authentication showing a valid relationship between the owner of a domain name and the owner of the server that has been given an IP address. While not very thorough, this validation is strong enough to often be used for [[whitelist]]ing purposes, mainly because [[Spam (electronic)|spammers]] and [[Phishing|phishers]] usually can't pass verification for it when they use [[zombie computer]]s to forge domains.
* System logging or monitoring tools often receive entries with the relevant devices specified only by IP addresses. To provide more human-usable data, these programs often perform a reverse lookup before writing the log, thus writing a name rather than the IP address

==See also==
*[[Forward-confirmed reverse DNS]]

==References==
{{reflist}}

==External links==
*{{dmoz|Computers/Internet/Protocols/DNS/Web_Tools|Web-based DNS lookup tools}}
* [http://dns.icann.org ICANN DNS Operations]
* RFC 2317 documents a way to do rDNS delegation for [[Classless Inter-Domain Routing|CIDR]] blocks
* [https://tools.ietf.org/html/rfc3596 RFC 3596 DNS Extensions to Support IP Version 6]
* RDNS policies: [http://postmaster.aol.com/Postmaster.Errors.php#whatisrdns AOL], [http://customer.comcast.com/help-and-support/internet/fix-a-554-error/ Comcast], [http://www.craigslist.org/about/help/rdns_failure Craigslist], [https://www.misk.com/kb/reverse-dns Misk.com]

[[Category:Searching]]
[[Category:Domain name system]]

[[nl:Domain Name System#Omgekeerde lookups]]'''
>>EOP<<
176<|###|>Contextual Query Language
'''Contextual Query Language''' (CQL), previously known as '''Common Query Language''',<ref>[http://www.loc.gov/standards/sru/cql/spec.html CQL: the Contextual Query Language: Specifications] SRU: Search/Retrieval via URL, Standards, Library of Congress</ref> is a [[formal language]] for representing queries to [[information retrieval]] systems such as [[search engine]]s, [[bibliography|bibliographic catalogs]] and [[museum]] collection information. Based on the [[semantics]] of [[Z39.50]], its design objective is that queries be human readable and writable, and that the language be intuitive while maintaining the expressiveness of more complex [[query language]]s. It is being developed and maintained by the Z39.50 Maintenance Agency, part of the [[Library of Congress]].

== Examples of query syntax ==

Simple queries:

<blockquote><tt>dinosaur<br/>
"complete dinosaur"<br/>
title = "complete dinosaur"<br/>
title exact "the complete dinosaur"</tt></blockquote>

Queries using [[Boolean logic]]:

<blockquote><tt>dinosaur or bird<br/>
Palomar assignment and "ice age"<br/>
dinosaur not reptile<br/>
dinosaur and bird or dinobird<br/>
(bird or dinosaur) and (feathers or scales)<br/>
"feathered dinosaur" and (yixian or jehol)</tt></blockquote>

Queries accessing [[index (publishing)|publication indexes]]:

<blockquote><tt>publicationYear < 1980<br/>
lengthOfFemur > 2.4<br/>
bioMass >= 100</tt></blockquote>

Queries based on the proximity of words to each other in a document:

<blockquote><tt>ribs prox/distance<=5 chevrons<br/>
ribs prox/unit=sentence chevrons<br/>
ribs prox/distance>0/unit=paragraph chevrons</tt></blockquote>

Queries across multiple [[Dimension (data warehouse)|dimensions]]:

<blockquote><tt>date within "2002 2005"<br/>
dateRange encloses 2003</tt></blockquote>

Queries based on [[Relevance (information retrieval)|relevance]]:

<blockquote><tt>subject any/relevant "fish frog"<br/>
subject any/rel.lr "fish frog"</tt></blockquote>

The latter example specifies using a specific [[algorithm]] for [[logistic regression]].<ref>[http://srw.cheshire3.org/contextSets/rel/ Relevance Ranking Context Set version 1.1]</ref>

== References ==
{{Reflist}}

== External links ==
* [http://www.loc.gov/standards/sru/cql/ CQL home page]
* [http://www.loc.gov/z3950/agency/ Z39.50 Maintenance Agency]
* [http://zing.z3950.org/cql/intro.html A Gentle Introduction to CQL]

{{Query languages}}

{{USGovernment|sourceURL=http://www.loc.gov/standards/sru/cql/}}
{{LOC-stub}}

[[Category:Searching]]
[[Category:Library science]]
[[Category:Library of Congress]]
[[Category:Query languages]]
[[Category:Knowledge representation languages]]
>>EOP<<
182<|###|>Desktop search
{{multiple issues|
{{Cleanup|date=October 2010}}
{{technical|date=October 2014}}
}}

[[File:Puggle-search.png|thumb|Puggle Desktop Search]]
[[File:AdunaAutoFocus5.png|thumb|OSL Desktop Search engines software Aduna AutoFocus 5]]
'''Desktop search''' tools search within a user's own [[computer files]] as opposed to searching the Internet. These tools are designed to find information on the user's PC, including web browser history, e-mail archives, text documents, sound files, images, and video.

One of the main advantages of desktop search programs is that search results are displayed quickly due to the use of proper indexes.

A variety of desktop search programs are now available; see [[List of search engines#Desktop search engines|this list]] for examples.

Desktop search emerged as a concern for large firms for two main reasons: untapped productivity and security. On the one hand, users needs to be able to quickly find relevant files, but on the other hand, they shouldn't have access to restricted files. According to analyst firm Gartner, up to 80% of some companies' data is locked up inside [[unstructured data]]  the information stored on an end user's PC, the directories (folders) and files they've created on a [[Computer network|network]], documents stored in repositories such as corporate [[intranet]]s and a multitude of other locations.<ref>{{Citation | url = http://www.computerweekly.com/Articles/2006/04/25/215622/security-special-report-who-sees-your-data.htm | title = Security special report: Who sees your data? | newspaper = Computer Weekly | date = 2006-04-25}}.</ref>  Moreover, many companies have structured or unstructured information stored in older [[file formats]] to which they don't have ready access.

Companies doing business in the [[United States]] are frequently required under regulatory mandates like [[Sarbanes-Oxley]], [[Health Insurance Portability and Accountability Act|HIPAA]] and [[FERPA]] to make sure that access to sensitive information is 100% controlled. This creates a challenge for IT organizations, which may not have a desktop search standard, or lack strict central control over end users [[downloading]] tools from the [[Internet]]. Some consumer-oriented desktop search tools make it possible to generate indexes outside the corporate [[Firewall (computing)|firewall]] and share those indexes with unauthorized users. In some cases, end users are able to index  but not preview  items they should not even know exist.{{Citation needed|date = November 2009}}

Historically, full desktop search comes from the work of [[Apple inc.|Apple Computer's]] [[Apple Advanced Technology Group|Advanced Technology Group]], resulting in the underlying [[AppleSearch]] technology in the early 1990s. It was used to build the [[Sherlock (software)|Sherlock]] search engine and then developed into [[Spotlight (software)|Spotlight]], which brought automated, non-timer-based full indexing into the operating system.

== Technologies ==
Most desktop search engines build and maintain an [[Index (search engine)|index database]] to achieve reasonable performance when searching several [[gigabyte]]s of [[data]]. Indexing usually takes place when the computer is idle and most search applications can be set to suspend indexing if a portable computer is running on batteries, in order to save power. There are notable exceptions, however: Voidtools' Everything Search Engine,<ref>{{cite web|title=Everything Search Engine|url=http://www.voidtools.com/|publisher=voidtools|accessdate=27 December 2013}}</ref> which performs searches over only filenames &mdash; not the files' contents &mdash; for NTFS volumes only, is able to build its index from scratch in just a few seconds. Another exception is Vegnos Desktop Search Engine,<ref>{{cite web|title=Vegnos|url=http://www.vegnos.com|publisher=Vegnos|accessdate=27 December 2013}}</ref> which performs searches over filenames and files' contents without building any indices. The benefits to not having indices is that, in addition to not requiring persistent storage, more powerful queries (e.g., [[regular expressions]]) can be issued, whereas indexed search engines are limited to keyword-based queries. An index may also not be up-to-date, when a query is performed. In this case, results returned will not be accurate (that is, a hit may be shown when it is no longer there, and a file may not be shown, when in fact it is a hit). Some products, such as Lookeen,<ref>{{cite web|title=Real-Time Indexing and Lookeen 8|url=http://www.lookeen.net/2884/News/real-time-ndexing-and-lookeen-8/|publisher=Lookeen|accessdate=26 October 2014}}</ref> have sought to remedy this disadvantage by building a real-time indexing function into the software. There are disadvantages to not indexing. Namely, the time to complete a query can be significant, and the issued query can also be resource-intensive.

Desktop search tools typically collect three types of information about files:
* file and folder names
* [[metadata]], such as titles, authors, comments in file types such as [[MP3]], [[Portable Document Format|PDF]] and [[JPEG]]
* file content (for supported types of documents only)

To search effectively within documents, the tools need to be able to parse many different types of documents. This is achieved by using filters that interpret selected file formats. For example, a ''Microsoft Office Filter'' might be used to search inside [[Microsoft Office]] documents.

Long-term goals for desktop search include the ability to search the contents of image files, sound files and video by context.<ref>[http://www.niallkennedy.com/blog/archives/2006/10/video-search.html "The current state of video search", by Niall Kennedy]</ref><ref>[http://www.niallkennedy.com/blog/archives/2006/10/audio-search.html "The current state of audio search", by Niall Kennedy]</ref>

The sector attracted considerable attention from the struggle between Microsoft and Google.<ref>[http://news.bbc.co.uk/1/hi/technology/3952285.stm "Search wars hit desktop computers". (Oct 2004) BBC News]</ref> According to market analysts, both companies were attempting to leverage their monopolies (of [[web browser]]s and [[search engine]]s, respectively) to strengthen their dominance. Due to [[Google]]'s complaint that users of Windows Vista cannot choose any competitor's desktop search program over the built-in one, an agreement was reached between [[US Justice Department]] and [[Microsoft]] that [[Windows Vista Service Pack 1]] would enable users to choose between the built-in and other desktop search programs, and select which one is to be the default.<ref>[http://goebelgroup.com/searchtoolblog/2007/06/20/microsoft-agrees-to-change-vista-desktop-search-tool/ "Microsoft agrees to change Vista Desktop Search Tool" (Jun 2007)]</ref>

As of September, 2011, Google ended life for Google Desktop, a program designed to make it easy for users to search their own PCs for emails, files, music, photos, Web pages and more. <ref>[http://googledesktop.blogspot.com/2011/09/google-desktop-update.html/ "Google Desktop Update" (Sept 2011)]</ref>  

X1 makes one of the leading desktop search products on the market. X1 Search 8 is a software alternative to Windows Desktop and Outlook Search, helping business professional sift through desktop files, emails, attachments, SharePoint data, and more. <ref>[http://www.computerworld.com/article/2475293/desktop-apps/x1-rises-again-with-desktop-search-8--virtual-edition.html/ "X1 rises again with Desktop Search 8, Virtual Edition" (May 2013)]</ref>   

==Platforms & their histories==
There are three main platforms that desktop search falls into. [[Microsoft Windows|Windows]], [[Mac OS|Mac]] OS & [[Linux]]. This article will focus on the history of these search platforms, the features they had, and how those features evolved.

'''Windows'''

Today's Windows Search replaced WDS (Windows Desktop Search). WDS, in turn, replaced Indexing Service. A "a base service that extracts content from files and constructs an indexed catalog to facilitate efficient and rapid searching"<ref>https://msdn.microsoft.com/en-us/library/ee805985%28v=vs.85%29.aspx</ref> Indexing service was originally released in August 1996, it was built in order to speed up manually searching for files on Personal Desktops and Corporate Computer Network. Indexing service helped by using Microsoft web servers to index files on the desired hard drives. Indexing was done by file format. By using terms that users provided, a search was conducted that matched terms to the data within the file formats. The largest issue that Indexing service faced was the fact that every time a file was added, it had to be indexed. This coupled with the fact that the indexing cached the entire index in RAM, made the hardware a huge limitation.<ref>https://msdn.microsoft.com/en-us/library/dd582937%28v=office.11%29.aspx</ref> This made indexing large amounts of files require extremely powerful hardware and very long wait times.

In 2003, Windows Desktop Search (WDS) replaced Microsoft Indexing Service. Instead of only matching terms to the details of the file format and file names, WDS brings in content indexing to all Microsoft files and text-based formats such as e-mail and text files. This means, that WDS looked into the files and indexed the content. Thus, when a user searched a term, WDS no longer matched just information such as file format types and file names, but terms, and values stored within those files. WDS also brought "Instant searching" meaning the user could type a character and the query would instantly start searching and updating the query as the user typed in more characters.<ref>http://web.archive.org/web/20110924212903/http://www.microsoft.com/windows/products/winfamily/desktopsearch/technicalresources/techfaq.mspx</ref> Windows Search apparently used up a lot of processing power, as Windows Desktop Search would only run if it was directly queried or while the PC was idle. Even only running while directly queried or while the computer was idled, indexing the entire hard drive still took hours. The index would be around 10% of the size of all the files that it indexed. For example, if the indexed files amounted to around 100GB of space, the index would, itself, be 10GB large.

With the release of Windows Vista came Windows Search 3.1. Unlike it's predecessors WDS and Windows Search 3.0, 3.1 could search through both indexed and non indexed locations seamlessly. Also, the RAM and CPU requirements were greatly reduced. Cutting back indexing times immensely. This brings us to the Windows Search 4.0 which is currently running on all PCs with Windows 7 and up.

'''Mac OS'''

Mac OS was the first to fully implement Desktop Search, it allowed users to fully search all documents with in their Macintosh computer. This means file format types, meta-data on those file formats and the content within the files. Released in 1994 two years before Windows Search was released, AppleSearch already had content searching. The biggest issue that AppleSearch had large resource requirements "AppleSearch requires at least a 68040 processor and 5MB of RAM."<ref>http://infomotions.com/musings/tricks/manuscript/1600-0001.html</ref> A Macintosh computer that had these specs cost around $1400 in today's dollars that's around $2050.<ref>http://stats.areppim.com/calc/calc_usdlrxdeflator.php</ref> On top of that, the software it self cost around $1400 for a single licenses.

In 1997, Sherlock was released alongside Mac OS 8.5. Sherlock, named after the famous fictional detective Sherlock Holmes, was integrated into Mac OS's file browser: Finder. Sherlock extended the desktop search to the world wide web. Allowing users to now search locally and externally. Adding the web to Sherlock was relatively easy as the plugins only needed to be written in a plain text file. Sherlock was included in every single Mac OS 8, 9 and 10 until 10.5.

Spotlight was released in 2005, on Mac OSX 10.4, is a Selection-based search which means the user invokes a query using only the mouse. It allows the user to search the Internet for more information about any keyword or phrase contained within a document or webpage. Spotlight also uses a built-in Oxford American Dictionary and calculator to offer quick access to definitions and small calculations.<ref>http://www.apple.com/pr/library/2005/04/12Apple-to-Ship-Mac-OS-X-Tiger-on-April-29.html</ref> While Spotlight had a initially long start-up time (for first time set up). The entire hard disk was indexed, and as files are added to the hard disk, the index is constantly being updated in the background. This is done using minimal CPU & RAM resources, making searching relatively easy and quick.

'''Linux'''

For Linux, we will primarily cover the Ubuntu distribution as it was and currently is still the most popular version of Linux. Strangely enough, Ubuntu didn't have desktop search until Feisty Fawn 7.04. Using Tracker<ref>http://arstechnica.com/information-technology/2007/07/afirst-look-at-tracker-0-6-0/</ref> desktop search, the desktop search feature was very similar to Mac OS's AppleSearch and Sherlock. Considering the fact that both are UNIX based systems. Tracker, was released in late 2007 was built to have a relatively low impact on system resources. But unfortunately occasionally had sporadic control over what resources it was using. It not only featured the basic features of file format sorting, and meta-data matching, but support for searching through emails and messages (instant messages) was added. Years later, in 2014 Recoll<ref>http://www.lesbonscomptes.com/recoll/usermanual/index.html#RCL.INDEXING</ref> was added to Linux distributions, it works with other search programs such as Tracker and Beagle to provide efficient full text search. This greatly increased the types of queries that Linux desktop searches could handle as well as file types. The wonderful thing about Recoll is that it allows for greater customization of what is indexed. For example, Recoll will index the entire hard disk by default, but will and can index just a few select directories instead of wasting time indexing directories you know you will never need to look at. It also allows for more search options, you may actually narrow down what kind of query you want to ask. For example you could search for just file types or by content.<ref>http://archive09.linux.com/feature/114283</ref>

==See also==
*[[List of search engines#Desktop search engines|List of desktop search engines]]

== References ==
<!--* [http://ims.dei.unipd.it/members/agosti/teaching/2006-07/ir/ Maristella Agosti's website]-->
{{reflist|2}}

== External links ==
* ''[http://www.slate.com/id/2111643/ Keeper Finders]'', by Paul Boutin, ''[[Slate (magazine)|Slate]]'', December 31, 2004 &mdash; A comparison of Google, Ask Jeeves, HotBot, MSN and Copernic desktop search tools.
* [http://www.goebelgroup.com/desktopmatrix.htm GoebelGroup.com's desktop search tools comparison chart] - Date of last update: 15 January 2007.
* [http://labnol.blogspot.com/2004/10/detailed-comparison-of-desktop-search.html A detailed comparison of desktop search tools] - dated 2004.
* [http://www.wikinfo.org/index.php/Comparison_of_desktop_search_software Comparison of desktop search software] - Date of last update: March 2008
* [http://tbox.codeplex.com/ TBox] - DevTool, with ability to do fast search by text files

{{Navigationbox Desktopsearch}}

{{DEFAULTSORT:Desktop Search}}
[[Category:Desktop search engines| ]]
[[Category:Searching]]
>>EOP<<
188<|###|>Search-based software engineering
{{copy edit|date=October 2013}}

{{Use dmy dates|date=November 2011}}

'''Search-based software engineering''' ('''SBSE''') is an approach to apply [[metaheuristic]] search techniques like [[genetic algorithms]], [[simulated annealing]] and [[tabu search]] to [[software engineering]] problems. It is inspired by the observation that many activities in [[software engineering]] can be formulated as [[Optimization (mathematics)|optimization]] problems. Due to the [[computational complexity]] of these problems, exact [[Optimization (mathematics)|optimization]] techniques of [[operations research]] like [[linear programming]] or [[dynamic programming]] are mostly impractical for large scale [[software engineering]] problems. Because of this, researchers and practitioners have used [[metaheuristic]] search techniques to find near optimal or good-enough solutions.

Broadly speaking SBSE problems can be divided into two types. The first are black-box optimization problems, for example, assigning people to tasks (a typical [[combinatorial optimization]] problem). 
With this sort of problem domain, the underlying problem could have come from the software industry, but equally it could have originated from any domain where people are assigned tasks. 
The second type are white-box problems where operations on source code need to be considered.<ref>
{{Cite conference
| doi = 10.1109/SCAM.2010.28
| conference = 10th IEEE Working Conference on Source Code Analysis and Manipulation (SCAM 2010)
| pages = 719
| last = Harman
| first = Mark
| title = Why Source Code Analysis and Manipulation Will Always be Important
| booktitle = 10th IEEE Working Conference on Source Code Analysis and Manipulation (SCAM 2010)
| year = 2010
}}</ref>

__TOC__

==Definition==

The basic idea of SBSE is to take a software engineering problem and convert it into a computational search problem which can be tackled with a [[metaheuristic]]. 
This essentially involves a number of stages. Firstly defining a search space (the set of possible solutions to the problem). 
This space is typically too large to be explored exhaustively and therefore a  [[metaheuristic]] is employed to sample this space. 
Secondly, a metric <ref>
{{Cite conference
| doi = 10.1109/METRIC.2004.1357891
| conference = 10th International Symposium on Software Metrics, 2004
| pages = 5869
| last = Harman
| first = Mark
|author2=John A. Clark
 | title = Metrics are fitness functions too
| booktitle = Proceedings of the 10th International Symposium on Software Metrics, 2004 
| year = 2004
}}</ref> (also called a fitness function, cost function, objective function or quality measure) is used to measure the quality of a potential solution. Many software engineering problems can be reformulated as a computational search problem.<ref>{{Cite journal
| doi = 10.1049/ip-sen:20030559
| issn = 1462-5970
| volume = 150
| issue = 3
| pages = 161175
| last = Clark
| first = John A.
| coauthors = Dolado, Jose Javier; Harman, Mark; Hierons, Robert M.; Jones, Bryan F.; Lumkin, M.; Mitchell, Brian S.; Mancoridis, Spiros; Rees, K.; Roper, Marc; Shepperd, Martin J.
| title = Reformulating software engineering as a search problem
| journal = IEE Proceedings - Software 
| year = 2003
}}</ref>

The term "[[search-based application]]", in contrast, refers to using [[search engine technology]], rather than search techniques, in another industrial application.

==Brief history==

One of the earliest attempts in applying [[Optimization (mathematics)|optimization]] to a [[software engineering]] problem was reported by [[Webb Miller]] and David Spooner in 1976 in the area of software testing.<ref>
{{Cite journal
| doi = 10.1109/TSE.1976.233818
| issn = 0098-5589
| volume = SE-2
| issue = 3
| pages = 223226
| last = Miller
| first = Webb
| last2 = Spooner
| first2 = David L. 
| title = Automatic Generation of Floating-Point Test Data
| journal = IEEE Transactions on Software Engineering
| year = 1976
}}</ref> 
In 1992, Xanthakis and his colleagues applied a search technique to a [[software engineering]] problem for the first time.<ref>S. Xanthakis, C. Ellis, C. Skourlas, A. Le Gall, S. Katsikas and K. Karapoulios, "Application of genetic algorithms to software testing," in ''Proceedings of the 5th International Conference on Software Engineering and its Applications'', Toulouse, France, 1992, pp.&nbsp;625636</ref> 
The term SBSE was first used in 2001 by [[Mark Harman (computer scientist)|Harman]] and Jones.<ref>
{{Cite journal
| doi = 10.1016/S0950-5849(01)00189-6
| issn = 0950-5849
| volume = 43
| issue = 14
| pages = 833839
| last = Harman
| first = Mark
| last2 = Jones
| first2 = Bryan F.
| title = Search-based software engineering
| journal = Information and Software Technology
| accessdate = 2013-10-31
| date = 2001-12-15
| url = http://www.sciencedirect.com/science/article/pii/S0950584901001896
}}</ref> Since then, the research community has grown to include more than 800 authors in 2013, from approximately 270 institutions in 40 countries.{{Citation needed|date=October 2013}}

==Application areas==

Search-based software engineering is applicable to almost all phases of the [[software life cycle|software development process]]. [[Software testing]] has been one of the major applications of search techniques in [[software engineering]].<ref>
{{Cite journal
| doi = 10.1002/stvr.294
| issn = 1099-1689
| volume = 14
| issue = 2
| pages = 105156
| last = McMinn
| first = Phil
| title = Search-based software test data generation: a survey
| journal = Software Testing, Verification and Reliability
| accessdate = 2013-10-31
| year = 2004
| url = http://onlinelibrary.wiley.com/doi/10.1002/stvr.294/abstract
}}</ref> Search techniques have also been applied to other [[software engineering]] activities, for instance, [[requirements analysis]],<ref>
{{Cite journal
| doi = 10.1016/j.infsof.2003.07.002
| issn = 0950-5849
| volume = 46
| issue = 4
| pages = 243253
| last = Greer
| first = Des
| last2 = Ruhe
| first2 = Guenther
| title = Software release planning: an evolutionary and iterative approach
| journal = Information and Software Technology
| accessdate = 2013-09-06
| date = 2004-03-15
| url = http://www.sciencedirect.com/science/article/pii/S095058490300140X
}}</ref>
<ref>{{Cite conference
| doi = 10.1109/SBES.2009.23
| conference = XXIII Brazilian Symposium on Software Engineering, 2009. SBES '09
| pages = 207215
| last = Colares
| first = Felipe
| last2 = Souza
| first2 = Jerffeson
| last3 = Carmo
| first3 = Raphael
| last4 = Padua
| first4 = Clarindo
| last5 = Mateus
| first5 = Geraldo R.
| title = A New Approach to the Software Release Planning
| booktitle = XXIII Brazilian Symposium on Software Engineering, 2009. SBES '09
| year = 2009
}}</ref> [[software design]],<ref>
{{Cite journal
| doi = 10.1016/S0950-5849(01)00195-1
| issn = 0950-5849
| volume = 43
| issue = 14
| pages = 891904
| last = Clark
| first = John A.
| last2 = Jacob
| first2 = Jeremy L. 
| title = Protocols are programs too: the meta-heuristic search for security protocols
| journal = Information and Software Technology
| accessdate = 2013-10-31
| date = 2001-12-15
| url = http://www.sciencedirect.com/science/article/pii/S0950584901001951
}}</ref> [[software development]],<ref>
{{Cite journal
| doi = 10.1016/j.ins.2006.12.020
| issn = 0020-0255
| volume = 177
| issue = 11
| pages = 23802401
| last = Alba
| first = Enrique
| last2 = Chicano
| first2 = J. Francisco 
| title = Software project management with GAs
| journal = Information Sciences
| accessdate = 2013-10-31
| date = 2007-06-01
| url = http://www.sciencedirect.com/science/article/pii/S0020025507000175
}}</ref> and [[software maintenance]].<ref>
{{Cite conference
| doi = 10.1109/ICSM.2005.79
| conference = Proceedings of the 21st IEEE International Conference on Software Maintenance, 2005. ICSM'05
| pages = 240249
| last = Antoniol
| first = Giuliano
| last2 = Di Penta
| first2 = Massimiliano 
| last3 = Harman
| first3 = Mark
| title = Search-based techniques applied to optimization of project planning for a massive maintenance project
| booktitle = Proceedings of the 21st IEEE International Conference on Software Maintenance, 2005. ICSM'05
| year = 2005
}}</ref>

===Requirements engineering===

[[Requirements engineering]] is the process by which the needs of a software's users and environment are determined and managed. Search-based methods have been used for requirements selection and optimisation with the goal of finding the best possible subset of requirements that matches users' requests and different constraints such as limited resources and interdependencies between requirements. This problem is often tackled as a [[MCDM|multiple-criteria decision-making]] problem and, generally speaking, involves presenting the decision maker with a range of good compromises between cost and user satisfaction.<ref>
{{Cite thesis
| type = Ph.D.
| publisher = University of London
| last = Zhang
| first = Yuanyuan
| title = Multi-Objective Search-based Requirements Selection and Optimisation
| location = Strand, London, UK
| date = February 2010
| url = http://eprints.ucl.ac.uk/170695/
}}</ref>
<ref>
Y.&nbsp;Zhang and M.&nbsp;Harman and S.&nbsp;L.&nbsp;Lim, "[http://www.cs.ucl.ac.uk/fileadmin/UCL-CS/images/Research_Student_Information/RN_11_12.pdf Search Based Optimization of Requirements Interaction Management]," Department of Computer Science, University College London, Research Note RN/11/12, 2011.
</ref>

===Debugging and maintenance===

Identifying a [[software bug]] (or a [[code smell]]) and then [[debugging]] (or [[refactoring]]) the software is largely a manual and labor-intensive endeavor, though the process is supported by a number of tools. One objective of SBSE is to automatically identify bugs (for example via [[mutation testing]]), then automatically fix them.

[[Genetic programming]], a biologically-inspired technique which involves evolving programs through the use of crossover and mutation, has been used to search for repairs to programs by altering a few lines of source code. The [http://dijkstra.cs.virginia.edu/genprog/ GenProg Evolutionary Program Repair] software was shown to be able to repair 55 out of 105 bugs for approximately $8 each.<ref>{{Cite conference
| doi = 10.1109/ICSE.2012.6227211
| conference = 2012 34th International Conference on Software Engineering (ICSE)
| pages = 313
| last = Le Goues
| first = Claire
| last2 = Dewey-Vogt
| first2 = Michael
| last3 = Forrest
| first3 = Stephanie
| last4 = Weimer
| first4 = Westley 
| title = A systematic study of automated program repair: Fixing 55 out of 105 bugs for $8 each
| booktitle = 2012 34th International Conference on Software Engineering (ICSE)
| year = 2012
}}</ref>

[[Coevolution]] has also been used as an approach. It follows a predator and prey metaphor where a population of programs and a population of [[Unit Testing|unit tests]] evolve together and influence each other.<ref>{{Cite conference
| doi = 10.1109/CEC.2008.4630793
| conference = IEEE Congress on Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on Computational Intelligence)
| pages = 162168
| last = Arcuri
| first = Andrea
| last2 = Yao
| first2 = Xin 
| title = A novel co-evolutionary approach to automatic software bug fixing
| booktitle = IEEE Congress on Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on Computational Intelligence)
| year = 2008
}}</ref>

===Testing===

Search-based software engineering has been applied to software testing, including automatic generation of test cases (test data), test case minimization and test case prioritization. [[Regression testing]] has also received some attention.

===Optimizing software===
The use of SBSE in [[program optimization]], or modifying a piece of software to make it more efficient in terms of speed and resource use, has been the object of developing research interest and success. Genetic programming has been used to improve programs. In one instance, a 50,000 line program was genetically improved, resulting in a program 70 times faster on average.<ref>
{{Cite journal
| last = Langdon
| first = William B.
| last2 = Harman
| first2 = Mark 
| title = Optimising Existing Software with Genetic Programming
| journal = IEEE Transactions on Evolutionary Computation
| url = http://www0.cs.ucl.ac.uk/staff/w.langdon/ftp/papers/Langdon_2013_ieeeTEC.pdf
}}</ref>

===Project management===
A number of decisions which are normally made by a project manager can be done automatically, for example, project scheduling.<ref>
{{Cite conference
| publisher = ACM
| doi = 10.1145/2330163.2330332
| isbn = 978-1-4503-1177-9
| pages = 12211228
| last = Minku
| first = Leandro L.
| last2 = Sudholt
| first2 = Dirk
| last3 = Yao
| first3 = Xin 
| title = Evolutionary algorithms for the project scheduling problem: runtime analysis and improved design
| booktitle = Proceedings of the fourteenth international conference on Genetic and evolutionary computation conference
| location = New York, NY, USA
| series = GECCO '12
| accessdate = 2013-10-31
| year = 2012
| url = http://doi.acm.org/10.1145/2330163.2330332
}}</ref>

==Tools==

There are a number of tools available for SBSE approaches. These include tools like [[OpenPAT]].<ref> 
{{cite conference
|ref        = harv
|last       = Mayo
|first      = M.
|coauthors  = Spacey, S.
|title      = Predicting Regression Test Failures Using Genetic Algorithm-Selected Dynamic Performance Analysis Metrics
|url        = http://rd.springer.com/chapter/10.1007/978-3-642-39742-4_13
|format     = PDF
|journal    = Proceedings of the 5th International Symposium on Search-Based Software Engineering (SSBSE)
|volume     = 8084
|pages      = 158171
|year       = 2013
}}</ref>
and Evosuite <ref>(http://www.evosuite.org/)</ref>
and a code coverage measurement for Python
<ref>
https://pypi.python.org/pypi/coverage
</ref>

==Methods and techniques==

There are a number of methods and techniques available. 
A non-exhaustive list of these tools includes:

[[profiling (computer programming)|Profiling]]
<ref>http://java-source.net/open-source/profilers</ref> via [[instrumentation]] in order to monitor certain parts of a program as it is executed.

Obtaining an [[abstract syntax tree]] associated with the program, which can be automatically examined to gain insights into the structure of a program.

Applications of [[program slicing]] relevant to SBSE include [[software maintenance]], [[Optimization (computer science)|optimization]], [[Program analysis (computer science)|program analysis]].

[[Code coverage]] allows measuring how much of the code is executed with a given 
set of input data.

[[Static program analysis]]

==Industry acceptance==

As a relatively new area of research, SBSE does not yet benefit from broad industry acceptance. One issue is that software engineers are reluctant to adopt tools over which they have little control or that generate solutions that are quite different from the ones humans would produce.<ref>
{{cite web
 |url        = http://shape-of-code.coding-guidelines.com/2013/10/18/programming-using-genetic-algorithms-isnt-that-what-humans-already-do/
 |title      = Programming using genetic algorithms: isnt that what humans already do ;-)
 |last       = Jones
 |first      = Derek
 |date       = 18 October 2013
 |website    = The Shape of Code
 |accessdate = 31 October 2013
}}
</ref>
In the context of SBSE use in fixing or improving programs, developers need to be confident that any automatically produced modification does not generate unexpected behavior outside the scope of a system's requirements and testing environment. Considering that fully automated programming has yet to be achieved, a desirable property of such modifications would be that they need to be easily understood by humans to favor program maintainability.<ref>
{{Cite journal
| doi = 10.1007/s11219-013-9208-0
| issn = 1573-1367
| volume = 21
| issue = 3
| pages = 421443
| last = Le Goues
| first = Claire
| last2 = Forrest 
| first2 = Stephanie 
| last3 = Weimer
| first3 = Westley
| title = Current challenges in automatic software repair
| journal = Software Quality Journal
| accessdate = 2013-10-31
| date = 2013-09-01
| url = http://link.springer.com/article/10.1007/s11219-013-9208-0
}}
</ref>

Another concern is that SBSE might make the software engineer redundant. Researchers have argued that, on the contrary, the motivation for SBSE is to enhance the relationship between the engineer and the program.<ref>
{{Cite conference
| publisher = IEEE Press
| conference = First International Workshop on Combining Modelling with Search-Based Software Engineering,First International Workshop on Combining Modelling with Search-Based Software Engineering
| pages = 4950
| last = Simons
| first = Christopher L.
| title = Whither (away) software engineers in SBSE?
| location = San Francisco, USA
| accessdate = 2013-10-31
| date = May 2013
| url = http://eprints.uwe.ac.uk/19938/
}}</ref>

==See also==
{{Portal|Software Testing}}
*[[Program analysis (computer science)]]
*[[Dynamic program analysis]]

==References==
{{reflist|colwidth=30em}}

==External links==
*[http://crestweb.cs.ucl.ac.uk/resources/sbse_repository/ Repository of publications on SBSE]
*[http://neo.lcc.uma.es/mase/ Metaheuristics and Software Engineering]
*[http://sir.unl.edu/portal/index.php  Software-artifact Infrastructure Repository]
*[http://2013.icse-conferences.org/ International Conference on Software Engineering]
*[http://www.sigevo.org/wiki/tiki-index.php Genetic and Evolutionary Computation (GECCO)]
*[http://scholar.google.co.uk/citations?view_op=search_authors&hl=en&mauthors=label:sbse Google Scholar page on Search-based software engineering]

[[Category:2001 introductions]]
[[Category:Software engineering]]
[[Category:Software testing]]
[[Category:Searching]]
[[Category:Search algorithms]]
[[Category:Optimization algorithms and methods]]
[[Category:Genetic algorithms]]
>>EOP<<
194<|###|>Locate (Unix)
{{lowercase}}
'''<code>locate</code>''', a [[Unix]] utility first created in 1983,<ref>
Ref: [[Usenix]] ''';login:''', Vol 8, No 1, February/March, 1983, p. 8.
</ref>
serves to find [[computer file|file]]s on [[filesystem]]s. It searches through a prebuilt [[database]] of files generated by '''<code>updatedb</code>''' or by a [[Daemon (computing)|daemon]] and compressed using [[incremental encoding]]. It operates significantly faster than <code>[[find]]</code>, but requires regular updating of the database. This sacrifices overall efficiency (because of the regular interrogation of filesystems even when no user needs information) and absolute accuracy (since the database does not update in [[Real-time computing|real time]]) for significant speed improvements (particularly on very large filesystems).

The GNU version forms a part of [[GNU Findutils]].

Some versions can also index network filesystems.

==mlocate==
mlocate is a locate/updatedb implementation.

[https://fedorahosted.org/mlocate/ mlocate site]

==References==
<references/>

==External links==
* {{man|1|locate|FreeBSD}}
* [https://www.gnu.org/software/findutils/findutils.html GNU Findutils]

Variants:
* {{wayback|url=http://slocate.trakker.ca/|title=slocate (Secure Locate)|date=20090204031919}}
** {{man|1|slocate|die.net}}
* [http://carolina.mff.cuni.cz/~trmac/blog/mlocate/ <code>mlocate</code>] - faster updates
** {{man|1|locate|die.net|mlocate}}
* [http://rlocate.sourceforge.net/ rlocate] - always up-to-date
* [http://www.kde-apps.org/content/show.php/KwickFind+(Locate+GUI+Frontend)?content=54817 KwickFind] - KDE GUI frontend for locate
* [http://www.locate32.net/ Locate32 for Windows] Windows analog of GNU locate with GUI, released under GNU license

{{unix commands}}

[[Category:GNU Project software]]
[[Category:Unix file system-related software]]
[[Category:Searching]]

{{Unix-stub}}
>>EOP<<
200<|###|>Inversion (discrete mathematics)
{{contradict-other-multiple|Permutation|Lehmer code|Factorial number system|date=March 2013}}
[[File:Inversion set and vector of a permutation.svg|thumb|right|380px|The permutation (4,1,5,2,6,3) has the inversion vector (0,1,0,2,0,3) and the inversion set {(1,2),(1,4),(3,4),(1,6),(3,6),(5,6)}. The inversion vector [[w:Factorial number system|converted]] to decimal is 373.]]
[[File:Inversion set 16; wp(13,11, 7,15).svg|thumb|250px|Inversion set of the permutation<br>(0,15, 14,1, 13,2, 3,12,<br>11,4, 5,10, 6,9, 8,7)<br>showing the pattern of the<br>[[ThueMorse sequence]]]]
In [[computer science]] and [[discrete mathematics]], an '''inversion''' is a pair of places of a sequence where the elements on these places are out of their natural [[total order|order]].

== Definitions ==

Formally, let <math>(A(1), \ldots, A(n))</math> be a sequence of ''n'' distinct numbers.  If <math>i < j</math> and <math>A(i) > A(j)</math>, then the pair <math>(i, j)</math> is called an inversion of <math>A</math>.{{sfn|Cormen|Leiserson|Rivest|Stein|2001|pp=39}}{{sfn|Vitter|Flajolet|1990|pp=459}}

The '''inversion number''' of a sequence is one common measure of its sortedness.{{sfn|Barth|Mutzel|2004|pp=183}}{{sfn|Vitter|Flajolet|1990|pp=459}}  Formally, the inversion number is defined to be the number of inversions, that is, 
:<math>\text{inv}(A) = \# \{(A(i),A(j)) \mid i < j \text{ and } A(i) > A(j)\}</math>.{{sfn|Barth|Mutzel|2004|pp=183}}  
Other measures of (pre-)sortedness include the minimum number of elements that can be deleted from the sequence to yield a fully sorted sequence, the number and lengths of sorted "runs" within the sequence, and the smallest number of exchanges needed to sort the sequence.{{sfn|Mahmoud|2000|pp=284}} Standard [[comparison sort]]ing algorithms can be adapted to compute the inversion number in time {{math|O(''n'' log ''n'')}}.

The '''inversion vector''' ''V(i)'' of the sequence is defined for ''i'' = 2, ..., ''n'' as <math>V[i] = \left\vert\{k \mid k < i \text{ and } A(k) > A(i)\}\right\vert</math>.  In other words each element is the number of elements preceding the element in the original sequence that are greater than it.  Note that the inversion vector of a sequence has one less element than the sequence, because of course the number of preceding elements that are greater than the first is always zero.  Each permutation of a sequence has a unique inversion vector and it is possible to construct any given permutation of a (fully sorted) sequence from that sequence and the permutation's inversion vector.{{sfn|Pemmaraju|Skiena|2003|pp=69}}

==Weak order of permutations==
The set of permutations on ''n'' items can be given the structure of a [[partial order]], called the '''weak order of permutations''', which forms a [[lattice (order)|lattice]].

To define this order, consider the items being permuted to be the integers from 1 to ''n'', and let Inv(''u'') denote the set of inversions of a permutation ''u'' for the natural ordering on these items. That is, Inv(''u'') is the set of ordered pairs (''i'', ''j'') such that 1  ''i'' < ''j''  ''n'' and ''u''(''i'') > ''u''(''j''). Then, in the weak order, we define ''u''  ''v'' whenever Inv(''u'')  Inv(''v'').

The edges of the [[Hasse diagram]] of the weak order are given by permutations ''u'' and ''v'' such that ''u < v'' and such that ''v'' is obtained from ''u'' by interchanging two consecutive values of ''u''. These edges form a [[Cayley graph]] for the [[symmetric group|group of permutations]] that is isomorphic to the [[skeleton (topology)|skeleton]] of a [[permutohedron]].

The identity permutation is the minimum element of the weak order, and the permutation formed by reversing the identity is the maximum element.

== See also ==
{{wikiversity|Inversion (discrete mathematics)}}
{{commons|Category:Inversion (discrete mathematics)|Inversion (discrete mathematics)}}
* [[Factorial number system]] (a factorial number is a reflected inversion vector)
* [[Permutation group#Transpositions, simple transpositions, inversions and sorting|Transpositions, simple transpositions, inversions and sorting]]
* [[DamerauLevenshtein distance]]
* [[Parity of a permutation]]

'''Sequences in the [[On-Line Encyclopedia of Integer Sequences|OEIS]]:'''
* [https://oeis.org/wiki/Index_to_OEIS:_Section_Fa#factorial Index entries for sequences related to factorial numbers]
* Reflected inversion vectors: {{OEIS link|A007623}} and {{OEIS link|A108731}}
* Sum of inversion vectors, cardinality of inversion sets: {{OEIS link|A034968}}
* Inversion sets of finite permutations interpreted as binary numbers: {{OEIS link|A211362}} &nbsp; (related permutation: {{OEIS link|A211363}})
* Finite permutations that have only 0s and 1s in their inversion vectors: {{OEIS link|A059590}} &nbsp; (their inversion sets: {{OEIS link|A211364}})
* Numbers of permutations of n elements with k inversions; Mahonian numbers: {{OEIS link|A008302}} &nbsp; (their row maxima; Kendall-Mann numbers: {{OEIS link|A000140}})
* Number of connected labeled graphs with n edges and n nodes: {{OEIS link|A057500}}
* Arrays of permutations with similar inversion sets and inversion vectors: {{OEIS link|A211365}}, {{OEIS link|A211366}}, {{OEIS link|A211367}}, {{OEIS link|A211368}}, {{OEIS link|A211369}}, {{OEIS link|A100630}}, {{OEIS link|A211370}}, {{OEIS link|A051683}}

== References ==
{{reflist|4|refs=}}

=== Source bibliography ===
{{refbegin|1}}
* {{cite journal|ref=harv|first1=Wilhelm|last1=Barth|first2=Petra|last2=Mutzel|author2-link=Petra Mutzel|title=Simple and Efficient Bilayer Cross Counting|journal=[[Journal of Graph Algorithms and Applications]]|volume=8|issue=2|pages=179&ndash;194|year=2004|doi=10.7155/jgaa.00088}}
* {{cite book|ref=harv
 | first1=Thomas H.|last1=Cormen|authorlink1=Thomas H. Cormen
 | last2=Leiserson|first2=Charles E.|authorlink2=Charles E. Leiserson
 | last3=Rivest|first3=Ronald L.|authorlink3=Ron Rivest
 | last4=Stein|first4=Clifford|authorlink4=Clifford Stein
 | title = [[Introduction to Algorithms]]
 | publisher = MIT Press and McGraw-Hill
 | year = 2001
 | isbn = 0-262-53196-8
 | edition = 2nd
 }}
* {{cite book|ref=harv|title=Sorting: a distribution theory|chapter=Sorting Nonrandom Data|volume=54|series=Wiley-Interscience series in discrete mathematics and optimization|first=Hosam Mahmoud|last=Mahmoud|publisher=Wiley-IEEE|year=2000|isbn=978-0-471-32710-3}}
* {{cite book|ref=harv|title=Computational discrete mathematics: combinatorics and graph theory with Mathematica|chapter=Permutations and combinations|first1=Sriram V.|last1=Pemmaraju|first2=Steven S.|last2=Skiena|publisher=Cambridge University Press|year=2003|isbn=978-0-521-80686-2}}
* {{cite book|ref=harv|title=Algorithms and Complexity|volume=1|editor1-first=Jan|editor1-last=van Leeuwen|editor1-link=Jan van Leeuwen|edition=2nd|publisher=Elsevier|year=1990|isbn=978-0-444-88071-0|chapter=Average-Case Analysis of Algorithms and Data Structures|first1=J.S.|last1=Vitter|first2=Ph.|last2=Flajolet}}
{{refend}}

=== Further reading ===
* {{cite journal|ref=harv|journal=Journal of Integer Sequences|volume=4|year=2001|title=Permutations with Inversions|first=Barbara H.|last=Margolius}}

=== Presortedness measures ===
* {{cite journal|ref=harv|journal=Lecture Notes in Computer Science|year=1984|volume=172|pages=324&ndash;336|doi=10.1007/3-540-13345-3_29|title=Measures of presortedness and optimal sorting algorithms|first=Heikki|last=Mannila|authorlink=Heikki Mannila}}
* {{cite journal|ref=harv|first1=Vladimir|last1=Estivill-Castro|first2=Derick|last2=Wood|title=A new measure of presortedness|journal=Information and Computation|volume=83|issue=1|pages=111&ndash;119|year=1989|doi=10.1016/0890-5401(89)90050-3}}
* {{cite journal|ref=harv|first=Steven S.|last=Skiena|year=1988|title=Encroaching lists as a measure of presortedness|journal=BIT|volume=28|issue=4|pages=755&ndash;784|doi=10.1007/bf01954897}}

[[Category:Permutations]]
[[Category:Order theory]]
[[Category:String similarity measures]]
[[Category:Sorting algorithms]]
[[Category:Combinatorics]]
[[Category:Discrete mathematics]]
>>EOP<<
206<|###|>Edit distance
In [[computer science]], '''edit distance''' is a way of quantifying how dissimilar two [[String (computing)|strings]] (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other. Edit distances find applications in [[natural language processing]], where automatic [[Spell checker|spelling correction]] can determine candidate corrections for a misspelled word by selecting words from a dictionary that have a low distance to the word in question. In [[bioinformatics]], it can be used to quantify the similarity of [[macromolecule]]s such as [[DNA]], which can be viewed as strings of the letters A, C, G and T.

Several definitions of edit distance exist, using different sets of string operations. One of the most common variants is called [[Levenshtein distance]], named after the Soviet Russian computer scientist [[Vladimir Levenshtein]]. In this version, the allowed operations are the removal or insertion of a single character, or the substitution of one character for another. Levenshtein distance may also simply be called "edit distance", although several variants exist.<ref name="navarro">{{Cite doi/10.1145.2F375360.375365}}</ref>{{rp|32}}

==Formal definition and properties==
Given two strings {{mvar|a}} and {{mvar|b}} on an alphabet {{mvar|}} (e.g. the set of [[ASCII]] characters, the set of [[byte]]s [0..255], etc.), the edit distance d({{mvar|a}}, {{mvar|b}}) is the minimum-weight series of edit operations that transforms {{mvar|a}} into {{mvar|b}}. One of the simplest sets of edit operations is that defined by Levenshtein in 1966:<ref name="slp"/>

:'''Insertion''' of a single symbol. If {{mvar|a}} = {{mvar|u}}{{mvar|v}}, then inserting the symbol {{mvar|x}} produces {{mvar|u}}{{mvar|x}}{{mvar|v}}. This can also be denoted {{mvar|x}}, using  to denote the empty string.
:'''Deletion''' of a single symbol changes {{mvar|u}}{{mvar|x}}{{mvar|v}} to {{mvar|u}}{{mvar|v}} ({{mvar|x}}).
:'''Substitution''' of a single symbol {{mvar|x}} for a symbol {{mvar|y}} = {{mvar|x}} changes {{mvar|u}}{{mvar|x}}{{mvar|v}} to {{mvar|u}}{{mvar|y}}{{mvar|v}} ({{mvar|x}}{{mvar|y}}).

In Levenshtein's original definition, each of these operations has unit cost (except that substitution of a character by itself has zero cost), so the Levenshtein distance is equal to the minimum ''number'' of operations required to transform {{mvar|a}} to {{mvar|b}}. A more general definition associates non-negative weight functions {{mvar|w}}<sub>ins</sub>({{mvar|x}}), {{mvar|w}}<sub>del</sub>({{mvar|x}}) and {{mvar|w}}<sub>sub</sub>({{mvar|x}}&nbsp;{{mvar|y}}) with the operations.<ref name="slp">{{cite book |author1=Daniel Jurafsky |author2=James H. Martin |title=Speech and Language Processing |publisher=Pearson Education International |pages=107111}}</ref>

Additional primitive operations have been suggested. A common mistake when typing text is '''transposition''' of two adjacent characters commonly occur, formally characterized by an operation that changes {{mvar|u}}{{mvar|x}}{{mvar|y}}{{mvar|v}} into {{mvar|u}}{{mvar|y}}{{mvar|x}}{{mvar|v}} where {{mvar|x}}, {{mvar|y}}  {{mvar|}}.<ref name="ukkonen83">{{cite conference |author=Esko Ukkonen |title=On approximate string matching |conference=Foundations of Computation Theory |year=1983 |pages=487495 |publisher=Springer}}</ref><ref name="ssm"/>
For the task of correcting [[Optical character recognition|OCR]] output, '''merge''' and '''split''' operations have been used which replace a single character into a pair of them or vice-versa.<ref name="ssm">{{cite journal |first1=Klaus U. |last1=Schulz |first2=Stoyan |last2=Mihov |year=2002 |id={{citeseerx|10.1.1.16.652}} |title=Fast string correction with Levenshtein automata |journal=International Journal of Document Analysis and Recognition |volume=5 |issue=1 |pages=6785 |doi=10.1007/s10032-002-0082-8}}</ref>

Other variants of edit distance are obtained by restricting the set of operations. [[Longest common subsequence]] (LCS) distance is edit distance with insertion and deletion as the only two edit operations, both at unit cost.<ref name="navarro"/>{{rp|37}} Similarly, by only allowing substitutions (again at unit cost), [[Hamming distance]] is obtained; this must be restricted to equal-length strings.<ref name="navarro"/>
[[JaroWinkler distance]] can be obtained from an edit distance where only transpositions are allowed.

===Example===
The [[Levenshtein distance]] between "kitten" and "sitting" is 3. The minimal edit script that transforms the former into the latter is:

# '''k'''itten  '''s'''itten (substitution of "s" for "k")
# sitt'''e'''n  sitt'''i'''n (substitution of "i" for "e")
# sittin  sittin'''g''' (insertion of "g" at the end).

LCS distance (insertions and deletions only) gives a different distance and minimal edit script:

# delete '''k''' at 0
# insert '''s''' at 0
# delete '''e''' at 4
# insert '''i''' at 4
# insert '''g''' at 6

for a total cost/distance of 5 operations.

===Properties===
Edit distance with non-negative cost satisfies the axioms of a [[Metric (mathematics)|metric]], giving rise to a [[metric space]] of strings, when the following conditions are met:<ref name="navarro"/>{{rp|37}}

* Every edit operation has positive cost;
* for every operation, there is an inverse operation with equal cost.

With these properties, the metric axioms are satisfied as follows:

:{{mvar|d}}({{mvar|a}}, {{mvar|a}}) = 0, since each string can be trivially transformed to itself using exactly zero operations.
:{{mvar|d}}({{mvar|a}}, {{mvar|b}}) > 0 when {{mvar|a}} = {{mvar|b}}, since this would require at least one operation at non-zero cost.
:{{mvar|d}}({{mvar|a}}, {{mvar|b}}) = {{mvar|d}}({{mvar|b}}, {{mvar|a}}) by equality of the cost of each operation and its inverse.
:Triangle inequality: {{mvar|d}}({{mvar|a}}, {{mvar|c}})  {{mvar|d}}({{mvar|a}}, {{mvar|b}}) + {{mvar|d}}({{mvar|b}}, {{mvar|c}}).<ref>{{cite conference |author1=Lei Chen |author2=Raymond Ng |title=On the marriage of L-norms and edit distance |conference=Proc. 30th Int'l Conf. on Very Large Databases (VLDB) |volume=30 |year=2004}}</ref>

Levenshtein distance and LCS distance with unit cost satisfy the above conditions, and therefore the metric axioms. Variants of edit distance that are not proper metrics have also been considered in the literature.<ref name="navarro"/>

Other useful properties of unit-cost edit distances include:

* LCS distance is bounded above by the sum of lengths of a pair of strings.<ref name="navarro"/>{{rp|37}}
* LCS distance is an upper bound on Levenshtein distance.
* For strings of the same length, Hamming distance is an upper bound on Levenshtein distance.<ref name="navarro"/>

Regardless of cost/weights, the following property holds of all edit distances:

* When {{mvar|a}} and {{mvar|b}} share a common prefix, this prefix has no effect on the distance. Formally, when {{mvar|a}} = {{mvar|uv}} and {{mvar|b}} = {{mvar|uw}}, then {{mvar|d}}({{mvar|a}}, {{mvar|b}}) = {{mvar|d}}({{mvar|v}}, {{mvar|w}}).<ref name="ssm"/> This allows speeding up many computations involving edit distance and edit scripts, since common prefixes and suffixes can be skipped in linear time.

==Computation==
===Basic algorithm===
{{main|WagnerFischer algorithm}}
Using Levenshtein's original operations, the edit distance between <math>a = a_1\ldots a_n</math> and <math>b = b_1\ldots b_m</math> is given by <math>d_{mn}</math>, defined by the recurrence<ref name="slp"/>

:<math>d_{i0} = \sum_{k=1}^{i} w_\mathrm{del}(b_{k}), \quad  for\; 1 \leq i \leq m</math>
:<math>d_{0j} = \sum_{k=1}^{j} w_\mathrm{ins}(a_{k}), \quad  for\; 1 \leq j \leq n</math>
:<math>d_{ij} = \begin{cases} d_{i-1, j-1} & \quad a_{j} = b_{i}\\ \min \begin{cases} d_{i-1, j} + w_\mathrm{del}(b_{i})\\ d_{i,j-1} + w_\mathrm{ins}(a_{j}) \\ d_{i-1,j-1} + w_\mathrm{sub}(a_{j}, b_{i}) \end{cases} & \quad a_{j} \neq b_{i}\end{cases} , \quad  for\; 1 \leq i \leq m, 1 \leq j \leq n.</math>

This algorithm can be generalized to handle transpositions by adding another term in the recursive clause's minimization.<ref name="ukkonen83"/>

The straightforward, [[Recursion (computer science)|recursive]] way of evaluating this recurrence takes [[exponential time]]. Therefore, it is usually computed using a [[dynamic programming]] algorithm that is commonly credited to [[WagnerFischer algorithm|Wagner and Fischer]],<ref>{{cite journal |author1=R. Wagner |author2=M. Fischer |title=The string-to-string correction problem |journal=J. ACM |volume=21 |year=1974 |pages=168178 |doi=10.1145/321796.321811}}</ref> although it has a history of multiple invention.<ref name="slp"/><ref name="ukkonen83"/>
After completion of the WagnerFischer algorithm, a minimal sequence of edit operations can be read off as a backtrace of the operations used during the dynamic programming algorithm starting at <math>d_{mn}</math>.

This algorithm has a [[time complexity]] of ({{mvar|m}}{{mvar|n}}). When the full dynamic programming table is constructed, its [[space complexity]] is also ({{mvar|m}}{{mvar|n}}); this can be improved to (min({{mvar|m}},{{mvar|n}})) by observing that at any instant, the algorithm only requires two rows (or two columns) in memory. However, this optimization makes it impossible to read off the minimal series of edit operations.<ref name="ukkonen83"/> A linear-space solution to this problem is offered by [[Hirschberg's algorithm]].<ref>{{cite book |last=Skiena |first=Steven |authorlink=Steven Skiena |title = The Algorithm Design Manual |publisher=[[Springer Science+Business Media]] |edition=2nd |year = 2010 |isbn=1-849-96720-2}}</ref>{{rp|634}}

===Improved algorithms===
Improving on the WagnerFisher algorithm described above, [[Esko Ukkonen|Ukkonen]] describes several variants,<ref>{{cite journal |title=Algorithms for approximate string matching |journal=Information and Control |volume=64 |issue=13 |year=1985 |url=http://www.cs.helsinki.fi/u/ukkonen/InfCont85.PDF}}</ref> one of which takes two strings and a maximum edit distance {{mvar|s}}, and returns min({{mvar|s}}, {{mvar|d}}). It achieves this by only computing and storing a part of the dynamic programming table around its diagonal. This algorithm takes time O({{mvar|s}}min({{mvar|m}},{{mvar|n}})), where {{mvar|m}} and {{mvar|n}} are the lengths of the strings. Space complexity is O({{mvar|s}}2) or O({{mvar|s}}), depending on whether the edit sequence needs to be read off.<ref name="ukkonen83"/>

==Applications==
Edit distance finds applications in [[computational biology]] and natural language processing, e.g. the correction of spelling mistakes or OCR errors, and [[approximate string matching]], where the objective is to find matches for short strings in many longer texts, in situations where a small number of differences is to be expected.

Various algorithms exist that solve problems beside the computation of distance between a pair of strings, to solve related types of problems.

* [[Hirschberg's algorithm]] computes the optimal [[Sequence alignment|alignment]] of two strings, where optimality is defined as minimizing edit distance.
* [[Approximate string matching]] can be formulated in terms of edit distance. Ukkonen's 1985 algorithm takes a string {{mvar|p}}, called the pattern, and a constant {{mvar|k}}; it then builds a [[deterministic finite state automaton]] that finds, in an arbitrary string {{mvar|s}}, a substring whose edit distance to {{mvar|p}} is at most {{mvar|k}}<ref>{{cite journal |author=Esko Ukkonen |title=Finding approximate patterns in strings |journal=J. Algorithms |volume=6 |pages=132137 |year=1985 |doi=10.1016/0196-6774(85)90023-9}}</ref> (cf. the [[AhoCorasick string matching algorithm|AhoCorasick algorithm]], which similarly constructs an automaton to search for any of a number of patterns, but without allowing edit operations). A similar algorithm for approximate string matching is the [[bitap algorithm]], also defined in terms of edit distance.
* [[Levenshtein automaton|Levenshtein automata]] are finite-state machines that recognize a set of strings within bounded edit distance of a fixed reference string.<ref name="ssm"/>

==References==
{{reflist|30em}}

[[Category:String similarity measures]]
>>EOP<<
212<|###|>WagnerFischer algorithm
In [[computer science]], the '''WagnerFischer algorithm''' is a [[dynamic programming]] algorithm that computes the [[edit distance]] between two strings of characters.

==History==
The WagnerFischer algorithm has a history of [[multiple invention]]. Navarro lists the following inventors of it, with date of publication, and acknowledges that the list is incomplete:<ref name="navarro"/>{{rp|43}}
* Vintsyuk, 1968
* [[NeedlemanWunsch algorithm|Needleman and Wunsch]], 1970
* Sankoff, 1972
* Sellers, 1974
* Wagner and Fischer, 1974
* Lowrance and Wagner, 1975

==Calculating distance==
The WagnerFischer algorithm computes edit distance based on the observation that if we reserve a [[Matrix (mathematics)|matrix]] to hold the edit distances between all [[prefix (computer science)|prefix]]es of the first string and all prefixes of the second, then we can compute the values in the matrix by [[flood fill]]ing the matrix, and thus find the distance between the two full strings as the last value computed.

A straightforward implementation, as [[pseudocode]] for a function ''EditDistance'' that takes two strings, ''s'' of length ''m'', and ''t'' of length ''n'', and returns the Levenshtein distance between them, looks as follows. Note that the inputs strings are one-indexed, while the matrix ''d'' is zero-indexed, and <code>[i..k]</code> is a closed range.

  '''int''' EditDistance('''char''' s[1..m], '''char''' t[1..n])
    ''// For all i and j, d[i,j] will hold the Levenshtein distance between''
    ''// the first i characters of s and the first j characters of t.''
    ''// Note that d has (m+1)  x(n+1) values.
    '''let''' d be a 2-d array of '''int''' with dimensions [0..m, 0..n]
   
    '''for''' i '''in''' [0..m]
      d[i, 0]  i ''// the distance of any first string to an empty second string''
    '''for''' j '''in''' [0..n]
      d[0, j]  j ''// the distance of any second string to an empty first string''
   
    '''for''' j '''in''' [1..n]
      '''for''' i '''in''' [1..m]
        '''if''' s[i] = t[j] '''then'''  <!-- not: s[i-1] = t[j-1] -->
          d[i, j]  d[i-1, j-1]       ''// no operation required''
        '''else'''
          d[i, j]  minimum of
                     (
                       d[i-1, j] + 1,  ''// a deletion''
                       d[i, j-1] + 1,  ''// an insertion''
                       d[i-1, j-1] + 1 ''// a substitution''
                     )
   
    '''return''' d[m,n]

Two examples of the resulting matrix (hovering over an underlined number reveals the operation performed to get that number):
<center>
{|
|
{|class="wikitable"
|-
| 
| 
!k 
!i 
!t 
!t 
!e 
!n
|-
| ||0 ||1 ||2 ||3 ||4 ||5 ||6
|-
!s
|1 ||{{H:title|substitution of 'k' for 's'|1}} ||2 ||3 ||4 ||5 ||6
|-
!i
|2 ||2 ||{{H:title|'i' equals 'i'|1}} ||2 ||3 ||4 ||5
|-
!t
|3 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3 ||4
|-
!t
|4 ||4 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3 
|-
!i
|5 ||5 ||4 ||3 ||2 ||{{H:title|substitution of 'e' for 'i'|2}} ||3
|-
!n
|6 ||6 ||5 ||4 ||3 ||3 ||{{H:title|'n' equals 'n'|2}}
|-
!g
|7 ||7 ||6 ||5 ||4 ||4 ||{{H:title|insert 'g'|3}}
|}
|
{|class="wikitable"
|
|
!S
!a
!t
!u
!r
!d
!a
!y
|-
| 
|0 ||1 ||2 ||3 ||4 ||5 ||6 ||7 ||8
|-
!S
|1 ||{{H:title|'S' equals 'S'|0}} ||{{H:title|insert 'a'|1}} ||{{H:title|insert 't'|2}} ||3 ||4 ||5 ||6 ||7
|-
!u
|2 ||1 ||1 ||2 ||{{H:title|'u' equals 'u'|2}} ||3 ||4 ||5 ||6
|-
!n
|3 ||2 ||2 ||2 ||3 ||{{H:title|substitution of 'r' for 'n'|3}} ||4 ||5 ||6
|-
!d
|4 ||3 ||3 ||3 ||3 ||4 ||{{H:title|'d' equals 'd'|3}} ||4 ||5 
|-
!a
|5 ||4 ||3 ||4 ||4 ||4 ||4 ||{{H:title|'a' equals 'a'|3}} ||4
|-
!y
|6 ||5 ||4 ||4 ||5 ||5 ||5 ||4 ||{{H:title|'y' equals 'y'|3}}
|}
|}
</center>

The [[invariant (mathematics)|invariant]] maintained throughout the algorithm is that we can transform the initial segment <code>s[1..i]</code> into <code>t[1..j]</code> using a minimum of <code>d[i,j]</code> operations. At the end, the bottom-right element of the array contains the answer.

===Proof of correctness===
As mentioned earlier, the [[invariant (mathematics)|invariant]] is that we can transform the initial segment <code>s[1..i]</code> into <code>t[1..j]</code> using a minimum of <code>d[i,j]</code> operations. This invariant holds since:
* It is initially true on row and column 0 because <code>s[1..i]</code> can be transformed into the empty string <code>t[1..0]</code> by simply dropping all <code>i</code> characters. Similarly, we can transform <code>s[1..0]</code> to <code>t[1..j]</code> by simply adding all <code>j</code> characters.
* If <code>s[i] = t[j]</code>, and we can transform <code>s[1..i-1]</code> to <code>t[1..j-1]</code> in <code>k</code> operations, then we can do the same to <code>s[1..i]</code> and just leave the last character alone, giving <code>k</code> operations.
* Otherwise, the distance is the minimum of the three possible ways to do the transformation:
** If we can transform <code>s[1..i]</code> to <code>t[1..j-1]</code> in <code>k</code> operations, then we can simply add <code>t[j]</code> afterwards to get <code>t[1..j]</code> in <code>k+1</code> operations (insertion).
** If we can transform <code>s[1..i-1]</code> to <code>t[1..j]</code> in <code>k</code> operations, then we can remove <code>s[i]</code> and then do the same transformation, for a total of <code>k+1</code> operations (deletion).
** If we can transform <code>s[1..i-1]</code> to <code>t[1..j-1]</code> in <code>k</code> operations, then we can do the same to <code>s[1..i]</code>, and exchange the original <code>s[i]</code> for <code>t[j]</code> afterwards, for a total of <code>k+1</code> operations (substitution).
* The operations required to transform <code>s[1..n]</code> into <code>t[1..m]</code> is of course the number required to transform all of <code>s</code> into all of <code>t</code>, and so <code>d[n,m]</code> holds our result.

This proof fails to validate that the number placed in <code>d[i,j]</code> is in fact minimal; this is more difficult to show, and involves an [[Reductio ad absurdum|argument by contradiction]] in which we assume <code>d[i,j]</code> is smaller than the minimum of the three, and use this to show one of the three is not minimal.

===Possible improvements===
Possible improvements to this algorithm include:
* We can adapt the algorithm to use less space, [[Big O notation|''O'']](''m'') instead of ''O''(''mn''), since it only requires that the previous row and current row be stored at any one time.
* We can store the number of insertions, deletions, and substitutions separately, or even the positions at which they occur, which is always <code>j</code>.
* We can normalize the distance to the interval <code>[0,1]</code>.
* If we are only interested in the distance if it is smaller than a threshold ''k'', then it suffices to compute a diagonal stripe of width ''2k+1'' in the matrix. In this way, the algorithm can be run in [[Big O notation|''O'']](''kl'') time, where ''l'' is the length of the shortest string.<ref>{{cite book |author=Gusfield, Dan |title=Algorithms on strings, trees, and sequences: computer science and computational biology |publisher=Cambridge University Press |location=Cambridge, UK |year=1997 |isbn=0-521-58519-8 }}</ref>
* We can give different penalty costs to insertion, deletion and substitution. We can also give penalty costs that depend on which characters are inserted, deleted or substituted.
* This algorithm [[parallel computing|parallelizes]] poorly, due to a large number of [[data dependency|data dependencies]]. However, all the <code>cost</code> values can be computed in parallel, and the algorithm can be adapted to perform the <code>minimum</code> function in phases to eliminate dependencies.
* By examining diagonals instead of rows, and by using [[lazy evaluation]], we can find the Levenshtein distance in ''O''(''m'' (1 + ''d'')) time (where ''d'' is the Levenshtein distance), which is much faster than the regular dynamic programming algorithm if the distance is small.<ref>{{cite journal |author=Allison L |title=Lazy Dynamic-Programming can be Eager |journal=Inf. Proc. Letters |volume=43 |issue=4 |pages=20712 |date=September 1992 |url=http://www.csse.monash.edu.au/~lloyd/tildeStrings/Alignment/92.IPL.html |doi=10.1016/0020-0190(92)90202-7}}</ref>

==Seller's variant for string search==
By initializing the first row of the matrix with zeros, we obtain a variant of the WagnerFischer algorithm that can be used for [[fuzzy string searching|fuzzy string search]] of a string in a text.<ref name="navarro">{{cite doi|10.1145/375360.375365}}</ref> This modification gives the end-position of matching substrings of the text. To determine the start-position of the matching substrings, the number of insertions and deletions can be stored separately and used to compute the start-position from the end-position.<ref>Bruno Woltzenlogel Paleo. [http://www.logic.at/people/bruno/Papers/2007-GATE-ESSLLI.pdf An approximate gazetteer for GATE based on levenshtein distance]. Student Section of the European Summer School in Logic, Language and Information ([[European Summer School in Logic, Language and Information|ESSLLI]]), 2007.</ref>

The resulting algorithm is by no means efficient, but was at the time of its publication (1980) one of the first algorithms that performed approximate search.<ref name="navarro"/>

== References ==
{{Reflist|30em}}

{{DEFAULTSORT:Wagner-Fischer algorithm}}
[[Category:Algorithms on strings]]
[[Category:String similarity measures]]
>>EOP<<
218<|###|>Arts and Humanities Citation Index
{{ infobox bibliographic database
| image       = 
| caption     = 
| producer    =Thomson Reuters 
| country     =United States 
| history     = 
| languages   = 
| providers   =Web of Science, Dialog Bluesheets 
| cost        =Subscription 
| disciplines =Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, Religion, Television, Theater, and Radio 
| depth       =Index, abstract, citation indexing, author 
| formats     =original research articles, reviews, editorials, chronologies, abstracts,   scripts, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, book reviews, films, music, and theatrical performances 
| temporal    =1975 to present 
| geospatial  =global 
| number      = 
| updates     = 
| p_title     = 
| p_dates     = 
| ISSN        = 
| web         = 
| titles      =  
}}

The '''''Arts & Humanities Citation Index''''' ('''A&HCI'''), also known as '''''Arts & Humanities Search''''', is a [[citation index]], with abstracting and indexing for more than 1,700 arts and humanities journals, and coverage of disciplines that includes social and natural science journals. Part of this database is derived from [[Current Contents]] records. Furthermore the print counterpart is Current Contents.

Subjects covered are the Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, History, Religion, Television, Theater, and Radio. 

Available citation (source) coverage includes articles, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, as well as citations to reviews of books, films, music, and theatrical performances. 

This database can be accessed online through ''[[Web of Science]]''. It provides access to current and retrospective bibliographic information and cited references. It also covers individually selected, relevant items from approximately 1,200 titles, mostly arts and humanities journals but with an unspecified number of titles from other disciplines.

According to Thomson Reuters, the ''Arts & Humanities Search'', can be accessed via Dialog, DataStar, and OCLC, with weekly updates and backfiles to 1980.<ref name=dialog-blue>
{{Cite web
  | title =Arts & Humanities Search (File 255) 
  | publisher =Dialog bluesheets  
  | date = 
  | url =http://library.dialog.com/bluesheets/html/bl0439.html 
  | format =Online web page 
  | accessdate =2011-07-03}}</ref><ref name=Iowa>
Description of Arts & Humanities Search. 
{{Cite web
  | title =e-Library catalog
  | publisher =Iowas State University  
  | year =2008 
  | url =http://www.lib.iastate.edu/collections/db/artshm.html
  | format =Online web page 
  | accessdate =2011-07-03}}</ref><ref name=Iowa-wos>
Description of Web of Science coverage.  
{{Cite web
  | title =e-Library catalog
  | publisher =Iowas State University  
  | year =2008 
  | url =http://www.lib.iastate.edu/collections/db/websci.html
  | format =Online web page 
  | accessdate =2011-07-03}}</ref><ref name=TR>
See the page entitled "Tech Specs" 
{{Cite web
  | title =Database description
  | publisher =Thomson Reuters  
  | year = 
  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/#tab3
  | format =Online web page 
  | accessdate =2011-07-03}}</ref>
==History==
The index was originally developed by the [[Institute for Scientific Information]], which was later acquired by [[Thomson Scientific]]. It is now published by [[Thomson Reuters]]' IP & Science division.

==See also==
* [[Science Citation Index]]
* [[Social Sciences Citation Index]]

==References==
{{Reflist}}

== External links ==
* {{Official|http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/}} at Thomson Reuters.
* [http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=H Subject categories] of the Arts and Humanities Citation Index.

{{Thomson Reuters}}
[[Category:Citation indices]]
[[Category:Thomson Reuters]]

{{DEFAULTSORT:Arts And Humanities Citation Index}}
>>EOP<<
224<|###|>CAB Direct (database)
{{Redirect|Global Health|other uses|Global health}}
{{italic title}}
{{Infobox Bibliographic Database
|title =CAB Abstracts 
|image = 
|caption = 
|producer =[[CABI (organisation)|CABI]]
|country =United Kingdom 
|history =1973 to present 
|languages =Fifty languages, English abstracts 
|providers =Datastar, Dialog bluesheets, STN International, CAB Direct (CABI's own platform), Thomson-Reuters [[Web of Knowledge]], EBSCO, OvidSP, Dimdi 
|cost = 
|disciplines =applied life sciences - agriculture, environment, veterinary sciences, applied economics, food science and nutrition 
|depth =bibliographic, abstracting and indexing 
|formats =journal articles, abstracts, proceedings, books, book chapters, monographs, annual reports, handbooks, bulletins, newsletters, discussion papers, field notes, technical information, thesis papers  
|temporal =1973-Present 
|geospatial =Global - international 
|number =6 million + 
|updates =
|p_title = 
|p_dates = 
|ISSN =
|web = 
|titles =http://www.cabi.org/default.aspx?site=170&page=1028  
}}
{{Infobox Bibliographic Database
|title =Global Health bibliographic database 
|image = 
|caption = 
|producer = 
|country = 
|history = 
|languages = 50 languages (158 countries)
|providers =CAB Direct, SilverPlatter, Web of Knowledge, EBSCO, OvidSP, Dialog, Dimdi 
|cost = 
|disciplines =international health research (medical and public)
|depth =bibliographic, abstracting and indexing 
|formats =scientific journals, reports, books and conferences 
|temporal =1973 to present 
|geospatial =global-international 
|number =1.2 million scientific records 
|updates = 
|p_title = 
|p_dates = 
|ISSN =
|web = 
|titles =  
}}

'''CAB Direct''' is a source of references for the ''[[life sciences|applied life sciences]]'' It incorporates two  bibliographic databases: '''''CAB Abstracts''''' and '''''Global Health'''''. CAB Direct is an access point for multiple [[bibliographic databases]] produced by ''CABI''. This database contains 8.8 million [[bibliographic record]]s, which includes  85,000 full text articles. It also includes noteworthy literature reviews. News articles and reports are also part of this combined database.<ref name=direct>{{cite web
  | title =CAB Direct 
  | work = 
  | publisher =CABI  
  | date =July 2010 
  | url =http://www.cabdirect.org/ 
  | accessdate =2010-07-18}}</ref>

In the U.K., in 1947, the ''Imperial Agricultural Bureaux'' became the ''Commonwealth Agricultural Bureaux'' or ''CAB''. In 1986 the ''Commonwealth Agricultural Bureaux'' became ''[[CAB International]]'' or ''CABI''  <ref name=history-cabi>{{cite web
  | title =Our history 
  | work =Bulleted history 
  | publisher =CABI  
  | date =July 2010 
  | url =http://www.cabi.org/default.aspx?site=170&page=1388 
  | accessdate =2010-07-18}}</ref>

==CAB Abstracts==
'''CAB Abstracts''' is an applied life sciences bibliographic database emphasising [[agricultural]] literature, which is international in scope. It contains 6 million records, with coverage from 1973 to present day, adding 300,000 abstracts per year. Subject coverage includes [[agriculture]], [[environmental science|environment]], [[veterinary]] sciences, [[applied economics]], [[food science]] and nutrition. Database covers international issues in agriculture, [[forestry]], and allied disciplines in the life sciences. Indexed publications are from 150 countries in 50 languages, including English abstracts for most articles. Literature coverage includes journals, proceedings,  books, and a large collection of agricultural serials. Other non-journal formats are also indexed.<ref name=cabAb>
{{cite web
  | title =CAB Abstracts 
  | work = 
  | publisher =CABI  
  | date =July 2010 
  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=125 
  | accessdate =2010-07-18}}</ref><ref name=TRcababse>{{cite web
  | title =CAB Abstracts (Web of Knowledge) 
  | work = 
  | publisher =Thomson Reuters 
  | date =July 2010 
  | url =http://science.thomsonreuters.com/training/cab/#overview 
  | accessdate =2010-07-18}}</ref><ref name=ovidCABabs>{{cite web
  | title =CAB Abstracts 
  | work =Coverage is 1973-Present 
  | publisher =Ovid Technologies, Inc  
  | date =December 2010 
  | url =http://www.ovid.com/site/catalog/DataBase/31.jsp?top=2&mid=3&bottom=7&subsection=10  
  | accessdate =2010-12-10}}</ref> 
===CAB Abstracts Archive===
'''CAB Abstracts Archive''' is a searchable database produced by ''CABI''. It is created from 600 volumes of printed abstracts,  which are the collected and published [[scientific research]] from 1910 to 1972, and then digitized to form the archive. This archive database contains more than 1.8 million records which covers agriculture, [[veterinary]] science, nutrition and the environment. Subject coverage also includes [[biodiversity]], [[pest control]], [[environmental pollution]], [[animal disease]] (including [[zoonotic disease]]s), [[nutrition]], and [[food production]]. [[Natural resource management]] includes plant and [[animal breeding]]. CAB Abstracts Archive is also  indexed in other databases, which also serve as access points. These other databases are ''CAB Direct'', [[Web of Knowledge]], [[EBSCOhost]], [[Ovid Technologies|OvidSP]], and [[Dialog]].

The following print journals (digitized) comprise CAB Abstracts Archive:
                                                
:Animal Breeding Abstracts, Dairy Science Abstracts, Field Crop Abstracts, 
:Forestry Abstracts, Horticultural Science Abstracts, Nematological Abstracts, 
:Nutrition Abstracts and Reviews Series A: Human and Experimental, 
:Nutrition Abstracts and Reviews Series B: Livestock Feeds and Feeding,  
:Plant Breeding Abstracts, Review of Agricultural Entomology, 
:Review of Medical and Veterinary Mycology, Review of Plant Pathology, 
:Review of Medical and Veterinary Entomology, Review of Plant Pathology, 
:Soils and Fertilizers, Tropical Veterinary Bulletin, Veterinary Bulletin  
:and Weed Abstracts.

===Weed Abstracts===
'''''Weed Abstracts''''', derived from CAB Abstracts, is an abstracts database focused on [[scientific journal|published research]] regarding [[weed]]s and [[herbicides]]. This includes [[plant biology|weed biology]], encompassing [[research|research areas]] from [[genetics]] to [[ecology]], including [[parasitic]], [[poisonous]], [[allergenic]] and [[aquatic plant|aquatic]] weeds. Further coverage includes all topics related to [[weed control]], in both [[farming|crop]] and non-crop situations. Research on herbicides, includes formulations, [[herbicide resistance]] and the effects of [[herbicide residues]] in the environment. 10,000 records are add to this database per year. 

'''''Weed Abstracts''''' is updated weekly with summaries from notable English and foreign language journal articles, reports, conferences and books about weeds and herbicides. With the back-file, coverage is from 1990 to present day bringing the total of available research summaries to 130,000 records.<ref name=weedAb>{{cite web
  | title =Weed Abstracts 
  | work = 
  | publisher =CABI  
  | date =July 2010 
  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=2203
  | accessdate =2010-07-20}}</ref>

==Global Health database==
'''''Global Health''''' is a bibliographic database which focuses on [[scientific literature|research literature]] in [[public health]] and [[Health science|medical health]] science sectors (including practice). Information (see infobox above) in indexed in more than 5000 [[academic journals]], and indexed from other sources such as reports, books and conferences.  Global Health contains over 1.2 million [[scientific]] records from 1973 to the present, with an addition of  90,000 indexed and abstracted records per year. Sources are abstracted from publications in 158 countries written in 50 languages. Any relevant non-English-language papers are translated into English. Proceedings, patents, thesis papers, electronic publications and relevant but difficult-to-find literature sources are also part of this database.<ref name=glbl-hlth-cabi>{{cite web
  | title =Global Health overview 
  | work = 
  | publisher =CABI  
  | date =July 2010 
  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=328 
  | accessdate =2010-07-18}}</ref><ref name=TRglblhlth>{{cite web
  | title =Global Health (Web of Knowledge) 
  | work = 
  | publisher =Thomson Reuters 
  | date =July 2010 
  | url =http://thomsonreuters.com/content/PDF/scientific/globalhealth_fs.pdf 
  | format =Free PDF download 
  | accessdate =2010-07-18}}</ref><ref name=ovidGH>{{cite web
  | title =Global Health (Ovid) 
  | work = 
  | publisher =Ovid Technologies Inc. 
  | date =July 2010 
  | url =http://www.ovid.com/site/catalog/DataBase/30.jsp?top=2&mid=3&bottom=7&subsection=10 
  | accessdate =2010-07-18}}</ref> 

===Global Health Archive===
'''''Global Health Archive''''' is a searchable database produced by CABI. It is created from 800,000 records, from six printed abstract journals,  which are collected published scientific research from 1910 to 1972, digitized to form the archive. Global Health Archive is also  indexed in other databases, which also serve as access points. These other databases are ''CAB Direct'', [[Web of Knowledge]], [[EBSCOhost]], [[Ovid Technologies|OvidSP]], and [[Dialog]].<ref name=ghArchive/>

When combined with the ''Global Health'' database indexing coverage can be from 1910 to present day. Hence, coverage is made up of past [[epidemics]], from rates and patterns of disease [[Transmission (medicine)|transmission]], duration of [[pandemics]], timing of epidemiological peaks, [[geographic distribution]] of diseases, and [[World Health Organization|government preparedness]] and [[quarantine]] provisions.  The following can also be taken  into account:  effects on different age and [[social groups]], severity in developing vs. developed countries, [[symptoms]], causes of [[Human|mortality]] - such as secondary problems like [[pneumonia]] - and mortality rates.<ref name=ghArchive>{{cite web
  | title =Global Health Archive 
  | work = 
  | publisher =CABI 
  | date =March 2010  
  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=2221 
  | accessdate =2010-07-18}}</ref><ref name=ovidGHA>{{cite web
  | title =Global Health Archive (Ovid)
  | work = 
  | publisher =Ovid Technologies Inc. 
  | date =July 2010 
  | url =http://www.ovid.com/site/catalog/DataBase/1748.jsp?top=2&mid=3&bottom=7&subsection=10 
  | accessdate =2010-07-18}}</ref> 

====Journal and topic coverage====
Records for this database are derived from the following journals throughout certain years:<ref name=ghArchive/><ref name=ovidGHA/>

:Tropical Diseases Bulletin (1912-83),
:Abstracts on Hygiene and Communicable Diseases (1926-83), 
:Review of Veterinary and Medical Entomology (1913-72), 
:Review of Veterinary and Medical Mycology (1943-72) 
:Nutrition Abstracts and Reviews (1931-72), and Helminthological Abstracts (1932-72).

Subject coverage includes [[Public health]], [[tropical disease|Tropical]] and [[Communicable disease]]s, Nutrition, [[Parasitology]], [[Entomology]], and [[Mycology]].

===Tropical Diseases Bulletin===
'''''Tropical Diseases Bulletin''''' is a bibliographic and abstracts database which focuses on research published regarding [[infectious disease]]s and [[public health]] in [[developing countries]] and the [[tropics]] and [[subtropics]]. This includes research areas from [[epidemiology]] to [[diagnosis]], [[therapy]] to [[disease prevention]], [[tropical medicine]], and related aspects of [[travel medicine]]. Published research coverage on [[patients]] and populations encompasses the health of marginalized populations: [[immigrant]]s, [[refugee]]s, and [[indigenous peoples]].<ref name=tropical/>

Back-file coverage is from 1990 to present day, with an accessible base of 195,000 abstracts and the addition of 11,000 records per year. As a monthly journal '''''Tropical Diseases Bulletin''''' is also available in print. This print journal has author, subject and serials cited indexes.  Coverage of the print back-file is to 1912. A searchable, electronic database version of this journal is part of the ''Global Health Archive'' (see above).<ref name=tropical>
{{cite web
  | title =Tropical Diseases Bulletin
  | work = 
  | publisher =CABI 
  | year =2010  
  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=2201
  | accessdate =2010-07-18}}</ref>

==Organic Research Database==
This indexing database focuses on scientific literature pertaining to all topics in  [[organic farming]], in both the [[temperate zone|temperate]] and [[tropical zone]]s. This includes [[sustainability|sustainability issues]] and [[soil science|soil fertility]]. Coverage is global; literature is obtained from 125 countries. The temporal coverage spans 30 years, 180,000 organic research abstracts, along with the addition of 8000 records per year. Linking to full text articles, guided searches, broad subject categorization along with subject refinement are also provided. The editorial advisory board of this database also commission reviews pertaining to organic farming.<ref name=organic>
{{cite web
  | title =Organic Research Database
  | work =Description and bibliographic information 
  | publisher =CABI 
  | year =2011  
  | url =http://www.cabi.org/organicresearch/default.aspx?site=154&page=932
  | accessdate =2011-01-03}}</ref><ref name=usda>
{{cite web
  | title =Primary Research and Literature Databases
  | work = focus on sustainable and alternative agricultural topics
  | publisher =[[USDA]] - [[National Agriculture Library]] - [[AFSIC]] 
  | year =2011  
  | url =http://afsic.nal.usda.gov/nal_display/index.php?info_center=2&tax_level=2&tax_subject=288&level3_id=0&level4_id=0&level5_id=0&topic_id=1597&&placement_default=0
  | accessdate =2011-01-03}}</ref>

==CABI full text repository==
'''''CABI full text repository''''' is integrated into all ''CABI databases'' including CAB Abstracts, and Global Health. Both of these are online and print journals. Coverage includes 70,000 full text articles, through agreements with third party publishers. Eighty percent of the content is exclusive to CABI.<ref name=full-text/>  

The full text repository is made up of fifty percent journal articles, and equal percentage of conference (proceeding) papers, and other accessible literature is also included. Eighty percent of the articles are in English and coverage includes 56 countries. Also included in this database are relevant but hard to find materials which crosses disciplines consisting of [[agriculture]], [[health sciences|health]] and the [[life sciences]]. Main stream literature and hard to find materials of equal relevance are given equal access.<ref name=full-text>{{cite web
  | title =CABI full text 
  | work = 
  | publisher =CABI 
  | date =March 2010  
  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=2227 
  | accessdate =2010-07-18}}</ref>

''CABI full text repository'' is indexed in other databases, which also serve as access points, consisting of ''Web of Knowledge (Thomson Reuters)'', ''CAB Direct'', ''OvidSP, Dialog, Dimdi, and EBSCOhost''.

==References==
{{Reflist}}

[[Category:Bibliographic database providers]]
[[Category:Bibliographic indexes]]
[[Category:Citation indices]]
[[Category:Environmental science]]
[[Category:Global health]]
>>EOP<<
230<|###|>Web of Science
{{Merge from|Web of Knowledge|date=December 2014|discuss=Talk:Web of Science#Merge}}
{{Infobox Bibliographic Database
|title = Web of Science
|image = [[File:Web of science next generation.png|thumb|350px|Web of Science]]
|caption = 
|producer = Thomson Reuters 
|country = United States 
|history = 
|languages = 
|providers = Various institutions and commercial organizations 
|cost = 
|disciplines = Science, social science, arts, humanities (supports 256 disciplines) 
|depth = citation indexing,  author, topic title, subject keywords, abstract,  periodical title, author's address, publication year 
|formats = full text articles, reviews, editorials, chronologies, abstracts, proceedings (journals and book-based ), technical papers 
|temporal = 1900 to present 
|geospatial = 
|number = 90 million + 
|updates = 
|p_title = 
|p_dates = 
|ISSN =
|web =http://thomsonreuters.com/products_services/science/science_products/a-z/web_of_science 
|titles =http://thomsonreuters.com/content/science/pdf/Web_of_Science_factsheet.pdf
}}
'''Web of Science''' (WoS, previously known as [[Web of Knowledge]]) is an online subscription-based scientific [[citation index]]ing service maintained by [[Thomson Reuters]] that provides a comprehensive citation search. It gives access to multiple databases that reference cross-disciplinary research, which allows for in-depth exploration of specialized sub-fields within an [[academic discipline|academic or scientific discipline]].<ref>Drake, Miriam A. Encyclopedia of Library and Information Science. New York, N.Y.: Marcel Dekker, 2004.</ref>

==Background==
A citation index is built on the fact that citations in science serve as linkages between similar research items, and lead to matching or related scientific literature, such as [[academic journal|journal articles]], [[conference proceedings]], abstracts, etc. In addition, literature which shows the greatest impact in a particular field, or more than one discipline, can be easily located through a citation index. For example, a paper's influence can be determined by linking to all the papers that have cited it. In this way, current trends, patterns, and emerging fields of research can be assessed. [[Eugene Garfield]], the "father of citation indexing of academic literature,"<ref>Jacso, Peter. The impact of Eugene Garfield through the prizm of Web of Science. Annals of Library and Information Studies, Vol. 57, September 2010, P. 222. [http://nopr.niscair.res.in/bitstream/123456789/10235/4/ALIS%2057%283%29%20222-247.pdf PDF]</ref> who launched the [[Science Citation Index]] (SCI), which in turn led to the Web of Science,<ref>Garfield, Eugene, Blaise Cronin, and Helen Barsky Atkins. The Web of Knowledge: A Festschrift in Honor of Eugene Garfield. Medford, N.J.: Information Today, 2000.</ref> wrote: 

{{Quote|Citations are the formal, explicit linkages between papers that have particular points in common. A citation index is built around these linkages. It lists publications that have been cited and identifies the sources of the citations. Anyone conducting a literature search can find from one to dozens of additional papers on a subject just by knowing one that has been cited. And every paper that is found provides a list of new citations with which to continue the search.
The simplicity of citation indexing is one of its main strengths. <ref>Garfield, Garfield, Eugene. Citation indexing: Its theory and application in science, technology, and humanities. New York: Wiley, 1979, P. 1. [http://garfield.library.upenn.edu/ci/chapter1.PDF PDF]</ref>}}

==Coverage==
[[File:Web_of_Science_Core_Collection.png|thumb|200px|Accessing the Web of Science via the [[Web of Knowledge]]]]
Expanding the coverage of Web of Science, in November 2009 Thomson Reuters introduced ''Century of Social Sciences''. This service contains files which trace social science research back to the beginning of the 20th century,<ref name=InfoToNov2009>"''Thomson Reuters introduces century of social sciences''". Information Today 26.10 (2009): 10. General OneFile. Web. 23 June 2010.  [http://find.galegroup.com/gps/infomark.do?&contentSet=IAC-Documents&type=retrieve&tabID=T003&prodId=IPS&docId=A211794482&source=gale&srcprod=ITOF&userGroupName=mlin_c_marlpl&version=1.0  Document URL].</ref><ref name=ComLibNov2009>Thomson Reuters introduces century of social sciences." Computers in Libraries 29.10 (2009): 47. General OneFile. Internet. 23 June 2010. [http://find.galegroup.com/gps/infomark.do?&contentSet=IAC-Documents&type=retrieve&tabID=T003&prodId=IPS&docId=A211236981&source=gale&srcprod=ITOF&userGroupName=mlin_c_marlpl&version=1.0 Document URL]</ref> and Web of Science now has indexing coverage from the year 1900 to the present.<ref name=oview>
{{Cite web |title =Overview - Web of Science| publisher =Thomson Reuters| year = 2010
  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/web_of_science
  | format =Overview of coverage gleaned from promotional language.  
  | accessdate =2010-06-23}}</ref><ref name=UoOL>
{{Cite web| last = Lee| first =Sul H.| title =Citation Indexing and ISI's Web of Science 
  | publisher =The University of Oklahoma Libraries| year =2010
  | url =http://www.ou.edu/webhelp/librarydemos/isi/ | format =Discussion of finding literature manually. Description of [[citation index]]ing, and Web of Science.| accessdate =2010-06-23}}</ref> The multidisciplinary coverage of the Web of Science encompasses over 50,000 scholarly books, 12,000 journals and 160,000 conference proceedings<ref name="Web of Science">[http://wokinfo.com/citationconnection/realfacts/#regional Web of Science. Thomson Reuters, 2014]</ref> (as of September 3, 2014). The selection is made on the basis of [[impact factor|impact evaluations]] and comprise [[open-access journal]]s, spanning multiple [[academic discipline]]s. The coverage includes: the [[science]]s, [[social science]]s, [[the arts|arts]], and humanities, and goes across disciplines.<ref name=oview/><ref name=facts/> However, Web of Science does not index all journals, and its coverage in some fields is less complete than in others.

Furthermore, as of September 3, 2014 the total file count of the Web of Science was 90 million records, which included over a billion cited references. This citation service on average indexes around 65 million items per year, and it is described as the largest accessible citation database.<ref name=facts>[http://wokinfo.com/citationconnection/  Bulleted fact sheet]. Thomson Reuters. 2014.</ref>

Titles of foreign-language publications are translated into English and so cannot be found by searches in the original language.<ref name=harvard-search>{{Cite web
  |title =Some Searching Conventions
  | publisher =President and Fellows of Harvard College   | date = December 3, 2009   | url =http://hcl.harvard.edu/research/guides/citationindex/#some   | format =    | accessdate =2010-06-23}}</ref>

==Citation databases==
Web of Science consist of seven online databases:<ref name=included/><ref name="Web of Science">[http://wokinfo.com/media/pdf/WoSFS_08_7050.pdf Jo Yong-Hak. Web of Science. Thomson Reuters, 2013]</ref>
*[[Conference Proceedings Citation Index]] covers more than 160,000 conference titles in the Sciences starting from 1990 to the present day
*[[Science Citation Index Expanded]] covers more than 8,500 notable journals encompassing 150 disciplines. Coverage is from the year 1900 to the present day.
*[[Social Sciences Citation Index]] covers more than 3,000 journals in social science disciplines.  Range of coverage is from the year 1900 to the present day.
*[[Arts & Humanities Citation Index]] covers more than  1,700 arts and humanities journals starting from 1975. In addition, 250 major scientific and social sciences journals are also covered. 
*[[Index Chemicus]] lists more than 2.6 million compounds. The time of coverage is from 1993 to present day.
*[[Current Chemical Reactions]] indexes over one million reactions, and the range of coverage is from 1986 to present day. The '' INPI '' archives from 1840 to 1985 are also indexed in this database.
*[[Book Citation Index]] covers more than 60,000 editorially selected books starting from 2005.

=== Contents ===
The seven [[citation index|citation indices]] listed above contain references which have been cited by other articles. One may use them to undertake cited reference search, that is, locating articles that cite an earlier, or current publication. One may search citation databases by topic, by author, by source title, and by location. Two chemistry databases,  ''Index Chemicus'' and  ''Current Chemical Reactions'' allow for the creation of structure drawings, thus enabling users to locate [[chemical compound]]s and reactions. Institutions such as universities and research departments generally access the Web of Science through the [[Web of Knowledge]] platform. (An example of a typical search.<ref>[http://cires.colorado.edu/~jjose/P-Cited/DeCarlo06_ISI.pdf A typical Web of Science search example.]</ref>)

===Abstracting and indexing===
The following  types of literature are indexed: scholarly books, [[peer review]]ed journals, original research articles, reviews, editorials, chronologies, abstracts, as well as other items. Disciplines included in this index are  [[agriculture]], [[biological sciences]], [[engineering]], medical and [[life sciences]], [[physics|physical]] and [[chemical sciences]], [[anthropology]], law, [[library science]]s, [[architecture]], dance, music, film, and theater. Seven citation databases encompasses coverage of the above disciplines.<ref name=UoOL/><ref name=included>
{{Cite web |title =Coverage - Web of Science| publisher =Thomson Reuters| year = 2010
  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/web_of_science
  | format =Overview of coverage gleaned from promotional language.  
  | accessdate =2010-06-23}}</ref><ref name="Web of Science" />

==Limitations in the use of citation analysis==
As with other scientific approaches, scientometrics and bibliometrics have their own limitations. Recently, a criticism was voiced pointing toward certain deficiencies of the journal impact factor (JIF) calculation process, based on Thomson Reuters Web of Science, such as: journal citation distributions usually are highly skewed towards established journals; journal impact factor properties are field-specific and can be easily manipulated by editors, or even by changing the editorial policies; this makes the entire process essentially nontransparent.<ref name="Declaration">[http://am.ascb.org/dora/ San Francisco Declaration on Research Assessment: Putting science into the assessment of research, December 16, 2012]</ref>

Regarding the more objective journal metrics, there is a growing view that for greater accuracy it must be supplemented with an article-based assessment and peer-review.<ref name="Declaration" /> Thomson Reuters replied to criticism in general terms by stating that "no one metric can fully capture the complex contributions scholars make to their disciplines, and many forms of scholarly achievement should be considered."<ref>Thomson Reuters Statement Regarding the San Francisco Declaration on Research Assessment [http://researchanalytics.thomsonreuters.com/]</ref>

== See also ==

{{Div col|3}}
*[[List of academic journal search engines]]
*[[CSA (database company)|CSA databases]]
*[[Dialog (online database)]]
*[[Energy Citations Database]]
*[[Energy Science and Technology Database]]
*[[ETDEWEB]]
*[[Geographic Names Information System]]
*[[Materials Science Citation Index]]
*[[PASCAL (database)|PASCAL database]]
* [[PubMed Central]]
* [[SciELO]]
* [[VINITI Database RAS]]
* [[Web development tools]]
{{Div col end}}

== References ==
{{Reflist}}

==External links==
* [http://scientific.thomson.com/products/wos/ Web of Science]
* [http://www.webofknowledge.com/ Web of Knowledge]
* [http://web.archive.org/web/20110521161422/http://hcl.harvard.edu/research/guides/citationindex/ Searching the Citation Indexes (Web of Science)] Harvard College Library. 2010. (archive)
* [http://video.mit.edu/watch/web-of-science-12339/ MIT Web of Science video tutorial]. 2008.

{{Thomson Reuters}}
{{DEFAULTSORT:Web Of Science}}
[[Category:Bibliographic databases]]
[[Category:Full text scholarly online databases]]
[[Category:Thomson family]]
[[Category:Thomson Reuters]]
[[Category:Citation indices]]
[[Category:Scholarly search services]]
>>EOP<<
236<|###|>Shazam (service)
{{EngvarB|date=February 2014}}
{{Use dmy dates|date=February 2014}}
{{Infobox company
| name             = Shazam Entertainment Ltd.
| logo             = [[File:Shazam logo.png|160px]]
| caption          = 
| type             = 
| traded_as        = 
| genre            = <!-- Only used with media and publishing companies -->
| fate             = 
| predecessor      = 
| successor        = 
| foundation       = United States ({{Start date|1999}})
| founder          = {{unbulleted list|Chris Barton|Philip Inghelbrecht|Dhiraj Mukherjee|Avery Wang}}
| defunct          = <!-- {{End date|df=yes|YYYY|MM|DD}} -->
| location_city    = London
| location_country = United Kingdom
| location         = 
| locations        = 7 offices (2014)
| area_served      = Worldwide
| key_people       = {{unbulleted list|Rich Riley (CEO)|Andrew Fisher (Executive chairman)}}
| industry         = 
| products         = [[Application software|Apps]]
| services         = 
| revenue          = 
| operating_income = 
| net_income       = 
| aum              = <!-- Only used with financial services companies -->
| assets           = 
| equity           = 
| num_employees    = 
| divisions        = 
| subsid           = 
| homepage         = {{URL|//www.shazam.com/}} 
| footnotes        = 
| intl             = 
}}
'''Shazam''' is a British app for smartphones, PCs<ref>{{Cite web|url = http://apps.microsoft.com/windows/en-au/app/shazam/5593d150-02c7-4714-ab8f-007d5d251688|title = Shazam|date = |accessdate = 7 January 2015|website = Shazam app for Windows in the Windows Store|publisher = Microsoft Corporation|last = |first = }}</ref> and Macs, which is best known for its music identification capabilities. Shazam Entertainment Limited was founded in 1999 by Chris Barton, Philip Inghelbrecht, Avery Wang and Dhiraj Mukherjee.<ref name="DirectorDec2009">{{cite news | url=http://www.director.co.uk/magazine/2009/11%20December/shazam_63_04.html | title=Shazam names that tune | date=December 2009 | accessdate=26 September 2012 | last=Woodward | first=David | newspaper=Director}}</ref> The company is best known for its music identification technology, but has expanded to integrations with cinema, advertising, TV and retail environments.<ref>http://www.billboard.com/biz/articles/news/digital-and-mobile/6207061/shazam-launches-resonate-tv-sales-platform</ref>

Shazam uses a smartphone or Mac's built-in microphone to gather a brief sample of audio being played.  It creates an [[acoustic fingerprint]] based on the sample, and compares it against a central database for a match.  If it finds a match, it sends information such as the artist, song title, and album back to the user. Some implementations of Shazam incorporate relevant links to services such as [[iTunes]], [[YouTube]], [[Spotify]] or [[Zune]]. In December of 2013, Shazam was one of the top ten apps in the world, according to its CEO.<ref>http://video.cnbc.com/gallery/?video=3000222563#.</ref> The Shazam app has more than 100 million monthly active users and has been used on more than 500 million mobile devices.<ref>http://thenextweb.com/insider/2014/08/20/shazam-now-100-million-monthly-active-users-mobile/</ref> In October of 2014, Shazam announced its technology has been used to identify 15 billion songs.<ref>http://www.siliconrepublic.com/digital-life/item/38714-15-billion-songs-have-been</ref>
==Features==
Shazam offers two types of applications; a free app simply called Shazam and a paid app called Shazam Encore. The service was expanded in September 2012 to enable TV users in the US to identify featured music, access cast information and get links to show information online, as well as adding social networking capabilities.<ref name="TV tags"/>

In February of 2014, Shazam announced a redesign of the app, which included a new look and additional features, including lyric-viewing options, access to music videos and related videos, unique recommendations, improved biographies and discographies and additional functionality for use with TV shows. The update also featured a News Feed, and Auto-Shazam, a feature introduced in December of 2013, which runs in the background of users mobile devices to automatically identify media.<ref>http://www.digitaltveurope.net/151662/shazam-unveils-app-redesign/</ref>  

In July of 2014, Shazam announced the launch of Shazam for Mac, a desktop version of the app, which when enabled, runs in the background and automatically recognizes any song played on or near the computer, including songs playing in the background of TV shows or YouTube videos.<ref>http://mashable.com/2014/07/31/shazam-mac-app/</ref> Apples launch of iOS 8 in September of 2014 came with the seamless integration of Shazam into Apples intelligent personal assistant Siri function.<ref>http://www.jbgnews.com/2014/09/shazam-partners-with-apple-to-bring-music-recognition-to-siri/504608.html</ref> 

==Devices==
Shazam is a free or low-cost application that runs on [[Android (operating system)|Android]], [[Apple Inc.|Apple]] [[iPhone]] iOS, [[BlackBerry]] OS, and [[Windows Phone|Windows]] systems. The application is similar on most phones and the result is shown on the screen complete with details on Artist, Album, Title, Genre, Music label, lyrics, a thumbnail image of the song/album artwork, links to download the song on [[iTunes]] or the [[Amazon MP3]] store and, where relevant, show the song's video on YouTube and give the option of playing the song on [[Rdio]]. Shazam is also available for Mac, as a desktop application.<ref>http://mashable.com/2014/07/31/shazam-mac-app/ </ref> 

== Function ==
Shazam works by analyzing the captured sound and seeking a match based on an [[acoustic fingerprint]] in a database of more than 11 million songs.<ref>[//www.shazam.com/music/web/about.html Shazam  About Shazam<!-- Bot generated title -->]</ref>

[[File:Spectrogram of violin.png|thumb|A spectrogram of the sound of a violin.]]
[[File:Target zone2.png|thumb|The target zone of a song scanned by Shazam.{{clarify|date=September 2012}}]]
Shazam identifies songs based on an audio fingerprint based on a time-frequency graph called a [[spectrogram]].

Shazam stores a catalogue of audio fingerprints in a database. The user tags a song for 10 seconds and the application creates an audio fingerprint.

Once it creates the fingerprint of the audio, Shazam starts the search for matches in the database. If there is a match, it returns the information to the user; otherwise it returns a "song not known" dialogue.<ref>[http://soyoucode.com/2011/how-does-shazam-recognize-song How does Shazam work to recognize a song ? | So, you code ?<!-- Bot generated title -->]</ref>

Shazam can identify prerecorded music being broadcast from any source, such as a radio, television, cinema or music in a club, provided that the background noise level is not high enough to prevent an acoustic fingerprint being taken, and that the song is present in the software's database.

==History==
The company was founded in 1999 by Barton and Inghelbrecht, who were students at [[University of California, Berkeley]], and Mukherjee, who worked at a London-based internet consulting firm called Viant.{{Citation needed|date=September 2013}} In need of a digital signal processing specialist, the founding team then hired Wang, who was a PhD student from [[Stanford University]]. {{as of|September 2012}}, Wang is the only member of the original team to remain in the company,<ref name="DirectorDec2009" /> and serves as Shazam's Chief Scientist.<ref name="Shazam Team">{{cite web|title=About Shazam  Team|url=//www.shazam.com/music/web/team.html|accessdate=27 September 2012}}</ref>

[[Rich Riley]] joined Shazam as CEO in April 2013 to increase the companys growth,<ref>http://www.huffingtonpost.co.uk/2013/08/30/shazam-rich-riley_n_3762179.html</ref> after over 13 years at Yahoo!<ref>http://www.billboard.com/biz/articles/news/digital-and-mobile/1560025/shazam-names-rich-riley-new-ceo-aiming-for-eventual-ipo </ref> and with more than 17 years of experience as an entrepreneur and leading Internet executive.<ref>http://www.crunchbase.com/person/rich-riley </ref> "I look forward to extending our dominance in media engagement, from our roots in music to our leadership position in second-screen TV and want to ensure that Shazam is the company that helps people recognize and engage with the world around them, Riley said in a statement at the time.<ref>http://www.billboard.com/biz/articles/news/digital-and-mobile/1560025/shazam-names-rich-riley-new-ceo-aiming-for-eventual-ipo </ref> Riley replaced Andrew Fisher, who was hired from [[Infospace]] into the CEO role in 2005 to strengthen industry partnerships and grow the userbase.<ref name=DirectorDec2009 /> Fisher is now executive chairman.

===Partnerships===
The first partnership was with Entertainment UK, part of Woolworths, whom they approached to digitise their music catalogue of 1.5 million songs in return for permission to create a proprietary database. As the service grew to have a worldwide userbase, it needed to keep its database up-to-date, which it does by having relationships with labels globally.<ref name="DirectorDec2009" /> By December 2008, the database had grown to 8 million songs.<ref>{{cite news|last=Reisinger|first=Don|title=Shazam adds 2 million tracks to music library|url=http://news.cnet.com/8301-17939_109-10113274-2.html|accessdate=29 September 2012|newspaper=CNET|date=4 December 2008}}</ref>

In February 2013, Shazam announced a partnership with the music store [[Beatport]], adding its library of [[electronic music]] to the service.<ref name=bb-shazambeatport>{{cite web|title=Beatport's Matthew Adell on Shazam Deal, Why Music Biz Is a 'Disaster Model'|url=http://www.billboard.com/biz/articles/news/digital-and-mobile/1538517/beatports-matthew-adell-on-shazam-deal-why-music-biz-is|work=Billboard.biz|accessdate=21 September 2013}}</ref> On 3 April 2013, Shazam announced an exclusive partnership with [[Saavn]], an Indian online music streaming service. The deal will add nearly 1 million songs in [[Languages of India|Indian languages]] to Shazam's database.<ref>[http://www.financialmirror.com/newsml_story.php?id=5458 Shazam Forms Exclusive New Partnership with Saavn for the Best Indian Music Discovery Experience<!-- Bot generated title -->]</ref><ref>{{cite news| url=http://blogs.wsj.com/speakeasy/2013/04/03/shazam-broadens-its-horizons/ | work=The Wall Street Journal | title=Shazam Broadens Its Horizons  Speakeasy  WSJ}}</ref><ref>[http://techcrunch.com/2013/04/03/shazam-partners-with-the-spotify-of-india-saavn-to-improve-its-south-asian-music-recognition/ Shazam Partners With The Spotify Of India, Saavn, To Improve Its South Asian Music Recognition | TechCrunch<!-- Bot generated title -->]</ref><ref>[http://www.medianama.com/2013/04/223-shazam-saavn-tieup/ Updated: Shazam Ties Up With Saavn To Identify Hindi & Regional Music; Implications  MediaNama<!-- Bot generated title -->]</ref> In July 2014, Shazam announced a partnership with Rdio that allows Shazam users to stream full songs within the app.<ref>http://www.billboard.com/biz/articles/news/digital-and-mobile/6157583/shazam-partners-with-rdio-to-stream-full-songs-inside </ref>

In addition to music, Shazam has announced collaborations with partners across television, advertising and cinema. In May of 2014, NCM Media Networks announced a partnership with Shazam to incorporate Shazam into FirstLook pre-show segments that run in Regal, AMC and Cinemark theaters.<ref>http://techcrunch.com/2014/05/14/shazam-partners-with-ncm/ </ref> In November of 2014, NCM and Shazam announced that NCM FirstLook pre-shows are now Shazam enabled on over 20,000 movie screens across the United States.<ref>http://mashable.com/2014/11/07/shazam-firstlook/ </ref>

In August of 2014, Shazam announced the launch of Resonate, a sales product that allows TV networks to access its technology and user base. The news included the announcement of partnerships with AMC, A+E, dick clark productions and FUSE.<ref>http://www.billboard.com/biz/articles/news/digital-and-mobile/6207061/shazam-launches-resonate-tv-sales-platform </ref>

Shazam recently announced a partnership with Sun Broadcast Group on Shazam for Radio, a new offering that will allow radio stations to push customized content to listeners on Sun Broadcasts over 8,000 radio stations in the U.S.<ref>http://thenextweb.com/insider/2014/10/09/shazam-makes-big-move-interactive-radio-content/ </ref>

===Early days of the service===
Initially, in 2002, the service was launched only in the UK and was known as "2580", as the number was the [[shortcode]] that customers dialled from their mobile phone to get music recognised.<ref name=DirectorDec2009 /> The phone would automatically hang up after 30 seconds. A result was then sent to the user in the form of a text message containing the song title and artist name. At a later date, the service also began to add hyperlinks in the text message to allow the user to download the song online.<ref name=CNETUKApril06>{{cite news|last=Lim|first=Andrew|title=Shazam & AQA: The answer is on your mobile|url=http://crave.cnet.co.uk/mobiles/shazam-and-aqa-the-answer-is-on-your-mobile-49264359/|accessdate=29 September 2012|newspaper=CNET UK|date=24 April 2006}}</ref>

Shazam launched in the US on the AT&T Wireless network in 2004 in a joint offering with Musicphone, a now defunct San Francisco-based company. The service was free at launch with AT&T saying that it would charge USD0.99 for each use in future.<ref name="cnet040415">{{cite news | url=http://news.cnet.com/Dial-that-tune-comes-to-U.S./2110-1039_3-5192105.html | title=Dial-that-tune comes to U.S. | work=CNET | date=15 April 2004 | accessdate=29 September 2012 | author=Charny, Ben}}</ref>

In 2006, users were charged 0.60 per call or had unlimited use for 4.50 a month, as well as an online service to keep track of all tags.<ref name=CNETUKApril06/>

===Smartphone app===
Shazam for iPhone 2.0 debuted on 10 July 2008, with the launch of Apple's App Store. The free app simplified the service by enabling the user to launch iTunes and buy the song directly if the user was on a Wi-Fi connection <ref name=CNET080710>{{cite news|last=Rosoff|first=Matt|title=Shazam on iPhone could change music discovery|url=http://news.cnet.com/8301-13526_3-9988219-27.html|accessdate=29 September 2012|newspaper=CNET|date=10 July 2008}}</ref> (at the time, iTunes did not allow music downloads over 3G). It was also possible to launch the iPhone YouTube app, if a video was available.<ref name=CNET080716>{{cite news|last=Dolcourt|first=Jessica|title=First Look video: Shazam for iPhone|url=http://download.cnet.com/8301-2007_4-9992639-12.html|accessdate=29 September 2012|newspaper=CNET|date=16 July 2008}}</ref>

In 2008, the service struggled to identify classical music.<ref>{{cite news|last=Ho|first=Kevin|title=iPhone apps: Testing Shazam's limits  classical music|url=http://news.cnet.com/8301-13544_3-9993320-35.html|accessdate=29 September 2012|newspaper=CNET|date=17 July 2008}}</ref>

Shazam launched on the [[Android operating system|Android platform]] in October 2008. The Android app connected to [[Amazon Appstore|Amazon's MP3 store]] instead of iTunes.<ref name=AndroidLaunch>{{cite news|last=Reisinger|first=Don|title=Shazam moves to Android, works with Amazon MP3 Store|url=http://news.cnet.com/8301-17939_109-10071167-2.html|accessdate=29 September 2012|newspaper=CNET|date=21 October 2008}}</ref>

Alongside the iOS 3 update in July 2009, Shazam updated its app to include a number of new features: marking the tag with GPS coordinates; sending tags to others as 'postcards', enabling them to buy the song; and Twitter integration.<ref>{{cite news|last=Lee|first=Nicola|title=Latest Shazam lets you track musical journey in iPhone OS 3.0|url=http://download.cnet.com/8301-2007_4-10267205-12.html|accessdate=29 September 2012|newspaper=CNET|date=17 June 2009}}</ref>

The app launched on the [[Windows Marketplace for Mobile|Windows Mobile Marketplace]] in October 2009 as a [[freemium]] offering, with the first release of Shazam Encore. The free version was now limited to five tags per month: users typically tagged ten songs per month. Encore, priced at USD4.69, added several features such as song popularity charts and recommendations.<ref name=CNETWindowsLaunch>{{cite news|last=Dolcourt|first=Jessica|title=Shazam debuts in Windows Marketplace for Mobile|url=http://reviews.cnet.com/8301-12261_7-10368986-10356022.html|accessdate=30 September 2012|newspaper=CNET|date=7 October 2009}}</ref> Encore first appeared for iPhone in November 2009.<ref>{{cite news|last=Dolcourt|first=Jessica|title=Shazam iPhone app gets premium Encore|url=http://download.cnet.com/8301-2007_4-10393035-12.html|accessdate=30 September 2012|newspaper=CNET|date=9 November 2009}}</ref>

By December 2009, Shazam was downloaded 10 million times in 150 countries across 350 mobile operators. Around eight percent of users purchased a track after it was identified by the service.<ref name=DirectorDec2009 /> Its success led to a funding round from [[Kleiner Perkins Caufield & Byers]] in October 2009.<ref name=DirectorDec2009 /><ref>{{cite news|last=Saint|first=Nick|title=Shazam Draws Investment, Is Already Profitable|url=http://www.businessinsider.com/shazam-draws-investment-is-already-profitable-2009-10|accessdate=30 September 2012|newspaper=Business Insider|date=15 October 2009}}</ref> In January 2011, Apple announced that Shazam was the fourth most downloaded free app of all time on the App Store, while rival [[SoundHound]] had the top paid iPad app.<ref>{{cite news|last=Reisinger|first=Don|title=Apple reveals top apps of all time|url=http://news.cnet.com/8301-13506_3-20028889-17.html|accessdate=30 September 2012|newspaper=CNET|date=19 January 2011}}</ref>

Early adopters of the free application are still allowed unlimited tagging.<ref>[http://androidforums.com/android-applications/132182-shazam-how-preserve-unlimited-tagging-feature-after-reflash-root.html#post1488306 Shazam: How to preserve the "unlimited tagging" feature after REFLASH and Root?  Android Forums<!-- Bot generated title -->]</ref>

[[GetJar]], an app store for Android, Blackberry and Symbian, added Shazam in November 2010.<ref>{{cite news|last=Reisinger|first=Don|title=AT&T ladles out GetJar apps  iPhone excluded|url=http://news.cnet.com/8301-13506_3-20022340-17.html|accessdate=30 September 2012|newspaper=CNET|date=10 November 2010}}</ref>

In January 2011, Shazam and [[Spotify]] announced a partnership for iOS and Android to help users identify music with Shazam and listen to tracks through Spotify.<ref>{{cite news|last=Morris|first=Natali|title=Space love|url=http://cnettv.cnet.com/8301-13991_53-20028388-10391624.html|accessdate=30 September 2012|newspaper=CNET|date=13 January 2011}}</ref>

While Shazam already had Facebook and Twitter share buttons, deeper Facebook integration was released in March 2011. With Shazam Friends users can see what their Facebook friends have tagged, listen to the tracks and buy them.<ref>{{cite news|last=McCarthy|first=Caroline|title=Music app Shazam gets new Facebook features|url=http://news.cnet.com/8301-13577_3-20045965-36.html|accessdate=1 October 2012|newspaper=CNET|date=22 March 2011}}</ref>

With Shazam 5.0, released in April 2012, the app begins 'listening' as soon as it is launched and can take as little as one second to identify media. In addition to music, the app can identify TV programs and ads, if they are Shazam-enabled.<ref>{{cite news|last=Parker|first=Jason|title=Shazam for iOS adds TV to its list of media it can identify|url=http://reviews.cnet.com/8301-19512_7-57408964-233/shazam-for-ios-adds-tv-to-its-list-of-media-it-can-identify/|accessdate=1 October 2012|newspaper=CNET|date=3 April 2012}}</ref>

In August 2012, Shazam announced the service had been used to tag five billion songs, TV shows and advertisements. In addition, Shazam claimed to have over 225 million users across 200 countries.<ref>{{cite news|last=Sawers|first=Paul|title=Shazam: Five billion songs, TV shows and ads tagged|url=http://thenextweb.com/insider/2012/08/07/shazam-five-billion-songs-tv-shows-and-ads-tagged/|accessdate=30 September 2012|newspaper=The Next Web|date=7 August 2012}}</ref> A month later, the service claimed to have more than 250 million users with 2 million active users per week.<ref name="TV tags">{{cite news|last=Kinder|first=Lucy|title=Shazam hits 250 million users and adds TV tagging capability|url=http://www.telegraph.co.uk/technology/news/9547632/Shazam-hits-250-million-users-and-adds-TV-tagging-capability.html|accessdate=17 September 2012|newspaper=The Telegraph|date=17 September 2012|location=London}}</ref> The Shazam app currently has more than 100 million monthly active users and has been used on more than 500 million mobile devices.<ref>http://thenextweb.com/insider/2014/08/20/shazam-now-100-million-monthly-active-users-mobile/ </ref> In October of 2014, Shazam announced its technology has been used to identify 15 billion songs.<ref>http://www.siliconrepublic.com/digital-life/item/38714-15-billion-songs-have-been </ref>

The Shazam app was listed among Techland's 50 Best Android Applications for 2013.<ref>{{cite news |url=http://techland.time.com/2013/07/01/50-best-android-apps-for-2013/slide/pulse-news/ | title=50 Best Android Apps for 2013 | author=Jared Newman | work=Techland | accessdate=30 June 2013 | date=1 July 2013}}</ref>

In August 2014, Shazam announced there would be no more updates for Shazam(RED) after August 7.<ref>[https://support.shazam.com/hc/en-us/articles/202604996-Important-News-About-SHAZAM-RED Important News About Shazam(RED)]  Shazam Support</ref> Current users are advised to switch to the free version with tags transferred and ads removed (for free).

Apples launch of iOS 8 in September of 2014 came with the seamless integration of Shazam into Apples intelligent personal assistant Siri function.<ref>http://www.jbgnews.com/2014/09/shazam-partners-with-apple-to-bring-music-recognition-to-siri/504608.html </ref>

In October of 2014, Shazam introduced version 8.0 of the app, which features a new and improved News feed, as well as a section featuring Shazam charts and an explore option which lets user explore Shazamed tracks near them and around the world.<ref>http://appadvice.com/appnn/2014/10/shazam-8-0-features-interactive-notifications-in-ios-8-revamped-news-feed-and-more </ref>

===Desktop app=== 
Shazam announced the launch of Shazam for Mac, a desktop application, in July of 2014. When enabled, the app runs in the background of a Mac and automatically recognizes any song played on or near the computer, including songs playing in the background of TV shows or YouTube videos.<ref>http://mashable.com/2014/07/31/shazam-mac-app/ </ref>

==Similar apps==

*[[SoundHound]], previously known as Midomi, uses [[Query by humming]] to identify songs.{{citation needed|date=October 2012}}
*[[Gracenote]]'s MusicID-Stream has the main advantage of having the largest database of all music IDs (with more than 28 million songs).{{citation needed|date=October 2012}}
*Musipedia is a music search engine that works differently from others because instead of using techniques to identify recorded music, it can identify pieces of music from a single melody or rhythm.{{citation needed|date=October 2012}}
*Play by Yahoo Music.
*Bing music identification.
*Sony TrackID
*Path also has a music-identification feature.<ref>{{cite news|last=Cabebe|first=Jaymar|title=Path: The smaller, simpler alternative to Facebook|url=http://news.cnet.com/8301-1035_3-57416066-94/path-the-smaller-simpler-alternative-to-facebook/|accessdate=1 October 2012|newspaper=CNET|date=18 April 2012}}</ref>
*Stream That Song by Orange Innovation UK Ltd

==Patent infringement lawsuit==
In May 2009, Tune Hunter accused Shazam of violating {{US Patent|6941275}}, which covers music identification and purchase in a portable device.<ref>{{cite news|last=Ogg|first=Erica|title=Apple, AT&T, Samsung, Verizon, and others sued over Shazam app|url=http://news.cnet.com/8301-13579_3-10241309-37.html|accessdate=29 September 2012|newspaper=CNET|date=14 May 2009}}</ref> Shazam settled the case in January 2010.<ref>{{cite news
|title=Shazam Settles Patent Infringement Case With Tune Hunter
|url=http://techcrunch.com/2010/01/06/shazam-tune-hunter-settlement/
|date=Jan 6, 2010
|first=Robin
|last=Wauters
}}</ref>

==Funding==

As of September 2012, Shazam had raised $32 million in funding.<ref name="Techcrunch">{{cite news|last=Kincaid|first=Jason|title=Shazam Raises A Huge Round to the Tune of $32 Million|url=http://techcrunch.com/2011/06/22/shazam-raises-a-huge-round-to-the-tune-of-32-million/|accessdate=20 September 2012|newspaper=TechCrunch|date=22 June 2011}}</ref> In July 2013, [[Carlos Slim]] invested $40 million in Shazam for an undisclosed share.<ref>[http://www.ft.com/intl/cms/s/0/97d2c46a-e58d-11e2-8d0b-00144feabdc0.html#axzz2ag6DhVhD Carlos Slim invests $40m in music app Shazam  FT.com<!-- Bot generated title -->]</ref> And in March of 2014, Shazam confirmed another $20 million in new funding, raising the total value of the company to half a billion dollars.<ref>http://www.billboard.com/biz/articles/5930359/shazam-confirms-20m-in-new-funding-raising-value-to-500m </ref>

==See also==
* [[Query by humming]]
* [[Acoustic fingerprint]]
* [[Spectrogram]]
* [[Sound recording copyright symbol]]

==References==
{{Reflist|30em}}

==Further reading==
* {{cite news |last=Dredge |first=Stuart |title=Shazam: 'TV advertising is going to become our primary revenue stream' |url=http://www.guardian.co.uk/media/appsblog/2013/feb/27/shazam-tv-advertising-future |accessdate=27 February 2013 |newspaper=[[The Guardian]] |date=27 February 2013|location=London}}

==External links==
* {{Official website|www.shazam.com/music/web/home.html}}

[[Category:Companies based in London]]
[[Category:Acoustic fingerprinting]]
[[Category:Android (operating system) software]]
[[Category:BlackBerry software]]
[[Category:IOS software]]
[[Category:Symbian software]]
[[Category:Music search engines]]
[[Category:Companies established in 1999]]
[[Category:Windows Phone software]]
>>EOP<<
