0<|###|>Category:Searching
{{Commons category|Searching}}
[[Category:Information retrieval]]
<!-- used to be on cat search engines that was merged into this, but didn't think it really applied... [[Category:Information technology]] -->
>>EOP<<
6<|###|>IFACnet
'''IFACnet''', the KnowledgeNet for Professional Accountants, is the global, multilingual search engine developed by the [[International Federation of Accountants]] (IFAC) and its members to provide professional accountants worldwide with one-stop access to [[good practice guidance]], articles, management tools and other resources. This enterprise search engine was launched on October 2, 2006 by INDEZ. Originally marketed to professional accountants in business, IFACnet was expanded in March 2007 to provide resources and information relevant to small and medium accounting practices. It now includes resources and information for accountants in all sectors of the profession.

The following 31 organizations participate in IFACnet:

*[[American Institute of Certified Public Accountants]] (AICPA)
*[[Association of Chartered Certified Accountants]] (ACCA)
*[[Canadian Institute of Chartered Accountants]]
*[[Certified General Accountants Association of Canada]]
*[[Chartered Institute of Management Accountants]] (CIMA)
*[[Chartered Institute of Public Finance and Accountancy]]
*[[CMA Canada]]
*[[Compagnie Nationale des Commissaires aux Comptes]]
*[[Conseil Superieur de l'Ordre des Experts-Comptables]]
*[[Consiglio Nazionale Dottori Commercialisti]]
*[[CPA Australia]]
*[[Delegation Internationale Pour l'Audit et la Comptabilite]]
*[[Hong Kong Institute of Certified Public Accountants]] (HKICPA)
*[[International Federation of Accountants]]  (IFAC)
*[[Institut der Wirtschaftspruefer in Deutschland]] e.V. (IDW)
*[[Institute of Certified Public Accountants in Ireland]]
*[[Institute of Certified Public Accountants of Singapore]]
*[[Institute of Chartered Accountants of Australia]]
*[[Institute of Chartered Accountants in England & Wales]] (ICAEW)
*[[Institute of Chartered Accountants in Ireland]]
*[[Institute of Chartered Accountants of India]]
*[[Institute of Chartered Accountants of Pakistan]]
*[[Institute of Chartered Accountants of Scotland]] (ICAS)
*[[Institute of Management Accountants]]
*[[Japanese Institute of Certified Public Accountants]] (JICPA)
*[[Koninklijk Nederlands Instituut van Registeraccountants]] (Royal NIVRA)
*[[Malaysian Institute of Accountants]]
*[[Malta Institute of Accountants]]
*[[National Association of State Boards of Accountancy]] (NASBA)
*[[South African Institute of Chartered Accountants]] (SAICA)
*[[Union of Chambers of Certified Public Accountants of Turkey]] (TURMOB)

==External links==
*[http://www.ifacnet.com/ IFACnet - A KnowledgeNet for Professional Accountants]
*[http://www.ifac.org/ International Federation of Accountants Homepage]

[[Category:Information retrieval]]
[[Category:Internet search engines]]
[[Category:Accounting organizations]]
>>EOP<<
12<|###|>BASE (search engine)
{{multiple issues|
{{notability|Web|date=February 2012}}
{{refimprove|date=June 2009}}
{{primary sources|date=February 2012}}
{{one source|date=February 2012}}
{{no footnotes|date=February 2012}}
}}

'''BASE''' ('''Bielefeld Academic Search Engine''') is a multi-disciplinary [[search engine]] to scholarly internet resources, created by [[Bielefeld University]] Library in [[Bielefeld]], [[Germany]]. It is based on search technology provided by [[Fast Search & Transfer]] (FAST), a [[Norway|Norwegian]] company. It [[Web harvesting|harvests]] OAI metadata from scientific [[Digital repository|digital repositories]] that implement the [[Open Archives Initiative Protocol for Metadata Harvesting]] (OAI-PMH), and are [[Index (search engine)|indexed]] using FAST's software. In addition to OAI [[metadata]], the library indexes selected web sites and local data collections, all of which can be searched via a single search interface.

It allows those who use the search engine to search metadata, when available, as well as conducting [[full text search]]es. It contrasts with commercial search engines in multiple ways, including in the types and kinds of resources it searches and the information it offers about the results it finds. Where available, [[Bibliographic database|bibliographic data]] is provided, and the results may be sorted by multiple fields, such as by author or year of publication.

== See also ==
* [[List of academic databases and search engines]]

==External links==
* [http://www.base-search.net/ BASE search]

[[Category:Internet search engines]]
[[Category:Information retrieval]]
[[Category:Open access (publishing)]]
[[Category:Bibliographic databases]]


{{software-stub}}
>>EOP<<
18<|###|>Poliqarp
'''Poliqarp''' is an [[open source]] [[search engine]] designed to process [[text corpus|text corpora]], among others the [[National Corpus of Polish]] created at the Institute of Computer Science, [[Polish Academy of Sciences]].

==Features==

* Custom [[query language]].
* Two-level [[regular expressions]]:
** operating at the level of characters in words
** operating at the level of words in statements/paragraphs
* Good performance
* Compact corpus representation (compared to similar projects)
* Portability across operating systems: [[Linux]]/[[BSD]]/[[Win32]]
* Lack of portability across [[endianness]] (current release works only on little endian devices)

==External links==

* [http://www.korpus.pl/index.php?lang=en&page=welcome Polish corpus website (in English)]
* [http://poliqarp.sourceforge.net/ Project website on SourceForge]
* [http://poliqarp.suxx.pl/ Search plugin for Firefox]
<br />
[[Category:Information retrieval]]
>>EOP<<
24<|###|>Poison words
{{Missing information|Examples of poison words|date=September 2008}}
{{Unreferenced|date=December 2007}}

'''Poison words''', or '''forbidden words''', is the name given to words or phrases that trigger suspicion, mistrust and loss of respect, or are of inappropriate character for a given web site in its consideration for a [[search engine]].

There is no definite list of poison words which all natural language processing tools incorporate. 

This is different from harmless but useless words that are called [[Stop words]].

Adult (obscene) words can put a web page in an adult category where it is filtered out by various filters at search engines, so this is one set of poison words. But some consider any words that lower your ranking in a search engine as poison words. Some people consider any words that encourage ads to pervade a whole site and displace much higher earning ads as poison words.

== See also ==

* [[Bayesian poisoning]]
* [[Natural language processing]]
* [[Text mining]]
* [[Index (search engine)|Search engine indexing]]

== External links ==


{{SearchEngineOptimization}}

[[Category:Information retrieval]]
[[Category:Searching]]
>>EOP<<
30<|###|>Noisy text analytics
'''Noisy text analytics''' is a process of [[information extraction]] whose goal is to automatically extract structured or semistructured information from [[noisy text|noisy unstructured text data]]. While [[Text analytics]] is a growing and mature field that has great value because of the huge amounts of data being produced, processing of noisy text is gaining in importance because a lot of common applications produce noisy text data. Noisy unstructured text data is found in informal settings such as [[online chat]], [[Text messaging|text messages]], [[e-mail]]s, [[message boards]], [[newsgroups]], [[blogs]], [[wikis]] and [[web pages]]. Also, text produced by processing spontaneous speech using [[automatic speech recognition]] and printed or handwritten text using [[optical character recognition]] contains processing noise. Text produced under such circumstances is typically highly noisy containing spelling errors, [[abbreviation]]s, non-standard words, false starts, repetitions, missing [[punctuation]]s, missing [[letter case]] information, pause filling words such as um and uh and other texting and [[speech disfluencies]]. Such text can be seen in large amounts in [[contact centre (business)|contact centers]], [[chat room]]s, [[optical character recognition]] (OCR) of text documents, [[short message service]] (SMS) text, etc. Documents with [[historical language]] can also be considered noisy with respect to todays knowledge about the language. Such text contains important historical, religious, ancient medical knowledge that is useful. The nature of the noisy text produced in all these contexts warrants moving beyond traditional text analysis techniques.

== Techniques for noisy text analysis ==
Missing punctuation and the use of non-standard words can often hinder standard [[natural language processing]] tools such as [[Part-of-speech tagging]]
and [[parsing]]. Techniques to both learn from the noisy data and then to be able to process the noisy data are only now being developed. 

== Possible source of noisy text ==
* [[World wide web]]: Poorly written text is found in web pages, [[online chat]], [[blogs]], [[wikis]], [[discussion forum]]s, [[newsgroups]]. Most of these data are unstructured and the style of writing is very different from, say, well-written news articles. Analysis for the web data is important because they are sources for market buzz analysis, market review, [[trend estimation]], etc. Also, because of the large amount of data, it is necessary to find efficient methods of [[information extraction]], [[Statistical classification|classification]], [[automatic summarization]] and analysis of these data.
* [[Contact centre (business)|Contact centers]]: This is a general term for help desks, information lines and customer service centers operating in domains ranging from computer sales and support to mobile phones to apparels. On an average a person in the developed world interacts at least once a week with a contact center agent. A typical contact center agent handles over a hundred calls per day. They operate in various modes such as voice, [[online chat]] and [[E-mail]]. The contact center industry produces gigabytes of data in the form of [[E-mails]], chat logs, voice conversation [[Transcription (linguistics)|transcription]]s, customer feedback, etc. A bulk of the contact center data is voice conversations. Transcription of these using state of the art [[automatic speech recognition]] results in text with 30-40% [[word error rate]]. Further, even written modes of communication like online chat between customers and agents and even the interactions over email tend to be noisy. Analysis of contact center data is essential for customer relationship management, customer satisfaction analysis, call modeling, customer profiling, agent profiling, etc., and it requires sophisticated techniques to handle poorly written text.
* Printed Documents: Many libraries, government organizations and national defence organizations have vast repositories of [[hard copy]] documents. To retrieve and process the content from such documents, they need to be processed using [[Optical Character Recognition]]. In addition to printed text, these documents may also contain handwritten annotations. OCRed text can be highly noisy depending on the font size, quality of the print etc. It can range from 2-3% [[word error rate]]s to as high as 50-60% [[word error rate]]s. Handwritten annotations can be particularly hard to decipher, and error rates can be quite high in their presence.
* [[Text messaging|Short Messaging Service]] (SMS): Language usage over computer mediated discourses, like chats, emails and SMS texts, significantly differs from the standard form of the language. An urge towards shorter message length facilitating faster typing and the need for semantic clarity, shape the structure of this non-standard form known as the texting language.

== References ==
*[http://www.springerlink.com/content/ql711884654q/?p=c6beb20b8dfa4389b5e4daf2dd63618e&pi=0 "Special Issue on Noisy Text Analytics - International Journal on Document Analysis and Recognition (2007), Springer, Guest Editors Craig Knoblock, Daniel Lopresti, Shourya Roy and L. Venkata Subramaniam, Vol. 10, No. 3-4, December 2007."]
*[http://arXiv.org/abs/0810.0332 "Wong, W., Liu, W. & Bennamoun, M. Enhanced Integrated Scoring for Cleaning Dirty Texts. In: IJCAI Workshop on Analytics for Noisy Unstructured Text Data (AND); Hyderabad, India."].
<references />


==See also==
* [[Text analytics]]
* [[Information extraction]]
* [[Computational linguistics]]
* [[Natural language processing]]
* [[Named entity recognition]]
* [[Text mining]]
* [[Automatic summarization]]
* [[Statistical classification]]
* [[Data quality]]

[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval]]
[[Category:Statistical natural language processing]]

[[es:Extraccion de la informacion]]
>>EOP<<
36<|###|>Mean reciprocal rank
{{Refimprove|date=June 2007}}
'''Mean reciprocal rank''' is a [[statistic]] measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the [[multiplicative inverse]] of the rank of the first correct answer. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:<ref>{{cite conference | title=Proceedings of the 8th Text Retrieval Conference | booktitle=TREC-8 Question Answering Track Report | author=E.M. Voorhees |year=1999 | pages=77&ndash;82}}</ref>

:<math> \text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}. \!</math>

The reciprocal value of the mean reciprocal rank corresponds to the [[harmonic mean]] of the ranks.

== Example ==
For example, suppose we have the following three sample queries for a system that tries to translate English words to their plurals.  In each case, the system makes three guesses, with the first one being the one it thinks is most likely correct:

{| class="wikitable"
|-
! Query
! Results
! Correct response
! Rank
! Reciprocal rank
|-
| cat
| catten, cati, '''cats'''
| cats
| 3
| 1/3
|-
| torus
| torii, '''tori''', toruses
| tori
| 2
| 1/2
|-
| virus
| '''viruses''', virii, viri
| viruses
| 1
| 1
|}

Given those three samples, we could calculate the mean reciprocal rank as (1/3&nbsp;+&nbsp;1/2&nbsp;+&nbsp;1)/3 = 11/18 or about 0.61.

This basic definition does not specify what to do if...
# none of the proposed results are correct (use reciprocal rank 0), or if
# there are multiple correct answers in the list. Consider using [[Information_retrieval#Mean_average_precision|mean average precision (MAP)]].

See also [[Information retrieval]] and [[Question answering]].<ref>{{cite conference | title=Evaluating web-based question answering systems | booktitle=Proceedings of LREC | author=D. R. Radev, H. Qi, H. Wu, W. Fan |year=2002 }}</ref>

==References==
{{Reflist}}

[[Category:Summary statistics]]
[[Category:Information retrieval]]
>>EOP<<
42<|###|>GLIMPSE
{{Other uses|Glimpse (disambiguation)}}
{{Infobox software
| name = Glimpse
| logo = 
| screenshot =
| caption =
| developer = [[Internet WorkShop]]
| status = 
| latest release version = 4.18.6 (source) / 4.18.5 (binary) 
| latest release date = {{release date|2012|06|09}}
| operating system = [[Cross-platform]]
| programming language = [[C (programming language)|C]]
| genre = [[Search algorithm|Search]] and [[index (search engine)|index]]
| license = 
| website = {{URL|http://webglimpse.net/}}
}}
'''GLIMPSE''' is a text indexing and [[text retrieval|retrieval]] [[software]] program originally developed at the [[University of Arizona]] by [[Udi Manber]], [[Sun Wu]], and [[Burra Gopal]].  It was released under the ISC [[open source]] license in September 2014.

GLIMPSE stands for GLobal IMPlicit SEarch. While many text indexing schemes create quite large indexes (usually around 50% of the size of the original text), a GLIMPSE-created index is only 2-4% of the size of the original text.

GLIMPSE uses and takes a great deal of inspiration from [[Agrep]], which was also developed at the University of Arizona, but GLIMPSE uses a high level index whereas Agrep parses all the text each time.

The basic algorithm is similar to other text indexing and retrieval engines, except that the text records in the index are huge, consisting of multiple files each. This index is searched using a boolean matching algorithm like most other text indexing and retrieval engines. After one or more of these large text records is matched, Agrep is used to actually scan for the exact text desired. While this is slower than traditional totally indexed approaches, the advantage of the smaller index is seen to be advantageous to the individual user. This approach would not work particularly well across websites, but it would work reasonably well for a single site, or a single workstation. In addition, the smaller index can be created more quickly than a full index.

==References==
{{Reflist}}

==External links==
*[http://webglimpse.net/ Glimpse and WebGlimpse home page]
*[http://webglimpse.net/pubs/glimpse.pdf Original Glimpse paper] (PDF)

[[Category:Information retrieval]]
[[Category:Free search engine software]]
[[Category:Search engine software]]
>>EOP<<
48<|###|>Communication engine
{{Orphan|date=February 2009}}
A '''communication engine''' is a tool that sends user requests to several other [[communication protocols]] and/or [[database]]s and aggregates the results into a single list or displays them according to their source. Communication engines enable users to enter communication account authorization once and access several communication avenues simultaneously. Communication engines operate on the premise that the [[World Wide Web]] is too large for any one engine to index it all and that more productive results can be obtained by combining the results from several engines dynamically. This may save the user from having to use multiple engines separately.

[[Category:Information retrieval]]
[[Category:Computing terminology]]


{{Web-stub}}
>>EOP<<
54<|###|>List of enterprise search vendors
== Free and open source [[enterprise search]] software ==
<!--
################# READ THIS

Please do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.
Also, read the definition of enterprise search before adding a new search engine
-->
*[[Apache Solr]]
*[[DataparkSearch]]
*[[ElasticSearch]]
*[[Htdig|ht://Dig]]
*[[ApexKB]]
*[[mnoGoSearch]]
*[[OpenSearchServer]]
*[[Searchdaimon]]
*[[Sphinx_(search_engine)|Sphinx]]
*[[Zettair]]

== Vendors of open source enterprise search software ==
* [[30 Digits]] - Implementation, consulting, support, and value-add components for [[Lucene]] and [[Solr]]
* [[Apache Software Foundation]] - The foundation is the entity behind the [[Lucene]] family of products
* [[LucidWorks]] (former Lucid Imagination) - Commercial support, training and services for [[Lucene]] and [[Solr]]
* Customermatrix (acquired Polyspot, CRM development and products for [[Lucene]]) 
* [[Searchblox]] - Commercial product for [[Lucene]] and [[ElasticSearch]]
* [[Sematext]] - Consulting, development and products for [[Lucene]], [[Solr]], [[Nutch]], and [[Hadoop]]
* [[FlaxUK|Flax]] - Architecture, development and support for [[Lucene]], [[Solr]] and [[Xapian]]

== Vendors of proprietary enterprise search software ==
<!--
################# READ THIS

Please do not add web links or companies which do not have Wikipedia articles. They will be summarily deleted.

-->
*[[AskMeNow]]
*[[Attivio]]
*[[Concept Searching Limited]]
*[[Content Analyst Company|Content Analyst Company LLC]]
*[[Coveo]]
*[[Dassault Systemes]] (acquired [[Exalead]])
*[[Denodo]]
*[[Dieselpoint, Inc.]]
*[[dtSearch Corp.]]
*[[EMC Corp.]]
*[[Exorbyte GmbH]]
*[[Expert System S.p.A.]]
*[[Exterro, Inc.]]
*[[Fabasoft Mindbreeze|Fabasoft]]
*[[Funnelback]]
*[[Google Search Appliance]]
*[[HP]] (acquired [[Autonomy Corporation]] which in turn acquired [[Verity]] K2 and Ultraseek)
*[[IBM]] (acquired [[Vivisimo]], rebranded "[[Watson (computer)|Watson]]")
*[[Inbenta]]
*[[inter:gator Enterprise Search]]
*[[ISYS Search Software]]
*[[Lookeen]]
*[[Mark Logic|MarkLogic]]
*[[Microsoft]] (includes [[Microsoft Search Server]], [[Fast Search & Transfer]])
*[[Fabasoft Mindbreeze|Mindbreeze]] 
*[[Neofonie]] (includes WeFind)
*[[Omniture]] (acquired by [[Adobe Systems]])
*[[Open Text Corporation]]
*[[Oracle Corporation]] (includes [[Oracle_Corporation#Oracle_Secure_Enterprise_Search|Secure Enterprise Search]] and [[Endeca Technologies Inc.]])
*[[Q-go]]
*[[Q-Sensei]]
*[[Recommind (software company)|Recommind]]
*[[SAP AG|SAP]] (includes SAP NetWeaver Enterprise Search, Search Services in SAP NetWeaver AS ABAP, and Search and Classification TREX)
*[[Silent Eight]]
*[[Sinequa]]
*[[SLI_Systems]]
*[[Sophia Search Limited]]
* [[Swiftype]]
*[[TeraText]]
*[[Thunderstone Software]]
*[[X1 Technologies, Inc.]]
*[[ZyLAB Technologies]]
*[[ZL Technologies]]

== External links ==
* [http://www.dmoz.org/Computers/Software/Information_Retrieval/Fulltext/ DMOZ category Information Retrieval/Fulltext]
{{Companies by industry}}

{{DEFAULTSORT:Enterprise search vendors}}
[[Category:Information retrieval]]
[[Category:Searching]]
[[Category:Search engine software|*Enterprise search vendors]]
[[Category:Lists of software]]
[[Category:Lists of companies by industry|Enterprise search vendors]]
>>EOP<<
60<|###|>European Summer School in Information Retrieval
The '''European Summer School in Information Retrieval''' (ESSIR) is a scientific event founded in 1990, which starts off a series of Summer Schools to provide high quality teaching of information retrieval on advanced topics. ESSIR is typically a week-long event consisting of guest lectures and seminars from invited lecturers who are recognized experts in the field.
The aim of ESSIR is to give to its participants a common ground in different aspects of '''[[Information Retrieval]] (IR)'''. Maristella Agosti in 2008 stated that: ''The term IR identifies the activities that a person  the user  has to conduct to choose, from a collection of documents, those that can be of interest to him to satisfy a specific and contingent information need.''<ref>Agosti, M.: Information Access using the Guide of User Requirements. In: ''Information Access through Search Engines and Digital Libraries''. Agosti, M. ed., Springer-Verlag Berlin Heidelberg, pp. 1-12, (2008).</ref>

IR is a discipline with many facets and at the same time influences and is influenced by many other scientific disciplines. Indeed, IR ranges from [[Computer Science]] to [[Information Science]] and beyond; moreover, a large number of IR methods and techniques are adopted and absorbed by several technologies. The IR core methods and techniques are those for designing and developing IR systems, Web search engines, and tools for information storing and querying in Digital Libraries. IR core subjects are: system architectures, algorithms, formal theoretical models, and evaluation of the diverse systems and services that implement functionalities of storing and retrieving documents from multimedia document collections, and over wide area networks such as the [[Internet]].

ESSIR aims to give a deep and authoritative insight of the core IR methods and subjects along these three dimensions and also for this reason it is intended for researchers starting out in IR, for industrialists who wish to know more about this increasingly important topic and for people working on topics related to management of information on the [[Internet]].

Two books have been prepared as readings in IR from editions of ESSIR, the first one is ''Lectures on Information Retrieval''
,<ref>Agosti, M., Crestani, F. and Pasi, G. (Eds): Lectures on Information Retrieval. Revised Lectures of Third European Summer-School, ESSIR 2000 Varenna, Italy, September 1115, 2000. LNCS Vol. 1980, Springer-Verlag, Berlin Heidelberg, 2001.</ref> the second one is ''Advanced Topics in Information Retrieval''.<ref>Melucci, M., and Baeza-Yates, R. (Eds): Advanced Topics in Information Retrieval. The Information Retrieval Series, Vol. 33, Springer-Verlag, Berlin Heidelberg, 2011.</ref>

== ESSIR Editions ==
ESSIR series started in 1990 coming out from the successful experience of the Summer School in Information Retrieval (SSIR) conceived and designed by Nick Belkin, [[Rutgers University]], U.S.A., and Maristella Agosti, [[University of Padua]], Italy, for an Italian audience in 1989.

{| class="wikitable" border="1"
|-
! Edition
! Web Site
! Location
! Organiser(s)
|-
|  9th
|  [http://www.ugr.es/~essir2013/ ESSIR 2013]
|  Granada, Spain
|  Juan M. Fernadez-Luna and Juan F. Huete
|-
|  8th
|  [http://essir.uni-koblenz.de/ ESSIR 2011]
|  Koblenz, Germany
|  Sergej Sizov and Steffen Staab
|-
|  7th
|  [http://essir2009.dei.unipd.it/ ESSIR 2009]
|  Padua, Italy
|  Massimo Melucci and Ricardo Baeza-Yates
|-
|  6th
|  [http://www.dcs.gla.ac.uk/essir2007/ ESSIR 2007]
|  Glasgow, Scotland, United Kingdom
|  Iadh Ounis and Keith van Rijsbergen
|-
|  5th
|  [http://www.cdvp.dcu.ie/ESSIR2005/ ESSIR 2005]
|  Dublin, Ireland
|  Alan Smeaton
|-
|  4th
|  [http://www-clips.imag.fr/mrim/essir03/main_essir.html ESSIR 2003]
|  Aussois (Savoie), France
|  Catherine Berrut and Yves Chiaramella
|-
|  3rd
|  [http://www.itim.mi.cnr.it/Eventi/essir2000/index.htm ESSIR 2000]
|  Varenna, Italy
|  Maristella Agosti, Fabio Crestani, and Gabriella Pasi
|-
|  2nd
|  [http://www.dcs.gla.ac.uk/essir/ ESSIR 1995]
|  Glasgow, United Kingdom
|  Keith van Rijsbergen
|-
|  1st
|  [http://ims.dei.unipd.it/websites/essir/essir1990.html ESSIR 1990]
|  Brixen, Italy
|  Maristella Agosti
|}

==Notes==
{{reflist}}

==External links==
* [http://ims.dei.unipd.it/websites/essir/home.html ESSIR presentation page of the IMS Research Group]
* [http://ims.dei.unipd.it IMS Research Group, Department of Information Engineering - University of Padua, Italy]
* [http://www.dei.unipd.it/ Department of Information Engineering - University of Padua, Italy]
* [http://www.unipd.it/en/index.htm University of Padua, Italy]

[[Category:Information retrieval]]
[[Category:Summer schools]]
>>EOP<<
66<|###|>Instant indexing
{{multiple issues|
{{Orphan|date=February 2009}}
{{Refimprove|article|date=November 2006}}
}}

'''Instant indexing''' is a feature offered by [[Internet]] [[search engine]]s that enables users to submit content for immediate inclusion into the [[search engine indexing|index]].

==Delayed inclusion==
Certain search engine services may require an extended period of time for inclusion, which is seen as a delay and a frustration by [[website]] administrators who wish to have their websites appear in [[search engine results page|search engine results]]{{Citation needed|date=February 2007}}.

Delayed inclusion may due to the size of the index that the service must maintain or due to corporate, political or social policies{{Citation needed|date=February 2007}}. Some services only index content collected by a [[web crawling|crawler program]] which does not allow for manual adding of content to index{{Citation needed|date=February 2007}}.

==Criticisms==
A criticism of instant indexing is that certain services filter results manually or via algorithms that prevent instant inclusion to avoid inclusion of content that violates the service's policies.{{Citation needed|date=February 2007}}

Instant indexing impacts the timeliness of the content included in the index. Given the manner in which many [[web crawling|crawlers]] operate in the case of Internet search engines, websites are only visited if a some other website links to them. Unlinked web sites are never visited (see [[invisible web]]) by the crawler because it cannot reach the website during its traversal. It is assumed that unlinked websites are less authoritative and less popular, and therefore of less quality. Over time, if a website is popular or authoritative, it is assumed that other websites will eventually link to it. If a search engine service provides instant indexing, it bypasses this quality control mechanism by not requiring incoming links. This infers that the search engine's service produces lower quality results.

Select search services that offer such a service typically also offer [[paid inclusion]], also referred to as [[pay per click|inorganic search]]. This may reduce the quality of search results.

==External links==
* {{cite web | url = http://www.web-cite.com/search_marketing/000078.html | title = Don't Blink: Instant Indexing? | publisher = Web-Cite Exposure | date = 2003-03-26 | accessdate = 2006-09-23 |archiveurl = http://web.archive.org/web/20060427184004/http://www.web-cite.com/search_marketing/000078.html <!-- Bot retrieved archive --> |archivedate = 2006-04-27}}
* {{cite web | url = http://www.earthstation9.com/index.html?us_searc.htm | title = The Wonderful World of Search Engines and Web Directories  A Search Engine Guide | author = Stan Daniloski | publisher = Earth Station 9 | date = 2004-09-17 | accessdate = 2006-09-23}}

== See also ==
* [[Search engine]]
* [[Search engine indexing]]
* [[Web crawling]]

[[Category:Internet terminology]]
[[Category:Information retrieval]]


{{website-stub}}
>>EOP<<
72<|###|>Search engine (computing)
{{more footnotes|date=August 2014}}
{{one source|date=August 2014}}
A '''search engine''' is an [[information retrieval|information retrieval system]] designed to help find information stored on a [[computer system]]. The search results are usually presented in a list and are commonly called ''hits''. Search engines help to minimize the time required to find information and the amount of information which must be consulted, akin to other techniques for managing [[information overload]]. {{Citation needed|date=December 2007}}

The most public, visible form of a search engine is a [[Web search engine]] which searches for information on the [[World Wide Web]].

==How search engines work==
Search engines provide an [[interface (computer science)|interface]] to a group of items that enables users to specify criteria about an item of interest and have the engine find the matching items. The criteria are referred to as a [[search query]]. In the case of text search engines, the search query is typically expressed as a set of words that identify the desired [[concept]] that one or more [[document]]s may contain.<ref>Voorhees, E.M. [http://www.indexnist.gov/itl/iad/894.02/works/papers/nlp_ir.ps Natural Language Processing and Information Retrieval]. National Institute of Standards and Technology. March 2000.</ref> There are several styles of search query [[syntax]] that vary in strictness. It can also switch names within the search engines from previous sites.  Whereas some text search engines require users to enter two or three words separated by [[Whitespace (computer science)|white space]], other search engines may enable users to specify entire documents, pictures, sounds, and various forms of [[natural language]]. Some search engines apply improvements to search queries to increase the likelihood of providing a quality set of items through a process known as [[query expansion]].

[[Image:search-engine-diagram-en.svg|right|thumb|Index-based search engine]]

The list of items that meet the criteria specified by the query is typically sorted, or ranked. Ranking items by relevance (from highest to lowest) reduces the time required to find the desired information. [[probability|Probabilistic]] search engines rank items based on measures of [[String metric|similarity]] (between each item and the query, typically on a scale of 1 to 0, 1 being most similar) and sometimes [[popularity]] or [[authority]] (see [[Bibliometrics]]) or use [[relevance feedback]]. [[Boolean logic|Boolean]] search engines typically only return items which match exactly without regard to order, although the term ''boolean search engine'' may simply refer to the use of boolean-style syntax (the use of operators [[Logical_conjunction|AND]], [[Logical_disjunction|OR]], NOT, and [[Exclusive_or|XOR]]) in a probabilistic context.

To provide a set of matching items that are sorted according to some criteria quickly, a search engine will typically collect [[metadata]] about the group of items under consideration beforehand through a process referred to as [[Index (search engine)|indexing]]. The index typically requires a smaller amount of [[computer storage]], which is why some search engines only store the indexed information and not the full content of each item, and instead provide a method of navigating to the items in the [[serp|search engine result page]]. Alternatively, the search engine may store a copy of each item in a [[cache (computing)|cache]] so that users can see the state of the item at the time it was indexed or for archive purposes or to make repetitive processes work more efficiently and quickly.

Other types of search engines do not store an index. Crawler, or spider type search engines (a.k.a. real-time search engines) may collect and assess items at the time of the search query, dynamically considering additional items based on the contents of a starting item (known as a seed, or seed URL in the case of an Internet crawler). [[Meta search engine]]s store neither an index nor a cache and instead simply reuse the index or results of one or more other search engines to provide an aggregated, final set of results.

==See also==
{{Portal|Computer Science}}
{{div col|colwidth=30em}}
*[[Automatic summarization]]
*[[Bibliographic database]]
*[[Desktop search]]
*[[Emanuel Goldberg]] (inventor of early search engine)
*[[Enterprise search]]
*[[Federated search]]
*[[Full text search]]
*[[Human search engine]]
*[[Image search]]
*[[Index (search engine)]]
*[[Inverted index]]
*[[List of search engines]]
*[[List of enterprise search vendors]]
*[[Medical literature retrieval]]
*[[Metasearch engine]]
*[[Search engine optimization]]
*[[Search suggest drop-down list]]
*[[Selection-based search]]
*[[Semantic search]]
* [[Solver (computer science)]]
*[[Spamdexing]]
*[[SQL]]
*[[Text mining]]
*[[Vertical search]]
*[[Video search engine]]
*[[Web search engine]]
{{div col end}}

==References==
{{Reflist}}
{{Internet search}}

{{DEFAULTSORT:Search Engine (Computing)}}
[[Category:Information retrieval]]
[[Category:Data search engines| Search engine]]
>>EOP<<
78<|###|>Information Retrieval Specialist Group
{{Unreferenced|date=January 2010}}

The '''Information Retrieval Specialist Group''' ('''IRSG''') or '''BCS-IRSG''' is a Specialist Group of the [[British Computer Society]] concerned with supporting communication between researchers and practitioners, promoting the use of [[Information Retrieval]] (IR) methods in industry and raising public awareness. There is a newsletter called ''The Informer'', an annual European Conference (ECIR), and continual organisation and sponsorship of conferences, workshops and seminars. The current chair is Dr. Andy MacFarlane.{{Citation needed|date=January 2010}}

==European Conference on Information Retrieval==
Organising [[European Conference on Information Retrieval|ECIR]] is one of the major activities of the Information Retrieval Specialist Group. The conference began in 1979 and has grown to become one of the major Information Retrieval conferences alongside [[Special Interest Group on Information Retrieval|SIGIR]] receiving hundreds of paper and poster submissions every year from around the world.{{Citation needed|date=January 2010}} ECIR was initially established by the IRSG under the name "Annual Colloquium on Information Retrieval Research", and held in the UK until 1997. It was renamed ECIR in 2003 to better reflect its status as an international conference.

== External links ==
* [http://irsg.bcs.org/ IRSG website]

[[Category:Information retrieval|Specialist Group]]
[[Category:BCS Specialist Groups]]
>>EOP<<
84<|###|>Queries per second
{{refimprove|date=February 2010}}
'''Queries Per Second''' (QPS) is a common measure of the amount of search traffic an [[information retrieval]] system, such as a [[search engine]] or a [[database]], receives during one second.<ref>[http://www.microsoft.com/enterprisesearch/en/us/search-glossary.aspx#Q Microsoft's search glossary]</ref><ref>[http://www.answers.com/topic/qps QPS definition at answers.com]</ref>

High-traffic systems must watch their QPS in order to know when to scale the system to handle more load.

== References ==
{{reflist}}

[[Category:Units of measurement]]
[[Category:Information retrieval]]

{{computer-stub}}
>>EOP<<
90<|###|>Federated search
{{Citations missing|date=June 2008}}

'''Federated search''' is an [[information retrieval]] technology that allows the simultaneous search of multiple searchable resources.  A user makes a single query request which is distributed to the [[search engine]]s participating in the federation.  The federated search then aggregates the results that are received from the [[search engine]]s for presentation to the user.

==Purpose==
Federated search came about to meet the need of searching multiple  disparate content sources with one query.  This allows a user to search multiple databases at once in real time, arrange the results from the various databases into a useful form and then present the results to the user.

==Process==
As described by Peter Jacso (2004<ref>Thoughts About Federated Searching.  Jacso, Peter, Information Today,  Oct 2004, Vol. 21, Issue 9</ref>), federated searching consists of (1) transforming a [[Web search query|query]] and broadcasting it to a group of disparate databases or other web resources, with the appropriate syntax, (2) merging the results collected from the databases, (3) presenting them in a succinct and unified format with minimal duplication, and (4) providing a means, performed either automatically or by the portal user, to sort the merged result set.

Federated search portals, either commercial or open access, generally search public access [[bibliographic databases]], public access Web-based library catalogues ([[OPAC]]s), Web-based search engines like [[Google]] and/or open-access, government-operated or corporate data collections. These individual information sources send back to the portal's interface a list of results from the search query. The user can review this hit list.  Some portals will merely [[screen scrape]] the actual database results and not directly allow a user to enter the information source's application. More sophisticated ones will de-dupe the results list by merging and removing duplicates. There are additional features available in many portals, but the basic idea is the same: to improve the accuracy and relevance of individual searches as well as reduce the amount of time required to search for resources.

This process allows federated search some key advantages when compared with existing crawler-based search engines.  Federated search need not place any requirements or burdens on owners of the individual information sources, other than handling increased traffic.  Federated searches are inherently as current as the individual information sources, as they are searched in real time.

==Implementation==
[[File:Fed_search.png|thumb|alt=federated search engine|Federating across three search engines]]

One application of federated searching is the [[metasearch engine]]; however, this is not a complete solution as many documents are not currently indexed. These documents are on what is known as the [[deep Web]], or invisible Web. Many more information sources are not yet stored in electronic form. [[Google Scholar]] is one example of many projects trying to address this.

When the search vocabulary or [[data model]] of the search system is different from the data model of one or more of the foreign target systems the query must be translated into each of the foreign target systems.  This can be done using simple data-element translation or may require [[semantic translation]].

A challenge faced in the implementation of federated search engines is scalability, in other words, the performance of the site as the number of information sources comprising the federated search engine increase. One federated search engine that has begun to address this issue is [[WorldWideScience]], hosted by the [[U.S. Department of Energy]]'s [[Office of Scientific and Technical Information]].  WorldWideScience <ref>[http://www.worldwidescience.org WorldWideScience]</ref> is composed of more than 40 information sources, several of which are federated search portals themselves.  One such portal is Science.gov <ref name="Science.gov">[http://www.science.gov Science.gov]</ref> which itself federates more than 30 information sources representing most of the R&D output of the U.S. Federal government.  Science.gov returns its highest ranked results to WorldWideScience, which then merges and ranks these results with the search returned by the other information sources that comprise WorldWideScience.<ref name="Science.gov"/> This approach of cascaded federated search enables large number of information sources to be searched via a single query.

Another application [[Sesam]] running in both Norway and Sweden has been built on top of an open sourced platform specialised for federated search solutions. Sesat,<ref>[http://sesat.no Sesat]</ref> an acronym for [[Sesam Search Application Toolkit]], is a platform that provides much of the framework and functionality required for handling parallel and pipelined searches and displaying them elegantly in a user interface, allowing engineers to focus on the index/database configuration tuning.

==Challenges==

When federated search is performed against secure data sources, the users' credentials must be passed on
to each underlying search engine, so that appropriate security is maintained.  If the user has different
login credentials for different systems, there must be a means to map their login ID to each search
engine's security domain.<ref>[http://www.ideaeng.com/tabId/98/itemId/124/Mapping-Security-Requirements-to-Enterprise-Search.aspx Mapping Security Requirements to Enterprise Search]</ref>

Another challenge is mapping results list navigators into a common form.  Suppose 3 real-estate sites are searched, each provides a list of hyperlinked city names to click on, to see matches only in each city.  Ideally these facets would be combined into one set, but that presents additional technical challenges.<ref>[http://www.ideaeng.com/tabId/98/itemId/154/20-Differences-Between-Internet-vs-Enterprise-Se.aspx#fed_facets 20+ Differences Between Internet vs. Enterprise Search - part 1]</ref>  The system also needs to understand "next page" links if it's going to allow the user to page through the combined results.

==Further reading==
*[http://www.libraryjournal.com/article/CA6571320.html Federated Search 101. Linoski, Alexis, Walczyk, Tine, Library Journal, Summer 2008 Net Connect, Vol. 133]{{Dead link|date=November 2010}} Note: this content has been moved [http://www.accessmylibrary.com/article-1G1-182034526/federated-search-101-alexis.html here], but you will need a remote access account through your local library to get the whole article.

*Cox, Christopher N. Federated Search: Solution or Setback for Online Library Services. Binghamton, NY: Haworth Information Press, 2007.[http://lccn.loc.gov/2006101753 Table of Contents]
*[http://www.altsearchengines.com/2009/01/11/federated-search-finds-content-that-google-cant-reach-part-i-of-iii/ Federated Search Primer. Lederman, S., AltSearchEngines, January 2009] {{Dead link|date=July 2010}} Note: This material has been reposted [http://deepwebtechblog.com/federated-search-finds-content-that-google-can%E2%80%99t-reach-part-i-of-iii/ here], on the blog of a commercial search engine company.

* Milad Shokouhi and Luo Si, Federated Search, Foundations and Trends in Information Retrieval: Vol. 5: No 1, pp 1-102., [http://dx.doi.org/10.1561/1500000010 http://dx.doi.org/10.1561/1500000010]

==See also==
* [[Search aggregator]]
* [[Deep Web]]

==References==
{{Reflist}}
{{Internet search}}

{{DEFAULTSORT:Federated Search}}
[[Category:Information retrieval]]
[[Category:Internet terminology]]
[[Category:Searching]]
[[Category:Internet search algorithms]]
[[Category:Applications of distributed computing]]
>>EOP<<
96<|###|>Multi-document summarization
'''Multi-document summarization''' is an automatic procedure aimed at [[information extraction|extraction of information]] from multiple texts written about the same topic. Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the [[news aggregators]] performing the next step down the road of coping with [[information overload]].

==Key benefits==
Multi-[[document summarization]] creates information reports that are both concise and comprehensive.
With different opinions being put together & outlined, every topic is described from multiple perspectives within a single document.
While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.
Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.

==Technological challenges==
The multi-document summarization task has turned out to be much more complex than [[automatic summarization|summarizing a single document]], even a very large one. This difficulty arises from inevitable thematic diversity within a large set of documents. A good summarization technology aims to combine the main themes with completeness, readability, and conciseness. Document Understanding Conferences,<ref>http://www-nlpir.nist.gov/projects/duc/index.html</ref> conducted annually by [[NIST]], have developed sophisticated evaluation criteria for techniques accepting the multi-document summarization challenge.

An ideal multi-document summarization system does not simply shorten the source texts but presents information organized around the key aspects to represent a wider diversity of views on the topic. When such quality is achieved, an automatic multi-document summary is perceived more like an overview of a given topic. The latter implies that such text compilations should also meet other basic requirements for an overview text compiled by a human. The multi-document summary quality criteria are as follows:
*clear structure, including an outline of the main content, from which it is easy to navigate to the full text sections
*text within sections is divided into meaningful paragraphs
*gradual transition from more general to more specific thematic aspects
*good [[readability]]

The latter point deserves additional note - special care is taken in order to ensure that the automatic overview shows:
*no paper-unrelated "[[communication noise|information noise]]" from the respective documents (e.g., web pages)
*no dangling references to what is not mentioned or explained in the overview
*no text breaks across a sentence
*no semantic [[Redundancy (information theory)|redundancy]].

==Real-life systems==
The multi-document summarization technology is now coming of age - a view supported by a choice of advanced web-based systems that are currently available.
* Ultimate Research Assistant<ref>http://ultimate-research-assistant.com/</ref> - performs text mining on Internet search results to help summarize and organize them and make it easier for the user to perform online research. Specific text mining techniques used by the tool include concept extraction, text summarization, hierarchical concept clustering (e.g., automated taxonomy generation), and various visualization techniques, including tag clouds and mind maps. 
* iResearch Reporter<ref>http://www.iresearch-reporter.com/</ref> - Commercial Text Extraction and Text Summarization system, free demo site accepts user-entered query, passes it on to Google search engine, retrieves multiple relevant documents, produces categorized, easily  readable natural language summary reports covering multiple documents in retrieved set, all extracts linked to original documents on the Web, post-processing, entity extraction, event and relationship extraction, text extraction, extract clustering, linguistic analysis, multi-document, full text, natural language processing, categorization rules, clustering, linguistic analysis, text summary construction tool set.
* Newsblaster<ref>http://newsblaster.cs.columbia.edu</ref> is a system that helps users find news that is of the most interest to them. The system automatically collects, clusters, categorizes, and summarizes news from several sites on the web ([[CNN]], [[Reuters]], [[Fox News]], etc.) on a daily basis, and it provides users an interface to browse the results.
* NewsInEssence<ref>http://www.newsinessence.com</ref> may be used to retrieve and summarize a cluster of articles from the web. It can start from a [[Uniform Resource Locator|URL]] and retrieve documents that are similar, or it can retrieve documents that match a given set of keywords. NewsInEssence also downloads news articles daily and produces news clusters from them.
* NewsFeed Researcher<ref>http://newsfeedresearcher.com</ref> is a news portal performing continuous [[automatic summarization]] of documents initially clustered by the [[news aggregators]] (e.g., [[Google News]]). NewsFeed Researcher is backed by a free online engine covering major events related to business, technology, U.S. and international news. This tool is also available in on-demand mode allowing a user to build a summaries on selected topics.
* Scrape This<ref>http://www.scrapethis.com</ref> is like a search engine, but instead of providing links to the most relevant websites based on a query, it scrapes the pertinent information off of the relevant websites and provides the user with a consolidated multi-document summary, along with dictionary definitions, images, and videos.
* JistWeb<ref>http://www.jastatechnologies.com/productList.html</ref> is a query specific multiple document summariser.

As auto-generated multi-document summaries increasingly resemble the overviews written by a human, their use of extracted text snippets may one day face [[copyright]] issues in relation to the [[fair use]] copyright concept.

==Bibliography==
* Gunes Erkan and Dragomir R. Radev. Lexrank: Graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR), 2004. [http://clair.si.umich.edu/~radev/papers/lprj.pdf]
* Dragomir R. Radev, Hongyan Jing, Malgorzata Stys, and Daniel Tam. Centroid-based summarization of multiple documents. Information Processing and Management, 40:919938, December 2004. [http://clair.si.umich.edu/~radev/papers/centroid.pdf]
* Kathleen R. McKeown and Dragomir R. Radev. Generating summaries of multiple news articles. In Proceedings, ACM Conference on Research and Development in Information Retrieval SIGIR'95, pages 7482, Seattle, Washington, July 1995. [http://clair.si.umich.edu/~radev/papers/sigir95.pdf]
* C.-Y. Lin, E. Hovy, "From single to multi-document summarization: A prototype system and its evaluation", In "Proceedings of the ACL", pp.&nbsp;457464, 2002
*Kathleen McKeown, Rebecca J. Passonneau, David K. Elson, Ani Nenkova, Julia Hirschberg, "Do Summaries Help? A Task-Based Evaluation of Multi-Document Summarization", SIGIR05, Salvador, Brazil, August 1519, 2005 [http://www.cs.columbia.edu/~ani/papers/f98-mckeown.pdf]
*R. Barzilay, N. Elhadad, K. R. McKeown, "Inferring strategies for sentence ordering in multidocument news summarization", Journal of Artificial Intelligence Research, v. 17, pp.&nbsp;3555, 2002
*M. Soubbotin, S. Soubbotin, "Trade-Off Between Factors Influencing Quality of the Summary", Document Understanding Workshop (DUC), Vancouver, B.C., Canada, October 910, 2005 [http://duc.nist.gov/pubs/2005papers/freetext.sergei.pdf]
* C Ravindranath Chowdary, and P. Sreenivasa Kumar. "Esum: an efficient system for query-specific multi-document summarization." In ECIR (Advances in Information Retrieval), pp.&nbsp;724728. Springer Berlin Heidelberg, 2009.

==See also==
* [[Automatic summarization]]
* [[Text mining]]
* [[News aggregators]]

==References==
{{reflist}}

==External links==
{{External links|date=September 2010}}
*[http://www-nlpir.nist.gov/projects/duc/index.html Document Understanding Conferences]
*[http://www1.cs.columbia.edu/nlp/projects.html Columbia NLP Projects]
*[http://lada.si.umich.edu:8080/clair/nie1/nie.cgi NewsInEssence: Web-based News Summarization]

{{Natural Language Processing}}

{{DEFAULTSORT:Multi-Document Summarization}}
[[Category:Natural language processing]]
[[Category:Information retrieval]]
>>EOP<<
102<|###|>Cosine similarity
'''Cosine similarity''' is a measure of similarity between two vectors of an [[inner product space]] that measures the [[cosine]] of the angle between them. The cosine of 0 is 1, and it is less than 1 for any other angle. It is thus a judgement of orientation and not magnitude: two vectors with the same orientation have a Cosine similarity of 1, two vectors at 90 have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude. Cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1].

Note that these bounds apply for any number of dimensions, and Cosine similarity is most commonly used in high-dimensional positive spaces. For example, in [[Information Retrieval]] and [[text mining]], each term is notionally assigned a different dimension and a document is characterised by a vector where the value of each dimension corresponds to the number of times that term appears in the document. Cosine similarity then gives a useful measure of how similar two documents are likely to be in terms of their subject matter.<ref>Singhal, Amit (2001). "Modern Information Retrieval: A Brief Overview". Bulletin of the IEEE Computer Society Technical Committee on Data Engineering 24 (4): 3543.</ref>

The technique is also used to measure cohesion within [[cluster (computing)|cluster]]s in the field of [[data mining]].<ref>P.-N. Tan, M. Steinbach & V. Kumar, "Introduction to Data Mining", , Addison-Wesley (2005), ISBN 0-321-32136-7, chapter 8; page 500.</ref>

''Cosine distance'' is a term often used for the complement in positive space, that is: <math>D_C(A,B) = 1 - S_C(A,B)</math>. It is important to note, however, that this is not a proper [[distance metric]] as it does not have the triangle inequality property and it violates the coincidence axiom; to repair the triangle inequality property whilst maintaining the same ordering, it is necessary to convert to Angular distance (see below.)

One of the reasons for the popularity of Cosine similarity is that it is very efficient to evaluate, especially for sparse vectors, as only the non-zero dimensions need to be considered.

==Definition==

The cosine of two vectors can be derived by using the [[Euclidean vector#Dot product|Euclidean dot product]] formula:

:<math>\mathbf{a}\cdot\mathbf{b}
=\left\|\mathbf{a}\right\|\left\|\mathbf{b}\right\|\cos\theta</math>

Given two [[Vector (geometric)|vectors]] of attributes, ''A'' and ''B'', the cosine similarity, ''cos()'', is represented using a [[dot product]] and [[Magnitude (mathematics)#Euclidean vectors|magnitude]] as

:<math>\text{similarity} = \cos(\theta) = {A \cdot B \over \|A\| \|B\|} = \frac{ \sum\limits_{i=1}^{n}{A_i \times B_i} }{ \sqrt{\sum\limits_{i=1}^{n}{(A_i)^2}} \times \sqrt{\sum\limits_{i=1}^{n}{(B_i)^2}} }</math>

The resulting similarity ranges from &minus;1 meaning exactly opposite, to 1 meaning exactly the same, with 0 usually indicating independence, and in-between values indicating intermediate similarity or dissimilarity.

For text matching, the attribute vectors ''A'' and ''B'' are usually the [[tf-idf|term frequency]] vectors of the documents.  The cosine similarity can be seen as a method of normalizing document length during comparison.

In the case of [[information retrieval]], the cosine similarity of two documents will range from 0 to 1, since the term frequencies ([[tf-idf]] weights) cannot be negative. The angle between two term frequency vectors cannot be greater than&nbsp;90.

If the attribute vectors are normalized by subtracting the vector means (e.g., <math>A - \bar{A}</math>), the measure is called centered cosine similarity and is equivalent to the [[Pearson_product-moment_correlation_coefficient#For_a_sample|Pearson Correlation Coefficient]].

=== Angular similarity ===

The term "cosine similarity" has also been used on occasion to express a different coefficient, although the most common use is as defined above. Using the same calculation of similarity, the normalised angle between the vectors can be used as a bounded similarity function within [0,1], calculated from the above definition of similarity by:
:<math>1 - \frac{ \cos^{-1}( \text{similarity} )}{ \pi} </math>
in a domain where vector coefficients may be positive or negative, or
:<math>1 - \frac{ 2 \cdot \cos^{-1}( \text{similarity} ) }{ \pi }</math>
in a domain where the vector coefficients are always positive.  

Although the term "cosine similarity" has been used for this angular distance, the term is oddly used as the cosine of the angle is used only as a convenient mechanism for calculating the angle itself and is no part of the meaning. The advantage of the angular similarity coefficient is that, when used as a difference coefficient (by subtracting it from 1) the resulting function is a proper [[distance metric]], which is not the case for the first meaning. However for most uses this is not an important property. For any use where only the relative ordering of similarity or distance within a set of vectors is important, then which function is used is immaterial as the resulting order will be unaffected by the choice.

=== Confusion with "Tanimoto" coefficient ===

The cosine similarity may be easily confused with the Tanimoto metric - a specialised form of a similarity coefficient with a similar algebraic form:

:<math>T(A,B) = {A \cdot B \over \|A\|^2 +\|B\|^2 - A \cdot B}</math>

In fact, this algebraic form [[Jaccard index#Tanimoto_Similarity_and_Distance|was first defined by Tanimoto]] as a mechanism for calculating the [[Jaccard coefficient]] in the case where the sets being compared are represented as [[bit vector]]s. While the formula extends to vectors in general, it has quite different properties from cosine similarity and bears little relation other than its superficial appearance.

=== Ochiai coefficient ===
This coefficient is also known in biology as Ochiai coefficient, or Ochiai-Barkman coefficient, or Otsuka-Ochiai coefficient:<ref>''Ochiai A.'' Zoogeographical studies on the soleoid fishes found Japan and its neighboring regions. II // Bull. Jap. Soc. sci. Fish. 1957. V. 22. No 9. P. 526-530.</ref><ref>''Barkman J.J.'' Phytosociology and ecology of cryptogamic epiphytes, including a taxonomic survey and description of their vegetation units in Europe.  Assen. Van Gorcum. 1958. 628 p.</ref>
:<math>K =\frac{n(A \cap B)}{\sqrt{n(A) \times n(B)}}</math>
Here, <math>A</math> and <math>B</math> are sets, and <math>n(A)</math> is the number of elements in <math>A</math>. If sets are represented as [[bit vector]]s, the Ochiai coefficient can be seen to be the same as the cosine similarity.

== Properties ==
Cosine similarity is related to [[Euclidean distance]] as follows. Denote Euclidean distance by the usual <math>\|A - B\|</math>, and observe that

:<math>\|A - B\|^2 = (A - B)^\top (A - B) = \|A\|^2 + \|B\|^2 - 2 A^\top B</math>

by [[Polynomial expansion|expansion]]. When {{mvar|A}} and {{mvar|B}} are normalized to unit length, <math>\|A\|^2 = \|B\|^2 = 1</math> so the previous is equal to

:<math>2 (1 - \cos(A, B))</math>

'''Null distribution:''' For data which can be negative as well as positive, the [[null distribution]] for cosine similarity is the distribution of the dot product of two independent random unit vectors. This distribution has a [[mean]] of zero and a [[variance]] of <math>1/n</math> (where <math>n</math> is the number of dimensions), and although the distribution is bounded between -1 and +1, as <math>n</math> grows large the distribution is increasingly well-approximated by the [[normal distribution]].<ref>{{cite journal
 | author = Spruill, Marcus C
 | year = 2007
 | title = Asymptotic distribution of coordinates on high dimensional spheres
 | journal = Electronic communications in probability
 | volume = 12 | pages = 234-247
 | doi = 10.1214/ECP.v12-1294
}}</ref><ref>[http://stats.stackexchange.com/questions/85916/distribution-of-dot-products-between-two-random-unit-vectors-in-mathbbrd CrossValidated: Distribution of dot products between two random unit vectors in RD]</ref>
For other types of data, such as bitstreams (taking values of 0 or 1 only), the null distribution will take a different form, and may have a nonzero mean.<ref>{{cite journal
 | author = Graham L. Giller 
 | year = 2012
 | title = The Statistical Properties of Random Bitstreams and the Sampling Distribution of Cosine Similarity
 | journal = Giller Investments Research Notes
 | number = 20121024/1
 | doi = 10.2139/ssrn.2167044
}}</ref>

== Soft Cosine Measure ==
'''Soft cosine measure''' <ref>{{cite journal|last1=Sidorov|first1=Grigori|last2=Gelbukh|first2=Alexander|last3=Gomez-Adorno|first3=Helena|last4=Pinto|first4=David|title=Soft Similarity and Soft Cosine Measure: Similarity of Features in Vector Space Model|journal=Computacion y Sistemas|volume=18|issue=3|pages=491504|doi=10.13053/CyS-18-3-2043|url=http://cys.cic.ipn.mx/ojs/index.php/CyS/article/view/2043|accessdate=7 October 2014}}</ref>
is a measure of soft similarity between two vectors, i.e., the measure that considers similarity of pairs of features. The traditional '''cosine similarity''' considers the [[vector space model]] (VSM) features as independent or completely different, while the '''soft cosine measure''' proposes considering the similarity of features in VSM, which allows generalization of the concepts of cosine measure and also the idea of similarity (soft similarity).

For example, in the field of [[natural language processing]] (NLP) the similarity between features is quite intuitive. Features such as words, n-grams or syntactic n-grams<ref>{{cite book|last1=Sidorov|first1=Grigori|last2=Velasquez|first2=Francisco|last3=Stamatatos|first3=Efstathios|last4=Gelbukh|first4=Alexander|last5=Chanona-Hernandez|first5=Liliana|title=Syntactic Dependency-based N-grams as Classification Features|publisher=LNAI 7630|isbn=978-3-642-37798-3|pages=111|url=http://link.springer.com/chapter/10.1007%2F978-3-642-37798-3_1|accessdate=7 October 2014}}</ref> can be quite similar, though formally they are considered as different features in the VSM. For example, words play and game are different words and thus are mapped to different dimensions in VSM; yet it is obvious that they are related semantically. In case of [[n-grams]] or syntactic n-grams, [[Levenshtein distance]] can be applied (in fact, Levenshtein distance can be applied to words as well).

For calculation of the soft cosine measure, the matrix {{math|'''s'''}} of similarity between features is introduced. It can be calculated using Levenshtein distance or other similarity measures, e.g., various [[WordNet]] similarity measures. Then we just multiply by this matrix.  

Given two {{math|''N''}}-dimension vectors a and b, the soft cosine similarity is calculated as follows:

:<math>\begin{align}
    \operatorname{soft\_cosine}_1(a,b)=
    \frac{\sum\nolimits_{i,j}^N s_{ij}a_ib_j}{\sqrt{\sum\nolimits_{i,j}^N s_{ij}a_ia_j}\sqrt{\sum\nolimits_{i,j}^N s_{ij}b_ib_j}},
\end{align}
</math>

where {{math|''s<sub>ij</sub>'' {{=}} similarity(feature<sub>''i''</sub>, feature<sub>''j''</sub>)}}.

If there is no similarity between features ({{math|''s<sub>ii</sub>'' {{=}} 1}}, {{math|''s<sub>ij</sub>'' {{=}} 0}} for {{math|''i'' = ''j''}}), the given equation is equivalent to the conventional cosine similarity formula.

The complexity of this measure is quadratic, which makes it perfectly applicable to real world tasks. The complexity can be even transformed to linear.

== See also ==
* [[Srensen similarity index|Srensen's quotient of similarity]]
* [[Hamming distance]]
* [[Correlation]]
* [[Dice's coefficient]]
* [[Jaccard index]]
* [[SimRank]]
* [[Information retrieval]]

==References==
{{reflist}}

== External links ==
* [http://www.appliedsoftwaredesign.com/archives/cosine-similarity-calculator/ Online Cosine Similarity Calculator]
* [http://mathforum.org/kb/message.jspa?messageID=5658016&tstart=0 Weighted cosine measure]
* [http://blog.christianperone.com/?p=2497 A tutorial on cosine similarity using Python]

{{DEFAULTSORT:Cosine Similarity}}
[[Category:Information retrieval]]
>>EOP<<
108<|###|>Datanet
{{Use mdy dates|date=September 2011}}
''This article is about the U.S. National Science Foundation Office of Cyberinfrastructure .

On September 28, 2007, the U.S. [[National Science Foundation]] Office of Cyberinfrastructure announced a request for proposals with the name '''Sustainable Digital Data Preservation and Access Network Partner (DataNet)'''.<ref name="datanetprogram">{{cite web
|url=http://www.nsf.gov/funding/pgm_summ.jsp?pims_id=503141
|publisher=National Science Foundation
|title=Sustainable Digital Data Preservation and Access Network Partners (DataNet) Program Summary
|date=September 28, 2007
|accessdate=October 3, 2007
}}</ref>  The lead paragraph of its synopsis describes the program as:

<blockquote>Science and engineering research and education are increasingly digital and increasingly data-intensive.  Digital data are not only the output of research but provide input to new hypotheses, enabling new scientific insights and driving innovation. Therein lies one of the major challenges of this scientific generation: how to develop the new methods, management structures and technologies to manage the diversity, size, and complexity of current and future data sets and data streams.  This solicitation addresses that challenge by creating a set of exemplar national and global data research infrastructure organizations (dubbed DataNet Partners) that provide unique opportunities to communities of researchers to advance science and/or engineering research and learning.</blockquote>

The introduction in the solicitation<ref name="datanetsolicitation">{{cite web
|url=http://www.nsf.gov/publications/pub_summ.jsp?ods_key=nsf07601
|publisher=National Science Foundation
|title=Sustainable Digital Data Preservation and Access Network Partners Program Announcements & Information
|date=September 28, 2007
|accessdate=October 3, 2007
}}</ref> goes on to say:

<blockquote>Chapter 3 (Data, Data Analysis, and Visualization) of [http://www.nsf.gov/pubs/2007/nsf0728/index.jsp NSFs Cyberinfrastructure Vision for 21st century Discovery] presents a vision in which science and engineering digital data are routinely deposited in well-documented form, are regularly and easily consulted and analyzed by specialists and non-specialists alike, are openly accessible while suitably protected, and are reliably preserved. The goal of this solicitation is to catalyze the development of a system of science and engineering data collections that is open, extensible and evolvable.</blockquote>

The initial plan called for a $100 million initiative: five awards of $20&nbsp;million each over five years with the possibility of continuing funding.  Awards were given in two rounds. In the first round, for which  full proposals were due on March 21, 2008, two DataNet proposals were awarded. [[DataONE]],<ref>{{cite web|author=William Michener et al |url=https://www.dataone.org |title=DataONE: Observation Network for Earth |publisher=www.dataone.org | accessdate=2013-01-19}}</ref> led by William Michener at the [[University of New Mexico]] covers ecology, evolutionary, and earth science. The Data Conservancy,<ref>{{cite web|author=Sayeed Choudhury et al |url=https://dataconservancy.org |title=Data Conservancy |publisher=dataconservancy.org | accessdate=2013-01-19}}</ref> led by Sayeed Choudhury of [[Johns Hopkins University]], focuses on astronomy, earth science, life sciences, and social science. 

For the second round, preliminary proposals were due on October 6, 2008 and full proposals on February 16, 2009. Awards from the second round were greatly delayed, and funding was reduced substantially from $20 million per project to $8 million.<ref>{{cite web|author=National Science Foundation |url=http://www.nsf.gov/awardsearch/simpleSearchResult?queryText=%22datanet+full+proposal%3A%22 |title=NSF DataNet Awards |publisher=www.nsf.gov | accessdate=2013-01-19}}</ref> Funding for three second round projects began in Fall 2011. SEAD: Sustainable Environment through Actionable Data,<ref>{{cite web|author=[[Margaret Hedstrom]] et al |url=http://sead-data.net/ |title=SEAD Sustainable Environment - Actionable Data |publisher=sead-data.net | accessdate=2013-01-19}}</ref> led by [[Margaret Hedstrom]] of the [[University of Michigan]], seeks to provide data curation software and services for the "long tail" of small- and medium-scale data producers in the domain of sustainability science. The DataNet Federation Consortium,<ref>{{cite web|author=[[Reagan Moore]] et al |url=http://datafed.org/ |title=DataNet Federation Consortium |publisher=datafed.org | accessdate=2013-01-19}}</ref> led by Reagan Moore of the [[University of North Carolina]], uses the integrated Rule-Oriented Data System (iRODS) to provide data grid infrastructure for science and engineering. ''Terra Populus'',<ref>{{cite web|author=[[Steven Ruggles]] et al |url=http://www.terrapop.org/ |title=Terra Populus: Integrated Data on Population and the Environment |publisher=terrapop.org | accessdate=2013-01-19}}</ref> led by [[Steven Ruggles]] of the [[University of Minnesota]] focuses on tools for data integration across the domains of social science and environmental data, allowing interoperability of the three major data formats used in these domains: microdata, areal data, and raster data.

==References==
{{reflist|30em}}

==External links==
* [http://www.dataone.org DataONE]
* [http://dataconservancy.org/ Data Conservancy]
* [http://sead-data.net/ SEAD Sustainable Environment - Actionable Data]
* [http://datafed.org/ DataNet Federation Consortium]
* [http://www.terrapop.org/ Terra Populus: Integrated Data on Population and the Environment] 
 

[[Category:National Science Foundation]]
[[Category:Science and technology in the United States]]
[[Category:Information retrieval]]
[[Category:Digital library projects]]
>>EOP<<
114<|###|>Nearest neighbor search
'''Nearest neighbor search''' ('''NNS'''), also known as '''proximity search''', '''similarity search''' or '''[[Closest pair of points problem|closest point search]]''',  is an [[optimization problem]] for finding closest (or most similar) points. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set ''S'' of points in a space ''M'' and a query point ''q''&nbsp;&nbsp;''M'', find the closest point in ''S'' to ''q''. [[Donald Knuth]] in vol. 3 of ''[[The Art of Computer Programming]]'' (1973) called it the '''post-office problem''', referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a ''k''-NN search, where we need to find the ''k'' closest points.

Most commonly ''M'' is a  [[metric space]] and dissimilarity is expressed as a [[distance metric]], which is symmetric and satisfies the [[triangle inequality]]. Even more common, ''M'' is taken to be the ''d''-dimensional [[vector space]] where dissimilarity is measured using the [[Euclidean distance]], [[Taxicab geometry|Manhattan distance]] or other [[Statistical distance|distance metric]]. However, the dissimilarity function can be arbitrary. One example are asymmetric [[Bregman divergence]]s, for which the triangle inequality does not hold.<ref name=Cayton2008>{{Cite journal
 | last1 = Cayton | first1 = Lawerence
 | year = 2008
 | title =  Fast nearest neighbor retrieval for bregman divergences.
 | journal = Proceedings of the 25th international conference on Machine learning
 | pages = 112119
}}</ref>

==Applications==

The nearest neighbor search problem arises in numerous fields of application, including:
*[[Pattern recognition]] - in particular for [[optical character recognition]]
*[[Statistical classification]]- see [[k-nearest neighbor algorithm]]
*[[Computer vision]]
*[[Computational Geometry]] - see [[Closest pair of points problem]]
*[[Database]]s - e.g. [[content-based image retrieval]]
*[[Coding theory]] - see [[Decoding methods|maximum likelihood decoding]]
*[[Data compression]] - see [[MPEG-2]] standard
*[[Recommender system|Recommendation systems]], e.g. see [[Collaborative filtering]]
*[[Internet marketing]] - see [[contextual advertising]] and [[behavioral targeting]]
*[[DNA sequencing]]
*[[Spell checking]] - suggesting correct spelling
*[[Plagiarism detection]]
*[[Contact searching algorithms in FEA]]
*[[Similarity score]]s for predicting career paths of professional athletes.
*[[Cluster analysis]] - assignment of a set of observations into subsets (called clusters) so that observations in the same cluster are similar in some sense, usually based on [[Euclidean distance]]
*[[Chemical similarity]]
*[[Motion planning#Sampling-Based Algorithms|Sampling-Based Motion Planning]]

==Methods==

Various solutions to the NNS problem have been proposed.  The quality and usefulness of the algorithms are determined by the time complexity of queries as well as the space complexity of any search data structures that must be maintained. The informal observation usually referred to as the [[curse of dimensionality]] states that there is no general-purpose exact solution for NNS in high-dimensional Euclidean space using polynomial preprocessing and polylogarithmic search time.

===Linear search===
The simplest solution to the NNS problem is to compute the distance from the query point to every other point in the database, keeping track of the "best so far".  This algorithm, sometimes referred to as the naive approach, has a [[running time]] of ''O''(''dN'') where ''N'' is the [[cardinality]] of ''S'' and ''d'' is the dimensionality of ''M''.  There are no search data structures to maintain, so linear search has no space complexity beyond the storage of the database. Naive search can, on average, outperform space partitioning approaches on higher dimensional spaces.<ref>{{cite web|title=A quantitative analysis and performance study for similarity search methods in high dimensional spaces|author=Weber, Schek, Blott | url=http://www.vldb.org/conf/1998/p194.pdf}}</ref>

===Space partitioning===
Since the 1970s, [[branch and bound]] methodology has been applied to the problem. In the case of Euclidean space this approach is known as [[spatial index]] or spatial access methods. Several [[Space partitioning|space-partitioning]] methods have been developed for solving the NNS problem.  Perhaps the simplest is the [[k-d tree]], which iteratively bisects the search space into two regions containing half of the points of the parent region.  Queries are performed via traversal of the tree from the root to a leaf by evaluating the query point at each split. Depending on the distance specified in the query, neighboring branches that might contain hits may also need to be evaluated. For constant dimension query time, average complexity is ''O''(log&nbsp;''N'') <ref>{{cite web|title=An introductory tutorial on KD trees|author=Andrew Moore | url=http://www.autonlab.com/autonweb/14665/version/2/part/5/data/moore-tutorial.pdf?branch=main&language=en}}</ref> in the case of randomly distributed points, worst case complexity analyses have been performed.<ref name=Lee1977>{{Cite journal
 | last1 = Lee | first1 = D. T. | author1-link = Der-Tsai Lee
 | last2 = Wong | first2 = C. K.
 | year = 1977
 | title = Worst-case analysis for region and partial region searches in multidimensional binary search trees and balanced quad trees
 | journal = Acta Informatica
 | volume = 9
 | issue = 1
 | pages = 2329
 | doi = 10.1007/BF00263763
 | postscript = .
}}</ref>
Alternatively the [[R-tree]] data structure was designed to support nearest neighbor search in dynamic context, as it has efficient algorithms for insertions and deletions such as the [[R* tree]].<ref>{{cite doi|10.1145.2F223784.223794}}</ref> R-trees can yield nearest neighbors not only for Euclidean distance, but can also be used with other distances.

In case of general metric space branch and bound approach is known under the name of [[metric trees]]. Particular examples include [[vp-tree]] and [[BK-tree]].

Using a set of points taken from a 3-dimensional space and put into a [[Binary space partitioning|BSP tree]], and given a query point taken from the same space, a possible solution to the problem of finding the nearest point-cloud point to the query point is given in the following description of an algorithm.  (Strictly speaking, no such point may exist, because it may not be unique.  But in practice, usually we only care about finding any one of the subset of all point-cloud points that exist at the shortest distance to a given query point.)  The idea is, for each branching of the tree, guess that the closest point in the cloud resides in the half-space containing the query point.  This may not be the case, but it is a good heuristic.  After having recursively gone through all the trouble of solving the problem for the guessed half-space, now compare the distance returned by this result with the shortest distance from the query point to the partitioning plane.  This latter distance is that between the query point and the closest possible point that could exist in the half-space not searched.  If this distance is greater than that returned in the earlier result, then clearly there is no need to search the other half-space.  If there is such a need, then you must go through the trouble of solving the problem for the other half space, and then compare its result to the former result, and then return the proper result.  The performance of this algorithm is nearer to logarithmic time than linear time when the query point is near the cloud, because as the distance between the query point and the closest point-cloud point nears zero, the algorithm needs only perform a look-up using the query point as a key to get the correct result.

===Locality sensitive hashing===

[[Locality sensitive hashing]] (LSH) is a technique for grouping points in space into 'buckets' based on some distance metric operating on the points. Points that are close to each other under the chosen metric are mapped to the same bucket with high probability.<ref>{{cite web|author=A. Rajaraman and J. Ullman| url=http://infolab.stanford.edu/~ullman/mmds.html |title=Mining of Massive Datasets, Ch. 3. |year=2010}}</ref>

===Nearest neighbor search in spaces with small intrinsic dimension===

The [[cover tree]] has a theoretical bound that is based on the dataset's [[doubling constant]]. The bound on search time is ''O''(''c''<sup>12</sup>&nbsp;log&nbsp;''n'') where ''c''  is the [[Expansivity constant|expansion constant]] of the dataset.

===Vector approximation files===

In high dimensional spaces, tree indexing structures become useless because an increasing percentage of the nodes need to be examined anyway. To speed up linear search, a compressed version of the feature vectors stored in RAM is used to prefilter the datasets in a first run. The final candidates are determined in a second stage using the uncompressed data from the disk for distance calculation.<ref>{{cite web|title=An Approximation-Based Data Structure for Similarity Search|author=Weber, Blott}}</ref>

===Compression/clustering based search===
The VA-file approach is a special case of a compression based search, where each feature component is compressed uniformly and independently. The optimal compression technique in multidimensional spaces is Vector Quantization (VQ), implemented through clustering. The database is clustered and the most "promising" clusters are retrieved. Huge gains over VA-File, tree-based indexes and sequential scan have been observed.<ref>{{cite web|title=Adaptive cluster-distance bounding for similarity search in image databases|author=Ramaswamy, Rose, ICIP 2007}}</ref><ref>{{cite web|title=Adaptive cluster-distance bounding for high-dimensional indexing|author=Ramaswamy, Rose, TKDE 2010}}</ref> Also note the parallels between clustering and LSH.

===Greedy walks===
One possible way to solve NNS is to construct a graph <math>G(V,E)</math>, where every point <math>x_i \in S </math> is uniquely associated with vertex <math>v_i \in V </math>. The search of the point in the set ''S'' closest to the query ''q'' takes the form of the search of vertex in the graph <math>G(V,E)</math>.
One of the basic vertex search algorithms in graphs with metric objects is the greedy search algorithm. It starts from the random vertex <math>v_i \in V </math>. The algorithm computes a distance value from the query q to each vertex from the neighborhood <math>\{v_j:(v_i,v_j) \in E\}</math> of  the current vertex <math>v_i</math>, and then selects a vertex with the minimal distance value. If the distance value between the query and the selected vertex is smaller than the one between the query and the current element, then the algorithm moves to the selected vertex, and it becomes new current vertex. The algorithm stops when it reaches a local minimum: a vertex whose neighborhood does not contain a vertex that is closer to the query than the vertex itself.
This idea was exploited in VoroNet system <ref name=voroNet>{{Cite journal
 | last1 = Olivier | first1 = Beaumont  
 | last2 = Kermarrec | first2 = Anne-Marie
 | last3 = Marchal | first3 = Loris 
 | last4 = Riviere | first4 = Etienne   
 | year = 2006
 | title = VoroNet: A scalable object network based on Voronoi tessellations
 | journal = INRIA
 | volume = RR-5833
 | issue = 1
 | pages = 2329
 | doi = 10.1007/BF00263763
 | postscript = .
}}</ref> for the plane, in RayNet system <ref name=rayNet>{{Cite journal
 | last1 = Olivier | first1 = Beaumont  
 | last2 = Kermarrec | first2 = Anne-Marie
 | last4 = Riviere | first4 = Etienne   
 | year = 2007
 | title = Peer to Peer Multidimensional Overlays: Approximating Complex Structures
 | journal = Principles of Distributed Systems
 | volume =  4878
 | issue = .
 | pages = 315328
 | doi = 10.1007/978-3-540-77096-1_23
 | isbn = 978-3-540-77095-4
 | postscript = .
}}</ref> for the <math>\mathbb{E}^n</math> and for the general metric space in Metrized Small World algorithm <ref name=msw2014>{{Cite journal
 | last1 = Malkov | first1 = Yury  
 | last2 = Ponomarenko | first2 = Alexander
 | last3 = Krylov | first3 = Vladimir 
 | last4 = Logvinov | first4 = Andrey   
 | year = 2014
 | title = Approximate nearest neighbor algorithm based on navigable small world graphs
 | journal = Information Systems
 | volume = 45
 | pages = 6168
 | doi = 10.1016/j.is.2013.10.006
 | postscript = .
}}</ref>

==Variants==

There are numerous variants of the NNS problem and the two most well-known are the [[K-nearest neighbor algorithm|''k''-nearest neighbor search]] and the [[&epsilon;-approximate nearest neighbor search]].

===<span id="K-nearest neighbor"> ''k''-nearest neighbor </span>===

[[K-nearest neighbor algorithm|''k''-nearest neighbor search]] identifies the top ''k'' nearest neighbors to the query.  This technique is commonly used in predictive analytics to estimate or classify a point based on the consensus of its neighbors. ''k''-nearest neighbor graphs are graphs in which every point is connected to its ''k'' nearest neighbors.

===Approximate nearest neighbor===
In some applications it may be acceptable to retrieve a "good guess" of the nearest neighbor. In those cases, we can use an algorithm which doesn't guarantee to return the actual nearest neighbor in every case, in return for improved speed or memory savings. Often such an algorithm will find the nearest neighbor in a majority of cases, but this depends strongly on the dataset being queried.

Algorithms that support the approximate nearest neighbor search include [[Locality-sensitive hashing#LSH algorithm for nearest neighbor search|locality-sensitive hashing]], [[best bin first]] and [[balanced box-decomposition tree]] based search.<ref>S. Arya, [[David Mount|D. M. Mount]], [[Nathan Netanyahu|N. S. Netanyahu]], R. Silverman and A. Wu, An optimal algorithm for approximate nearest neighbor searching, Journal of the ACM, 45(6):891-923, 1998. [http://www.cse.ust.hk/faculty/arya/pub/JACM.pdf]</ref>

===Nearest neighbor distance ratio===

[[Nearest neighbor distance ratio]] do not apply the threshold on the direct distance from the original point to the challenger neighbor but on a ratio of it depending on the distance to the previous neighbor. It is used in [[Content-based image retrieval|CBIR]] to retrieve pictures through a "query by example" using the similarity between local features. More generally it is involved in several [[Pattern matching|matching]] problems.

===Fixed-radius near neighbors===

[[Fixed-radius near neighbors]] is the problem where one wants to efficiently find all points given in [[Euclidean space]] within a given fixed distance from a specified point. The data structure should work on a distance which is fixed however the query point is arbitrary.

===All nearest neighbors===

For some applications (e.g. [[entropy estimation]]), we may have ''N'' data-points and wish to know which is the nearest neighbor ''for every one of those N points''. This could of course be achieved by running a nearest-neighbor search once for every point, but an improved strategy would be an algorithm that exploits the information redundancy between these ''N'' queries to produce a more efficient search. As a simple example: when we find the distance from point ''X'' to point ''Y'', that also tells us the distance from point ''Y'' to point ''X'', so the same calculation can be reused in two different queries.

Given a fixed dimension, a semi-definite positive norm (thereby including every  [[lp space|L<sup>p</sup> norm]]), and ''n'' points in this space, the nearest neighbour of every point can be found in ''O''(''n''&nbsp;log&nbsp;''n'') time and the ''m'' nearest neighbours of every point can be found in ''O''(''mn''&nbsp;log&nbsp;''n'') time.<ref>{{citation
 | last = Clarkson | first = Kenneth L. | author-link = Kenneth L. Clarkson
 | contribution = Fast algorithms for the all nearest neighbors problem
 | doi = 10.1109/SFCS.1983.16
 | pages = 226232
 | title = 24th IEEE Symp. Foundations of Computer Science, (FOCS '83)
 | year = 1983| isbn = 0-8186-0508-1 }}.</ref><ref name=Vaidya>{{Cite journal
 | doi = 10.1007/BF02187718
 | last1 = Vaidya | first1 = P. M.
 | year = 1989
 | title = An ''O''(''n''&nbsp;log&nbsp;''n'') Algorithm for the All-Nearest-Neighbors Problem 
 | journal = [[Discrete and Computational Geometry]]
 | volume = 4
 | issue = 1
 | pages = 101115
 | url = http://www.springerlink.com/content/p4mk2608787r7281/?p=09da9252d36e4a1b8396833710ef08cc&pi=8
 | postscript = .
}}</ref>

==See also==
{{div col|colwidth=20em}}
* [[Range search]]
* [[Set cover problem]]
*[[Statistical distance]]
*[[Closest pair of points problem]]
*[[Ball tree]]
*[[Cluster analysis]]
*[[Neighbor joining]]
*[[Content-based image retrieval]]
*[[Curse of dimensionality]]
*[[Digital signal processing]]
*[[Dimension reduction]]
*[[Fixed-radius near neighbors]]
*[[Fourier analysis]]
*[[Instance-based learning]]
*[[k-nearest neighbor algorithm|''k''-nearest neighbor algorithm]]
*[[Linear least squares (mathematics)|Linear least squares]]
*[[Locality sensitive hashing]]
*[[Multidimensional analysis]]
*[[Nearest-neighbor interpolation]]
*[[Principal component analysis]]
*[[Singular value decomposition]]
*[[Time series]]
*[[Voronoi diagram]]
*[[Wavelet]]
*[[MinHash]]
{{div col end}}

==Notes==
<references/>

==References==
*Andrews, L.. A template for the nearest neighbor problem.  ''C/C++ Users Journal'', vol. 19, no 11 (November 2001), 40 - 49, 2001, ISSN:1075-2838, [http://www.ddj.com/architect/184401449 www.ddj.com/architect/184401449]
*Arya, S., D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Y. Wu.  An Optimal Algorithm for Approximate Nearest Neighbor Searching in Fixed Dimensions.  ''Journal of the ACM'', vol. 45, no. 6, pp.&nbsp;891923
*Beyer, K., Goldstein, J., Ramakrishnan, R., and Shaft, U. 1999. When is nearest neighbor meaningful? In Proceedings of the 7th ICDT, Jerusalem, Israel.
*Chung-Min Chen and Yibei Ling - A Sampling-Based Estimator for Top-k Query. ICDE 2002: 617-627
*Samet, H. 2006. Foundations of Multidimensional and Metric Data Structures. Morgan Kaufmann. ISBN 0-12-369446-9
*Zezula, P., Amato, G., Dohnal, V., and Batko, M. Similarity Search - The Metric Space Approach. Springer, 2006. ISBN 0-387-29146-6

==Further reading==
*{{cite book | last = Shasha | first = Dennis | title = High Performance Discovery in Time Series | publisher = Springer | location = Berlin | year = 2004 | isbn = 0-387-00857-8 }}

==External links==
*[http://simsearch.yury.name/tutorial.html Nearest Neighbors and Similarity Search]  a website dedicated to educational materials, software, literature, researchers, open problems and events related to NN searching. Maintained by Yury Lifshits
*[http://sswiki.tierra-aoi.net Similarity Search Wiki]  a collection of links, people, ideas, keywords, papers, slides, code and data sets on nearest neighbours
*[http://www.kgraph.org KGraph]  a C++ library for fast approximate nearest neighbor search with user-provided distance metric by Wei Dong.
*[http://www.cs.ubc.ca/research/flann/ FLANN]  a library for performing fast approximate nearest neighbor searches in high dimensional spaces  by Marius Muja and David G. Low
*[http://sisap.org/?f=library Metric Spaces Library]  An open-source C-based library for metric space indexing by Karina Figueroa, Gonzalo Navarro, Edgar Chavez
*[https://github.com/searchivarius/NonMetricSpaceLib  Non-Metric Space Library]  An open-source C++ library for exact and approximate searching in non-metric and metric spaces
*[http://www.cs.umd.edu/~mount/ANN/ ANN]  A library for Approximate Nearest Neighbor searching by David M. Mount and Sunil Arya
*[http://www.irisa.fr/texmex/people/jegou/ann.php Product Quantization]  Matlab implementation of approximate nearest neighbor search in the compressed domain by Herve Jegou
*[http://lsd.fi.muni.cz/trac/messif MESSIF]  Metric Similarity Search Implementation Framework by Michal Batko and David Novak
*[http://www.obsearch.net/ OBSearch]  Similarity Search engine for Java (GPL); implementation by Arnoldo Muller, developed during Google Summer of Code 2007
*[http://mrim.imag.fr/georges.quenot/freesoft/knnlsb/ KNNLSB]  K Nearest Neighbors Linear Scan Baseline (distributed, LGPL); implementation by Georges Quenot (LIG-CNRS)
*[http://neartree.sourceforge.net/ NearTree]  An API for finding nearest neighbors among points in spaces of arbitrary dimensions by Lawrence C. Andrews and Herbert J. Bernstein
*[http://nearpy.io/ NearPy]  Python framework for fast approximated nearest neighbor search by Ole Krause-Sparmann
*[http://www.cgal.org/Pkg/SpatialSearchingD dD Spatial Searching] in [[CGAL]]  the Computational Geometry Algorithms Library
*[https://github.com/ryanrhymes/panns Panns]  A Python library for searching approximate nearest neighbors, optimized for large dataset with high dimensional features, developed by Liang Wang

{{DEFAULTSORT:Nearest Neighbor Search}}
[[Category:Approximation algorithms]]
[[Category:Classification algorithms]]
[[Category:Data mining]]
[[Category:Discrete geometry]]
[[Category:Geometric algorithms]]
[[Category:Information retrieval]]
[[Category:Machine learning]]
[[Category:Numerical analysis]]
[[Category:Mathematical optimization]]
[[Category:Searching]]
[[Category:Search algorithms]]
>>EOP<<
120<|###|>Dragomir R. Radev
'''Dragomir R. Radev''' is a [[University of Michigan]] computer science professor and [[Columbia University]] computer science adjunct professor working on [[natural language processing]] and [[information retrieval]].  
He is currently working on the fields of open domain [[question answering]],  [[multi-document summarization]], and the application of NLP in Bioinformatics and Political Science.

Radev received his PhD in [[Computer Science]] from [[Columbia University]] in 1999. He is the secretary of [http://www.aclweb.org [[Association for Computational Linguistics|ACL]]] (2006present) and associate editor of [http://www.jair.org JAIR].

== Awards ==
As [[NACLO]] founder, Radev shared the [[Linguistic Society of America]] 2011 [http://www.lsadc.org/info/lsa-awards.cfm ''Linguistics, Language and the Public Award'']. He is the  Co-winner of the [http://polmeth.wustl.edu/about.php?page=awards Gosnell Prize (2006)].

== IOL==
Radev has served as the coach and led the US national team in the [[International Linguistics Olympiad|International Linguistics Olympiad (IOL)]] to several gold medals [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073][http://www.nsf.gov/news/news_summ.jsp?cntn_id=109891].

== Books ==
* Puzzles in Logic, Languages and Computation (2013) <ref>{{Cite web|url = http://www.springer.com/education+%26+language/linguistics/book/978-3-642-34371-1|title = Puzzles in Logic, Languages and Computation|date = |accessdate = |website = |publisher = |last = |first = }}</ref>
* Mihalcea and Radev (2011) [http://www.cambridge.org/gb/knowledge/isbn/item5980387/?site_locale=en_GB ''Graph-based methods for NLP and IR'']

== Selected Papers ==
* SIGIR 1995 Generating summaries of multiple news articles
* ANLP 1997 Building a generation knowledge source using internet-accessible newswire
* Computational Linguistics 1998 Generating natural language summaries from multiple on-line sources
* ACL 1998 Learning correlations between linguistic indicators and semantic constraints: Reuse of context dependent descriptions of entities
* ANLP 2000 Ranking suspected answers to natural language questions using predictive annotation
* CIKM 2001 Mining the web for answers to natural language questions
* AAAI 2002 Towards CST-enhanced summarization
* ACL 2003 Evaluation challenges in large-scale multi-document summarization: the Mead project
* Information Processing and Management 2004 Centroid-based summarization of multiple documents
* J. of Artificial Intelligence Research 2004 LexRank: Graph-based lexical centrality as salience in text summarization
* J. of the American Association of Information Science and Technology 2005 Probabilistic question answering on the web
* Communications of the ACM 2005 NewsInEssence: summarizing online news topics
* EMNLP 2007 Semi-supervised classification for extracting protein interaction sentences using dependency parsing
* Bioinformatics 2008 Identifying gene-disease associations using centrality on a literature mined gene-interaction network
* IEEE Intelligent Systems 2008 natural language processing and the web
* NAACL 2009 Generating surveys of scientific paradigms
* Nucleic Acids Research 2009 Michigan molecular interactions r2: from interacting proteins to pathways
* J. of the American Association of Information Science and Technology 2009 Visual overviews for discovering key papers and influences across research fronts
* KDD 2010 Divrank: the interplay of prestige and diversity in information networks
* American J. of Political Science 2010 How to Analyze Political Attention with Minimal Assumptions and Costs
* Arxiv 2011 The effect of linguistic constraints on the large scale organization of language
* J. of Biomedical Semantics 2011 Mining of vaccine-associated ifn-gamma gene interaction networks using the vaccine ontology

==External links==
* [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073 Team USA Brings Home the Linguistics Gold]
* [http://www.eecs.umich.edu/eecs/about/articles/2011/Radev-LSA11.html Dragomir Radev, Co-Founders Recognized as NACLO Receives Linguistics, Language and the Public Award]
* [http://www.eecs.umich.edu/eecs/about/articles/2010/Radev-Linguistics.html Dragomir Radev Coaches US Linguistics Team to Multiple Wins]
* [http://www.eecs.umich.edu/eecs/about/articles/2009/Radev-ACM-DM.html Dragomir Radev Honored as ACM Distinguished Scientist]
* [http://www.eecs.umich.edu/eecs/etc/news/shownews.cgi?428 Prof. Dragomir Radev Receives Gosnell Prize]

== References ==
{{reflist}}
<!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. --->
*
*
*
*

{{Persondata <!-- Metadata: see [[Wikipedia:Persondata]]. -->
| NAME              = Radev, Dragomir R.
| ALTERNATIVE NAMES =
| SHORT DESCRIPTION = American computer scientist
| DATE OF BIRTH     =
| PLACE OF BIRTH    =
| DATE OF DEATH     =
| PLACE OF DEATH    =
}}

{{DEFAULTSORT:Radev, Dragomir R.}}
[[Category:Year of birth missing (living people)]]
[[Category:Living people]]

[[Category:Columbia University alumni]]
[[Category:American computer scientists]]
[[Category:University of Michigan faculty]]
[[Category:Natural language processing]]
[[Category:Information retrieval]]
>>EOP<<
126<|###|>Category:Substring indices
{{cat main|Substring index}}

[[Category:String (computer science)]]
[[Category:Algorithms on strings]]
[[Category:String data structures]]
[[Category:Database index techniques]]
[[Category:Information retrieval]]
[[Category:Bioinformatics algorithms]]
>>EOP<<
132<|###|>Search engine technology
{{multiple issues|
{{Refimprove|date=May 2014}}
{{Tone|article|date=January 2013}}
}}
A search engine is a type of computer software used to search data in the form of text or a database for specified information.<ref>{{cite web|title=Define search engine|url=http://www.webopedia.com/TERM/S/search_engine.html|accessdate=1 June 2014}}</ref>

Search engines normally consist of spiders (also known as bots) which roam the web searching for links and keywords. They send collected data back to the indexing software which categorizes and adds the links to databases with their related keywords. When you specify a search term the engine does not scan the whole web but extracts related links from the database.

==History of Search Technology==

{{Empty section|date=July 2014}}

== The Memex ==

The concept of hypertext and a memory extension originates from an article that was published in [[The Atlantic Monthly]] in July 1945 written by [[Vannevar Bush]], titled [[As We May Think]].  Within this article Vannevar urged scientists to work together to help build a body of knowledge for all mankind. He then proposed the idea of a virtually limitless, fast, reliable, extensible, associative memory storage and retrieval system. He named this device a [[memex]].<ref>{{cite journal|last1=Yeo|first1=Richard|title=Before Memex: Robert Hooke, John Locke, and Vannevar Bush on External Memory|journal=Science in Context|date=30 January 2007|volume=20|issue=01|page=21|doi=10.1017/S0269889706001128}}</ref>

Bush regarded the notion of associative indexing as his key conceptual contri- bution. As he explained, this was a provision whereby any item may be caused at will to select immediately and automatically another. This is the essential feature of the memex. The process of tying two items together is the important thing. This linking (as we now say) constituted a trail of documents that could be named, coded, and found again. Moreover, after the original two items were coupled, numerous items could be joined together to form a trail; they could be reviewed in turn, rapidly or slowly, by deflecting a lever like that used for turning the pages of a book. It is exactly as though the physical items had been gathered together from widely separated sources and bound together to form a new book<ref>{{cite journal|title=Before Memex: Robert Hooke, John Locke, and Vannevar Bush on External Memory|journal=Science in Context|date=30 January 2007|volume=20|issue=01|pages=2147|doi=10.1017/S0269889706001128|accessdate=1 June 2014|postscript=The example Bush gives is a quest to find information on the relative merits of the Turkish short bow and the English long bow in the crusades}}</ref>

All of the documents used in the memex would be in the form of microfilm copy acquired as such or, in the case of personal records, transformed to microfilm by the machine itself. Memex would also employ new retrieval techniques based on a new kind of associative indexing the basic idea of which is a provision whereby any item may be caused at will to select immediately and automatically another to create personal "trails" through linked documents. The new procedures, that Bush anticipated facilitating information storage and retrieval would lead to the development of wholly new forms of encyclopedia.

The most important mechanism, conceived by Bush and considered as closed to the modern hypertext systems is the associative trail. It would be a way to create a new linear sequence of microfilm frames across any arbitrary sequence of microfilm frames by creating a chained sequence of links in the way just described, along with personal comments and side trails.
The essential feature of the memex [is] the process of tying two items together... When the user is building a trail, he names it in his code book, and taps it out on his keyboard. Before him are the two items to be joined, projected onto adjacent viewing positions. At the bottom of each there are a number of blank code spaces, and a pointer is set to indicate one of these on each item. The user taps a single key, and the items are permanently joined... Thereafter, at any time, when one of these items is in view, the other can be instantly recalled merely by tapping a button below the corresponding code space.

In the article of Bush is not described any automatic search, nor any universal metadata scheme such as a standard library classification or a hypertext element set. Instead, when the user made an entry, such as a new or annotated manuscript, or image, he was expected to index and describe it in his personal code book. Later on, by consulting his code book, the user could retrace annotated and generated entries.

In 1965 Bush took part in the project INTREX of MIT, for developing technology for mechanization the processing of information for library use. In his 1967 essay titled "Memex Revisited", he pointed out that the development of the digital computer, the transistor, the video, and other similar devices had heightened the feasibility of such mechanization, but costs would delay its achievements. He was right again.

Ted Nelson, who later did pioneering work with first practical hypertext system and coined the term "hypertext" in the 1960s, credited Bush as his main influence.<ref>{{cite web|title=The MEMEX of Vannevar Bush|url=http://history-computer.com/Internet/Dreamers/Bush.html}}</ref>

== SMART ==

Gerard Salton, who died on August 28 of 1995, was the father of modern search technology. His teams at Harvard and Cornell developed the SMART informational retrieval system. Saltons Magic Automatic Retriever of Text included important concepts like the vector space model, Inverse Document Frequency (IDF), Term Frequency (TF), term discrimination values, and relevancy feedback mechanisms.

He authored a 56 page book called A Theory of Indexing which explained many of his tests upon which search is still largely based.

== String Search Engines ==

In 1987 an article was published detailing the development of a character string search engine (SSE) for rapid text retrieval on a double-metal 1.6-m n-well CMOS solid-state circuit with 217,600 transistors lain out on a 8.62x12.76-mm die area. The SSE accommodated a novel string-search architecture which combines a 512-stage finite-state automaton (FSA) logic with a content addressable memory (CAM) to achieve an approximate string comparison of 80 million strings per second. The CAM cell consisted of four conventional static RAM (SRAM) cells and a read/write circuit. Concurrent comparison of 64 stored strings with variable length was achieved in 50 ns for an input text stream of 10 million characters/s, permitting performance despite the presence of single character errors in the form of character codes. Furthermore, the chip allowed nonanchor string search and variable-length `don't care' (VLDC) string search.<ref>{{cite journal|last=Yamada|first=H.|author2=Hirata, M. |author3=Nagai, H. |author4= Takahashi, K. |title=A high-speed string-search engine|journal=IEEE Journal of Solid-State Circuits|date=Oct 1987|volume=22|issue=5|pages=829834|doi=10.1109/JSSC.1987.1052819|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1052819&url=http%3A%2F%2Fieeexplore.ieee.org%2Fstamp%2Fstamp.jsp%3Ftp%3D%26arnumber%3D1052819|accessdate=30 May 2014|publisher=IEEE}}</ref>

<!-- Potential source for article expansion:  http://ieeexplore.ieee.org/search/searchresult.jsp?queryText%3Dsearch-engine&sortType=asc_p_Publication_Year&pageNumber=1&resultAction=SORT -->

== Web Search Engines ==

=== Archie ===

The first web search engines was Archie, created in 1990<ref name="intelligent-technologies">{{cite book|author1=Priti Srinivas Sajja|author2=Rajendra Akerkar|title=Intelligent technologies for web applications|date=2012|publisher=CRC Press|location=Boca Raton|isbn=978-1-4398-7162-1|page=87|url=http://books.google.com/books?id=HqXxoWK7tucC&pg=PA87&lpg=PA87&dq=the+University+of+Nevada+System+Computing+Services+group+developed+Veronica.&source=bl&ots=Xt7TQz0a6Y&sig=vusKa34uORNCBI6lT3-sEy5qv-Q&hl=en&sa=X&ei=KzqOU7PCDcOlyATtt4L4DA&ved=0CEoQ6AEwBQ#v=onepage&q=the%20University%20of%20Nevada%20System%20Computing%20Services%20group%20developed%20Veronica.&f=false|accessdate=3 June 2014}}</ref> by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program "archives," but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on. For more information on where Archie is today, see:
http://www.bunyip.com/products/archie/

The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: Some administrator decides that he wants to make files available from his computer. He sets up a program on his computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, he or she connects to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol.

Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, "anonymous" FTP sites became repositories for files, allowing all users to post and retrieve them.

Even with archive sites, many important files were still scattered on small FTP servers. Unfortunately, these files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file.

Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.<ref name="wileyhistory">{{cite web|title=A History of Search Engines|url=http://www.wiley.com/legacy/compbooks/sonnenreich/history.html|publisher=Wiley|accessdate=1 June 2014}}</ref>

=== Veronica ===

In 1993, the University of Nevada System Computing Services group developed Veronica.<ref name="intelligent-technologies"/> It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.<ref name="wileyhistory"/>

=== The Lone Wanderer ===

The World Wide Web Wanderer, developed by Matthew Gray in 1993<ref>{{cite book|author1=Priti Srinivas Sajja|author2=Rajendra Akerkar|title=Intelligent technologies for web applications|date=2012|publisher=CRC Press|location=Boca Raton|isbn=978-1-4398-7162-1|page=86|url=http://books.google.com/books?id=HqXxoWK7tucC&pg=PA87&lpg=PA87&dq=the+University+of+Nevada+System+Computing+Services+group+developed+Veronica.&source=bl&ots=Xt7TQz0a6Y&sig=vusKa34uORNCBI6lT3-sEy5qv-Q&hl=en&sa=X&ei=KzqOU7PCDcOlyATtt4L4DA&ved=0CEoQ6AEwBQ#v=onepage&q=the%20University%20of%20Nevada%20System%20Computing%20Services%20group%20developed%20Veronica.&f=false|accessdate=3 June 2014}}</ref> was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database.

Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of time a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained.

In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways.

ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot doesn't run about eating up Net bandwidth.  Unfortunately, the disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they don't submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.<ref name="wileyhistory"/>

=== Excite ===

Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.
Their project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.<ref name="wileyhistory"/>

Excite was the first serious commercial search engine which launched in 1995.<ref>{{cite web|title=The Major Search Engines|url=http://www.pccua.edu/kholland/major_search_engines.htm|accessdate=1 June 2014|date=21 January 2014}}</ref> It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million.

=== Yahoo! ===

In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos.

As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory.

The Wanderer captured only URLs, which made it difficult to find things that werent explicitly described by their URL. Because URLs are rather cryptic to begin with, this didnt help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites.

=== Lycos ===

At Carnegie Mellon University during the July of 1994, Michael Mauldin, on leave from CMU,developed the Lycos search engine.

== Types of Web Search Engines ==

Search engines on the web are sites enriched with facility to search the content stored on other sites.  There is difference in the way various search engines work, but they all perform three basic tasks.<ref>{{cite book|author1=Priti Srinivas Sajja|author2=Rajendra Akerkar|title=Intelligent technologies for web applications|date=2012|publisher=CRC Press|location=Boca Raton|isbn=978-1-4398-7162-1|page=85|url=http://books.google.com/books?id=HqXxoWK7tucC&pg=PA87&lpg=PA87&dq=the+University+of+Nevada+System+Computing+Services+group+developed+Veronica.&source=bl&ots=Xt7TQz0a6Y&sig=vusKa34uORNCBI6lT3-sEy5qv-Q&hl=en&sa=X&ei=KzqOU7PCDcOlyATtt4L4DA&ved=0CEoQ6AEwBQ#v=onepage&q=the%20University%20of%20Nevada%20System%20Computing%20Services%20group%20developed%20Veronica.&f=false|accessdate=3 June 2014}}</ref>

# Finding and selecting full or partial content based on the keywords provided.
# Maintaining index of the content and referencing to the location they find
# Allowing users to look for words or combinations of words found in that index.

The process begins when a user enters a query statement into the system through the interface provided.

{| class="wikitable"
|-
! Type
! Example
! Description
|-
| Conventional
| librarycatalog
| Search by keyword, title, author, etc.
|-
| Text-based
| Lexis-Nexis,Google,Yahoo!
| Search by keywords. Limited search using queries in natural language.
|-
| Multimedia
| QBIC, WebSeek, SaFe
| Search by visual appearance (shapes, colors,..)
|-
| Q/A
| [[Stack Exchange]], NSIR
| Search in (restricted) natural language
|-
| Clustering Systems
| Vivisimo, Clusty
|
|-
| Research Systems
| Lemur, Nutch
|
|}

There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two.

Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine.

Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index.

In both cases, when you query a search engine to locate information, you're actually searching through the index that the search engine has created you are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index hasn't been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated.

So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for.

One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing.

Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered "important" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking.

Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. [[Google]]), database or structured data search engines (e.g. [[Dieselpoint]]), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and [[Yahoo!]], utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity.

==Search engine categories==

===Web search engines===
Search engines that are expressly designed for searching web pages, documents, and images were developed to facilitate searching through a large, nebulous blob of unstructured resources. They are engineered to follow a multi-stage process: crawling the infinite stockpile of pages and documents to skim the figurative foam from their contents, indexing the foam/buzzwords in a sort of semi-structured form (database or something), and at last, resolving user entries/queries to return mostly relevant results and links to those skimmed documents or pages from the inventory.

====Crawl====
In the case of a wholly textual search, the first step in classifying web pages is to find an index item that might relate expressly to the search term. In the past, search engines began with a small list of URLs as a so-called seed list, fetched the content, and parsed the links on those pages for relevant information, which subsequently provided new links. The process was highly cyclical and continued until enough pages were found for the searchers use.
These days, a continuous crawl method is employed as opposed to an incidental discovery based on a seed list. The crawl method is an extension of aforementioned discovery method. Except there is no seed list, because the system never stops worming.

Most search engines use sophisticated scheduling algorithms to decide when to revisit a particular page, to appeal to its relevance. These algorithms range from constant visit-interval with higher priority for more frequently changing pages to adaptive visit-interval based on several criteria such as frequency of chance, popularity, and overall quality of site. The speed of the web server running the page as well as resource constraints like amount of hardware or bandwidth also figure in.

====Link map====
The pages that are discovered by web crawls are often distributed and fed into another computer that creates a veritable map of resources uncovered. The bunchy clustermass looks a little like a graph, on which the different pages are represented as small nodes that are connected by  links between the pages. 
The excess of data is stored in multiple data structures that permit quick access to said data by certain algorithms that compute the popularity score of pages on the web based on how many links point to a certain web page, which is how people can access any number of resources concerned with diagnosing psychosis. Another example would be the accessibility/rank of web pages containing information on Mohamed Morsi versus the very best attractions to visit in Cairo after simply entering Egypt as a search term. One such algorithm, [[PageRank]], proposed by Google founders Larry Page and Sergey Brin, is well known and has attracted a lot of attention because it highlights repeat mundanity of web searches courtesy of students that dont know how to properly research subjects on Google.
The idea of doing link analysis to compute a popularity rank is older than PageRank. Other variants of the same idea are currently in use  grade schoolers do the same sort of computations in picking kickball teams. But in all seriousness, these ideas can be categorized into three main categories: rank of individual pages and nature of web site content. Search engines often differentiate between internal links and external links, because web masters and mistresses are not strangers to shameless self-promotion. Link map data structures typically store the anchor text embedded in the links as well, because anchor text can often provide a very good quality summary of a web pages content.

===Database Search Engines===
Searching for text-based content in databases presents a few special challenges from which a number of specialized search engines flourish. Databases can be slow when solving complex queries (with multiple logical or string matching arguments). Databases allow pseudo-logical queries which full-text searches do not use. There is no crawling necessary for a database since the data is already structured. However, it is often necessary to index the data in a more economized form to allow a more expeditious search.

===Mixed Search Engines===
Sometimes, data searched contains both database content and web pages or documents. Search engine technology has developed to respond to both sets of requirements. Most mixed search engines are large Web search engines, like Google. They search both through structured and unstructured data sources. Take for example, the word ball. In its simplest terms, it returns more than 40 variations on Wikipedia alone. Did you mean a ball, as in the social gathering/dance? A soccer ball? The ball of the foot? Pages and documents are crawled and indexed in a separate index. Databases are indexed also from various sources. Search results are then generated for users by querying these multiple indices in parallel and compounding the results according to rules.

<!-- 
Working on article, loosely pasting in snippets of information to use in improving 
article later, leaving all this in comments while I work on it

LOTS OF WORK TO DO

Potential sections to research into..

== Models of Information Retrieval ==
=== Boolean Model ===
=== Vector Model ===

== Document Preprocessing ==
# Tokenization ===
# Stemming ===
# The Porter Algorithm
# Storing, indexing, and searching text
#Inverted indexes

== Word Distributions ==
The Zipf distribution
The Benford distribution
Heap's law. TF*IDF. Vector space similarity and ranking.

== Retrieval evaluation ==
 Precision and Recall. F-measure. Reference collections. The TREC conferences.

== Automated indexing/labeling ==
. Compression and coding. Optimal codes.

== String matching ==
. Approximate matching.

== Query expansion ==. Relevance feedback.

== Text classification ==
. Naive Bayes. Feature selection. Decision trees.

Linear classifiers. k-nearest neighbors. Perceptron. Kernel methods. Maximum-margin classifiers. Support vector machines. Semi-supervised learning.
Lexical semantics and Wordnet.
Latent semantic indexing. Singular value decomposition. Vector space clustering. k-means clustering. EM clustering.
Random graph models. Properties of random graphs: clustering coefficient, betweenness, diameter, giant connected component, degree distribution.
Social network analysis. Small worlds and scale-free networks. Power law distributions. Centrality.
Graph-based methods. Harmonic functions. Random walks. PageRank. Hubs and authorities. Bipartite graphs. HITS. Models of the Web.

Crawling the web. Webometrics. Measuring the size of the web. The Bow-tie-method.
Hypertext retrieval. Web-based IR. Document closures. Focused crawling.
Question answering
Burstiness. Self-triggerability
Information extraction
Adversarial IR. Human behavior on the web. Text summarization

== Search Engine Parts ==

There are three main parts to every search engine: Spider, Index, and Web Interface.

=== Spider === 
   
A spider crawls the web. It follows links and scans web pages. All search engines have periods of deep crawl and quick crawl. During a deep crawl, the spider follows all links it can find and scans web pages in their entirety. During a quick crawl, the spider does not follow all links and may not scan pages in their entirety.

The job of the spider is to discover new pages and to collect copies of those pages, which are then analyzed in the index.

==== Crawl Rate ====

Pages that are considered important get crawled frequently. The crawl rate depends directly on link popularity and domain authority.

If many links point to a website, it may be an important site, so it makes sense to crawl it more often than a site with fewer links. This is also a money-saving issue. If search engines were to crawl all sites at an equal rate, it would take more time overall and cost more as a result.

=== Index ===

The index is the place where search engines keep basic copies of web pages and sort search results. When you a do a search, search engines do not search the web; they show results from their index. The number of pages in the index does not represent the entire web, but the number of pages that the spider has discovered, scanned and saved.

The index is the place where search engineers apply algorithms, and it is the place where rankings are partially determined. Search engineers may choose to apply an algorithm to the entire index, or only to a portion of it.

==== Datacenters and Different Indexes ====

Search engines have multiple datacenters around the world. When you enter a search term, your query is directed to the closest datacenter.

Different datacenters may have slightly different indexes, especially during an update. As a result, search results may differ depending on your location.

== History ==

=== Meta Tags ===

Meta tags were designed to help search engines sort web pages. Pages included keywords in meta tags telling search engines about the contents of each page. For a short time meta tags worked and helped search engines serve relevant results, but over time marketers learned they could easily rank by stuffing those tags with keywords.

As a result, search engine optimization in those days became about cramming "loans, loans, loans, loans, loans" into the meta tag. Search engines got spammed beyond being of any use, and many faced an exodus of users as a result.

Yahoo started as web directory in 1994 and outsourced their search until 2004. Google launched in 1996 and did not have a successful business model until 2001. Microsoft did not come on the search engine scene until 2003.
or more information on search engine history, you may want to investigate Search Engine History, a site entirely devoted to this topic. It also touches on the history of search engine optimization. Additionally, Web Master World has an excellent thread that covers the history of SEO.

Web Interface

When you search using a web interface (like Google.com), in many cases results are already presorted to a certain extent. The degree to which results are presorted depends on the complexity of the algorithm. If the time to apply an algorithm to the index is considerable, then that algorithm is applied in advance. On the other hand, some algorithms are applied at the time when the search query is requested.

Search queries go through analysis to determine the possible intent behind the query. Google is currently leading in this area.

Stop Words

"Stop words" are words that are frequently used in the English language. Those words include a, the, all, also, but, down, full, much etc. They are words that are used by everyone regardless of the topic. Generally, search engines ignore "stop" words and will usually correct your search to exclude them. For example, when you search for "cat and dog" search engines will exclude "and" and only search for "cat" "dog."

Google does use stop words to an extent.

Keyword Density

Keyword density is a measure of how often a word appears on the page in relation to other words. It is an over-hyped measurement that doesnt help in search rankings. Search engines use far more than keyword density for on-page analysis. Their technology includes the location of terms on the page, word proximity and natural language processing.

Google has purchased Applied Semantics for its AdSense Network, but may also be using this technology for on-page analysis. Additionally, please keep in mind that one of Googles current projects involves scanning thousands of books, from which it may learn more about natural language patterns.

Location of Terms on The Page

By analyzing how terms are located in relation to each other on the page, search engines can determine partial relevancy of the page. The closer terms are to each other, the more relevant a page is.

In many cases, keywords appear separately from each other throughout the page. This is considered normal in most cases, but be sure to include a term together at least once in the title, heading or paragraph.

Link Analysis

Link analysis is at the core of all search engine relevancy. Apart from Page Rank and general link popularity, Google looks at: link anchor text, the page from which the link comes, age of the link, location of the link, title of the page from which the link comes, authority of the linking page and more.

Links are the biggest quality indicators that search engines have at the moment. Before search engines existed, and before the web was commercialized it was much harder to find information. All you had to rely on was links. There were few if any spammers, and people who found interesting sites shared those sites with others by placing a link. Also, the first web pages and servers were universities and colleges; this is why Google is biased toward .edu domains  they were the first on the scene, and usually contain quality content and resources.

As the web became commercial and Googles Page Rank well known, links became a form of advertising, where a link could be bought or artificially made by spammers. This is the reason for Googles bias toward older links and links from trusted domains.

Yahoo put less weight on link analysis than Google, while Ask.com is more about "authoritative hubs." Ask.com generally has a harder time ranking documents unless theres a community around a topic.

Size and Length of the Page

Theres no "best" page copy length for ranking on search results. Search engines have specifically addressed this issue, and both long content and short content have equal chances to rank.

Behavioral Feedback

All major search engines such as Google, Yahoo, Live and Ask collect user feedback about web pages. They look at search queries, prior search queries, time interval between those queries and semantic relationships in order to learn more about intent. They also track click through rates for different listings. If, for example, users click on a listing and then go back right away, search engines may remove that listing and artificially lower its position for one or more keywords.

This brings up the fact that user experience is becoming an important part of SEO. As search engines collect more data, they are constantly learning to interpret it. As they get better at it, retaining users on your pages for a certain time period (maybe a benchmark for an industry) may become an important factor in the SEO game.

Behavior feedback is currently used in personalized search.
<ref>{{cite web|url=http://www.seochat.com/c/a/search-engine-news/the-history-of-search-and-search-technology/|accessdate=1 June 2014}}</ref>
-->

==See also==
*[[Database search engine]]
*[[Enterprise search]]
*[[Search engine]]
*[[Disambiguation]]
*[[Search engine indexing]]
*[[Web crawler]]
*[[Structured Search]]

==External links==
* [http://www.searchtools.com/info/database-search.html Searching for Text Information in Databases]
* [http://www.urbandictionary.com/define.php?term=Searchency Searchency]

==References==
{{reflist}}

{{DEFAULTSORT:Search Engine Technology}}
[[Category:Internet search engines]]
[[Category:Information retrieval]]
>>EOP<<
138<|###|>Okapi BM25
In [[information retrieval]], '''Okapi BM25''' is a [[ranking function]] used by [[search engine]]s to rank matching documents according to their [[Relevance (information retrieval)|relevance]] to a given search query. It is based on the [[Probabilistic relevance model|probabilistic retrieval framework]] developed in the 1970s and 1980s by [[Stephen E. Robertson]], [[Karen Sparck Jones]], and others.

The name of the actual ranking function is BM25. To set the right context, however, it usually referred to as "Okapi BM25", since the Okapi information retrieval system, implemented at [[London]]'s [[City University, London|City University]] in the 1980s and 1990s, was the first system to implement this function.

BM25, and its newer variants, e.g. BM25F (a version of BM25 that can take document structure and anchor text into account), represent state-of-the-art [[TF-IDF]]-like retrieval functions used in document retrieval, such as [[web search]].

== The ranking function ==

BM25 is a [[Bag of words model|bag-of-words]] retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of the inter-relationship between the query terms within a document (e.g., their relative proximity). It is not a single function, but actually a whole family of scoring functions, with slightly different components and parameters. One of the most prominent instantiations of the function is as follows.

Given a query <math>Q</math>, containing keywords <math>q_1, ..., q_n</math>, the BM25 score of a document <math>D</math> is:

:<math> \text{score}(D,Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})},</math>

where <math>f(q_i, D)</math> is <math>q_i</math>'s [[term frequency]] in the document <math>D</math>, <math>|D|</math> is the length of the document <math>D</math> in words, and <math>avgdl</math> is the average document length in the text collection from which documents are drawn. <math>k_1</math> and <math>b</math> are free parameters, usually chosen, in absence of an advanced optimization, as <math>k_1 \in [1.2,2.0]</math> and <math>b = 0.75</math>.<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schutze. ''An Introduction to Information Retrieval'', Cambridge University Press, 2009, p. 233.</ref> <math>\text{IDF}(q_i)</math> is the IDF ([[inverse document frequency]]) weight of the query term <math>q_i</math>. It is usually computed as:

:<math>\text{IDF}(q_i) = \log \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5},</math>

where <math>N</math> is the total number of documents in the collection, and <math>n(q_i)</math> is the number of documents containing <math>q_i</math>.

There are several interpretations for IDF and slight variations on its formula. In the original BM25 derivation, the IDF component is derived from the [[Binary Independence Model]].

Please note that the above formula for IDF shows potentially major drawbacks when using it for terms appearing in more than half of the corpus documents. These terms' IDF is negative, so for any two almost-identical documents, one which contains the term and one which does not contain it, the latter will possibly get a larger score.
This means that terms appearing in more than half of the corpus will provide negative contributions to the final document score. This is often an undesirable behavior, so many real-world applications would deal with this IDF formula in a different way:

* Each summand can be given a floor of 0, to trim out common terms;
* The IDF function can be given a floor of a constant <math>\epsilon</math>, to avoid common terms being ignored at all;
* The IDF function can be replaced with a similarly shaped one which is non-negative, or strictly positive to avoid terms being ignored at all.

== IDF information theoretic interpretation ==
Here is an interpretation from information theory. Suppose a query term <math>q</math> appears in <math>n(q)</math> documents. Then a randomly picked document <math>D</math> will contain the term with probability <math>\frac{n(q)}{N}</math> (where <math>N</math> is again the cardinality of the set of documents in the collection). Therefore, the [[information]] content of the message "<math>D</math> contains <math>q</math>" is:

:<math>-\log \frac{n(q)}{N} = \log \frac{N}{n(q)}.</math>

Now suppose we have two query terms <math>q_1</math> and <math>q_2</math>. If the two terms occur in documents entirely independently of each other, then the probability of seeing both <math>q_1</math> and <math>q_2</math> in a randomly picked document <math>D</math> is:

:<math>\frac{n(q_1)}{N} \cdot \frac{n(q_2)}{N},</math>

and the information content of such an event is:

:<math>\sum_{i=1}^{2} \log \frac{N}{n(q_i)}.</math>

With a small variation, this is exactly what is expressed by the IDF component of BM25.

== Modifications ==
* At the extreme values of the coefficient <math>b</math> BM25 turns into ranking functions known as '''BM11''' (for <math>b=1</math>) and '''BM15''' (for <math>b=0</math>).<ref>http://xapian.org/docs/bm25.html</ref>
* '''BM25F'''<ref>Hugo Zaragoza, Nick Craswell, Michael Taylor, Suchi Saria, and Stephen Robertson. [http://trec.nist.gov/pubs/trec13/papers/microsoft-cambridge.web.hard.pdf ''Microsoft Cambridge at TREC-13: Web and HARD tracks.''] In Proceedings of TREC-2004.</ref>  is a modification of BM25 in which the document is considered to be composed from several fields (such as headlines, main text, anchor text) with possibly different degrees of importance.
* '''BM25+'''<ref>Yuanhua Lv and ChengXiang Zhai. [http://sifaka.cs.uiuc.edu/~ylv2/pub/cikm11-lowerbound.pdf ''Lower-bounding term frequency normalization.''] In Proceedings of CIKM'2011, pages 7-16.</ref> is an extension of BM25. BM25+ was developed to address one deficiency of the standard BM25 in which the component of term frequency normalization by document length is not properly lower-bounded; as a result of this deficiency, long documents which do match the query term can often be scored unfairly by BM25 as having a similar relevancy to shorter documents that do not contain the query term at all. The scoring formula of BM25+ only has one additional free parameter <math>\delta</math> (a default value is <math>1.0</math> in absence of a training data) as compared with BM25:

:<math> \text{score}(D,Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \left[ \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})} + \delta \right]</math>

== Footnotes ==
{{Reflist}}

== References ==
* {{cite conference|author=Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford | title=Okapi at TREC-3 | conference=[http://trec.nist.gov/pubs/trec3/t3_proceedings.html Proceedings of the Third Text REtrieval Conference (TREC 1994)]|location=Gaithersburg, USA|date=November 1994|url=http://trec.nist.gov/pubs/trec3/papers/city.ps.gz}}

* {{cite conference|author=Stephen E. Robertson, Steve Walker, and Micheline Hancock-Beaulieu|title=Okapi at TREC-7|conference=[http://trec.nist.gov/pubs/trec7/t7_proceedings.html Proceedings of the Seventh Text REtrieval Conference]|location=Gaithersburg, USA|date=November 1998|url=http://trec.nist.gov/pubs/trec7/papers/okapi_proc.pdf.gz}}

* {{cite doi|10.1016/S0306-4573(00)00015-7}}

* {{cite doi|10.1016/S0306-4573(00)00016-9}}

== External links ==
* {{cite book|last1=Robertson|first1=Stephen|last2=Zaragoza|first2=Hugo|title=The Probabilistic Relevance Framework: BM25 and Beyond|date=2009|publisher=NOW Publishers, Inc.|isbn=978-1-60198-308-4|url=http://staff.city.ac.uk/~sb317/papers/foundations_bm25_review.pdf}}

[[Category:Ranking functions]]
[[Category:Information retrieval]]
>>EOP<<
144<|###|>Information retrieval
{{Information science}}

'''Information retrieval''' ('''IR''') is the activity of obtaining [[information]] resources relevant to an information need from a collection of information resources.  Searches can be based on [[metadata]] or on [[Full text search|full-text]] (or other content-based) indexing.

Automated information retrieval systems are used to reduce what has been called "[[information overload]]". Many universities and [[public library|public libraries]] use IR systems to provide access to books, journals and other documents. [[Web search engine]]s are the most visible [[Information retrieval applications|IR applications]].

== Overview ==

An information retrieval process begins when a user enters a [[query string|query]] into the system. Queries are formal statements of [[information need]]s, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of [[relevance|relevancy]].

An object is an entity that is represented by information in a [[database]]. User queries are matched against the database information. Depending on the [[Information retrieval applications|application]] the data objects may be, for example, text documents, images,<ref name=goodron2000>{{cite journal |first=Abby A. |last=Goodrum |title=Image Information Retrieval: An Overview of Current Research |journal=Informing Science |volume=3 |number=2 |year=2000 }}</ref> audio,<ref name=Foote99>{{cite journal |first=Jonathan |last=Foote |title=An overview of audio information retrieval |journal=Multimedia Systems |year=1999 |publisher=Springer }}</ref> [[mind maps]]<ref name=Beel2009>{{cite journal |first=Joran |last=Beel |first2=Bela |last2=Gipp |first3=Jan-Olaf |last3=Stiller |contribution=Information Retrieval On Mind Maps - What Could It Be Good For? |contribution-url=http://www.sciplore.org/publications_en.php |title=Proceedings of the 5th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom'09) |year=2009 |publisher=IEEE |place=Washington, DC }}</ref> or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.

Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.<ref name="Frakes1992">{{cite book |last=Frakes |first=William B. |title=Information Retrieval Data Structures & Algorithms |publisher=Prentice-Hall, Inc. |year=1992 |isbn=0-13-463837-9 |url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes }}</ref>

== History ==
{{Rquote|right|But do you know that, although I have kept the diary [on a phonograph] for months past, it never once struck me how I was going to find any particular part of it in case I wanted to look it up?|[[John Seward|Dr Seward]]| [[Bram Stoker]]'s ''[[Dracula]]'',
 1897}}
The idea of using computers to search for relevant pieces of information was popularized in the article ''[[As We May Think]]'' by [[Vannevar Bush]] in 1945.<ref name="Singhal2001">{{cite journal |last=Singhal |first=Amit |title=Modern Information Retrieval: A Brief Overview |journal=Bulletin of the IEEE Computer Society Technical Committee on Data Engineering |volume=24 |issue=4 |pages=3543 |year =2001 |url=http://singhal.info/ieee2001.pdf }}</ref> The first automated information retrieval systems were introduced in the 1950s and 1960s. By 1970 several different techniques had been shown to perform well on small [[text corpora]] such as the Cranfield collection (several thousand documents).<ref name="Singhal2001" /> Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.

In 1992, the US Department of Defense along with the [[National Institute of Standards and Technology]] (NIST), cosponsored the [[Text Retrieval Conference]] (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that [[scalability|scale]] to huge corpora. The introduction of [[web search engine]]s has boosted the need for very large scale retrieval systems even further.

== Model types ==
[[File:Information-Retrieval-Models.png|thumb|500px|Categorization of IR-models (translated from [[:de:Informationsruckgewinnung#Klassifikation von Modellen zur Reprasentation naturlichsprachlicher Dokumente|German entry]], original source [http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&lng=eng&id= Dominik Kuropka]).]]
For effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.

=== First dimension: mathematical basis ===
* ''Set-theoretic'' models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are:
** [[Standard Boolean model]]
** [[Extended Boolean model]]
** [[Fuzzy retrieval]]
* ''Algebraic models'' represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value.
** [[Vector space model]]
** [[Generalized vector space model]]
** [[Topic-based vector space model|(Enhanced) Topic-based Vector Space Model]]
** [[Extended Boolean model]]
** [[Latent semantic indexing]] a.k.a. [[latent semantic analysis]]
* ''Probabilistic models'' treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the [[Bayes' theorem]] are often used in these models.
** [[Binary Independence Model]]
** [[Probabilistic relevance model]] on which is based the [[Probabilistic relevance model (BM25)|okapi (BM25)]] relevance function
** [[Uncertain inference]]
** [[Language model]]s
** [[Divergence-from-randomness model]]
** [[Latent Dirichlet allocation]]
* ''Feature-based retrieval models'' view documents as vectors of values of ''feature functions'' (or just ''features'') and seek the best way to combine these features into a single relevance score, typically by [[learning to rank]] methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just a yet another feature.

=== Second dimension: properties of the model ===
* ''Models without term-interdependencies'' treat different terms/words as independent. This fact is usually represented in vector space models by the [[orthogonality]] assumption of term vectors or in probabilistic models by an [[Independence (mathematical logic)|independency]] assumption for term variables.
* ''Models with immanent term interdependencies'' allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by [[dimension reduction|dimensional reduction]]) from the [[co-occurrence]] of those terms in the whole set of documents.
* ''Models with transcendent term interdependencies'' allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example a human or sophisticated algorithms.)

== Performance and correctness measures ==
{{main|Precision and recall}}

Many different measures for evaluating the performance of information retrieval systems have been proposed. The measures require a collection of documents and a query. All common measures described here assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice queries may be [[ill-posed]] and there may be different shades of relevancy.

=== Precision ===

Precision is the fraction of the documents retrieved that are [[Relevance (information retrieval)|relevant]] to the user's information need.

:<math> \mbox{precision}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{retrieved documents}\}|} </math>

In [[binary classification]], precision is analogous to [[positive predictive value]]. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called ''precision at n'' or ''P@n''.

Note that the meaning and usage of "precision" in the field of Information Retrieval differs from the definition of [[accuracy and precision]] within other branches of science and [[statistics]].

=== Recall ===

Recall is the fraction of the documents that are relevant to the query that are successfully retrieved.

:<math>\mbox{recall}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{relevant documents}\}|} </math>

In binary classification, recall is often called [[sensitivity and specificity|sensitivity]]. So it can be looked at as ''the probability that a relevant document is retrieved by the query''.

It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.

=== Fall-out ===
The proportion of non-relevant documents that are retrieved, out of all non-relevant documents available:

:<math> \mbox{fall-out}=\frac{|\{\mbox{non-relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{non-relevant documents}\}|} </math>

In binary classification, fall-out is closely related to [[sensitivity and specificity|specificity]] and is equal to <math>(1-\mbox{specificity})</math>. It can be looked at as ''the probability that a non-relevant document is retrieved by the query''.

It is trivial to achieve fall-out of 0% by returning zero documents in response to any query.

=== F-measure ===
{{main|F-score}}
The weighted [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score is:

:<math>F = \frac{2 \cdot \mathrm{precision} \cdot \mathrm{recall}}{(\mathrm{precision} + \mathrm{recall})}.\,</math>

This is also known as the <math>F_1</math> measure, because recall and precision are evenly weighted.

The general formula for non-negative real <math>\beta</math> is:
:<math>F_\beta = \frac{(1 + \beta^2) \cdot (\mathrm{precision} \cdot \mathrm{recall})}{(\beta^2 \cdot \mathrm{precision} + \mathrm{recall})}\,</math>.

Two other commonly used F measures are the <math>F_{2}</math> measure, which weights recall twice as much as precision, and the <math>F_{0.5}</math> measure, which weights precision twice as much as recall.

The F-measure was derived by van Rijsbergen (1979) so that <math>F_\beta</math> "measures the effectiveness of retrieval with respect to a user who attaches <math>\beta</math> times as much importance to recall as precision".  It is based on van Rijsbergen's effectiveness measure <math>E = 1 - \frac{1}{\frac{\alpha}{P} + \frac{1-\alpha}{R}}</math>.  Their relationship is <math>F_\beta = 1 - E</math> where <math>\alpha=\frac{1}{1 + \beta^2}</math>.

=== Average precision ===
<!-- [[Average precision]] redirects here -->
Precision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision <math>p(r)</math> as a function of recall <math>r</math>. Average precision computes the average value of <math>p(r)</math> over the interval from <math>r=0</math> to <math>r=1</math>:<ref name="zhu2004">{{cite journal |first=Mu |last=Zhu |contribution=Recall, Precision and Average Precision |contribution-url=http://sas.uwaterloo.ca/stats_navigation/techreports/04WorkingPapers/2004-09.pdf |year=2004 }}</ref>
:<math>\operatorname{AveP} = \int_0^1 p(r)dr</math>
That is the area under the precision-recall curve.
This integral is in practice replaced with a finite sum over every position in the ranked sequence of documents:
:<math>\operatorname{AveP} = \sum_{k=1}^n P(k) \Delta r(k)</math>
where <math>k</math> is the rank in the sequence of retrieved documents, <math>n</math> is the number of retrieved documents, <math>P(k)</math> is the precision at cut-off <math>k</math> in the list, and <math>\Delta r(k)</math> is the change in recall from items <math>k-1</math> to <math>k</math>.<ref name="zhu2004" />

This finite sum is equivalent to:
:<math> \operatorname{AveP} = \frac{\sum_{k=1}^n (P(k) \times \operatorname{rel}(k))}{\mbox{number of relevant documents}} \!</math>
where <math>\operatorname{rel}(k)</math> is an indicator function equaling 1 if the item at rank <math>k</math> is a relevant document, zero otherwise.<ref name="Turpin2006">{{cite journal |last=Turpin |first=Andrew |last2=Scholer |first2=Falk |title=User performance versus precision measures for simple search tasks |journal=Proceedings of the 29th Annual international ACM SIGIR Conference on Research and Development in information Retrieval (Seattle, WA, August 0611, 2006) |publisher=ACM |location=New York, NY |pages=1118 |doi=10.1145/1148170.1148176 |year=2006 |isbn=1-59593-369-7 }}</ref> Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.

Some authors choose to interpolate the <math>p(r)</math> function to reduce the impact of "wiggles" in the curve.<ref name=voc2010>{{cite journal |last=Everingham |first=Mark |last2=Van Gool |first2=Luc |last3=Williams |first3=Christopher K. I. |last4=Winn |first4=John |last5=Zisserman |first5=Andrew |title=The PASCAL Visual Object Classes (VOC) Challenge |journal=International Journal of Computer Vision |volume=88 |issue=2 |pages=303338 |publisher=Springer |date=June 2010 |url=http://pascallin.ecs.soton.ac.uk/challenges/VOC/pubs/everingham10.pdf |accessdate=2011-08-29 |doi=10.1007/s11263-009-0275-4 }}</ref><ref name="nlpbook">{{cite book |last=Manning |first=Christopher D. |last2=Raghavan |first2=Prabhakar |last3=Schutze |first3=Hinrich |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008 |url=http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html }}</ref> For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}:<ref name="voc2010" /><ref name="nlpbook" />
:<math>\operatorname{AveP} = \frac{1}{11} \sum_{r \in \{0, 0.1, \ldots, 1.0\}} p_{\operatorname{interp}}(r)</math>
where <math>p_{\operatorname{interp}}(r)</math> is an interpolated precision that takes the maximum precision over all recalls greater than <math>r</math>:
:<math>p_{\operatorname{interp}}(r) = \operatorname{max}_{\tilde{r}:\tilde{r} \geq r} p(\tilde{r})</math>.

An alternative is to derive an analytical <math>p(r)</math> function by assuming a particular parametric distribution for the underlying decision values. For example, a ''binormal precision-recall curve'' can be obtained by assuming decision values in both classes to follow a Gaussian distribution.<ref>K.H. Brodersen, C.S. Ong, K.E. Stephan, J.M. Buhmann (2010). [http://icpr2010.org/pdfs/icpr2010_ThBCT8.28.pdf The binormal assumption on precision-recall curves]. ''Proceedings of the 20th International Conference on Pattern Recognition'', 4263-4266.</ref>

=== R-Precision ===

Precision at '''R'''-th position in the ranking of results for a query that has '''R''' relevant documents. This measure is highly correlated to Average Precision. Also, Precision is equal to Recall at the '''R'''-th position.

=== Mean average precision ===
<!-- [[Mean average precision]] redirects here -->
Mean average precision for a set of queries is the mean of the average precision scores for each query.
:<math> \operatorname{MAP} = \frac{\sum_{q=1}^Q \operatorname{AveP(q)}}{Q} \!</math>
where ''Q'' is the number of queries.

=== Discounted cumulative gain ===
{{main|Discounted cumulative gain}}
DCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.

The DCG accumulated at a particular rank position <math>p</math> is defined as:

:<math> \mathrm{DCG_{p}} = rel_{1} + \sum_{i=2}^{p} \frac{rel_{i}}{\log_{2}i}. </math>

Since result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p (<math>IDCG_p</math>), which normalizes the score:

:<math> \mathrm{nDCG_{p}} = \frac{DCG_{p}}{IDCG{p}}. </math>

The nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the <math>DCG_p</math> will be the same as the <math>IDCG_p</math> producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.

=== Other Measures ===
* [[Mean reciprocal rank]]
* [[Spearman's rank correlation coefficient]]

=== Timeline ===

* Before the '''1900s'''
*: '''1801''': [[Joseph Marie Jacquard]] invents the [[Jacquard loom]], the first machine to use punched cards to control a sequence of operations.
*: '''1880s''': [[Herman Hollerith]] invents an electro-mechanical data tabulator using punch cards as a machine readable medium.
*: '''1890''' Hollerith [[Punched cards|cards]], [[keypunch]]es and [[Tabulating machine|tabulators]] used to process the [[1890 US Census]] data.
* '''1920s-1930s'''
*: [[Emanuel Goldberg]] submits patents for his "Statistical Machine a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents.
* '''1940s1950s'''
*: '''late 1940s''': The US military confronted problems of indexing and retrieval of wartime scientific research documents captured from Germans.
*:: '''1945''': [[Vannevar Bush]]'s ''[[As We May Think]]'' appeared in ''[[Atlantic Monthly]]''.
*:: '''1947''': [[Hans Peter Luhn]] (research engineer at IBM since 1941) began work on a mechanized punch card-based system for searching chemical compounds.
*: '''1950s''': Growing concern in the US for a "science gap" with the USSR motivated, encouraged funding and provided a backdrop for mechanized literature searching systems (Allen Kent ''et al.'') and the invention of citation indexing ([[Eugene Garfield]]).
*: '''1950''': The term "information retrieval" appears to have been coined by [[Calvin Mooers]].<ref>Mooers, Calvin N.; ''Theory Digital Handling Non-numerical Information'' (Zator Technical Bulletin No. 48) 5, cited in "information, n.". OED Online. December 2011. Oxford University Press.</ref>
*: '''1951''': Philip Bagley conducted the earliest experiment in computerized document retrieval in a master thesis at [[MIT]].<ref name="Doyle1975">{{cite book |last=Doyle |first=Lauren |last2=Becker |first2=Joseph |title=Information Retrieval and Processing |publisher=Melville |year=1975 |pages=410 pp. |isbn=0-471-22151-1 }}</ref>
*: '''1955''': Allen Kent joined [[Case Western Reserve University]], and eventually became associate director of the Center for Documentation and Communications Research. That same year, Kent and colleagues published a paper in American Documentation describing the precision and recall measures as well as detailing a proposed "framework" for evaluating an IR system which included statistical sampling methods for determining the number of relevant documents not retrieved.
*: '''1958''': International Conference on Scientific Information Washington DC included consideration of IR systems as a solution to problems identified. See: ''Proceedings of the International Conference on Scientific Information, 1958'' (National Academy of Sciences, Washington, DC, 1959)
*: '''1959''': [[Hans Peter Luhn]] published "Auto-encoding of documents for information retrieval."
* '''1960s''':
*: '''early 1960s''': [[Gerard Salton]] began work on IR at Harvard, later moved to Cornell.
*: '''1960''': [[Melvin Earl Maron]] and John Lary<!-- sic --> Kuhns<ref name="Maron2008">{{cite journal |title=An Historical Note on the Origins of Probabilistic Indexing |last=Maron | first=Melvin E. |journal=Information Processing and Management |volume=44 |year=2008 |pages=971972 |url=http://yunus.hacettepe.edu.tr/~tonta/courses/spring2008/bby703/maron-on-probabilistic%20indexing-2008.pdf |doi=10.1016/j.ipm.2007.02.012 |issue=2 }}</ref> published "On relevance, probabilistic indexing, and information retrieval" in the Journal of the ACM 7(3):216244, July 1960.
*: '''1962''':
*:* [[Cyril W. Cleverdon]] published early findings of the Cranfield studies, developing a model for IR system evaluation. See: Cyril W. Cleverdon, "Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems". Cranfield Collection of Aeronautics, Cranfield, England, 1962.
*:* Kent published ''Information Analysis and Retrieval''.
*: '''1963''':
*:* Weinberg report "Science, Government and Information" gave a full articulation of the idea of a "crisis of scientific information."  The report was named after Dr. [[Alvin Weinberg]].
*:* Joseph Becker and [[Robert M. Hayes]] published text on information retrieval. Becker, Joseph; Hayes, Robert Mayo. ''Information storage and retrieval: tools, elements, theories''. New York, Wiley (1963).
*: '''1964''':
*:* [[Karen Sparck Jones]] finished her thesis at Cambridge, ''Synonymy and Semantic Classification'', and continued work on [[computational linguistics]] as it applies to IR.
*:* The [[National Bureau of Standards]] sponsored a symposium titled "Statistical Association Methods for Mechanized Documentation." Several highly significant papers, including G. Salton's first published reference (we believe) to the [[SMART Information Retrieval System|SMART]] system.
*:'''mid-1960s''':
*::* National Library of Medicine developed [[MEDLARS]] Medical Literature Analysis and Retrieval System, the first major machine-readable database and batch-retrieval system.
*::* Project Intrex at MIT.
*:: '''1965''': [[J. C. R. Licklider]] published ''Libraries of the Future''.
*:: '''1966''': [[Don Swanson]] was involved in studies at University of Chicago on Requirements for Future Catalogs.
*: '''late 1960s''': [[F. Wilfrid Lancaster]] completed evaluation studies of the MEDLARS system and published the first edition of his text on information retrieval.
*:: '''1968''':
*:* Gerard Salton published ''Automatic Information Organization and Retrieval''.
*:* John W. Sammon, Jr.'s RADC Tech report "Some Mathematics of Information Storage and Retrieval..." outlined the vector model.
*:: '''1969''': Sammon's "A nonlinear mapping for data structure analysis" (IEEE Transactions on Computers) was the first proposal for visualization interface to an IR system.
* '''1970s'''
*: '''early 1970s''':
*::* First online systemsNLM's AIM-TWX, MEDLINE; Lockheed's Dialog; SDC's ORBIT.
*::* [[Theodor Nelson]] promoting concept of [[hypertext]], published ''Computer Lib/Dream Machines''.
*: '''1971''': [[Nicholas Jardine]] and [[Cornelis J. van Rijsbergen]] published "The use of hierarchic clustering in information retrieval", which articulated the "cluster hypothesis."<ref>{{cite journal|author=N. Jardine, C.J. van Rijsbergen|title=The use of hierarchic clustering in information retrieval|journal=Information Storage and Retrieval|volume=7|issue=5|pages=217240|date=December 1971|doi=10.1016/0020-0271(71)90051-9}}</ref>
*: '''1975''': Three highly influential publications by Salton fully articulated his vector processing framework and term discrimination model:
*::* ''A Theory of Indexing'' (Society for Industrial and Applied Mathematics)
*::* ''A Theory of Term Importance in Automatic Text Analysis'' ([[JASIS]] v. 26)
*::* ''A Vector Space Model for Automatic Indexing'' ([[Communications of the ACM|CACM]] 18:11)
*: '''1978''': The First [[Association for Computing Machinery|ACM]] [[Special Interest Group on Information Retrieval|SIGIR]] conference.
*: '''1979''': C. J. van Rijsbergen published ''Information Retrieval'' (Butterworths). Heavy emphasis on probabilistic models.
* '''1980s'''
*: '''1980''': First international ACM SIGIR conference, joint with British Computer Society IR group in Cambridge.
*: '''1982''': [[Nicholas J. Belkin]], Robert N. Oddy, and Helen M. Brooks proposed the ASK (Anomalous State of Knowledge) viewpoint for information retrieval. This was an important concept, though their automated analysis tool proved ultimately disappointing.
*: '''1983''': Salton (and Michael J. McGill) published ''Introduction to Modern Information Retrieval'' (McGraw-Hill), with heavy emphasis on vector models.
*: '''1985''': David Blair and [[Bill Maron]] publish: An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System
*: '''mid-1980s''': Efforts to develop end-user versions of commercial IR systems.
*:: '''19851993''': Key papers on and experimental systems for visualization interfaces.
*:: Work by [[Donald B. Crouch]], [[Robert R. Korfhage]], Matthew Chalmers, Anselm Spoerri and others.
*: '''1989''': First [[World Wide Web]] proposals by [[Tim Berners-Lee]] at [[CERN]].
* '''1990s'''
*: '''1992''': First [[Text Retrieval Conference|TREC]] conference.
*: '''1997''': Publication of [[Robert R. Korfhage|Korfhage]]'s ''Information Storage and Retrieval''<ref name="Korfhage1997">{{cite book |last=Korfhage |first=Robert R. |title=Information Storage and Retrieval |publisher=Wiley |year=1997 |pages=368 pp. |isbn=978-0-471-14338-3 |url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471143383,descCd-authorInfo.html }}</ref> with emphasis on visualization and multi-reference point systems.
*: '''late 1990s''': Web [[Web search engine|search engines]] implementation of many features formerly found only in experimental IR systems. Search engines become the most common and maybe best instantiation of IR models.

== Awards in the field ==

* [[Tony Kent Strix award]]
* [[Gerard Salton Award]]

==See also==

{{col-begin}}
{{col-break}}

* [[Adversarial information retrieval]]
* [[Collaborative information seeking]]
* [[Controlled vocabulary]]
* [[Cross-language information retrieval]]
* [[Data mining]]
* [[European Summer School in Information Retrieval]]
* [[Humancomputer information retrieval]]
* [[Information extraction]]
* [[Information Retrieval Facility]]
* [[Knowledge visualization]]
* [[Multimedia Information Retrieval]]
* [[List of information retrieval libraries]]
{{col-break}}
* [[Personal information management]]
* [[Relevance (Information Retrieval)]]
* [[Relevance feedback]]
* [[Rocchio Classification]]
* [[Index (search engine)|Search index]]
* [[Social information seeking]]
* [[Special Interest Group on Information Retrieval]]
* [[Structured Search]]
* [[Subject indexing]]
* [[Temporal information retrieval]]
* [[Tf-idf]]
* [[XML-Retrieval]]
* Key-objects

{{col-end}}

== References ==
{{reflist}}

==External links==
{{wikiquote}}
* [http://www.acm.org/sigir/ ACM SIGIR: Information Retrieval Special Interest Group]
* [http://irsg.bcs.org/ BCS IRSG: British Computer Society - Information Retrieval Specialist Group]
* [http://trec.nist.gov Text Retrieval Conference (TREC)]
* [http://www.isical.ac.in/~fire Forum for Information Retrieval Evaluation (FIRE)]
* [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval] (online book) by [[C. J. van Rijsbergen]]
* [http://ir.dcs.gla.ac.uk/wiki/ Information Retrieval Wiki]
* [http://ir-facility.org/ Information Retrieval Facility]
* [http://www.nonrelevant.net Information Retrieval @ DUTH]
* [http://nlp.stanford.edu/IR-book/ Introduction to Information Retrieval (online book) by Christopher D. Manning, Prabhakar Raghavan and Hinrich Schutze, Cambridge University Press. 2008.  ]

{{DEFAULTSORT:Information Retrieval}}
[[Category:Articles with inconsistent citation formats]]
[[Category:Information retrieval| ]]
[[Category:Natural language processing]]
>>EOP<<
150<|###|>Stop words
{{distinguish|Safeword}}
In [[computing]], '''stop words''' are words which are filtered out before or after [[Natural language processing|processing of natural language]] data (text).<ref>{{cite doi|10.1017/CBO9781139058452.002}}</ref> There is no single universal list of stop words used by all [[Natural language processing|processing of natural language]] tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these '''stop words''' to support [[phrase search]].

Any group of words can be chosen as the stop words for a given purpose. For some [[search engine]]s, these are some of the most common, short [[function word]]s, such as ''the'', ''is'', ''at'', ''which'', and ''on''. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as '[[The Who]]', '[[The The]]', or '[[Take That]]'. Other search engines remove some of the most common wordsincluding [[lexical word]]s, such as "want"from a query in order to improve performance.<ref>[http://blog.stackoverflow.com/2008/12/podcast-32 Stackoverflow]: "One of our major performance optimizations for the "related questions" query is removing the top 10,000 most common English dictionary words (as determined by Google search) before submitting the query to the SQL Server 2008 full text engine. Its shocking how little is left of most posts once you remove the top 10k English dictionary words. This helps limit and narrow the returned results, which makes the query dramatically faster".</ref>

[[Hans Peter Luhn]], one of the pioneers in [[information retrieval]], is credited with coining the phrase and using the concept. {{Citation needed|date=March 2013}}

== See also ==
{{Div col|cols=3}}
* [[Text mining]]
* [[Concept mining]]
* [[Information extraction]]
* [[Natural language processing]]
* [[Query expansion]]
* [[Stemming]]
* [[Index (search engine)|Search engine indexing]]
* [[Poison words]]
* [[Function words]]
{{Div col end}}

==References==
{{Reflist}}

== External links ==
* [http://xpo6.com/list-of-english-stop-words/  List of English Stop Words (PHP array, CSV) ]
* [http://dev.mysql.com/doc/refman/5.5/en/fulltext-stopwords.html  Full-Text Stopwords in MySQL ]
* [http://www.textfixer.com/resources/common-english-words.txt English Stop Words (CSV)]
* [http://mail.sarai.net/private/prc/Week-of-Mon-20080204/001656.html Hindi Stop Words]
* [http://solariz.de/de/deutsche_stopwords.htm German Stop Words],[http://aniol-consulting.de/uebersicht-deutscher-stop-words/ German Stop Words and phrases], another list of [http://www.ranks.nl/stopwords/german.html German stop words]
* [[:pl:Wikipedia:Stopwords|Polish Stop Words]]
* [https://code.google.com/p/stop-words/ Collection of stop words in 29 languages]
* [http://www.text-analytics101.com/2014/10/all-about-stop-words-for-text-mining.html A Detailed Explanation of Stop Words by Kavita Ganesan]


[[Category:Information retrieval]]
[[Category:Searching]]
{{Natural Language Processing}}
{{SearchEngineOptimization}}
>>EOP<<
156<|###|>Category:Directories
A directory maintains a list for reference or commercial purposes.  This category contains articles about directories.
{{Cat main|Directories}}
{{Commons cat|Directories}}

[[Category:Telephony]]
[[Category:Reference works]]
[[Category:Data management]]
[[Category:Information retrieval]]
>>EOP<<
162<|###|>Vector space model
'''Vector space model''' or '''term vector model''' is an algebraic model for representing text documents (and any objects, in general) as [[vector space|vectors]] of identifiers, such as, for example, index terms. It is used in [[information filtering]], [[information retrieval]], [[index (search engine)|index]]ing and relevancy rankings.  Its first use was in the [[SMART Information Retrieval System]].

==Definitions==

Documents and queries are represented as vectors.

:<math>d_j = ( w_{1,j} ,w_{2,j} , \dotsc ,w_{t,j} )</math>
:<math>q = ( w_{1,q} ,w_{2,q} , \dotsc ,w_{n,q} )</math>

Each [[Dimension (vector space)|dimension]] corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is [[tf-idf]] weighting (see the example below).

The definition of ''term'' depends on the application. Typically terms are single words, [[keyword (linguistics)|keyword]]s, or longer phrases. If words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the [[text corpus|corpus]]).

Vector operations can be used to compare documents with queries.

==Applications==

[[Image:vector space model.jpg|right|250px]]

[[Relevance (information retrieval)|Relevance]] [[ranking]]s of documents in a keyword search can be calculated, using the assumptions of [[semantic similarity|document similarities]] theory, by comparing the deviation of angles between each document vector and the original query vector where the query is represented as the same kind of vector as the documents.

In practice, it is easier to calculate the [[cosine]] of the angle between the vectors, instead of the angle itself:

:<math>
\cos{\theta} = \frac{\mathbf{d_2} \cdot \mathbf{q}}{\left\| \mathbf{d_2} \right\| \left \| \mathbf{q} \right\|}
</math>

Where <math>\mathbf{d_2} \cdot \mathbf{q}</math> is the intersection (i.e. the [[dot product]]) of the document (d<sub>2</sub> in the figure to the right) and the query (q in the figure) vectors, <math>\left\| \mathbf{d_2} \right\|</math> is the norm of vector d<sub>2</sub>, and <math>\left\| \mathbf{q} \right\|</math> is the norm of vector q. The [[Norm (mathematics)|norm]] of a vector is calculated as such:

:<math>
\left\| \mathbf{q} \right\| = \sqrt{\sum_{i=1}^n q_i^2}
</math>

As all vectors under consideration by this model are elementwise nonnegative, a cosine value of zero means that the query and document vector are [[orthogonal]] and have no match (i.e. the query term does not exist in the document being considered). See [[cosine similarity]] for further information.

==Example: tf-idf weights==

In the classic vector space model proposed by [[Gerard Salton|Salton]], Wong and Yang <ref>[http://doi.acm.org/10.1145/361219.361220 G. Salton , A. Wong , C. S. Yang, A vector space model for automatic indexing], Communications of the ACM, v.18 n.11, p.613-620, Nov. 1975</ref> the term-specific weights in the document vectors are products of local and global parameters. The model is known as [[tf-idf|term frequency-inverse document frequency]] model. The weight vector for document ''d'' is <math>\mathbf{v}_d = [w_{1,d}, w_{2,d}, \ldots, w_{N,d}]^T</math>, where

:<math>
w_{t,d} = \mathrm{tf}_{t,d} \cdot \log{\frac{|D|}{|\{d' \in D \, | \, t \in d'\}|}}
</math>

and
* <math>\mathrm{tf}_{t,d}</math> is term frequency of term ''t'' in document ''d'' (a local parameter)
* <math>\log{\frac{|D|}{|\{d' \in D \, | \, t \in d'\}|}}</math> is inverse document frequency (a global parameter). <math>|D|</math> is the total number of documents in the document set; <math>|\{d' \in D \, | \, t \in d'\}|</math> is the number of documents containing the term ''t''.

Using the cosine the similarity between document ''d<sub>j</sub>'' and query ''q'' can be calculated as:

:<math>\mathrm{sim}(d_j,q) = \frac{\mathbf{d_j} \cdot \mathbf{q}}{\left\| \mathbf{d_j} \right\| \left \| \mathbf{q} \right\|} = \frac{\sum _{i=1}^N w_{i,j}w_{i,q}}{\sqrt{\sum _{i=1}^N w_{i,j}^2}\sqrt{\sum _{i=1}^N w_{i,q}^2}}</math>

==Advantages==

The vector space model has the following advantages over the [[Standard Boolean model]]:

#Simple model based on linear algebra
#Term weights not binary
#Allows computing a continuous degree of similarity between queries and documents
#Allows ranking documents according to their possible relevance
#Allows partial matching

==Limitations==

The vector space model has the following limitations:

#Long documents are poorly represented because they have poor similarity values (a small [[scalar product]] and a [[curse of dimensionality|large dimensionality]])
#Search keywords must precisely match document terms; word [[substring]]s might result in a "[[false positive]] match"
#Semantic sensitivity; documents with similar context but different term vocabulary won't be associated, resulting in a "[[false negative]] match".
#The order in which the terms appear in the document is lost in the vector space representation.
#Theoretically assumes terms are statistically independent. 
#Weighting is intuitive but not very formal. 

Many of these difficulties can, however, be overcome by the integration of various tools, including mathematical techniques such as [[singular value decomposition]] and [[lexical database]]s such as [[WordNet]].

==Models based on and extending the vector space model==

Models based on and extending the vector space model include:
* [[Generalized vector space model]]
* [[Latent semantic analysis]]
* [[Term Discrimination]]
* [[Rocchio Classification]]
* [[random_indexing|Random Indexing]]

==Software that implements the vector space model==

The following software packages may be of interest to those wishing to experiment with vector models and implement search services based upon them.

===Free open source software===

* [[Apache Lucene]]. Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java.
* [http://semanticvectors.googlecode.com SemanticVectors]. Semantic Vector indexes, created by applying a Random Projection algorithm (similar to [[Latent semantic analysis]]) to term-document matrices created using Apache Lucene.
* [[Gensim]] is a Python+[[NumPy]] framework for Vector Space modelling. It contains incremental (memory-efficient) algorithms for [[Tfidf]], [[Latent Semantic Indexing]], [[Locality_sensitive_hashing#Random_projection|Random Projections]] and [[Latent Dirichlet Allocation]].
* [[Weka (machine learning)|Weka]]. Weka is popular data mining package for Java including WordVectors and Bag Of Words models.
* [http://codingplayground.blogspot.com/2010/03/compressed-vector-space.html Compressed vector space in C++] by Antonio Gulli
* [http://scgroup.hpclab.ceid.upatras.gr/scgroup/Projects/TMG/ Text to Matrix Generator (TMG)]  MATLAB toolbox that can be used for various tasks in text mining specifically  i) indexing, ii) retrieval, iii) dimensionality reduction, iv) clustering, v) classification. Most of TMG is written in MATLAB and parts in Perl. It contains implementations of LSI, clustered LSI, NMF and other methods.
* [http://senseclusters.sourceforge.net SenseClusters], an open source package, written in Perl, that supports context and word clustering using Latent Semantic Analysis and word co-occurrence matrices.
* [https://github.com/fozziethebeat/S-Space/wiki S-Space Package], a collection of algorithms for exploring and working with [[statistical semantics]].
* [http://www.cs.uni.edu/~okane/source/ISR/ Vector Space Model Software Workbench] Collection of 50 source code programs for education.

==Further reading==

* [[Gerard Salton|G. Salton]], A. Wong, and C. S. Yang (1975), "[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing]," ''Communications of the ACM'', vol. 18, nr. 11, pages 613620. ''(Article in which a vector space model was presented)''
* David Dubin (2004), [http://www.ideals.uiuc.edu/bitstream/2142/1697/2/Dubin748764.pdf The Most Influential Paper Gerard Salton Never Wrote] ''(Explains the history of the Vector Space Model and the non-existence of a frequently cited publication)''
* [http://isp.imm.dtu.dk/thor/projects/multimedia/textmining/node5.html Description of the vector space model]
* [http://www.miislita.com/term-vector/term-vector-3.html Description of the classic vector space model by Dr E. Garcia]
* [http://nlp.stanford.edu/IR-book/html/htmledition/vector-space-classification-1.html Relationship of vector space search to the "k-Nearest Neighbor" search]

==See also==
*[[Bag-of-words model]]
*[[Nearest neighbor search]]
*[[Compound term processing]]
*[[Inverted index]]
*[[w-shingling]]
*[[Eigenvalues and eigenvectors]]
*[[Conceptual Spaces]].

==References==
<references/>

[[Category:Vector space model|*]]
>>EOP<<
168<|###|>Agrep
{{lowercase|title=agrep}}
{{Infobox software
| name                   = agrep
| logo                   = <!-- Image name is enough -->
| logo caption           = 
| logo_size              = 
| logo_alt               = 
| screenshot             = <!-- Image name is enough -->
| caption                = 
| screenshot_size        = 
| screenshot_alt         = 
| collapsible            = 
| developer              = {{Plainlist|
* [[Udi Manber]]
* Sun Wu
}}
| released               = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->
| discontinued           = 
| latest release version = 
| latest release date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->
| latest preview version = 
| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->
| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->
| status                 = 
| programming language   = 
| operating system       = {{Plainlist|
* [[Unix-like]]
* [[OS/2]]
* [[DOS]]
* [[Microsoft Windows|Windows]]
}}
| platform               = 
| size                   = 
| language               = 
| language count         = <!-- DO NOT include this parameter unless you know what it does -->
| language footnote      = 
| genre                  = [[Pattern matching]]
| license                = 
| website                = <!-- {{URL|example.org}} -->
| standard               = 
}}

'''agrep''' (approximate [[grep]]) is a [[proprietary software|proprietary]] [[approximate string matching]] program, developed by [[Udi Manber]] and Sun Wu between 1988 and 1991, for use with the [[Unix]] operating system. It was later ported to [[OS/2]], [[DOS]], and [[Microsoft Windows|Windows]].

It selects the best-suited algorithm for the current query from a variety of the known fastest (built-in) [[string searching algorithm]]s, including Manber and Wu's [[bitap algorithm]] based on [[Levenshtein distance]]s.

agrep is also the [[search engine]] in the indexer program [[GLIMPSE]]. agrep is free for private and non-commercial use only, and belongs to the University of Arizona.

== Alternative implementations ==
A more recent agrep is the command-line tool provided with the [[TRE (computing)|TRE]] regular expression library. TRE agrep is more powerful than Wu-Manber agrep since it allows weights and total costs to be assigned separately to individual groups in the pattern. It can also handle Unicode.<ref>{{cite web | title=TRE - TRE regexp matching package - Features | url=http://laurikari.net/tre/about }}</ref> Unlike Wu-Manber agrep, TRE agrep is licensed under a [[BSD licenses#BSD-style licenses|2-clause BSD-like license]].

FREJ (Fuzzy Regular Expressions for Java) open-source library provides command-line interface which could be used in the way similar to agrep. Unlike agrep or TRE it could be used for constructing complex substitutions for matched text.<ref>{{cite web | title=FREJ - Fuzzy Regular Expressions for Java - Guide and Examples | url=http://frej.sf.net/rules.html }}</ref> However its syntax and matching abilities differs significantly from ones of ordinary [[regular expression]]s.

==References==
{{Reflist}}

==External links==
* Wu-Manber agrep
**[ftp://ftp.cs.arizona.edu/agrep/ For Unix]  (To compile under OSX 10.8, add <code>-Wno-return-type</code> to the <code>CFLAGs  = -O</code> line in the Makefile)
**[http://www.tgries.de/agrep For DOS, Windows and OS/2 home page]
*[http://wiki.christophchamp.com/index.php/Agrep_(command) Entry for "agrep" in Christoph's Personal Wiki]

*See also
**[http://laurikari.net/tre TRE regexp matching package]
**[http://www.bell-labs.com/project/wwexptools/cgrep/ cgrep a command line approximate string matching tool]
**[http://www.dcc.uchile.cl/~gnavarro/software/ nrgrep] a command line approximate string matching tool
**[http://finzi.psych.upenn.edu/R/library/base/html/agrep.html agrep as implemented in R]

[[Category:Searching]]
[[Category:Unix text processing utilities]]
>>EOP<<
174<|###|>Hybrid search engine
{{Notability|date=December 2009}}
A '''hybrid search engine''' ('''HSE''') is a type of [[computer]] [[search engine]] that uses different types of data with or without ontologies to produce the [[algorithm]]ically generated results based on [[web crawling]]. Previous types of search engines only use text to generate their results.

==References==
{{No footnotes|date=April 2010}}
*http://eprints.ecs.soton.ac.uk/17457/
*http://eprints.whiterose.ac.uk/3771/
*http://www.picollator.com

[[Category:Searching]]


{{web-stub}}
>>EOP<<
180<|###|>OpenGrok
{{multiple issues|
{{Advert|date=March 2012}}
{{Notability|Products|date=March 2012}}
}}

{{Infobox software
| name                   = OpenGrok
| logo                   = [[Image:OpenGrok Logo.png|150px|OpenGrok Logo]]
| screenshot             = 
| caption                =
| collapsible            = yes
| developer              = [[Sun Microsystems]]/[[Oracle Corporation]]
| latest release version = 0.12.1
| latest release date    = {{release date|2014|04|29}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Index (search engine)|Index]]er and [[cross-reference]]r with [[Revision control]]
| license                = [[CDDL]]
| website                = http://opengrok.github.com/OpenGrok/
}}

'''OpenGrok''' is a [[source code]] search and cross reference engine. It helps programmers to search, cross-reference and navigate source code trees.

It can understand various [[program (computing)|program]] [[file formats]] and [[version control]] histories like [[Monotone (software)|Monotone]], [[Source Code Control System|SCCS]], [[Revision Control System|RCS]], [[Concurrent Versions System|CVS]], [[Subversion (software)|Subversion]], [[Mercurial (software)|Mercurial]], [[Git (software)|Git]], [[IBM Rational ClearCase|Clearcase]], [[Perforce]] and [[Bazaar (software)|Bazaar]].<ref>https://github.com/OpenGrok/OpenGrok/wiki/Supported-Revision-Control-Systems</ref>

The name comes from the term ''[[grok]]'', a [[jargon]] term used in computing to mean "profoundly understand". The term ''[[grok]]'' originated in a science fiction novel by Robert A. Heinlein called ''[[Stranger in a Strange Land]]''.

OpenGrok is being developed mainly by [[Oracle Corporation]] (former [[Sun Microsystems]]) engineers with help from its community. OpenGrok is released under the terms of the [[Common Development and Distribution License]] (CDDL).

== Features ==

OpenGrok's features include:

* Full text Search
* Definition Search
* Identifier Search
* Path search
* History Search
* Shows matching lines
* Hierarchical Search
* query syntax like ''AND'', ''OR'', ''field'':
* Incremental update
* Syntax highlighting-Xref
* Quick navigation inside the file
* Interface for SCM
* Usable URLs
* Individual file download
* Changes at directory level
* Multi language support

== See also ==

* [[LXR Cross Referencer]]
* [[ViewVC]]
* [[FishEye (software)]]

== References ==

{{reflist}}

== External links ==
* [http://opengrok.github.com/OpenGrok/ OpenGrok project page]
* {{ohloh|opengrok}}
* [http://code.metager.de/source/ Metager]
* [http://BXR.SU/ Super User's BSD Cross Reference]

{{Sun Microsystems}}
{{Java (Sun)}}

[[Category:Cross-platform free software]]
[[Category:Free revision control software]]
[[Category:Source code]]
[[Category:Searching]]
[[Category:Java platform software]]
[[Category:Concurrent Versions System]]
[[Category:Subversion]]
[[Category:Code search engines]]


{{programming-software-stub}}
>>EOP<<
186<|###|>Social search
'''Social search''' or a '''social search engine''' is a type of [[web search]] that takes into account the [[Social Graph]] of the person initiating the search query. When applied to web searches the Social-Graph uses established algorithmic or machine-based approaches where relevance is determined by analyzing the text of each document or the link structure of the documents.<ref>[http://searchenginewatch.com/showPage.html?page=3623153 What's the Big Deal With Social Search?], SearchEngineWatch, Aug 15, 2006</ref> Search results produced by '''social search engine''' give more visibility to content created or "touched" by users in the Social Graph.

Social search takes many forms, ranging from simple [[social bookmarking|shared bookmarks]] or [[Tag (metadata)|tagging]] of content with descriptive labels to more sophisticated approaches that combine human intelligence with computer [[algorithm]]s.<ref>[http://www2.computer.org/portal/web/csdl/doi/10.1109/MC.2009.87 Chi, Ed H. Information Seeking Can Be Social, Computer, vol. 42, no. 3, pp. 42-46, Mar. 2009, ] {{doi|10.1109/MC.2009.87}}</ref><ref>[http://blog.delver.com/index.php/2008/07/31/taxonomy-of-social-search-approaches/ A Taxonomy of Social Search Approaches], Delver company blog, Jul 31, 2008</ref>
<ref>[http://www.springerlink.com/content/e12233858017h042/ Longo, Luca et al., Enhancing Social Search: A Computational Collective Intelligence Model of Behavioural Traits, Trust and Time. Transactions on Computational Collective Intelligence II, Lecture Notes in Computer Science, Volume 6450. ISBN 978-3-642-17154-3. Springer Berlin Heidelberg, 2010, p. 46 ] {{doi|10.1007/978-3-642-17155-0_3}}</ref><ref>[http://www.springerlink.com/content/gg3p6177pw6h10j8/ Longo, Luca et al., Information Foraging Theory as a Form of Collective Intelligence for Social Search. Computational Collective Intelligence. Semantic  Web, Social Networks and  Multiagent Systems Lecture Notes in Computer Science, 2009, Volume 5796/2009, 63-74] {{doi|10.1007/978-3-642-04441-0_5}}</ref>

The search experience takes into account varying sources of metadata, such as collaborative discovery of web pages, tags, social ranking, commenting on bookmarks, news, images, videos, knowledge sharing, podcasts and other web pages. Example forms of user input include social bookmarking or direct interaction with the search results such as promoting or demoting results the user feels are more or less relevant to their query.<ref>[http://venturebeat.com/2008/01/31/googles-marissa-mayer-social-search-is-the-future Googles Marissa Mayer: Social search is the future], VentureBeat, Jan 31, 2008</ref>

==History==

The term social search began to emerge between 2004 and 2005. The concept of social ranking can be considered to derive from Google's [[PageRank]] algorithm,{{citation needed|date=March 2009}} which assigns importance to web pages based on analysis of the link structure of the web, because PageRank is relying on the collective judgment of webmasters linking to other content on the web. Links, in essence, are positive votes by the webmaster community for their favorite sites.

In 2008, there were a few startup companies that focused on ranking search results according to one's [[social graph]] on [[social networks]].<ref>[http://online.wsj.com/public/article/SB121063460767286631.html New Sites Make It Easier To Spy on Your Friends], Wall Street Journal, May 13. 2008</ref><ref>[http://mashable.com/2007/08/27/social-search/ Social Search Guide: 40+ Social Search Engines], Mashable, Aug 27. 2007</ref> Companies in the social search space include  Evam-SOCOTO Wajam, Slangwho, [[Sproose]], [[Mahalo.com|Mahalo]], [[Jumper 2.0]], [[Qitera]], [[Scour Inc.|Scour]], [[Wink Technologies|Wink]], [[Eurekster]], [[Baynote]], [[Delver (Social Search)|Delver]], and OneRiot. Former efforts include [[Wikia Search]]. In 2008, a story on ''[[TechCrunch]]'' showed [[Google]] potentially adding in a voting mechanism to search results similar to [[Digg]]'s methodology.<ref>[http://www.techcrunch.com/2008/07/16/is-this-the-future-of-search/ Is This The Future Of Search?], TechCrunch, July 16, 2008</ref> This suggests growing interest in how social groups can influence and potentially enhance the ability of algorithms to find meaningful data for end users. There are also other services like Sentiment that turn search personal by searching within the users' social circles.

In October 2009, [[Google]] rolled out its "Social Search" feature; after a time in [[beta]], the feature was expanded to multiple languages in May 2011. Before the expansion however in 2010 [[Bing]] and [[Google]] were already taking into account re-tweets and Likes when providing search results.<ref>{{cite web|url = http://www.marchpr.com/blog/2013/04/seo-social-media-search/|title = Retweets and Likes influencing search results|date = 10 April 2013|accessdate = 1 December 2014| publisher = March Communications}}</ref> However, after a search deal with Twitter ended without renewal, Google began to retool its Social Search. In January 2012, Google released "Search plus Your World", a further development of Social Search. The feature, which is integrated into Google's regular search as an opt-out feature, pulls references to results from [[Google+]] profiles. The goal was to deliver better, more relevant and personalized search results with this integration. This integration however had some problems in which [[Google+]] still isn't wildly adopted or has much usage among many users.<ref name="HubSpot">{{cite web|url = https://blog.hubspot.com/blog/tabid/6307/bid/34058/Facebook-Announces-New-Social-Search-Feature-Called-Graph-Search.aspx|title = Facebook Announces New Social Search Feature|date = 15 January 2013|accessdate = 1 December 2014| publisher = HubSpot}}</ref>

In January 2013, [[Facebook]] announced a new search engine called [[Graph Search]] still in the beta stages. The goal in mind was to accomplish what [[Google]] failed at, skipping the results that are popular to the internet, in favor of the results that are popular within your social circle. Unlike [[Google]], [[Facebook]]'s Graph search differed in two large areas, first, people use Facebook frequently. This allows [[Facebook]] to use all it's user generated content that is uploaded everyday to improve the [[Facebook]] search experience.<ref name="HubSpot"/> Secondly, [[Facebook]] did not incorporate Google into Facebook search, instead Graph Search is powered by [[Bing]].This allows [[Bing]] results to show when Facebook's Graph Search can't find a match.<ref>{{cite web|url = http://www.forbes.com/sites/tomiogeron/2013/01/15/live-facebook-announces-graph-search/|title = Graph Search powered by Bing|date = 15 January 2013|accessdate = 1 December 2014| publisher = Forbes}}</ref>

==Concerns==

When Google announced "Search plus Your World" the reaction was mixed among tech companies. The company was subsequently criticized by [[Twitter]] for the perceived potential impact of "Search plus Your World" upon web publishers, describing the feature's release to the public as a "bad day for the web", while Google replied that Twitter refused to allow deep search crawling by Google of Twitter's content.<ref>{{cite web|url = http://www.cnbc.com/id/100381337#.|title = Twitter unhappy about Google's social search changes|date = 11 January 2012|accessdate = 11 January 2012|publisher = BBC News}}</ref> The criticism from [[Twitter]] wasn't without merits however, by [[Google]] integrating [[Google+]], they were essentially forcing people to switch from a social network on to theirs in order to improve search results. One famous example occurred when [[Google]] showed a link to Mark Zuckerberg's dormant [[Google+]] account rather than the active [[Facebook]] profile.<ref name="Google pushing Google">{{cite web|url = http://searchengineland.com/googles-knowledge-graph-finally-shows-social-networks-named-google-209171.|title = Google pushing Google+|date = 18 November 2014|accessdate = 1 December 2012|publisher = Third Door Media}}</ref> Further more this affected businesses in which if they do not have time to leverage all other social media sites, they knew they should use [[Google+]] to maximize their efforts since the data shows it impacts rankings more than [[Twitter]] and [[Facebook]].<ref>{{cite web|url = http://www.quicksprout.com/2014/01/31/how-social-signals-impact-search-engine-rankings/#.|title = Google+ impacts ranking more|date = 31 January 2014|accessdate = 1 December 2014|publisher = Quick Sprout}}</ref> in November 2014 these accusations started to die down because Google's Knowledge Graph started to finally show links to [[Facebook]], [[Twitter]], and other social media sites.<ref name="Google pushing Google"/>

[[Google]] was not the only one that garnished concerns over social search. After the introduction of [[Graph Search]] by [[Facebook]] many pointed out how [[Graph Search]] showed private information that isn't in web search.<ref>{{cite web|url = http://www.forbes.com/sites/tomiogeron/2013/01/15/live-facebook-announces-graph-search/|title = Graph Search results|date = 1 January 2013|accessdate = 1 December 2014|publisher = Forbes}}</ref> Information that was once obscure is now easier to dig up, which is why Facebook urges users to monitor post and pictures users are tagged in and filter and filter any content that users would not want to make public.<ref>{{cite web|url = http://www.forbes.com/sites/larrymagid/2013/01/15/facebooks-new-social-search-what-it-is-and-how-it-affects-your-privacy/|title = Graph Search Privacy Concerns|date = 15 January 2013|accessdate = 1 December 2014|publisher = Forbes}}</ref>

This in large points towards the biggest concern toward social search which is that social media networks don't have a vested interest in working with search engines. [[LinkedIn]] for example has taken steps to improve its own individual search functions in order to stray users from external search engines. Even [[Microsoft]] started working with [[Twitter]] in order to integrate some tweets into [[Bing]]'s search results in November 2013. Yet [[Twitter]] has its own search engine which points out how much value their data has and why they'd like to keep it in house.<ref>{{cite web|url = http://venturebeat.com/2014/06/30/microsoft-and-twitter-make-bing-a-better-social-search-engine/|title = Bing's twitter integration|date = 30 June 2014|accessdate = 1 December 2014|publisher = Venture Beat}}</ref> In the end though social search will never be truly comprehensive of the subjects that matter to people unless users opt to be completely public with their information.<ref>{{cite web|url = https://blog.hubspot.com/blog/tabid/6307/bid/34058/Facebook-Announces-New-Social-Search-Feature-Called-Graph-Search.aspx|title = User data will never be competently public|date = 15 January 2013|accessdate = 1 December 2014|publisher = HubSpot}}</ref>

==Social discovery==
Social discovery is the use of social preferences and personal information to predict what content will be desirable to the user.<ref name="Bailyn2012">{{cite book|last=Bailyn|first=Evan|title=Outsmarting Social Media: Profiting in the Age of Friendship Marketing|url=http://books.google.com/books?id=M97RiODwKHEC&pg=PT51|accessdate=20 January 2014|date=2012-04-12|publisher=Que Publishing|isbn=9780132861403|pages=51}}</ref> Technology is used to discover new people and sometimes new experiences shopping, meeting friends or even traveling.<ref>{{cite web|last=Burke|first=Amy|url=http://mashable.com/2013/07/08/social-discovery-apps/|publisher=Mashable|title=Are Social Discovery Apps Too Creepy?}}</ref>  The discovery of new people is often in real-time, enabled by [[mobile apps]]. However, social discovery is not limited to meeting people in real-time, it also leads to sales and revenue for companies via social media.<ref>{{cite web|last=Cubie|first=Gregor|url=http://www.thedrum.com/news/2013/10/02/social-discovery-sites-influence-retail-expanding-rakutens-playcom-numbers-find|publisher=The Drum|title=Social Discovery sites' influence on retail expanding}}</ref>  An example of retail would be the addition of social sharing with music, through the iTunes music store. There is a social component to discovering new music <ref>{{cite web|last=Constine|first=Josh|url=http://techcrunch.com/2013/09/10/bitcovery/|publisher=TechCrunch|title=Bitcovery Brings A Desperately Needed Social Discovery Layer To The iTunes Store}}</ref> Social discovery is at the basis of [[Facebook]]'s profitability, generating ad revenue by targeting the ads to users using the social connections to enhance the commercial appeal.<ref name="Bailyn2012"/>

==Developments==

[[Google]] may be falling behind in terms of social search, but in reality they see the potential and importance of this technology with [[Web 3.0]] and [[web semantics]]. The importance of social media lies within how Semantic search works. Semantic search understands much more, including where you are, the time of day, your past history, and many other factors including social connections, and social signals. The first step in order to achieve this will be to teach algorithms to understand the relationship between things.<ref>{{cite web|url = http://www.socialmediatoday.com/content/google-semantic-search|title = Google Semantic Search|date = 28 February 2014|accessdate = 1 December 2014|publisher = Social Media Today}}</ref>

However this is not possible unless social media sites decide to work with search engines, which is difficult since everyone would like to be the main toll bridge to the internet. As we continue on, and more articles are referred by social media sites, the main concern becomes what good is a search engine without the data of users.

One development that seeks to redefine search is the combination of [[distributed search]] with social search. The goal is a basic search service whose operation is controlled and maintained by the community itself. This would largely work like Peer to Peer networks in which users provide the data they seems appropriate. Since the data used by search engines belongs to the user they should have absolute control over it. The infrastructure required for a search engine is already available in the from of thousands of idle desktops and extensive residential broadband access.<ref>{{cite web|url = http://www2009.eprints.org/242/|title = Towards Distributed Social Search Engines|accessdate = 1 December 2014|publisher = EPrints}}</ref>

== See also ==
* [[Collaborative filtering]]
* [[Enterprise bookmarking]]
* [[Human search engine]]
* [[Relevance feedback]]
* [[Social software]]

== References ==
{{reflist}}
{{Internet search}}

[[Category:Searching]]
[[Category:Social search| ]]
[[Category:Social software|Search]]
>>EOP<<
192<|###|>Search/Retrieve via URL
'''Search/Retrieve via URL''' ('''SRU''') is a standard search protocol for [[Internet search]] queries, utilizing [[Contextual Query Language]] (CQL), a standard query syntax for representing queries.

==See also==
* [[Search/Retrieve Web Service]]

==External links==
* [http://www.loc.gov/standards/sru/ Search/Retrieve via URL] at [[Library of Congress]]

{{Internet search}}

{{DEFAULTSORT:Search Retrieve via URL}}
[[Category:Data search engines]]
[[Category:Searching]]
[[Category:Uniform resource locator]]

{{web-stub}}
>>EOP<<
198<|###|>Search by sound
Searching by sound for now has limited uses. There are a handful of applications, specifically for mobile devises that utilizes searching by sound. [[Shazam (service)]], [[Soundhound]], Midomi, and others has seen considerable success by using a simple algorithm to match an acoustic fingerprint to a song in a library. These applications takes a sample clip of a song, or a user generated melody and checks a music library to see where the clip matches with the song. From there, song information will be pulled up and displayed to the user. 

These kind of applications is mainly used for finding a song that the user does not already know. 

Searching by sound is not limited so just identifying [[songs]], but also for identifying [[melodies]], [[Music|tunes]] or [[advertisements]], [[sound library management]] and [[video files]].

==Acoustic Fingerprinting==
The way these apps search by sound is through generating an acoustic fingerprint; a digital summary of the sound. A microphone is used to pick up an audio sample, which is then broken down into a simple numeric signature, a code unique to each track. Using the same method of fingerprinting sounds, when Shazam picks up a sound clip, it will generate a signature for that clip. Then its simple pattern matching from there using an extensive audio music database. 

The practice of using [[acoustic fingerprints]] is not limited to just music however, but other areas of the entertainment business as well. Shazam also can identify television shows with the same technique of acoustic fingerprinting. Of course, this method of breaking down a sound sample into a unique signature is useless unless there is an extensive database of music with keys to match with the samples. Shazam has over 11 million songs in its database. <ref> http://www.slate.com/articles/technology/technology/2009/10/that_tune_named.html </ref>

Other services such as Midomi and Soundhound allow users to add to that library of music in order to expand the chances to match a sound sample with its corresponding sound. 

==Spectogram==
Generating a signature from the song is essential for searching by sound, and can be tricky. However, the way certain applications such as Shazam found a way around this issue by creating a spectrogram. 

Any piece of music can be translated to a time frequency graph called a spectrogram. For each song in its database, each song is basically a graph that plots the three dimensions of music, frequency vs amplitude (intensity) vs time. The algorithm then picks out the points which peaks in the graph, labeled as higher energy content. In practice, this seems to work out to about three points per song. <ref> http://www.soyoucode.com/2011/how-does-shazam-recognize-song </ref>

This is how a song can be identified with just two or three notes. This greatly reduces the impact that [[background noise]] has on searching by sound. The key values taken away from this would be frequency in hertz and time in seconds. Shazam builds their fingerprint catalog out as a hash table, where the key is the frequency. They do not just mark a single point in the spectrogram, rather they mark a pair of points: the peak intensity plus a second anchor point. <ref> Li-Chun Wang, Avery. "An Industrial-Strength Audio Search Algorithm." Columbia University. Web. 1 Dec. 2014. <http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf>. </ref> So their key is not just a single frequency, it is a hash of the frequencies of both points.  This leads to less hash collisions which in turn speeds up catalog searching by several orders of magnitude by allowing them to take greater advantage of the tables constant (O(1)) look-up time. <ref> "How Shazam Works." Free Wont. Web. 1 Dec. 2014. <http://laplacian.wordpress.com/2009/01/10/how-shazam-works/>. </ref>

This method of acoustic fingerprinting allows applications such as Shazam to have the ability to differentiate between two closely related covers, as well as not having to account for popularity of a certain song. 

==Query by Humming==
Midomi and Soundhound both utilize Query by Humming, or QbH. This is a branch off of acoustic fingerprints, but is still a musical retrieval system. After receiving a user generated hummed melody, which is the input query, and returns a ranked list of songs that is closest to the user query. 

==References==
{{reflist}}




[[Category:Searching]]
>>EOP<<
204<|###|>Jaccard index
The '''Jaccard index''', also known as the '''Jaccard similarity coefficient''' (originally coined ''coefficient de communaute'' by [[Paul Jaccard]]), is a [[statistic]] used for comparing the [[Similarity measure|similarity]] and [[diversity index|diversity]] of [[Sample (statistics)|sample]] sets. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the [[intersection (set theory)|intersection]] divided by the size of the [[Union (set theory)|union]] of the sample sets:

:<math> J(A,B) = {{|A \cap B|}\over{|A \cup B|}}.</math>

(If ''A'' and ''B'' are both empty, we define ''J''(''A'',''B'')&nbsp;=&nbsp;1.) Clearly, 
:<math> 0\le J(A,B)\le 1.</math>

The [[MinHash]] min-wise independent permutations [[locality sensitive hashing]] scheme may be used to efficiently compute an accurate estimate of the Jaccard similarity coefficient of pairs of sets, where each set is represented by a constant-sized signature derived from the minimum values of a [[hash function]].

The '''Jaccard distance''', which measures ''dis''similarity between sample sets, is complementary to the Jaccard coefficient and is obtained by subtracting the Jaccard coefficient from 1, or, equivalently, by dividing the difference of the sizes of the union and the intersection of two sets by the size of the union:

:<math> d_J(A,B) = 1 - J(A,B) = { { |A \cup B| - |A \cap B| } \over |A \cup B| }.</math>

An alternate interpretation of the Jaccard distance is as the ratio of the size of the [[symmetric difference]] <math>A \triangle B = (A \cup B) - (A \cap B)</math> to the union. 

This distance is a [[Distance function|metric]] on the collection of all finite sets.<ref name="lipkus">{{citation |last=Lipkus |first=Alan H
|title=A proof of the triangle inequality for the Tanimoto distance
|journal=J Math Chem |volume=26 |number=1-3 |year=1999 |pages=263265 }}</ref><ref>{{citation |last1=Levandowsky |first1=Michael |last2=Winter |first2=David |title=Distance between sets|journal=Nature |volume=234 |number=5 |year=1971 |pages=3435 |doi=10.1038/234034a0}}</ref>

There is also a version of the Jaccard distance for [[measure (mathematics)|measures]], including [[probability measure]]s. If <math>\mu</math> is a measure on a [[measurable space]] <math>X</math>, then we define the Jaccard coefficient by <math>J_\mu(A,B) = {{\mu(A \cap B)} \over {\mu(A \cup B)}}</math>, and the Jaccard distance by <math>d_\mu(A,B) = 1 - J_\mu(A,B) = {{\mu(A \triangle B)} \over {\mu(A \cup B)}}</math>. Care must be taken if <math>\mu(A \cup B) = 0</math> or <math>\infty</math>, since these formulas are not well defined in that case.

== Similarity of asymmetric binary attributes ==
Given two objects, ''A'' and ''B'', each with ''n'' [[binary numeral system|binary]] attributes, the Jaccard coefficient is a useful measure of the overlap that ''A'' and ''B'' share with their attributes.  Each attribute of ''A'' and ''B'' can either be 0 or 1.  The total number of each combination of attributes for both ''A'' and ''B'' are specified as follows:
:<math>M_{11}</math> represents the total number of attributes where ''A'' and ''B'' both have a value of 1.
:<math>M_{01}</math> represents the total number of attributes where the attribute of ''A'' is 0 and the attribute of ''B'' is 1.
:<math>M_{10}</math> represents the total number of attributes where the attribute of ''A'' is 1 and the attribute of ''B'' is 0.
:<math>M_{00}</math> represents the total number of attributes where ''A'' and ''B'' both have a value of 0.
Each attribute must fall into one of these four categories, meaning that
:<math>M_{11} + M_{01} + M_{10} + M_{00} = n.</math>

The Jaccard similarity coefficient, ''J'', is given as
:<math>J = {M_{11} \over M_{01} + M_{10} + M_{11}}.</math>

The Jaccard distance, ''d''<sub>''J''</sub>, is given as
:<math>d_J = {M_{01} + M_{10} \over M_{01} + M_{10} + M_{11}}.</math>

== Generalized Jaccard similarity and distance ==

If <math>\mathbf{x} = (x_1, x_2, \ldots, x_n)</math> and <math>\mathbf{y} = (y_1, y_2, \ldots, y_n)</math> are two vectors with all real <math>x_i, y_i \geq 0</math>, then their Jaccard similarity coefficient is defined as
:<math>J(\mathbf{x}, \mathbf{y}) = \frac{\sum_i \min(x_i, y_i)}{\sum_i \max(x_i, y_i)},</math>
and Jaccard distance
:<math>d_J(\mathbf{x}, \mathbf{y}) = 1 - J(\mathbf{x}, \mathbf{y}).</math>

With even more generality, if <math>f</math> and <math>g</math> are two non-negative measurable functions on a measurable space <math>X</math> with measure <math>\mu</math>, then we can define
:<math>J(f, g) = \frac{\int\min(f, g) d\mu}{\int \max(f, g)  d\mu},</math>
where <math>\max</math> and <math>\min</math> are pointwise operators. Then Jaccard distance is
:<math>d_J(f, g) = 1 - J(f, g).</math>

Then, for example, for two measurable sets <math>A, B \subseteq X</math>, we have <math>J_\mu(A,B) = J(\chi_A, \chi_B),</math> where <math>\chi_A</math> and <math>\chi_B</math> are the characteristic functions of the corresponding set.

== Tanimoto similarity and distance ==

<!-- [[Tanimoto score]] redirects here, please change that redirect if you change this section title -->

Various forms of functions described as  Tanimoto similarity  and Tanimoto distance occur  in the literature and on the Internet. Most of these are synonyms for Jaccard similarity and Jaccard distance, but some are mathematically different. Many sources<ref>For example {{cite book |first=Huihuan |last=Qian |first2=Xinyu |last2=Wu |first3=Yangsheng |last3=Xu |title=Intelligent Surveillance Systems |publisher=Springer |year=2011 |page=161 |isbn=978-94-007-1137-2 }}</ref> cite an  unavailable IBM Technical Report<ref>{{cite journal |last=Tanimoto |first=T. |title=An Elementary Mathematical theory of Classification and Prediction |journal=Internal IBM Technical Report |date=17 Nov 1957 |issue=8? |volume=1957 }}</ref> as the seminal reference.

In "A Computer Program for Classifying Plants", published in October 1960,<ref>{{cite journal |first=David J. |last=Rogers |first2=Taffee T. |last2=Tanimoto |title=A Computer Program for Classifying Plants |journal=[[Science (journal)|Science]] |volume=132 |issue=3434 |pages=11151118 |year=1960 |doi=10.1126/science.132.3434.1115 }}</ref> a method of classification based on a similarity ratio, and a derived distance function, is given. It seems that this is  the most authoritative  source for the meaning of the terms "Tanimoto similarity" and "Tanimoto Distance". The similarity ratio is equivalent to Jaccard similarity, but the distance function is ''not'' the same as Jaccard distance.

=== Tanimoto's definitions of similarity and distance ===

In that paper, a "similarity ratio" is  given over [[Bit array|bitmaps]], where each bit of a fixed-size array represents the presence or absence of a characteristic in the plant being modelled. The definition of the ratio is the number of common bits, divided by the number of bits set (i.e. nonzero) in either sample.

Presented in mathematical terms, if samples ''X'' and ''Y'' are bitmaps, <math>X_i</math> is the ''i''th bit of ''X'', and <math> \land , \lor </math> are [[bitwise operation|bitwise]] ''[[logical conjunction|and]]'', ''[[logical disjunction|or]]'' operators respectively, then the similarity ratio <math>T_s</math> is

: <math> T_s(X,Y) =  \frac{\sum_i ( X_i \land Y_i)}{\sum_i ( X_i \lor Y_i)}</math>

If each sample is modelled instead as a set of attributes, this value is  equal to the Jaccard coefficient of the two sets. Jaccard is not cited in the paper, and it seems likely that the authors were not aware of it.

Tanimoto goes on to define a "distance coefficient" based on this ratio, defined for bitmaps with non-zero similarity:

: <math>T_d(X,Y) = -\log_2 ( T_s(X,Y) ) </math>

This coefficient is, deliberately, not a distance metric. It is chosen to allow the possibility of two specimens, which are quite different from each other, to both be similar to a third. It is  easy to construct an example which disproves the property of [[Triangle inequality#Metric space|triangle inequality]].

=== Other definitions of Tanimoto distance ===

Tanimoto distance is often referred to, erroneously, as a synonym for Jaccard distance <math> 1 - T_s</math>. This function is a proper distance metric. "Tanimoto Distance" is often stated as being a proper distance metric, probably because of its confusion with Jaccard distance.

If Jaccard or Tanimoto similarity is expressed over a bit vector, then it can be written as

: <math>
f(A,B) =\frac{ A \cdot B}{\vert A\vert^2 +\vert B\vert^2 -  A \cdot B }
</math>

where the same calculation is expressed in terms of vector scalar product and magnitude. This representation relies on the fact that, for a bit vector (where the value of each dimension is either 0 or 1) then <math>A \cdot B = \sum_i A_iB_i = \sum_i ( A_i \land B_i)</math> and <math>{\vert A\vert}^2 = \sum_i A_i^2 = \sum_i A_i </math>.

This is a potentially confusing representation, because the function as expressed over vectors is more general, unless its domain is explicitly restricted. Properties of <math> T_s </math> do not necessarily extend to <math>f</math>. In particular, the difference function <math>1 - f</math> does not preserve [[triangle inequality]], and is not therefore a proper distance metric, whereas <math>1 - T_s </math> is.

There is a real danger that the combination of "Tanimoto Distance" being defined using this formula, along with the statement "Tanimoto Distance is a proper distance metric" will lead to the false conclusion that the function <math>1 - f</math> is in fact a distance metric over vectors or multisets in general, whereas its use in similarity search or clustering algorithms may fail to produce correct results.

Lipkus<ref name="lipkus" /> uses a definition of Tanimoto similarity which is equivalent to <math>f</math>, and refers to Tanimoto distance as the function <math> 1 - f</math>. It is however made clear within the paper that the context is restricted by the use of a (positive) weighting vector <math>W</math> such that, for any vector ''A'' being considered, <math> A_i \in \{0,W_i\} </math>. Under these circumstances, the  function  is a proper distance metric, and so a set of vectors governed by such a weighting vector forms a metric space under this function.

== See also ==
* [[Srensen similarity index]]
* [[simple matching coefficient]]
* [[Mountford's index of similarity]]
* [[Most frequent k characters]]
* [[Hamming distance]]
* [[Dice's coefficient]], which is equivalent: <math>J=D/(2-D)</math> and <math>D=2J/(1+J)</math>
* [[Tversky index]]
* [[Correlation]]
* [[Mutual information]], a normalized [[Mutual information#Metric|metricated]] variant of which is an entropic Jaccard distance.

==Notes==
{{reflist}}

{{More footnotes|date=March 2011}}

== References ==
*{{citation|first1=Pang-Ning|last1=Tan|first2=Michael|last2=Steinbach|first3=Vipin|last3=Kumar|title=Introduction to Data Mining|year=2005|isbn=0-321-32136-7}}.
*{{citation|first=Paul|last=Jaccard|authorlink=Paul Jaccard|year=1901|title=Etude comparative de la distribution florale dans une portion des Alpes et des Jura|journal=Bulletin de la Societe Vaudoise des Sciences Naturelles|volume=37|pages=547579}}.
*{{citation|first=Paul|last=Jaccard|authorlink=Paul Jaccard|year=1912|title=The distribution of the flora in the alpine zone|journal=New Phytologist|volume=11|pages=3750|doi=10.1111/j.1469-8137.1912.tb05611.x}}.

== External links ==
* [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap2_data.pdf Introduction to Data Mining lecture notes from Tan, Steinbach, Kumar]
* [http://sourceforge.net/projects/simmetrics/ SimMetrics a sourceforge implementation of Jaccard index and many other similarity metrics]
* [http://www.idea-miner.de/cgi-bin/INT_Tools/ver_vergleich_0_1/cmp_menu2.cgi Web based tool for comparing texts using Jaccard coefficient]
* [http://www.gettingcirrius.com/2011/01/calculating-similarity-part-2-jaccard.html Tutorial on how to calculate different similarities]
* Open Source [https://github.com/rockymadden/stringmetric/blob/master/core/src/main/scala/com/rockymadden/stringmetric/similarity/JaccardMetric.scala Jaccard] [[Scala programming language|Scala]] implementation as part of the larger [http://rockymadden.com/stringmetric/ stringmetric project]

{{DEFAULTSORT:Jaccard Index}}
[[Category:Index numbers]]
[[Category:Measure theory]]
[[Category:Clustering criteria]]
[[Category:String similarity measures]]
>>EOP<<
210<|###|>DamerauLevenshtein distance
In [[information theory]] and [[computer science]], the '''DamerauLevenshtein distance''' (named after [[Frederick J. Damerau]] and [[Vladimir I. Levenshtein]]<ref>{{cite conference |last1=Brill |first1=Eric |last2=Moore |first2=Robert C. |year=2000 |title=An Improved Error Model for Noisy Channel Spelling Correction |conference=Proceedings of the 38th Annual Meeting on Association for Computational Linguistics |pages=286293 |doi=10.3115/1075218.1075255 |url=http://acl.ldc.upenn.edu/P/P00/P00-1037.pdf}}</ref><ref name="bard"/><ref>{{cite conference |last1=Li |last2=et al. |year=2006|title=Exploring distributional similarity based models for query spelling correction |conference=Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics |pages=10251032 |doi=10.3115/1220175.1220304 |url=http://acl.ldc.upenn.edu/P/P06/P06-1129.pdf}}</ref>) is a [[Metric (mathematics)|distance]] ([[string metric]]) between two [[string (computer science)|strings]], i.e., finite sequence of symbols, given by counting the minimum number of operations needed to transform one string into the other, where an operation is defined as an insertion, deletion, or substitution of a single character, or a [[transposition (mathematics)|transposition]] of two '''adjacent''' characters.  In his seminal paper,<ref>{{Citation |last=Damerau |first=Fred J. |author-link=Frederick J. Damerau |title=A technique for computer detection and correction of spelling errors |journal=Communications of the ACM |publisher=ACM |volume=7 |issue=3 |pages=171176 |date=March 1964 |doi=10.1145/363958.363994}}</ref> Damerau not only distinguished these four edit operations but also stated that they correspond to more than 80% of all human misspellings. Damerau's paper considered only misspellings that could be corrected with at most one edit operation.

The DamerauLevenshtein distance differs from the classical [[Levenshtein distance]] by including transpositions among its allowable operations. The classical Levenshtein distance only allows insertion, deletion, and substitution operations.<ref>{{citation |url= <!-- copylink? http://profs.sci.univr.it/~liptak/ALBioinfo/files/levenshtein66.pdf--> |first=Vladimir I. |last=Levenshtein |title=Binary codes capable of correcting deletions, insertions, and reversals |journal=Soviet Physics Doklady |date=February 1966 |volume=10 |issue=8 |pages=707&ndash;710}}</ref> Modifying this distance by including transpositions of adjacent symbols produces a different distance measure, known as the DamerauLevenshtein distance.<ref name="bard">{{citation
 | last = Bard | first = Gregory V.
 | contribution = Spelling-error tolerant, order-independent pass-phrases via the DamerauLevenshtein string-edit distance metric
 | isbn = 1-920-68285-6
 | location = Darlinghurst, Australia
 | pages = 117124
 | publisher = Australian Computer Society, Inc.
 | series = Conferences in Research and Practice in Information Technology
 | title = Proceedings of the Fifth Australasian Symposium on ACSW Frontiers : 2007, Ballarat, Australia, January 30 - February 2, 2007
 | url = http://dl.acm.org/citation.cfm?id=1274531.1274545
 | volume = 68
 | year = 2007}}. The isbn produces two hits: a 2007 work and a 2010 work at World Cat.</ref>

While the original motivation was to measure distance between human misspellings to improve applications such as [[spell checker]]s, DamerauLevenshtein distance has also seen uses in biology to measure the variation between [[DNA]].<ref>The method used in: {{Citation
| last1  = Majorek         | first1 = Karolina A.
| last2  = Dunin-Horkawicz | first2 = Stanisaw
| last3  = Steczkiewicz    | first3 = Kamil
| last4  = Muszewska       | first4 = Anna
| last5  = Nowotny         | first5 = Marcin
| last6  = Ginalski        | first6 = Krzysztof
| last7  = Bujnicki        | first7 = Janusz M.
| display-authors = 2
| title   = The RNase H-like superfamily: new members, comparative structural analysis and evolutionary classification
| journal = Nucleic Acids Research
| volume  = 42
| issue   = 7
| pages   = 41604179
| year    = 2013
| doi     = 10.1093/nar/gkt1414
| url     = http://nar.oxfordjournals.org/content/42/7/4160.full
}}</ref>

== Definition ==
The DamerauLevenshtein distance between two strings <math>a</math> and <math>b</math> is given by <math>d_{a,b}(|a|,|b|)</math> where:

<math>\qquad d_{a,b}(i,j) = \begin{cases}
  \max(i,j) & \text{ if} \min(i,j)=0, \\
\min \begin{cases}
          d_{a,b}(i-1,j) + 1 \\
          d_{a,b}(i,j-1) + 1 \\
          d_{a,b}(i-1,j-1) + 1_{(a_i \neq b_j)} \\
          d_{a,b}(i-2,j-2) + 1 
       \end{cases} & \text{ if } i,j > 1 \text{ and } a_i = b_{j-1} \text{ and } a_{i-1} = b_j \\
  \min \begin{cases}
          d_{a,b}(i-1,j) + 1 \\
          d_{a,b}(i,j-1) + 1 \\
          d_{a,b}(i-1,j-1) + 1_{(a_i \neq b_j)}
       \end{cases} & \text{ otherwise.}
\end{cases}</math>

where  <math>1_{(a_i \neq b_j)}</math> is the [[indicator function]] equal to 0 when  <math>a_i = b_j</math> and equal to 1 otherwise.

Each recursive call matches one of the cases covered by the DamerauLevenshtein distance:
* <math>d_{a,b}(i-1,j) + 1</math> corresponds to a deletion (from a to b).
* <math>d_{a,b}(i,j-1) + 1</math> corresponds to an insertion (from a to b).
* <math>d_{a,b}(i-1,j-1) + 1_{(a_i \neq b_j)} </math> corresponds to a match or mismatch, depending on whether the respective symbols are the same.
* <math>d_{a,b}(i-2,j-2) + 1 </math> corresponds to a [[transposition (mathematics)|transposition]] between two successive symbols.

== Algorithm ==
Presented here are two algorithms: the first,<ref>{{cite paper | author1 = B. J. Oommen | author2 = R. K. S. Loke | title = Pattern recognition of strings with substitutions, insertions, deletions and generalized transpositions | id = {{citeseerx|10.1.1.50.1459}} | doi=10.1016/S0031-3203(96)00101-X }}</ref> simpler one, computes what is known as the [[optimal string alignment]]{{Citation needed|date=May 2013}} (sometimes called the ''restricted edit distance''{{Citation needed|date=May 2013}}), while the second one<ref name="LW75">{{Citation |first1=Roy |last1=Lowrance |first2=Robert A. |last2=Wagner |title=An Extension of the String-to-String Correction Problem |journal=JACM |volume=22 |issue=2 |pages=177183 |date=April 1975 |doi=10.1145/321879.321880}}</ref> computes the DamerauLevenshtein distance with adjacent transpositions. Adding transpositions adds significant complexity. The difference between the two algorithms consists in that the ''optimal string alignment algorithm'' computes the number of edit operations needed to make the strings equal under the condition that '''no substring is edited more than once''', whereas the second one presents no such restriction.

Take for example the edit distance between '''CA''' and '''ABC'''. The DamerauLevenshtein distance LD('''CA''','''ABC''') = 2 because '''CA'''  '''AC'''  '''ABC''', but the optimal string alignment distance OSA('''CA''','''ABC''') = 3 because if the operation '''CA'''  '''AC''' is used, it is not possible to use '''AC'''  '''ABC''' because that would require the substring to be edited more than once, which is not allowed in OSA, and therefore the shortest sequence of operations is '''CA'''  '''A'''  '''AB'''  '''ABC'''. Note that for the optimal string alignment distance, the [[triangle inequality]] does not hold: OSA('''CA''','''AC''') + OSA('''AC''','''ABC''') < OSA('''CA''','''ABC'''), and so it is not a true metric.
 
===Optimal string alignment distance===
Firstly, let us consider a direct extension of the formula used to calculate [[Levenshtein distance]].  Below is [[pseudocode]] for a function ''OptimalStringAlignmentDistance'' that takes two strings, ''str1'' of length ''lenStr1'', and ''str2'' of length ''lenStr2'', and computes the optimal string alignment distance between them:

<syntaxhighlight lang="pascal">
 int OptimalStringAlignmentDistance(char str1[1..lenStr1], char str2[1..lenStr2])
    // d is a table with lenStr1+1 rows and lenStr2+1 columns
    declare int d[0..lenStr1, 0..lenStr2]

    // i and j are used to iterate over str1 and str2
    declare int i, j, cost

    // for loop is inclusive, need table 1 row/column larger than string length
    for i from 0 to lenStr1
        d[i, 0] := i
    for j from 1 to lenStr2
        d[0, j] := j

    // pseudo-code assumes string indices start at 1, not 0
    // if implemented, make sure to start comparing at 1st letter of strings
    for i from 1 to lenStr1
        for j from 1 to lenStr2
            if str1[i] = str2[j] then cost := 0
                                 else cost := 1
            d[i, j] := minimum(
                                 d[i-1, j  ] + 1,     // deletion
                                 d[i  , j-1] + 1,     // insertion
                                 d[i-1, j-1] + cost   // substitution
                             )
            if(i > 1 and j > 1 and str1[i] = str2[j-1] and str1[i-1] = str2[j]) then
                d[i, j] := minimum(
                                 d[i, j],
                                 d[i-2, j-2] + cost   // transposition
                              )                        
  
    return d[lenStr1, lenStr2]
</syntaxhighlight>

Basically this is the algorithm to compute [[Levenshtein distance]] with one additional recurrence:

<syntaxhighlight lang="pascal">
            if(i > 1 and j > 1 and str1[i] = str2[j-1] and str1[i-1] = str2[j]) then
                d[i, j] := minimum(
                                 d[i, j],
                                 d[i-2, j-2] + cost   // transposition
                              )
</syntaxhighlight>

===Distance with adjacent transpositions===
Here is the second algorithm that computes the true DamerauLevenshtein distance with adjacent transpositions (ActionScript 3.0); this function requires as an additional parameter the size of the alphabet (''C''), so that all entries of the arrays are in 0..(''C''&minus;1):

<syntaxhighlight lang='actionscript3'>static public function damerauLevenshteinDistance(a:Array, b:Array, C:uint):uint
{
    // "infinite" distance is just the max possible distance
    var INF:uint = a.length + b.length;

    // make and initialize the character array indices            
    var DA:Array = new Array(C);
    for (var k:uint = 0; k < C; ++k) DA[k]=0;

    // make the distance matrix H[-1..a.length][-1..b.length]
    var H:matrix = new matrix(a.length+2,b.length+2);
    
    // initialize the left and top edges of H
    H[-1][-1] = INF;
    for (var i:uint = 0; i <= a.length; ++i)
    {
        H[i][-1] = INF;
        H[i][ 0] = i;
    }
    for (var j:uint = 0; j <= b.length; ++j)
    {
        H[-1][j] = INF;
        H[ 0][j] = j;
    }

    // fill in the distance matrix H
    // look at each character in a
    for (var i:uint = 1; i <= a.length; ++i)
    {
        var DB:uint = 0;
        // look at each character in b
        for (var j:uint = 1; j <= b.length; ++j)
        {
            var i1:uint = DA[b[j-1]];
            var j1:uint = DB;
            var cost:uint;
            if (a[i-1] == b[j-1])
               {
                 cost = 0;
                 DB   = j;
               }
            else
               cost = 1;
            H[i][j] = Math.min(    H[i-1 ][j-1 ] + cost,  // substitution
                                   H[i   ][j-1 ] + 1,     // insertion
                                   H[i-1 ][j   ] + 1,     // deletion
                                   H[i1-1][j1-1] + (i-i1-1) + 1 + (j-j1-1));
        }
        DA[a[i-1]] = i;
    }
    return H[a.length][b.length];
}
</syntaxhighlight>

:<small>'''Note''': the algorithm given in the paper uses alphabet 1..C rather than the 0..''C''&minus;1 used here; the paper indexes arrays: H[&minus;1..|A|,&minus;1..|B|] and DA[1..C]; here DA[0..C&minus;1] is used; the paper seems to be missing the necessary line H[&minus;1,&minus;1]&nbsp;=&nbsp;INF</small>

To devise a proper algorithm to calculate unrestricted DamerauLevenshtein distance note that there always exists an optimal sequence of edit operations, where once-transposed letters are never modified afterwards. (This holds as long as the cost of a transposition, <math>W_T</math>, is at least the average of the cost of an insertion and deletion, i.e., <math>2W_T \ge W_I+W_D</math>.<ref name="LW75"/>) Thus, we need to consider only two symmetric ways of modifying a substring more than once: (1) transpose letters and insert an arbitrary number of characters between them, or (2) delete a sequence of characters and transpose letters that become adjacent after deletion. The straightforward implementation of this idea gives an algorithm of cubic complexity: <math>O\left (M \cdot N \cdot \max(M, N) \right )</math>, where ''M'' and ''N'' are string lengths. Using the ideas of Lowrance and Wagner,<ref name="LW75"/> this naive algorithm can be improved to be <math>O\left (M \cdot N \right)</math> in the worst case.

It is interesting that the [[bitap algorithm]] can be modified to process transposition. See the information retrieval section of{{ref|itman}} for an example of such an adaptation.

== Applications ==
DamerauLevenshtein distance plays an important role in [[natural language processing]]. In natural languages, strings are short and the number of errors (misspellings) rarely exceeds 2. In such circumstances, restricted and real edit distance differ very rarely. Oommen and Loke{{ref|OO}} even mitigated the limitation of the restricted edit distance by introducing ''generalized transpositions''. Nevertheless, one must remember that the restricted edit distance usually does not satisfy the [[triangle inequality]] and, thus, cannot be used with [[metric tree]]s.

=== DNA ===
Since [[DNA]] frequently undergoes insertions, deletions, substitutions, and transpositions, and each of these operations occurs on approximately the same timescale, the DamerauLevenshtein distance is an appropriate metric of the variation between two strands of DNA. More common in DNA, protein, and other bioinformatics related alignment tasks is the use of closely related algorithms such as [[NeedlemanWunsch algorithm]] or [[SmithWaterman algorithm]].

=== Fraud detection ===
The algorithm can be used with any set of words, like vendor names. Since entry is manual by nature there is a risk of entering a false vendor. A fraudster employee may enter one real vendor such as "Rich Heir Estate Services" versus a false vendor "Rich Hier State Services". The fraudster would then create a false bank account and have the company route checks to the real vendor and false vendor. The DamerauLevenshtein algorithm will detect the transposed and dropped letter and bring attention of the items to a fraud examiner.

== See also ==
* [[Approximate string matching]]
* [[Levenshtein automata]]
* [[Typosquatting]]

== References ==
{{Reflist|30em}}

== Further reading ==
* {{Citation |first=Gonzalo |last=Navarro |title=A guided tour to approximate string matching |journal=ACM Computing Surveys |volume=33 |issue=1 |pages=3188 |date=March 2001 |doi=10.1145/375360.375365 }}

{{DEFAULTSORT:Damerau-Levenshtein Distance}}
[[Category:String similarity measures]]
[[Category:Information theory]]
[[Category:Dynamic programming]]
>>EOP<<
216<|###|>Social Sciences Citation Index
The '''Social Sciences Citation Index''' ('''SSCI''') is an interdisciplinary  [[citation index]] product of  [[Thomson Reuters]]' Healthcare & Science division. It was developed by the [[Institute for Scientific Information]] (ISI) from the [[Science Citation Index]].

This citation database covers some 2,474 of the world's leading [[academic journal|journals]] of [[social sciences]] across more than 50 [[academic discipline|disciplines]].<ref>{{cite web
  | title = Social Sciences Citation Index 
  | url = http://scientific.thomson.com/products/ssci/
  | accessdate = 2008-06-11 }}</ref> It is made available online through the [[Web of Science]] service for a fee.  This database product provides information to identify the articles  cited most frequently and by what publisher and author.

== Criticism ==
In 2004 economists [[Daniel B. Klein]] and Eric Chiang conducted a survey of the Social Sciences Citation Index and identified a bias against free market oriented research. In addition to an ideological bias, Klein and Chiang also identified several methodological deficiencies that encouraged the over-counting of citations, and they argue that the Social Sciences Citation Index does a poor job reflecting the relevance and accuracy of articles.<ref>Daniel Klein and Eric Chiang. [http://econjwatch.org/articles/the-social-science-citation-index-a-black-box-with-an-ideological-bias The Social Science Citation Index: A Black Boxwith an Ideological Bias?] ''Econ Journal Watch'', Volume 1, Number 1, April 2004, pp 134-165.</ref>

==See also==
* [[Arts and Humanities Citation Index]]
* [[Science Citation Index]]

==References==
{{reflist}}

==External links==
* [http://thomsonreuters.com/products_services/science/science_products/a-z/social_sciences_citation_index Introduction to SSCI]

{{Thomson Reuters}}
[[Category:Thomson family]]
[[Category:Thomson Reuters]]
[[Category:Social sciences literature]]
[[Category:Citation indices]]

{{database-stub}}
{{sci-stub}}
>>EOP<<
222<|###|>Materials Science Citation Index
{{Third-party|date=February 2013}}
'''The Materials Science Citation Index''' is a [[citation index]], established in 1992, by [[Thomson ISI]] ([[Thomson Reuters]]). Its overall focus is [[citation|cited reference]] searching of the notable and significant [[science journal|journal literature]] in [[materials science]]. The database makes accessible the various [[physical properties|properties]], behaviors, and materials in the materials science discipline. This then encompasses [[applied physics]], [[ceramic engineering|ceramics]], [[Advanced composite materials (science & engineering)|composite materials]], [[metals]] and [[metallurgy]], [[polymer engineering]], [[semiconductors]], [[thin films]], [[biomaterial]]s, [[Dentistry|dental technology]], as well as [[optics]]. The [[database]] indexes relevant materials science information from over 6,000 [[scientific journal]]s that are part of the ISI database which is [[multidisciplinary]]. Author abstracts are searchable, which links articles sharing one or more [[bibliographic]] references. The database also allows a researcher to use an appropriate (or related to research) article as a base to search forward in time to discover more recently published articles that cite it.<ref name=msci-est>Pemberton, Julia K. "''Two new databases from ISI''." CD-ROM Professional 5.4 (1992): 107+. General OneFile. Web. 20 June 2010.</ref>

''Materials Science Citation Index'' lists 625 high impact journals, and is accessible via the [[Science Citation Index Expanded]] collection of databases.<ref name=msci-jnlList>[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=MS Materials Science Citation Index journal list]. Thomson Reuters. July 2010.</ref>

==Editions==
Coverage of Materials science is accomplished with the following editions:<ref name=MS-indexes>[http://science.thomsonreuters.com/mjl/scope/scope_scie/ Scope Notes]. Science Citation Index, Science Citation Index Expanded. Thomson Reuters. 2010.</ref><ref>[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=D Subject categories]. Science Citation Index Expanded. Thomson Reuters. 2010</ref>
*Materials Science, Ceramics
*Materials Science, Characterization & Testing
*Materials Science, Biomaterials
*Materials Science, Coatings & Films
*Materials Science, Composites
*Materials Science, Paper & Wood
*Materials Science, Multidisciplinary
*Materials Science, Textiles

==See also==
* [[Science Citation Index]]
* [[Academic publishing]]
* [[List of academic databases and search engines]]
* [[Social Sciences Citation Index]], which covers over 1500 journals, beginning with 1956
* [[Arts and Humanities Citation Index]], which covers over 1000 journals, beginning with 1975
* [[Impact factor]]
* [[VINITI Database RAS]]

==References==
{{Reflist}}

{{Thomson Reuters}}

[[Category:Thomson family]]
[[Category:Bibliographic databases]]
[[Category:Online databases]]
[[Category:Citation indices]]


{{science-journal-stub}}
>>EOP<<
228<|###|>Latin American Bibliography
{{Multiple issues|
{{orphan|date=March 2010}}
{{advert|date=August 2010}}
}}

The '''Latin American Bibliography''' refers to the set of [[databases]] and information services on [[academic journals]] from [[Latin America]] and the [[Caribbean]] created by the [[National Autonomous University of Mexico]] (UNAM) in the decade of the seventies.{{Clarify|date=August 2009}}

Nowadays, the Latin-American Bibliography is composed by the following databases: CLASE (''Latin-American Citations in [[Social Sciences]] and [[Humanities]]''); PERIODICA (''Index of Latin-American Journals in [[Science]]''); [[Latindex]] (''Regional Co-operative Information System for Scholarly Journals from [[Latin America]], the Caribbean, Spain and Portugal'').

These databases were created by a group of information professionals, who identified the need to register, preserve and give access to the Latin-American knowledge published in the main academic [[Academic journal|journals]] of the region. Within UNAM, the fostering institution of these information products was the Science and Humanities Information Center (CICH) created in 1971.

For the size of its collection of Latin-American journals, for the quantity of compiled records and for the duration and consistency of the project, the Latin-American Bibliography produced in the UNAM constitutes one of the most valuable resources for scholars and experts specializing in Latin-American affairs.{{Citation needed|date=August 2009}}

==Products==

Three databases are available through the web site of UNAMs General Directorate for Libraries [http://dgb.unam.mx General Directorate for Libraries]:

'''CLASE''' (''Latin-American Citations in Social Sciences and Humanities''). Bibliographical database, with more than 280,000 records, of which nearly 14,000 provide abstracts and links to the full text of the documents. It includes more than 1,400 journals specializing in Social Sciences, Humanities and Arts, from more than 20 countries of Latin America and the Caribbean. Documents not available in full text can be retrieved through the Document Supply Service of the Latin-American Serials Collection (Hemeroteca Latinoamericana) of the DGB. Direct link: [http://dgb.unam.mx/clase.html CLASE website]

'''PERIODICA''' (Index of Latin-American Journals in Science). Bibliographical database with more than 315,000 records, of which near 60,000 provide abstracts and links to the full text of the documents. The database indexes more than 1,500 journals specializing in Science and Technology, from more than 20 countries of Latin America and the Caribbean. Documents not available in full text can be retrieved through the Document Supply Service of the Latin-American Serials Collection (Hemeroteca Latinoamericana) of the DGB. Direct link: [http://dgb.unam.mx/periodica.html PERIODICA website]

'''[[Latindex]]''' (Regional Co-operative Information System for Scholarly Journals from Latin America, the Caribbean, Spain and Portugal). This initiative provides relevant information and data of the scholarly journals edited in the [[Iberoamerica]]n region. Three databases are produced through the collaborative work of the member institutions: '''Directory''',: with more than 17,000 records; '''Catalogue''', with more than 3,500 selected journals that fulfill international quality criteria and an '''Index of Electronic Journals''', offering nearly 3,000 links to available resources in full text. Direct link: [http://www.latindex.org Latindex website]

Currently, the Department of Latin-American Bibliography contributes to the production of two other Latin-American information products:

'''ASFA''' (''Aquatic Sciences and Fisheries Abstracts''). Bibliographical international database on Aquatic Sciences and [[Fisheries]], covering subject areas such as [[technology]] and [[Public administration|administration]] of the marine environments and its resources (salt and sweet waters), including its socioeconomic and juridical aspects. It offers abstracts of articles published in approximately 7,000 periodic publications, besides thesis, monographs and other not conventional literature. The contribution relative to the Mexican journals is produced in the Department of Latin-American Bibliography from 1981. Link: [http://www.fao.org/fishery/asfa ASFA website]

'''[[SciELO]] Mexico''' (''Scientific Electronic Library Online''). Open access electronic journals collection that includes a selection of the most recognized academic publications of the country in all areas of knowledge, previously selected accordingly to the most accepted criteria related to content and editorial standards. Currently it offers the full text of more than 2,500 articles from 28 academic Mexican journals. Direct link: [http://www.scielo.org.mx/scielo.php Scielo Mexico website]

Over the time, other databases were produced by the Department of Latin-American Bibliography during its more than 30 years of existence, namely:

'''BLAT''' (''Latin-American Bibliography I and II''), with information compiled from international sources, mainly documents from Latin-American origin (produced by Latin American authors and institutions) or those in which their object of study was related to the region. The database ceased in 1997. Another one was '''MEXINV''', as a subset of CLASE, offered bibliographical records of documents relative only to [[Mexico]]. This database ceased in the decade of the nineties.

===Institution===

Currently, the databases described above are produced by the Department of Latin-American Bibliography, part of the Assistant Office for Information Services of the General Directorate for Libraries (DGB) of the National Autonomous University of Mexico (UNAM). The original databases (BLAT, CLASE, PERIODICA, MEXINV and Latindex) were created by the Science and Humanities Information Center (CICH). Since the incorporation of the CICH to UNAMs General Directorate for Libraries in 1997, this institution acts as Responsible Editor.

==References==

*Alonso Gamboa, Jose Octavio. Servicios, productos, docencia e investigacion en informacion: la experiencia del Centro de Informacion Cientifica y Humanistica de la Universidad Nacional Autonoma de Mexico. Ciencias de la Informacion, vol. 24, no. 4, diciembre, 1993. p.&nbsp;201-208. URL: [http://www.bibliociencias.cu/gsdl/cgi-bin/library?e=d-000-00---0revistas--00-0-0--0prompt-10---4------0-1l--1-es-50---20-about---00031-001-1-0utfZz-8-00&cl=CL2.772&d=HASH01caacf727585263378aa110&x=1]

*Alonso Gamboa, Jose Octavio. Accesso a revistas latinoamericanas en Internet. Una opcion a traves de las bases de datos Clase y Periodica. Ciencia da Informacao, vol. 27, no. 1, Janeiro-abril, 1998, p.&nbsp;90-95. URL: http://www.scielo.br/pdf/ci/v27n1/12.pdf

*Alonso Gamboa, Jose Octavio y Felipe Rafael Reyna Espinosa. Compilacion de datos bibliometricos regionales usando las bases de datos CLASE y PERIODICA. Revista Interamericana de Bibliotecologia, 2005. Vol. 28, no. 1, enero-junio: 63-78. URL: http://bibliotecologia.udea.edu.co/revinbi/Numeros/2801/doc3_28.html

*Russell, Jane M.; Madera-Jaramillo, Maria J.; Hernandez- Garcia, Yoscelina y Ainsworth, Shirley. Mexican collaboration networks in the international and regional arenas. En: Kretschmer, H. & Havemann, F. (Eds.): Proceedings of WIS 2008, Berlin. Fourth International Conference on Webometrics, Informetrics and Scientometrics & Ninth COLLNET Meeting, Humboldt-Universitat zu Berlin, Institute for Library and Information Science (IBI). URL: http://www.collnet.de/Berlin-2008/RussellWIS2008mcn.pdf

[[Category:Bibliographic databases]]
[[Category:Scientific databases]]
[[Category:Citation indices]]
>>EOP<<
234<|###|>Rapid Evolution
{{Infobox Software
| name                   = Rapid Evolution
| screenshot             = <!--  Commented out: [[Image:RapidEvolutionScreenshot1.jpg|thumb|right|250px]] -->
| caption                = Screenshot of Rapid Evolution 2.9.0
| developer              = [[Jesse Bickmore]]
| frequently_updated     = yes
| operating system       = Any OS that supports Java
| genre                  = Music Software
| website                = [http://www.mixshare.com/ Mixshare]
}}
'''Rapid Evolution''' (also known as RE) is an [[open source]] [[software]] tool for [[DJs]], providing filtering and searching features suitable for musicians.  It can analyze audio files and automatically determine properties such as the musical key, [[beats per minute]] (BPM), beat intensity and [[ReplayGain]]. 

It supports file types [[MP3]], [[MP4]], [[WAV]], [[FLAC]], [[Ogg|OGG]], [[Advanced Audio Coding|AAC]] and [[APE tag|APE]].  It helps [[DJs]] to organize and profile their music, and assists in the process of mixing music by utilizing song metadata to be able to show harmonically compatible songs and songs of a similar style.  It allows DJs to save and remember which songs are good matches (like a personal, digital mixing journal) and to plan entire mix sets.

One of its uses is to assist in a [[DJ]] technique called [[harmonic mixing]]. Once the musical key and BPM is known for a set of songs, [[DJs]] can use [[music theory]] (such as the [[Circle of Fifths]]) to identify songs that are harmonically compatible.  The act of mixing harmonically can help eliminate [[consonance and dissonance|dissonant]] tones while mixing songs together.  Since identifying whether songs can be made harmonically compatible can be quite complex (once features such as pitch lock are introduced), the software assists DJs by being able to show them which songs in their collection can be made harmonically compatible with any particular song.  It can also assist DJs in the act of [[beatmatching]] by showing which songs are in a compatible BPM range, and the percent of BPM difference.

Rapid Evolution is created and released through Mixshare.com.  The metadata generated by Rapid Evolution is shared through the central servers at Mixshare.com, which can be browsed online.  There are 1 million songs added to the database sharing information such as key, BPM, styles and ratings.

==History==
Rapid Evolution was developed for the [[Microsoft Windows|Windows]] environment and released in 2003.  Starting in version 2.0 it was switched to run on the Java platform, allowing it to run in virtually any environment.  It is still actively developed.

Several improvements to the key detection algorithm have been introduced over the years.  Rapid Evolution is the only program which can detect advanced key modes, such as aeolian, ionian, dorian, phrygian, lydian and mixolydian.  To date, there has only been one serious comparison of key detection accuracy (including programs such as [[Mixed In Key]] and Mixmeister).  It was shown that Rapid Evolution is the most accurate.<ref>{{cite web|title=Key Detection Software Comparison|url=http://www.mixingonbeat.com/phpbb/viewtopic.php?t=2268|date=2006-04-26|accessdate=2008-03-21|publisher=MixingOnBeat}}</ref>

The program was open-sourced on November 2013. <ref>{{cite web |url=http://www.mixshare.com/cgi-bin/yabb2/YaBB.pl?num=1381954407|title=Open-sourcing forum thread |date=2013-10-13 |accessdate=2014-04-17 |publisher=Mixshare}}</ref>

==Community interest==
Rapid Evolution was originally a freeware program.<ref>{{cite web |url=http://www.mixshare.com/wiki/doku.php?id=testimonials|title=DJ Testimonials |date=2007-01-01 |accessdate=2008-03-21 |publisher=Mixshare}}</ref>Due to its vast feature set, Rapid Evolution tends to be suited more for experienced DJs versus beginners.  

== See also ==
*[[Harmonic mixing]]
*[[Music Theory]]
*[[DJing]]

== External links ==
*[http://www.mixshare.com/software Download Rapid Evolution]
*[http://www.mixshare.com Mixshare's Official website]
*[http://www.harmonic-mixing.com Harmonic-Mixing.com]
*[https://github.com/djqualia/RapidEvolution2 Source code for version 2]
*[https://github.com/djqualia/RapidEvolution3 Source code for version 3]

== References ==
{{reflist}}

[[Category:Music search engines]]
[[Category:OS X multimedia software]]
[[Category:Windows multimedia software]]
[[Category:Audio mixing software]]
>>EOP<<
