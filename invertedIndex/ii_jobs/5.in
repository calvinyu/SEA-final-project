5<|###|>Category:String similarity measures
{{Cat main|String metrics}}

[[Category:Algorithms on strings|Similarity]]
[[Category:Information retrieval]]
[[Category:Metric geometry]]
[[Category:Information theory]]
[[Category:String (computer science)]]
>>EOP<<
11<|###|>ChemRefer
{{Orphan|date=February 2009}}
{{Infobox website
| name = Chemrefer
| logo = [[Image:Chemrefer.png]]
| screenshot = 
| caption = 
| url = http://www.chemrefer.com
| commercial = Yes
| type = [[Search engine]]
| language = English
| registration = Not Applicable
| owner = ChemRefer Limited
| author = William James Griffiths
| launch date = 2006
| current status = Offline
| revenue = 
}}
'''ChemRefer''' is a service that allows searching of freely-available and full-text chemical and pharmaceutical literature that is published by authoritative sources.<ref>{{citation|journal=Science Articles |title= Science News Forum|publisher= SciScoop |date=May 19, 2006|url= http://www.sciscoop.com/story/2006/5/19/95844/6293}}</ref>

Features include basic and advanced search options, [[mouseover]] detailed view, an integrated chemical structure drawing and search tool, downloadable [[toolbar]], customized [[RSS]] feeds, and newsletter.

ChemRefer is primarily of use to readers who do not have subscriptions for accessing restricted chemical literature, and to publishers who offer either [[Open access (publishing)|open access]] or [[hybrid open access journal]]s and seek to attract further subscriptions by publicly releasing part of their archive.

==See also==
*[[Google Scholar]]
*[[Windows Live Academic]]
*[[BASE (search engine)|BASE]]
*[[PubMed]]

==References==
{{reflist}}

==External links==
===Recommendations & reviews===
*[http://www.rowland.harvard.edu/resources/library/lnn_archive/031706.php Cited as an "Internet Site of the Week"] by the library of the [[Rowland Institute for Science]] at [[Harvard University]]
*[http://infoweb.nrl.navy.mil/index.cfm?i=156 Recommended in the list of chemical literature databases] by the library of the [[United States Naval Research Laboratory]]
*[http://www.mta.ca/library/subject_chemistry.html Recommended in the list of chemical literature databases] by the library of [[Mount Allison University]]
*[http://depth-first.com/articles/2007/01/15/chemrefer-free-direct-access-to-the-primary-literature Review of ChemRefer] at Depth-First chemoinformatics magazine
*[http://recherche-technologie.wallonie.be/fr/particulier/menu/revue-athena/l-annuaire-de-liens/internet/moteurs-de-recherche/www-chemrefer-com.html?PROFIL=PART Recommended in the list of chemical literature databases] by the Technology Research Portal, Belgium
*[http://www.certh.gr/0E9BF53C.en.aspx Recommended in the list of chemical literature databases] by the Centre for Research and Technology, Thessaloniki

===Background===
*[http://www.reactivereports.com/56/56_0.html Interview with William James Griffiths] at Reactive Reports chemistry magazine
*[http://www.earlham.edu/~peters/fos/overview.htm Open access overview] by Professor Peter Suber, Earlham College

[[Category:Scholarly search services]]
[[Category:Chemistry literature]]
[[Category:Information retrieval]]
[[Category:Open access projects]]

{{searchengine-website-stub}}
>>EOP<<
17<|###|>Category:Electronic documents
[[Category:Documents]]
[[Category:Digital media]]
[[Category:Information retrieval]]
[[Category:Electronic publishing]]
>>EOP<<
23<|###|>Dynatext
{{primary sources|date=October 2011}}
'''DynaText''' is an [[SGML]] publishing tool. It was introduced in 1990, and was the first system to handle arbitrarily large SGML documents, and to render them according to multiple style-sheets that could be switched at will.

DynaText and its Web sibling DynaWeb won multiple [[Seybold]] and other awards [http://xml.coverpages.org/ebt-award.html][http://xml.coverpages.org/dynaweb3-dvi.html], and there are eleven US Patents related to the DynaText technology: 5,557,722; 5,644,776; 5,708,806; 5,893,109; 5,983,248; 6,055,544; 6,101,511; 6,101,512; 6,105,044; 6,167,409; and 6,546,406.

DynaText was developed by Electronic Book Technologies, Incorporated, of [[Providence, Rhode Island]]. EBT was founded by [[Louis Reynolds]], [[Steven DeRose]], [[Jeffrey Vogel]], and [[Andries van Dam]], and was sold to [[Inso]] corporation in 1996.

DynaText heavily influenced stylesheet technologies such as [[DSSSL]] and [[CSS]], and [[XML]] chairman [[Jon Bosak]] cites EBT chief architect [[Steven DeRose]] as the origin of the notion of [[well-formedness]] formalized in [[XML]], as well as DynaText for influencing the design of Web browsers in general [http://www.ibiblio.org/bosak/cv.htm].

[[Inso]] corporation went out of business in 2002. 

==References==
*[http://www.w3.org/History/19921103-hypertext/hypertext/Products/DynaText/Overview.html DynaText Notes] by [[Tim Berners-Lee]]

[[Category:Information retrieval]]

{{markup-languages-stub}}
>>EOP<<
29<|###|>Information needs
'''Information need''' is an individual or group's desire to locate and obtain [[information]] to satisfy a conscious or unconscious [[need]]. The information and need in information need are an inseparable interconnection. Needs and interests call forth information. The objectives of studying information needs are:
# The explanation of observed phenomena of information use or expressed need;
# The prediction of instances of information uses;
# The control and thereby improvement of the utilization of information manipulation of essentials conditions.

Information needs are related to, but distinct from [[information requirements]].  An example is that a need is hunger; the requirement is food.

== Background ==

The concept of information needs was coined by an American information gernalist [http://www.libsci.sc.edu/BOB/ISP/taylor2.htm Robert S. Taylor] in his article [http://doi.wiley.com/10.1002/asi.5090130405 "The Process of Asking Questions"] published in American Documentation (Now is Journal of the American Society of Information Science and Technology).

In this paper, Taylor attempted to describe how an inquirer obtains an answer from an [[information system]], by performing the process consciously or unconsciously; also he studied the reciprocal influence between the inquirer and a given system.

According to Taylor, information need has four levels:
# The conscious and unconscious need for information not existing in the remembered experience of the investigator. In terms of the query range, this level might be called the ideal question  the question which would bring from the ideal system exactly what the inquirer, if he could state his need. It is the actual, but unexpressed, need for information
# The conscious mental description of an ill-defined area of in decision. In this level, the inquirer might talk to someone else in the field to get an answer.
# A researcher forms a rational statement of his question. This statement is a rational and unambiguous description of the inquirers doubts.
# The question as presented to the information system.

There are variables within a system that influence the question and its formation. Taylor divided them into five groups: general aspects (physical and geographical factors); system input (What type of material is put into the system, and what is the unit item?); internal organization (classification, indexing, subject heading, and similar access schemes); question input (what part do human operators play in the total system?); output (interim feedback).

Herbert Menzel preferred demand studies to preference studies. Requests for information or documents that were actually made by scientists in the course of their activities form the data for demand studies. Data may be in the form of records of orders placed for bibliographics, calls for books from an interlibrary loan system, or inquires addressed to an information center or service. Menzel also investigated user study and defined information seeking behaviour from three angles:
# When approached from the point of view of the scientist or technologists, these are studies of scientists communication behaviour;
# When approached from the point of view of any communication medium, they are use studies;
# When approached from the science communication system, they are studies in the flow of information among scientists and technologists.

William J. Paisley moved from information needs/uses toward strong guidelines for information system. He studied the theories of information-processing behavior that will generate propositions concerning channel selection; amount of seeking; effects on productivity of information quality, quantity, currency, and diversity; the role of motivational and personality factors, etc. He investigated a concentric conceptual framework for user research. In the framework, he places the information users at the centre of ten systems, which are:
# The scientist within his culture.
# The scientist within a political system.
# The scientist within a membership group.
# The scientist within a reference group.
# The scientist within an invisible college.
# The scientist within a formal organization.
# The scientist within a work team.
# The scientist within his own head.
# The scientist within a legal/economical system.
# The scientist within a formal.

==See also==
* [[Information retrieval]]

==References==
* Menzel, Herbert. Information Needs and Uses in Science and Technology. Annual Review of Information Science and Technology, Vol. 1, Interscience Publishers 1966, pp 41-69.
* Paisley, William J. Information Needs and Uses. Annual Review of Information Science and Technology, Vol.3, Encyclopdia Britannica, Inc. Chicago 1968, pp.1-30.
* Taylor, Robert S. The Process of Asking Questions American Documentation, Vol.13, No. 4, October 1962, pp.391-396, DOI: 10.1002/asi.5090130405.
* Wilson, T.D. On User Studies and Information Needs. Journal of Documentation, Vol. 37, No. 1, 1981, pp.3-15

[[Category:Information retrieval]]
[[Category:Searching]]
>>EOP<<
35<|###|>Relevance (information retrieval)
{{Other uses|Relevance}}

In [[information science]] and [[information retrieval]], '''relevance''' denotes how well a retrieved document or set of documents meets the [[information need]] of the user. Relevance may include concerns such as timeliness, authority or novelty of the result.

== History ==

The concern with the problem of finding relevant information dates back at least to the first publication of scientific journals in the 17th century.

The formal study of relevance began in the 20th Century with the study of what would later be called [[bibliometrics]]. In the 1930s and 1940s, S. C. Bradford used the term "relevant" to characterize articles relevant to a subject (cf., [[Bradford's law]]). In the 1950s, the first information retrieval systems emerged, and researchers noted the retrieval of irrelevant articles as a significant concern. In 1958, B. C. Vickery made the concept of relevance explicit in an address at the International Conference on Scientific Information.<ref>Mizzaro, S. (1997). Relevance: The Whole History. Journal of the American Society for Information Science. 48, 810832.</ref>

Since 1958, information scientists have explored and debated definitions of relevance. A particular focus of the debate was the distinction between "relevance to a subject" or "topical relevance" and "user relevance".

Recently, Zhao and Callan (2010)<ref>Zhao, L. and Callan, J., Term Necessity Prediction, Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM 2010). Toronto, Canada, 2010.</ref> showed a connection between the [[Binary Independence Model|relevance probability]] and the [[vocabulary mismatch]] problem in retrieval, which could lead to at least 50-300% gains in retrieval accuracy.<ref>Zhao, L. and Callan, J., Automatic term mismatch diagnosis for selective query expansion, SIGIR 2012.</ref>

== Evaluation ==

The information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the [[Cranfield Experiments]] of the early 1960s and culminating in the [[Text Retrieval Conference|TREC]] evaluations that continue to this day as the main evaluation framework for information retrieval research.

In order to evaluate how well an [[information retrieval]] system retrieved topically relevant results, the relevance of retrieved results must be quantified. In [[Cranfield Experiments|Cranfield]]-style evaluations, this typically involves assigning a ''relevance level'' to each retrieved result, a process known as ''relevance assessment''. Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need).   Once relevance levels have been assigned to the retrieved results, [[Information retrieval#Performance measures|information retrieval performance measures]] can be used to assess the quality of a retrieval system's output.

In contrast to this focus solely on topical relevance, the information science community has emphasized user studies that consider user relevance. These studies often focus on aspects of [[human-computer interaction]] (see also [[human-computer information retrieval]]).

== Clustering and relevance ==

The [[cluster hypothesis]], proposed by [[C. J. van Rijsbergen]] in 1979, asserts that two documents that are similar to each other have a high likelihood of being relevant to the same information need. With respect to the embedding similarity space, the cluster hypothesis can be interpreted globally or locally.<ref name=diazthesis>F. Diaz, Autocorrelation and Regularization of Query-Based Retrieval Scores. PhD thesis, University of Massachusetts Amherst, Amherst, MA, February 2008, Chapter 3.</ref>    The global interpretation assumes that there exist some fixed set of underlying topics derived from inter-document similarity. These global clusters or their representatives can then be used to relate relevance of two documents (e.g. two documents in the same cluster should both be relevant to the same request). Methods in this spirit include:
* cluster-based information retrieval<ref name=croftcbir>W. B. Croft, A model of cluster searching based on classification, Information Systems, vol. 5, pp. 189195, 1980.</ref><ref name=griffithscbir>A. Griffiths, H. C. Luckhurst, and P. Willett, Using interdocument similarity information in document retrieval systems, Journal of the American Society for Information Science, vol. 37, no. 1, pp. 311, 1986.</ref>
* cluster-based document expansion such as [[latent semantic analysis]] or its language modeling equivalents.<ref name=lmcbir>X. Liu and W. B. Croft, Cluster-based retrieval using language models, in SIGIR 04: Proceedings of the 27th annual international conference on Research and development in information retrieval, (New York, NY, USA), pp. 186193, ACM Press, 2004.</ref>    It is important to ensure that clusters  either in isolation or combination  successfully model the set of possible relevant documents.

A second interpretation, most notably advanced by Ellen Voorhees,<ref name=voorheescbir>E. M. Voorhees, The cluster hypothesis revisited, in SIGIR 85: Proceedings of the 8th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 188196, ACM Press, 1985.</ref>    focuses on the local relationships between documents. The local interpretation avoids having to model the number or size of clusters in the collection and allow relevance at multiple scales. Methods in this spirit include,
* multiple cluster retrieval<ref name=griffithscbir/><ref name=voorheescbir/>
* spreading activation<ref name=preece>S. Preece, A spreading activation network model for information retrieval. PhD thesis, University of Illinois, Urbana-Champaign, 1981.</ref> and relevance propagation<ref name=relprop>T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma, A study of relevance propagation for web search, in SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 408415, ACM Press, 2005.</ref> methods
* local document expansion<ref name=docexpansion>A. Singhal and F. Pereira, Document expansion for speech retrieval, in SIGIR 99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 3441, ACM Press, 1999.</ref>
* score regularization<ref name=diazreg>F. Diaz, Regularizing query-based retrieval scores, Information Retrieval, vol. 10, pp. 531562, December 2007.</ref>
Local methods require an accurate and appropriate document similarity measure.

==Epistemological issues==
{{Section OR|date=May 2014}}
Are users best at evaluating the relevance of a given document, or is it better to use experts?
Most research about relevance in information retrieval in recent years have implicitly assumed that the users' evaluation of the output a given system should be used to increase "relevance" output. An alternative strategy would be to use journal [[impact factor]] to rank output and thus base relevance on expert evaluations. Other strategies, such as including diversity of the search results, may be used as well. The important thing to recognize is, however, that relevance is fundamentally a question of [[epistemology]], not [[psychology]]. (Peoples' psychology reflects certain epistemological influences).

==References==
 {{reflist}}

==Additional reading==
*Hjrland, B. (2010). The foundation of the concept of relevance. Journal of the American Society for Information Science and Technology, 61(2), 217-237.

*Relevance : communication and cognition. by Dan Sperber; Deirdre Wilson. 2nd ed. Oxford; Cambridge, MA: Blackwell Publishers, 2001. ISBN 978-0-631-19878-9

*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: nature and manifestations of relevance. Journal of the American Society for Information Science and Technology, 58(3), 1915-1933. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20II%20JASIST%20%2707.pdf pdf])

*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part III: Behavior and effects of relevance. Journal of the American Society for Information Science and Technology, 58(13), 2126-2144. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20III%20JASIST%20%2707.pdf pdf])

*Saracevic, T. (2007). Relevance in information science. Invited Annual Thomson Scientific Lazerow Memorial Lecture at School of Information Sciences, University of Tennessee. September 19, 2007. ([http://www.sis.utk.edu/lazerow2007 video])

[[Category:Information retrieval]]
>>EOP<<
41<|###|>Scientific data archiving
'''Scientific data archiving''' is  the [[Computer_data_storage#Volatility|long-term storage]] of [[scientific data]] and methods. The various scientific journals have differing policies regarding how much of their data and methods scientists are required to store in a public archive, and what is actually archived varies widely between different disciplines. Similarly, the major grant-giving institutions have varying attitudes towards public archival of data. In general, the tradition of science has been for publications to contain sufficient information to allow fellow researchers to replicate and therefore test the research. In recent years this approach has become increasingly strained as   research in some areas depends on large datasets which cannot easily be replicated independently.

[[Data archiving]] is more important in some fields than others.  In a few fields, all of the data necessary to replicate the work is already available in the journal article.  In drug development, a great deal of data is generated and must be archived so researchers can verify that the reports the drug companies publish accurately reflect the data.

The requirement of data archiving is a recent development in the [[history of science]].  It was made possible by advances in [[information technology]] allowing large amounts of data to be stored and accessed from central locations.  For example, the [[American Geophysical Union]] (AGU) adopted their first policy on data archiving in 1993, about three years after the beginning of the [[WWW]].<ref>Policy on Referencing Data in and Archiving Data for AGU Publications [http://www.agu.org/pubs/authors/policies/data_policy.shtml]</ref> This policy mandates that datasets cited in AGU papers must be archived by a recognised data center; it permits the creation of "data papers"; and it establishes AGU's role in maintaining data archives. But it makes no requirements on paper authors to archive their data.

Prior to organized data archiving, researchers wanting to evaluate or replicate a paper would have to request data and methods information from the author.  The science community expects authors to [[Data sharing (Science)|share supplemental data]].  This process was recognized as wasteful of time and energy and obtained mixed results.  Information could become lost or corrupted over the years.  In some cases, authors simply refuse to provide the information.

The need for data archiving and due diligence is greatly increased when the research deals with health issues or public policy formation.<ref>"The Case for Due Diligence When Empirical Research is Used in Policy Formation" by Bruce McCullough and Ross McKitrick. [http://economics.ca/2006/papers/0685.pdf]</ref><ref>[http://gking.harvard.edu/replication.shtml "Data Sharing and Replication" a website by Gary King]</ref>

==Selected policies by journals==

===The American Naturalist===
{{quote|[[The American Naturalist]]'' requires authors to deposit the data associated with accepted papers in a public archive. For gene sequence data and phylogenetic trees, deposition in [[GenBank]] or [[TreeBASE]], respectively, is required. There are many possible archives that may suit a particular data set, including the [[Dryad (repository)|Dryad]] repository for ecological and evolutionary biology data. All accession numbers for GenBank, TreeBASE, and Dryad must be included in accepted manuscripts before they go to Production. If the data is deposited somewhere else, please provide a link. If the data is culled from published literature, please deposit the collated data in Dryad for the convenience of your readers. Any impediments to data sharing should be brought to the attention of the editors at the time of submission so that appropriate arrangements can be worked out.|JSTOR<ref>[http://www.jstor.org/page/journal/amernatu/forAuthor.html#data Supporting Data and Material]</ref>}}

===Journal of Heredity===
{{quote|The primary data underlying the conclusions of an article are critical to the verifiability and transparency of the scientific enterprise, and should be preserved in usable form for decades in the future. For this reason, ''Journal of Heredity'' requires that newly reported nucleotide or amino acid sequences, and structural coordinates, be submitted to appropriate public databases (e.g., GenBank; the [[EMBL Nucleotide Sequence Database]]; DNA Database of Japan; the [[Protein Data Bank]] ; and [[Swiss-Prot]]). Accession numbers must be included in the final version of the manuscript. For other forms of data (e.g., microsatellite genotypes, linkage maps, images), the Journal endorses the principles of the Joint Data Archiving Policy (JDAP) in encouraging all authors to archive primary datasets in an appropriate public archive, such as Dryad, TreeBASE, or the Knowledge Network for Biocomplexity. Authors are encouraged to make data publicly available at time of publication or, if the technology of the archive allows, opt to embargo access to the data for a period up to a year after publication.

The American Genetic Association also recognizes the vast investment of individual researchers in generating and curating large datasets. Consequently, we recommend that this investment be respected in secondary analyses or meta-analyses in a gracious collaborative spirit.|oxfordjournals.org<ref>[http://www.oxfordjournals.org/our_journals/jhered/for_authors/msprep_submission.html#4.%20DATA%20ARCHIVING%20POLICY Data archiving policy]</ref>}}

===Molecular Ecology===
{{quote|[[Molecular Ecology]] expects that data supporting the results in the paper should be archived in an appropriate public archive, such as GenBank, [[Gene Expression Omnibus]], TreeBASE, Dryad, the [[Knowledge Network for Biocomplexity]], your own institutional or funder repository, or as Supporting Information on the Molecular Ecology web site. Data are important products of the scientific enterprise, and they should be preserved and usable for decades in the future. Authors may elect to have the data publicly available at time of publication, or, if the technology of the archive allows, may opt to embargo access to the data for a period up to a year after publication. Exceptions may be granted at the discretion of the editor, especially for sensitive information such as human subject data or the location of endangered species.|Wiley<ref>[http://www.wiley.com/bw/submit.asp?ref=0962-1083&site=1 Policy on data archiving]</ref>}}

===Nature===
{{quote|Such material must be hosted on an accredited independent site (URL and accession numbers to be provided by the author), or sent to the ''Nature'' journal at submission, either uploaded via the journal's online submission service, or if the files are too large or in an unsuitable format for this purpose, on CD/DVD (five copies). Such material cannot solely be hosted on an author's personal or institutional web site.<ref>[http://www.nature.com/authors/editorial_policies/availability.html "Availability of Data and Materials: The Policy of Nature Magazine]</ref>

''Nature'' requires the reviewer to determine if all of the supplementary data and methods have been archived.  The policy advises reviewers to consider several questions, including: "Should the authors be asked to provide supplementary methods or data to accompany the paper online? (Such data might include source code for modelling studies, detailed experimental protocols or mathematical derivations.)|[[Nature (journal)|Nature]]<ref>{{cite web|title=Guide to Publication Policies of the Nature Journals|date=March 14, 2007|url=http://www.nature.com/authors/gta.pdf}}</ref>}}

===''Science''===
{{quote|''Science'' supports the efforts of databases that aggregate published data for the use of the scientific community. Therefore, before publication, large data sets (including microarray data, protein or DNA sequences, and atomic coordinates or electron microscopy maps for macromolecular structures) must be deposited in an approved database and an accession number provided for inclusion in the published paper.<ref>[http://www.sciencemag.org/about/authors/prep/gen_info.dtl#datadep "General Policies of Science Magazine"]</ref>

"Materials and methods"  ''Science'' now requests that, in general, authors place the bulk of their description of materials and methods online as supporting material, providing only as much methods description in the print manuscript as is necessary to follow the logic of the text. (Obviously, this restriction will not apply if the paper is fundamentally a study of a new method or technique.)|[[Science (journal)|Science]]<ref>[http://www.sciencemag.org/about/authors/prep/prep_online.dtl Preparing Your Supporting Online Material]</ref>}}

== Royal Society Publishing==
{{quote|As a condition of acceptance authors agree to honour any reasonable request by other researchers for materials, methods, or data necessary to verify the conclusion of the article. Supplementary data up to 10Mb is placed on the Society's website free of charge and is publicly accessible. Large datasets must be deposited in a recognised public domain database by the author prior to submission. The accession number should be provided for inclusion in the published article.|{{citation needed |date=September 2013}}}}

==Policies by funding agencies==
In the United States, the [[National Science Foundation]] (NSF) has tightened requirements on data archiving.   Researchers seeking funding from NSF are now required to file a [[data management plan]] as a two-page supplement to the grant application.<ref>[http://news.sciencemag.org/scienceinsider/2010/05/nsf-to-ask-every-grant-applicant.html NSF to Ask Every Grant Applicant for Data Management Plan]</ref>

The NSF [[Datanet]] initiative has resulted in funding of the '''Data Observation Network for Earth''' ([[DataONE]]) project, which will provide scientific data archiving for ecological and environmental data produced by scientists worldwide. DataONE's stated goal is to preserve and provide access to multi-scale, multi-discipline, and multi-national data. The community of users for DataONE includes scientists, ecosystem managers, policy makers, students, educators, and the public.

==Data archives==
The following list refers to scientific data archives. See [[Data archive]] for social science archives. 
* [[CISL Research Data Archive]]
* [[Dryad (repository)|Dryad]]
* [[ESO/ST-ECF Science Archive Facility]]
* [http://www.ncdc.noaa.gov/paleo/treering.html International Tree-Ring Data Bank]
* [http://www.icpsr.umich.edu Inter-university Consortium for Political and Social Research]
* [http://knb.ecoinformatics.org Knowledge Network for Biocomplexity]
* [[National Archive of Computerized Data on Aging]]
* National Archive of Criminal Justice Data [http://www.icpsr.umich.edu/nacjd]
* [[National Climatic Data Center]]
* [[National Geophysical Data Center]]
* [[National Snow and Ice Data Center]]
* [[National Oceanographic Data Center]]
* [http://daac.ornl.gov Oak Ridge National Laboratory Distributed Active Archive Center]
* [[PANGAEA (data library)|Pangaea - Data Publisher for Earth & Environmental Science]]
* [[World Data Center]]
* [[DataONE]]

==References==
{{Reflist}}

==External links==
* [[Registry of Research Data Repositories]] ''re3data.org'' [http://service.re3data.org/search/results?term=]
* Statistical checklist required by ''Nature'' [http://www.nature.com/nature/authors/gta/Statistical_checklist.doc]
* Policies of ''Proceedings of the National Academy of Sciences (U.S.)'' [http://www.pnas.org/misc/iforc.shtml#policies]
* The US National Committee for CODATA [http://www7.nationalacademies.org/usnc-codata/Archiving.html]
* The Role of Data and Program Code Archives in the Future of Economic Research  [http://research.stlouisfed.org/wp/2005/2005-014.pdf]
* Data sharing and replication  Gary King website [http://gking.harvard.edu/replication.shtml]
* The Case for Due Diligence When Empirical Research is Used in Policy Formation by McCullough and McKitrick [http://economics.ca/2006/papers/0685.pdf]
* Thoughts on Refereed Journal Publication by Chuck Doswell [http://www.cimms.ou.edu/~doswell/pubreviews.html]
* How to encourage the right behaviour An opinion piece published in ''Nature'',  March, 2002.[http://www.nature.com/nature/journal/v416/n6876/full/416001b.html]
* [[NASA Astrophysics Data System]] [http://cdsads.u-strasbg.fr/]
* [[Panton Principles]] for Open Data in Science, at Citizendium [http://en.citizendium.org/wiki/Panton_Principles]
* [[Inter-university Consortium for Political and Social Research]] [http://www.icpsr.umich.edu]
[[Category:Information retrieval]]
[[Category:Knowledge representation]]
>>EOP<<
47<|###|>Cranfield Experiments
The '''Cranfield experiments''' were computer information retrieval experiments conducted by [[Cyril W. Cleverdon]] at [[Cranfield University]] in the 1960s, to evaluate the efficiency of indexing systems.<ref>Cleverdon, C. W. (1960). ASLIB Cranfield research project on the comparative efficiency of indexing systems. ASLIB Proceedings, XII, 421-431.</ref><ref>Cleverdon, C. W. (1967). The Cranfield tests on index language devices. Aslib Proceedings, 19(6), 173-194.</ref><ref>Cleverdon, C. W., & Keen, E. M. (1966). Factors determining the performance of indexing systems. Vol. 1: Design, Vol. 2: Results. Cranfield, UK: Aslib Cranfield Research Project. 
</ref> 

They represent the prototypical evaluation model of [[information retrieval]] systems, and this model has been used in large-scale information retrieval evaluation efforts such as the [[Text Retrieval Conference]] (TREC).

==See also==
*[[ASLIB]]
*[[Information history]]

==References==
{{Reflist}}

[[Category:Experiments]]
[[Category:Information retrieval]]


{{database-stub}}
>>EOP<<
53<|###|>Negative search
{{Multiple issues|
{{unreferenced|date=March 2009}}
{{orphan|date=February 2009}}
{{confusing|date=March 2009}}
}}

'''Negative Search''' is the elimination of information which is not relevant from a mass of content in order to present to a user a range of relevant content.

Negative Search is different from both Positive Search and Discovery Search. Positive Search uses the selection of relevant content as its primary mechanism. Discovery calculates relatedness (between user intent and content) to present users with relevant alternatives of which they may not have been aware.

Negative Search applies to those forms of searches where the user has the intention of finding a specific, actionable piece information but lacks the knowledge of what that specific information is or might be.

Negative Search can also apply to searches where the user has a clear understanding of '''Negative Intent''' (what they don't want) rather than what they do.

Examples of Negative Intent are:

- Job searching: someone knows they want a new job but they have no idea what it might be. They just know what they don't want.

- Online dating: someone is looking for a dating partner, but cannot identify what criteria they are looking for. They just know what they don't want.

- An investigator is looking for a car but has no other information on that car on which to base a search.

==Negative Search Classifiers==

If there are two forms of search (positive and negative) it follows that there are two forms of classifier models: '''Inclusive Classifiers''' and '''Exclusive Classifiers'''.

[[List of countries|Countries of the World]] are a good example of a MECE list. A positive search for the country Kenya would identify content referencing Kenya and present it. A Negative Search for the country Kenya would exclude all content relating to other countries in the world leaving the user with content of some relevance to Kenya.

==Irrelevancy as a Desirable Construct==

Positive Search tends to view Irrelevancy as undesirable. Having a system actively identify and pursue irrelevant content for the purpose of elimination from a [[user experience]] may prove a highly powerful mechanism.

It follows that Positive and Negative Search are not mutually exclusive and that a more powerful search may result from the combination of selection and elimination as tools to empower user experience in Negative Searches.

==Degrees of Passivity==

Positive Search involves an active search by a user with no degree of passivity (or openness). For example: "I am only interested in the Hilton Hotel in Vientiane on [[New Year\'s Eve|New Years Eve]]."

Discovery involves a simultaneous secondary more Passive search by the user while they are involved in a Positive search. For example: "I am interested in the Hilton Hotel in Vientiane on New Years Eve but if there's a better hotel, let me know"

Negative Search also involves an Active search but with a much higher degree of Passivity (or openness to discovery). For example: "I need a holiday and really don't care where as long as its good."

Searchers can be active in one dimension (Positive Search) while simultaneously being passive to alternatives or what they don't know they're looking for in many dimensions. In Discovery they are Passive in a small number of dimensions but in Negative Search they are Passive in many or all dimensions.

==References==
{{Reflist}}

[[Category:Information retrieval]]
>>EOP<<
59<|###|>Search-based application
'''Search-based applications''' ('''SBA''') are [[software applications]] in which a [[Search engine|search engine platform]] is used as the core infrastructure for information access and reporting. SBAs use [[Semantic technology|semantic technologies]] to aggregate, normalize and classify [[Unstructured data|unstructured]], [[Semi-structured data|semi-structured]] and/or [[Structured data|structured content]] across multiple repositories, and employ [[Natural language processing|natural language technologies]] for accessing the aggregated information.

== Pre-Conditions ==

Search based applications are fully packaged applications that:<ref>Worldwide Search and Discovery 2009 Vendor Shares: An Update on Market Trends, IDC #223926, July, 2010 by Susan Feldman and Hadley Reynolds.</ref>
* Are built on a search backbone to enable sub-second access to information in multiple formats and from multiple sources
* Are delivered as a unified work environment to support a specific task or workflow, for example: eDiscovery, financial services regulatory compliance, fraud detection, voice of the customer, sales prospecting, pharmaceutical research, anti-terrorism intelligence, or customer support.
* Integrate all the tools that are commonly needed for that specific task or workflow, including:
** Multi-source information access
** Authoring
** Collaboration
** Business process
** Reporting and analysis
** Alerting
** Visualization
* Provide pre-configured data integration with multiple repositories of information in multiple formats as appropriate for the application domain.
* Integrate domain knowledge to support the particular task, including industry taxonomies and vocabularies, internal processes, workflow for the task, connectors to specialized collections of information, and decision heuristics typical of the field.
* Provide a compelling user interface and interaction design that eliminates the need for users to pogo stick or continually jump from one application to another. This buffers the user from the complexity of operating separate applications and enables them to focus on getting work done.
* Are quick to deploy, easy to customize or extend, and economical to administer

== Practical Uses ==

SBAs are used for a variety of purposes, including:

* ''' Enterprise Business Applications:''' For example, [[Customer Relationship Management]] (CRM), [[Enterprise Resource Planning]] (ERP), [[Supply Chain Management]] (SCM), Compliance & Discovery, and [[Business Intelligence]] (BI)

* ''' Web Applications:''' Typically, B2B, B2C and C2B applications that [[Mashup (digital)|mash-up]] data and functionality from diverse sources (databases, Web content, user-generated content, mapping data and functions, etc.)

The use of a search platform as the core infrastructure for software applications has been enabled largely by two search engine features:  1) Scalability 2) Ad hoc access to multiple heterogeneous sources from a single point of access.

Search based applications have proven popular and effective because they provide a dynamic, scalable access infrastructure that can be integrated with other features that information workers need:  task-specific, and easy to use work environments that integrate features that are usually designed to be used as separate applications, collaborative features, domain knowledge, and security.

Search engines are not a replacement for database systems; they are a complement. They have been optimally engineered to facilitate access to information, not to record and store transactions. In addition, the mathematical and statistical processors integrated to date into search engines remain relatively simple. At present, therefore, databases still provide a more effective structure for complex analytical functions.Search applications also focus on providing quality results considering search relevancy.

==References==
{{Reflist}}

==Further Reading==
* Worldwide Search and Discovery 2009 Vendor Shares: An Update on Market Trends, IDC #223926, July, 2010 by Susan Feldman and Hadley Reynolds.
* Butler Group [http://www.butlergroup.com/webinarIntroduction.asp?mcr=EXA190509&scr=EXA190509 Webinar on Search Based Applications] explaining SBA and how they work
* Presentation on [http://www.informationbuilders.com/support/developers/presentations/?109 Search Based Applications] by   [[Information Builders]]
* IDC Executive Brief [http://www.exalead.com/software/forms/download.php?resourceid=69 "The Information Advantage: Information Access in Tomorrow's Enterprise,"] October 2009, downloadable from the [[Exalead|Exalead.com]] website. Adapted from [http://www.idc.com/getdoc.jsp?containerId=217936 Hidden Costs of Information Work: A Progress Report] and [http://www.idc.com/getdoc.jsp?containerId=219883 Worldwide Search and Discovery Software 20092013 Forecast Update and 2008 Vendor Shares] by Susan Feldman, IDC.
* IDC [http://www.kmworld.com/downloads/66062/Search_Market_Map_Chart.pdf Search and Discovery Software: 2009 Market Map]
* KMWorld article [http://www.kmworld.com/Articles/Editorial/Feature/Search-based-applications-support-critical-decision-making-66062.aspx Search-based applications support critical decision making]
* Kellblog post [http://www.kellblog.com/2010/02/11/idcs-definiton-of-search-based-applications/ IDC's Definition of Search-Based Applications]
* Steve-Kearns' [http://www.basistech.com/knowledge-center/search/2010-05-building-multilingual-search-based-applications.pdf Building Multilingual Search Based Applications] presentation at Apache Lucene EuroCon 2010 conference
* Information Today article [http://newsbreaks.infotoday.com/NewsBreaks/Attivio-Upgrades-Its-Active-Intelligence-Engine-67608.asp Attivio Upgrades Its Active Intelligence Engine]
* [http://lucidworks.com/blog/debugging-search-application-relevance-issues/ Debugging Search Application Relevance Issues] by Grant Ingersoll. Accessed October 22, 2014.
* [http://www.mind7.fr/en/information_intelligence.html Explanatory video on SBA's and Content Analysis]

== See also ==
{{col-begin}}
{{col-2}}
* [[Agile application]]
* [[Agile development]]
* [[Business Intelligence 2.0]] (BI 2.0)
* [[Enterprise Search]]
* [[Search oriented architecture]]
* [[Software as a service]]
* [[Lookeen]]
* [[Lucene]]
* [[Exalead]]
{{col-end}}

<!-- Categories -->
[[Category:Enterprise application integration]]
[[Category:Information retrieval]]
[[Category:Internet search engines| ]]
[[Category:Internet terminology]]
>>EOP<<
65<|###|>TeLQAS
'''TeLQAS''' (Telecommunication Literature Question Answering System) is an experimental [[question answering]] system developed for answering English questions in the [[telecommunications]] domain.<ref>Mahmoud R. Hejazi, Maryam S. Mirian , Kourosh Neshatian, Azam Jalali, and Bahadorreza Ofoghi, ''A Telecommunication Literature Question/Answering System Benefits from a Text Categorization Mechanism'', International Conference on Information and Knowledge Engineering (IKE2003), July 2003, USA.</ref>

==Architecture==
TeLQAS includes three main subsystems: an online subsystem, an offline subsystem, and an [[ontology]]. The online subsystem answers questions submitted by users in real time. During the online process, TeLQAS processes the question using a [[natural language processing]] component that implements [[part-of-speech tagging]] and simple [[syntactic parsing]]. The online subsystem also utilizes an inference engine in order to carry out necessary inference on small elements of knowledge. The offline subsystem automatically indexes documents collected by a ''focused [[web crawler]]'' from the web. An ontology server along with its [[API]] is used for knowledge representation.<ref>Kourosh Neshatian and Mahmoud R. Hejazi, ''An Object Oriented Ontology Interface for Information Retrieval Purposes in Telecommunication Domain'', International Symposium on Telecommunication (IST2003).</ref> The main concepts and classes of the ontology are created by domain experts. Some of these classes, however, can be instantiated automatically by the offline components.

==References==
<references/>

[[Category:Computational linguistics]]
[[Category:Information retrieval]]
[[Category:Natural language processing software]]
>>EOP<<
71<|###|>30 Digits
{{Use dmy dates|date=July 2013}}
{{multiple issues|
{{notability|Companies|date=August 2012}}
{{refimprove|date=August 2012}}
{{peacock|date=August 2012}}
{{advert|date=August 2012}}
}}
{{Infobox company
| logo = [[Image:30 Digits Logo.jpg|center]]
| name = 30 Digits GmbH
| type = [[Private company|Private]]
| foundation = 2008
| location = [[Munich]], Germany
| area_served = [[Europe]] <br/> [[North America]] <br/> [[South America]] <br/> [[Asia]]
| key_people = Justin Gilbreath (Managing Director) <br/> Mathis Koblin (Director of R&D)
| industry = [[Information access]] <br/> [[Information retrieval]] <br/> [[Web mining]] <br/> [[Open Source software]]
| products = [[Search Engines]] <br/> Information Discovery Suite <br/> [[Apache Lucene]] <br/> [[Apache Solr]] <br/> Web Extractor
| company_slogan = Linking People to Content
| homepage = {{url|http://www.30digits.com}}
}}

'''30 Digits''' is a privately held information access and retrieval company with headquarters in [[Munich, Germany]]<ref>{{cite web |url=http://www.digitalpublic.de/web-20-suchmaschinen-holen-auf |title=Web 2.0  Suchmaschinen holen auf}}</ref> located in the "Munchner Technologie Zentrum".<ref>{{cite web |url=http://www.mtz.de/index.php?id=12 |title=List of companies located in the MTZ (Munchner Technologie Zentrum)}}</ref> The company was founded in 2008 and offers software that is a mix of privately developed code and leading [[Open Source]] technology primarily from the [[Apache Software Foundation]].

The company focuses on [[enterprise information access]] solutions from areas ranging from call-center applications to [[enterprise search]] to database offloading. The company also focuses on solutions created out of unstructured content on the web being structured for analysis,<ref>{{cite web |url=http://www.crmmanager.de/magazin/artikel_2165_enterprise_20_wahlkampf.html |title=Enterprise 2.0: Was ein Unternehmen von Obamas Wahlkampf lernen kann}}</ref> often referred to as [[web harvesting]].  This can be for monitoring security threats or observing customer reactions to products.  It can even be used to gather complex address and other details about entities like properties.<ref>{{cite web |url=http://www.prweb.com/releases/2011/03/prweb5186764.htm |title=viewr Selects 30 Digits as Primary Property Data Provider }}</ref>  Sometimes the focuses blend together in areas like Market or Business Intelligence where both internal and external information needs extraction, analysis, and retrieval capabilities.

In addition to the software solutions, 30 Digits Professional Services offers services to assist customer in designing and deploying the correct solutions for the challenge at hand. {{citation needed|date=August 2012}} Trainings, support, and consulting are available both on 30 Digits software and the Open Source software they work with like [[Lucene]] and [[Solr]].<ref>{{cite web |url=http://www.aktiv-verzeichnis.de/details/30-digits-gmbh.html |title=Company description from the Aktiv Verzeichnis}}</ref>

==References==
{{reflist}}

==External links==
* [http://www.30digits.com  Company website]
* [http://www.imittelstand.de/mittelstandsliste/webcode/ww1213 Article (in German) placing 30 Digits Web Extractor in Top20 Business Intelligence tools for the "Initiative Mittelstand" 2009]
* [http://lucene.apache.org/  Lucene website]
* [http://lucene.apache.org/solr/ Solr website]

[[Category:Information retrieval]]
[[Category:Search engine software]]
[[Category:Software industry]]
[[Category:Software companies of Germany]]
[[Category:Business software]]
[[Category:Companies based in Munich]]
[[Category:Companies of Germany]]
[[Category:Companies of Europe]]
>>EOP<<
77<|###|>Taganode Local Search Engine
'''The Open Local Search Engine from Taganode''' is a [[search engine]] specifically targeting [[mobile phone]]s. It is based on local [[search algorithm]]s to find new places of interest within a specified distance.

The Taganode search engine offers an Open Developer [[Application programming interface|API]] that any one can use freely when writing new applications for [[iPhone]]s, [[Android (operating system)|Android phones]] and other platforms.

The search engine is optimized for mobile phones by low [[Bandwidth (computing)|bandwidth]] usage and only makes the simplest service calls to try to be compatible with as many mobile phones as possible. The Taganode search service is at this moment present in [[London]], [[Rome]], [[Venice]], [[Amsterdam]], [[Berlin]], [[Sweden]] and in [[Denmark]].

== References ==
<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->
*{{cite news|url=http://www.webfinanser.com/nyheter/164362/taganode-gratis-reseguide-finns-nu-aven-i-venedig/|title=Taganode  Gratis reseguide finns nu aven i Venedig |date=2009-10-12|work=Webfinanser|language=Swedish|accessdate=18 December 2009}}
*{{cite news|url=http://www.webfinanser.com/nyheter/159761/en-ny-och-innovativ-soktjanst-for-resenarer-i-europa/|title=En ny och innovativ soktjanst for resenarer i Europa|date=September 19, 2009|work=Webfinanser |language=Swedish|accessdate=18 December 2009}}

== External links ==
* [http://www.taganode.com Official site]
* [http://www.mynewsdesk.com/se/view/pressrelease/taganode-free-guide-now-available-in-rome-325701/  Taganode Service in Venice] (press release)

[[Category:Information retrieval]]
[[Category:Internet search engines]]
[[Category:Mobile phones]]
>>EOP<<
83<|###|>Champion list
{{orphan|date=January 2011}}

A '''champion list''', also called '''top doc''' or '''fancy list''' is a precomputed list sometimes used with the [[vector space model]] to avoid computing relevancy rankings for all documents each time a document collection is queried. The champion list contains a set of n documents with the highest weights for the given term. The number n can be chosen to be different for each term and is often higher for rarer terms. The weights can be calculated by for example [[tf-idf]].

[[Category:Information retrieval]]


{{computing-stub}}
>>EOP<<
89<|###|>Statistical semantics
{{linguistics}}
'''Statistical semantics''' is the study of "how the statistical patterns of human word usage can be used to figure out what people mean, at least to a level sufficient for information access" {{citation needed|date=July 2012}}<!--([[George Furnas|Furnas]], 2006)--this page has been moved and the new version no longer contains this quotation-->. How can we figure out what words mean, simply by looking at patterns of words in huge collections of text? What are the limits to this approach to understanding words?

==History==

The term ''Statistical Semantics'' was first used by [[Warren Weaver]] in his well-known paper on [[machine translation]].<ref>{{harvnb|Weaver|1955}}</ref> He argued that [[word sense disambiguation]] for machine translation should be based on the [[co-occurrence]] frequency of the context words near a given target word. The underlying assumption that "a word is characterized by the company it keeps" was advocated by [[J. R. Firth|J.R. Firth]].<ref>{{harvnb|Firth|1957}}</ref> This assumption is known in [[Linguistics]] as the [[Distributional hypothesis|Distributional Hypothesis]].<ref>{{harvnb|Sahlgren|2008}}</ref> Emile Delavenay defined ''Statistical Semantics'' as "Statistical study of meanings of words and their frequency and order of recurrence."<ref>{{harvnb|Delavenay|1960}}</ref> "[[George Furnas|Furnas]] ''et al.'' 1983" is frequently cited as a foundational contribution to Statistical Semantics.<ref>{{harvnb|Furnas|Landauer|Gomez|Dumais|1983}}</ref>  An early success in the field was [[Latent semantic analysis|Latent Semantic Analysis]].

==Applications of statistical semantics==

Research in Statistical Semantics has resulted in a wide variety of algorithms that use the Distributional Hypothesis to discover many aspects of [[semantics]], by applying statistical techniques to [[Text corpus|large corpora]]:
* Measuring the [[Semantic similarity|similarity in word meanings]] <ref>{{harvnb|Lund|Burgess|Atchley|1995}}</ref><ref>{{harvnb|Landauer|Dumais|1997}}</ref><ref>{{harvnb|McDonald|Ramscar|2001}}</ref><ref>{{harvnb|Terra|Clarke|2003}}</ref>
* Measuring the similarity in word relations <ref>{{harvnb|Turney|2006}}</ref>
* Modeling [[similarity-based generalization]] <ref>{{harvnb|Yarlett|2008}}</ref>
* Discovering words with a given relation <ref>{{harvnb|Hearst|1992}}</ref>
* Classifying relations between words <ref>{{harvnb|Turney|Littman|2005}}</ref>
* Extracting keywords from documents <ref>{{harvnb|Frank|Paynter|Witten|Gutwin|1999}}</ref><ref>{{harvnb|Turney|2000}}</ref>
* Measuring the cohesiveness of text <ref>{{harvnb|Turney|2003}}</ref>
* Discovering the different senses of words <ref>{{harvnb|Pantel|Lin|2002}}</ref>
* Distinguishing the different senses of words <ref>{{harvnb|Turney|2004}}</ref>
* Subcognitive aspects of words <ref>{{harvnb|Turney|2001}}</ref>
* Distinguishing praise from criticism <ref>{{harvnb|Turney|Littman|2003}}</ref>

==Related fields==

Statistical Semantics focuses on the meanings of common words and the relations between common words, unlike [[text mining]], which tends to focus on whole documents, document collections, or named entities (names of people, places, and organizations). Statistical Semantics is a subfield of [[computational semantics]], which is in turn a subfield of [[computational linguistics]] and [[natural language processing]].

Many of the applications of Statistical Semantics (listed above) can also be addressed by [[lexicon]]-based algorithms, instead of the [[text corpus|corpus]]-based algorithms of Statistical Semantics. One advantage of corpus-based algorithms is that they are typically not as labour-intensive as lexicon-based algorithms. Another advantage is that they are usually easier to adapt to new languages than lexicon-based algorithms. However, the best performance on an application is often achieved by combining the two approaches.<ref>{{harvnb|Turney|Littman|Bigham|Shnayder|2003}}</ref>

==See also==
{{Portal|Linguistics}}
*[[Latent semantic analysis]]
*[[Latent semantic indexing]]
*[[Text mining]]
*[[Information retrieval]]
*[[Natural language processing]]
*[[Computational linguistics]]
*[[Web mining]]
*[[Semantic similarity]]
*[[Co-occurrence]]
*[[Text corpus]]
*[[Semantic Analytics]]

==References==
{{reflist|2}}

===Sources===
{{refbegin}}
* {{cite book | last = Delavenay | first = Emile | year = 1960 | title = An Introduction to Machine Translation | location = New York, NY | publisher = [[Thames and Hudson]] | oclc = 1001646 | ref = harv }}

* {{cite journal | last = Firth | first = John R. | authorlink = John Rupert Firth | year = 1957 | title = A synopsis of linguistic theory 1930-1955 | journal = [[Studies in Linguistic Analysis]] | pages = 132 | location = Oxford | publisher = [[Philological Society]] | ref = harv }}
*: Reprinted in {{cite book | editor1-first = F.R. | editor1-last = Palmer | title = Selected Papers of J.R. Firth 1952-1959 | location = London | publisher = Longman | year = 1968 | oclc = 123573912 }}

* {{cite conference | last1 = Frank | first1 = Eibe | last2 = Paynter | first2 = Gordon W. | last3 = Witten | first3 = Ian H. | last4 = Gutwin | first4 = Carl | last5 = Nevill-Manning | first5 = Craig G. | year = 1999 | title = Domain-specific keyphrase extraction | booktitle = Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence | conference = [[International Joint Conference on Artificial Intelligence|IJCAI-99]] | volume = 2 | pages = 668673 | location = California | publisher = Morgan Kaufmann | isbn = 1-55860-613-0 | id = {{citeseerx|10.1.1.43.9100}} {{citeseerx|10.1.1.148.3598}} | ref = harv }}

* {{cite journal | last1 = Furnas | first1 = George W. | authorlink = George Furnas | last2 = Landauer | first2 = T. K. | last3 = Gomez | first3 = L. M. | last4 = Dumais | first4 = S. T. | year = 1983 | title = Statistical semantics: Analysis of the potential performance of keyword information systems | url = http://furnas.people.si.umich.edu/Papers/FurnasEtAl1983_BSTJ_p1753.pdf | journal = [[Bell System Technical Journal]] | volume = 62 | issue = 6 | pages = 17531806 | ref = harv | doi=10.1002/j.1538-7305.1983.tb03513.x}}

* {{cite conference | last = Hearst | first = Marti A. | year = 1992 | title = Automatic Acquisition of Hyponyms from Large Text Corpora | booktitle = Proceedings of the Fourteenth International Conference on Computational Linguistics | conference = [[COLING|COLING '92]] | pages = 539545 | location = Nantes, France | url = http://acl.ldc.upenn.edu/C/C92/C92-2082.pdf | doi = 10.3115/992133.992154 | id = {{citeseerx|10.1.1.36.701}} | ref = harv }}

* {{cite journal | last1 = Landauer | first1 = Thomas K. | last2 = Dumais | first2 = Susan T. | year = 1997 | title = A solution to Plato's problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge | journal = [[Psychological Review]] | volume = 104 | issue = 2 | pages = 211240 | url = http://lsa.colorado.edu/papers/plato/plato.annote.html | id = {{citeseerx|10.1.1.184.4759}} | ref = harv | doi=10.1037/0033-295x.104.2.211}}

* {{cite conference | last1 = Lund | first1 = Kevin | last2 = Burgess | first2 = Curt | last3 = Atchley | first3 = Ruth Ann | year = 1995 | title = Semantic and associative priming in high-dimensional semantic space | booktitle = Proceedings of the 17th Annual Conference of the Cognitive Science Society | publisher = [[Cognitive Science Society]] | pages = 660665 | url = http://locutus.ucr.edu/reprintPDFs/lba95csp.pdf | ref = harv }}

* {{cite conference | last1 = McDonald | first1 = Scott | last2 = Ramscar | first2 = Michael | year = 2001 | title = Testing the distributional hypothesis: The influence of context on judgements of semantic similarity | booktitle = Proceedings of the 23rd Annual Conference of the Cognitive Science Society | pages = 611616 | url = http://homepages.inf.ed.ac.uk/smcdonal/cogsci2001.pdf | id = {{citeseerx|10.1.1.104.7535}} | ref = harv }}

* {{cite conference | last1 = Pantel | first1 = Patrick | last2 = Lin | first2 = Dekang | year = 2002 | title = Discovering word senses from text | booktitle = Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining | isbn = 1-58113-567-X | conference = [[KDD Conference|KDD '02]] | pages = 613619 | id = {{citeseerx|10.1.1.12.6771}} | doi = 10.1145/775047.775138 | ref = harv }}

* {{cite journal | last1 = Sahlgren | first1 = Magnus | year = 2008 | title = The Distributional Hypothesis | url = http://soda.swedish-ict.se/3941/1/sahlgren.distr-hypo.pdf | journal = Rivista di Linguistica | volume = 20 | issue = 1 | pages = 3353 | ref = harv}}

* {{cite conference | last1 = Terra | first1 = Egidio L. | last2 = Clarke | first2 = Charles L. A. | year = 2003 | title = Frequency estimates for statistical word similarity measures | booktitle = Proceedings of the Human Language Technology and North American Chapter of Association of Computational Linguistics Conference 2003 | conference = HLT/NAACL 2003 | pages = 244251 | url = http://acl.ldc.upenn.edu/N/N03/N03-1032.pdf | id = {{citeseerx|10.1.1.12.9041}} | doi = 10.3115/1073445.1073477 | ref = harv }}

* {{cite journal | last = Turney | first = Peter D. |date=May 2000 | title = Learning algorithms for keyphrase extraction | journal = [[Information Retrieval (journal)|Information Retrieval]] | volume = 2 | issue = 4 | pages = 303336 | arxiv = cs/0212020 | id = {{citeseerx|10.1.1.11.1829}} | doi = 10.1023/A:1009976227802 | ref = harv }}

* {{cite journal | last = Turney | first = Peter D. | year = 2001 | title = Answering subcognitive Turing Test questions: A reply to French | journal = [[Journal of Experimental and Theoretical Artificial Intelligence]] | volume = 13 | issue = 4 | pages = 409419 | arxiv = cs/0212015 | id = {{citeseerx|10.1.1.12.8734}} | ref = harv | doi=10.1080/09528130110100270}}

* {{cite conference | last = Turney | first = Peter D. | year = 2003 | title = Coherent keyphrase extraction via Web mining | booktitle = Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence | conference = IJCAI-03 | location = Acapulco, Mexico | pages = 434439 | arxiv = cs/0308033 | id = {{citeseerx|10.1.1.100.3751}} | ref = harv }}

* {{cite conference | last = Turney | first = Peter D. | year = 2004 | title = Word sense disambiguation by Web mining for word co-occurrence probabilities | booktitle = Proceedings of the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text | conference = SENSEVAL-3 | location = Barcelona, Spain | pages = 239242 | arxiv = cs/0407065 | url = http://cogprints.org/3732/ | ref = harv }}

* {{cite journal | last = Turney | first = Peter D. | year = 2006 | title = Similarity of semantic relations |journal = [[Computational Linguistics (journal)|Computational Linguistics]] | volume = 32 | issue = 3 | pages = 379416 | arxiv = cs/0608100 | url = http://cogprints.org/5098/ | doi = 10.1162/coli.2006.32.3.379 | id = {{citeseerx|10.1.1.75.8007}} | ref = harv }}

* {{cite journal | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. |date=October 2003 | title = Measuring praise and criticism: Inference of semantic orientation from association | journal = [[ACM Transactions on Information Systems]] (TOIS) | volume = 21 | issue = 4 | pages = 315346 | arxiv = cs/0309034 | url = http://cogprints.org/3164/ | id = {{citeseerx|10.1.1.9.6425}} | doi = 10.1145/944012.944013 | ref = harv }}

* {{cite journal | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. | year = 2005 | title = Corpus-based Learning of Analogies and Semantic Relations | journal = [[Machine Learning (journal)|Machine Learning]] | volume = 60 | issue = 13 | pages = 251278 | arxiv = cs/0508103 | id = {{citeseerx|10.1.1.90.9819}} | doi = 10.1007/s10994-005-0913-1 | url = http://cogprints.org/4518/ | ref = harv }}

* {{cite conference | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. | last3 = Bigham | first3 = Jeffrey | last4 = Shnayder | first4 = Victor | year = 2003 | title = Combining Independent Modules to Solve Multiple-choice Synonym and Analogy Problems | booktitle = Proceedings of the International Conference on Recent Advances in Natural Language Processing | conference = RANLP-03 | location = [[Borovets]], Bulgaria | pages = 482489 | arxiv = cs/0309035 | id = {{citeseerx|10.1.1.5.2939}} | url = http://cogprints.org/3163/ | ref = harv }}

* {{cite book | last = Weaver | first = Warren | authorlink = Warren Weaver | year = 1955 | chapter = Translation | chapter-url = http://www.mt-archive.info/Weaver-1949.pdf | editor1-first = W.N. | editor1-last = Locke | editor2-first = D.A. | editor2-last = Booth | title = Machine Translation of Languages | location = [[Cambridge, Massachusetts]] | publisher = [[MIT Press]] | isbn = 0-8371-8434-7 | pages = 1523 | ref = harv }}

* {{cite thesis | last = Yarlett | first = Daniel G. | year = 2008 | title = Language Learning Through Similarity-Based Generalization | url = http://psych.stanford.edu/~michael/papers/Draft_Yarlett_Similarity.pdf | degree = PhD | publisher = Stanford University | ref = harv }}
{{refend}}

==External links==
* {{cite web | url = http://www.si.umich.edu/people/george-furnas | work = Faculty Profile | title = George Furnas | publisher = University of Michigan, School of Information | accessdate = 2010-07-12 }}
*[http://research.microsoft.com/%7Esdumais/ Susan Dumais]
*[http://www.pearsonkt.com/bioLandauer.shtml Thomas Landauer]
*[http://www.apperceptual.com/ Peter Turney]
*[http://waldron.stanford.edu/~michael/papers/ Michael Ramscar]
*[http://www.cs.ualberta.ca/~lindek/demos.htm Dekang Lin's Demos]
*[http://www.isi.edu/~pantel/Content/demos.htm Patrick Pantel's Demos]
*[http://www.nzdl.org/Kea/ Kea keyphrase extraction]
*[http://seokeywordanalysis.com/seotools/ Online keyphrase extractor]

{{DEFAULTSORT:Statistical Semantics}}
[[Category:Artificial intelligence applications]]
[[Category:Computational linguistics]]
[[Category:Information retrieval]]
[[Category:Semantics]]
[[Category:Statistical natural language processing]]
[[Category:Fields of application of statistics]]
>>EOP<<
95<|###|>Cross-language information retrieval
{{refimprove|date=September 2014}}

'''Cross-language information retrieval (CLIR)''' is a subfield of [[information retrieval]] dealing with retrieving information written in a language different from the language of the user's query. For example, a user may pose their query in English but retrieve relevant documents written in French. To do so, most of CLIR systems use translation techniques.  CLIR techniques can be classified into different categories based on different translation resources: 
* Dictionary-based CLIR techniques
* Parallel corpora based CLIR techniques
* Comparable corpora based CLIR techniques
* Machine translator based CLIR techniques

The first workshop on CLIR was held in Zurich during the SIGIR-96 conference.<ref>The proceedings of this workshop can be found in the book ''Cross-Language Information Retrieval'' (Grefenstette, ed; Kluwer, 1998) ISBN 0-7923-8122-X.</ref> Workshops have been held yearly since 2000 at the meetings of the [[Cross Language Evaluation Forum]] (CLEF).

The term "cross-language information retrieval" has many synonyms, of which the following are perhaps the most frequent: cross-lingual information retrieval, translingual information retrieval, multilingual information retrieval. The term "multilingual information retrieval" refers to CLIR in general, but it also has a specific meaning of cross-language information retrieval where a document collection is multilingual.

==See also==
*[[EXCLAIM]] (EXtensible Cross-Linguistic Automatic Information Machine)

==References==
<references />

==External links==
*[http://www.glue.umd.edu/~oard/research.html A resource page for CLIR]

{{DEFAULTSORT:Cross-Language Information Retrieval}}
[[Category:Information retrieval]]
[[Category:Natural language processing]]


{{linguistics-stub}}
>>EOP<<
101<|###|>Adversarial information retrieval
'''Adversarial information retrieval''' ('''adversarial IR''') is a topic in [[information retrieval]] related to strategies for working with a data source where some portion of it has been manipulated maliciously.  Tasks can include gathering, indexing, filtering, retrieving and ranking information from such a data source. Adversarial IR includes the study of methods to detect, isolate, and defeat such manipulation.

On the Web, the predominant form of such manipulation is [[spamdexing|search engine spamming]] (also known as spamdexing), which involves employing various techniques to disrupt the activity of [[web search engines]], usually for financial gain. Examples of spamdexing are [[Google bomb|link-bombing]], [[comment spam (disambiguation)|comment]] or [[referrer spam]], [[spam blog]]s (splogs), malicious tagging.  [[Reverse engineering]] of [[ranking function|ranking algorithms]], [[Ad filtering|advertisement blocking]], and [[web content filtering]] may also be considered forms of adversarial [[data manipulation]].<ref>B. Davison, M. Najork, and T. Converse (2006), [http://wayback.archive.org/web/20090320173324/http://www.acm.org/sigs/sigir/forum/2006D/2006d_sigirforum_davison.pdf SIGIR Worksheet Report: Adversarial Information Retrieval on the Web (AIRWeb 2006)]</ref>

Activities intended to poison the supply of useful data make search engines less useful for users. If search engines are more exclusionary they risk becoming more like directories and less dynamic.

== Topics ==
Topics related to Web spam (spamdexing):

* [[Link spam]]
* [[Keyword spamming]]
* [[Cloaking]]
* Malicious tagging
* Spam related to blogs, including [[spam in blogs|comment spam]], [[spam blog|splogs]], and [[sping|ping spam]]

Other topics:
* [[Click fraud]] detection
* Reverse engineering of  [[search engine]]'s [[ranking]] algorithm
* Web [[content filtering]]
* [[Ad filtering|Advertisement blocking]]
* Stealth [[web crawling|crawling]]
*[[Troll (Internet)]]
* Malicious tagging or voting in [[social networks]]
* [[Astroturfing]]
* [[Sockpuppetry]]

== History ==
The term "adversarial information retrieval" was first coined in 2000 by [[Andrei Broder]] (then Chief Scientist at [[Alta Vista]]) during the Web plenary session at the [[Text Retrieval Conference|TREC]]-9 conference.<ref>D. Hawking and N. Craswell (2004), [http://es.csiro.au/pubs/trecbook_for_website.pdf Very Large Scale Retrieval and Web Search (Preprint version)]</ref>

== See also ==
*[[Spamdexing]]
*[[Information retrieval]]

== References ==
{{reflist}}

== External links ==
*[http://airweb.cse.lehigh.edu/ AIRWeb]: series of workshops on Adversarial Information Retrieval on the Web
*[http://webspam.lip6.fr/ Web Spam Challenge]: competition for researchers on Web Spam Detection
*[http://wayback.archive.org/web/20100217125910/http://barcelona.research.yahoo.net/webspam/ Web Spam Datasets]: datasets for research on Web Spam Detection

{{DEFAULTSORT:Adversarial Information Retrieval}}
[[Category:Information retrieval]]
[[Category:Internet fraud]]
[[Category:Searching]]
>>EOP<<
107<|###|>Keyword optimization
#REDIRECT [[Search engine optimization]]

{{DEFAULTSORT:Keyword Optimization}}
[[Category:Information retrieval]]
[[Category:Internet marketing]]
>>EOP<<
113<|###|>Personalized search
{{essay-like|date=January 2015}}
{{original research|date=January 2015}}
'''Personalized search''' refers to search experiences that are tailored specifically to an individual's interests by incorporating information about the individual beyond specific query provided. Pitkow et al. describe two general approaches to personalizing search results, one involving modifying the users query and the other re-ranking search results.<ref>{{cite journal|last=Pitokow|first=James|author2=Hinrich Schutze |author3=Todd Cass |author4=Rob Cooley |author5=Don Turnbull |author6=Andy Edmonds |author7=Eytan Adar |author8=Thomas Breuel |title=Personalized search|journal=Communications of the ACM (CACM)|year=2002|volume=45|issue=9|pages=5055|url=http://portal.acm.org/citation.cfm?doid=567498.567526}}</ref>

==History==

Google introduced Personalized search in 2004 and it was implemented in 2005 to Google search. Google has personalized search set up for not just those who have a Google account but everyone as well. There is not very much information on how exactly Google personalizes their searches, however, it is believed that they use user language, location, and web history.<ref>http://personalization.ccs.neu.edu/paper.pdf</ref>

Early search engines, like [[Yahoo!]] and [[AltaVista]], found results based only on key words. Personalized search, as pioneered by [[Google]], has become far more complex with the goal to "understand exactly what you mean and give you exactly what you want."<ref>{{citation | last=Remerowski|first=Ted|title=National Geographic: Inside Google|year=2013}}</ref> Using mathematical algorithms, search engines are now able to return results based on the number of links to an from sites; the more links a site has, the higher it is placed on the page.<ref>{{cite AV media|last=Remerowski|first=Ted|title=National Geographic: Inside Google|year=2013}}</ref> Search engines have two degrees of expertise: the shallow expert and the deep expert. An expert from the shallowest degree serves as a witness who knows some specific information on a given event. A deep expert, on the other hand, has comprehensible knowledge that gives it the capacity to deliver unique information that is relevant to each individual inquirer.<ref>{{cite journal|last=Simpson|first=Thomas|title=Evaluating Google as an epistemic tool|journal=Metaphilosophy|year=2012|volume=43|issue=4|pages=969982}}</ref> If a person knows what he or she wants than the search engine will act as a shallow expert and simply locate that information. But search engines are also capable of deep expertise in that they rank results  indicating that those near the top are more relevant to a user's wants than those below.<ref>{{cite journal|last=Simpson|first=Thomas|title=Evaluating Google as an epistemic tool|journal=Metaphilosophy|year=2012|volume=43|issue=4|pages=969982}}</ref>

While many [[search engines]] take advantage of information about people in general, or about specific groups of people, personalized search depends on a user profile that is unique to the individual. Research systems that personalize search results model their users in different ways. Some rely on users explicitly specifying their interests or on demographic/cognitive characteristics.<ref>{{cite journal|last=Ma|first=Z.|author2=Pant, G. |author3=Sheng, O. |title=Interest-based personalized search.|journal=ACM TOIS|year=2007|volume=25|issue=5}}</ref><ref>{{cite journal|last=Frias-Martinez|first=E.|author2=Chen, S.Y. |author3=Liu, X. |title=Automatic cognitive style identification of digital library users for personalization.|journal=JASIST|year=2007|volume=58|issue=2|pages=237251|doi=10.1002/asi.20477}}</ref> But user supplied information can be hard to collect and keep up to date. Others have built implicit user models based on content the user has read or their history of interaction with Web pages.<ref>{{cite journal|last=Chirita|first=P.|author2=Firan, C. |author3=Nejdl, W. |title=Summarizing local context to personalize global Web search|journal=SIGIR|year=2006|pages=287296}}</ref><ref>{{cite journal|last=Dou|first=Z.|author2=Song, R. |author3=Wen, J.R. |title=A large-scale evaluation and analysis of personalized search strategies|journal=WWW|year=2007|pages=581590}}</ref><ref>{{cite journal|last=Shen|first=X.|coauthors=Tan, B. and Zhai, C.X.|title=Implicit user modeling for personalized search|journal=CIKM|year=2005|pages=824831}}</ref><ref>{{cite journal|last=Sugiyama|first=K.|author2=Hatano, K. |author3=Yoshikawa, M. |title=Adaptive web search based on user profile constructed without any effort from the user|journal=WWW|year=2004|pages=675684}}</ref><ref>{{cite journal|last=Teevan|first=J.|author2=Dumais, S.T. |author3=Horvitz, E. |title=Personalizing search via automated analysis of interests and activities|journal=SIGIR|year=2005|pages=415422|url=http://people.csail.mit.edu/teevan/work/publications/papers/tochi10.pdf}}</ref>

There are several publicly available systems for personalizing Web search results (e.g., [[Google Personalized Search]] and [[Bing (search engine)|Bing]]'s search result personalization<ref>{{cite web|last=Crook|first=Aidan, and Sanaz Ahari|title=Making search yours|url=http://www.bing.com/community/site_blogs/b/search/archive/2011/02/10/making-search-yours.aspx|publisher=Bing|accessdate=14 March 2011}}</ref>). However, the technical details and evaluations of these commercial systems are proprietary. One technique Google uses to personalize searches for its users is to track log in time and if the user has enabled web history in his browser. The more you keep going the same site through a search result from Google, it believes that you like that page. So when you do certain searches, Google's personalized search algorithm gives the page a boost, moving it up through the ranks. Even if you're signed out, Google may personalize your results because it keeps a 180 day record of what a particular web browser has searched for, linked to a cookie in that browser.<ref>{{cite web|last=Sullivan|first=Danny|title=Of "Magic Keywords" and Flavors Of Personalized Search At Google|url=http://searchengineland.com/flavors-of-google-personalized-search-139286|accessdate=21 April 2014}}</ref>

In order to better understand how personalized search results are being presented to the users, a group of researchers at Northeastern University set out to answer this question. By comparing an aggregate set of searches from logged in users against a control group, the research team found that 11.7% of results show differences due to personalization, however this varies widely by search query and result ranking position.<ref>{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|accessdate=21 April 2014}}</ref> Of various factors tested, the two that had measurable impact were being logged in with a Google account and the IP address of the searching users. It should also be noted that results with high degress of personalization include companies and politics. One of the factors driving personalization is localization of results, with company queries showing store locations relevant to the location of the user. So, for example, if you searched for "used car sales", Google may churn out results of local car dealerships in your area. On the other hand, queries with the least amount of personalization include factual queries ("what is") and health.<ref>{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|accessdate=21 April 2014}}</ref>

When measuring personalization, it is important to eliminate background noise. In this context, one type of background noise is the carry-over effect. The carry-over effect can be defined as follows: when you perform a search and follow it with a subsequent search, the results of the second search is influenced by the first search. An interesting point to note is that the top ranked URLs are less likely to change based off personalization, with most personalization occurring at the lower ranks. This is a style of personalization, based on recent search history, but it is not a consistent element of personalization because the phenomenon times out after 10 minutes, according to the researchers.<ref>{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|accessdate=21 April 2014}}</ref>

==The Filter Bubble==
{{Main|Filter bubble}}

Several concerns have been brought up regarding personalized search. It decreases the likelihood of finding new information by biasing search results towards what the user has already found. It introduces potential privacy problems in which a user may not be aware that their search results are personalized for them, and wonder why the things that they are interested in have become so relevant. Such a problem has been coined as the "filter bubble" by author [[Eli Pariser]]. He argues that people are letting major websites drive their destiny and make decisions based on the vast amount of data they've collected on individuals. This can isolate users in their own worlds or "filter bubbles" where they only see information that they want to, such a consequence of "The Friendly World Syndrome." As a result people are much less informed of problems in the developing world which can further widen the gap between the North (developed countries) and the South (developing countries).<ref>{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}</ref>

The methods of personalization, and how useful it is to promote certain results which have been showing up regularly in searches by like-minded individuals in the same community. The personalization method makes it very easy to understand how the Filter Bubble happens. As certain results are bumped up and viewed more by individuals, other results not favored by them are relegated to obscurity. As this happens on a community-wide level, it results in the community, consciously or not, sharing a skewed perspective of events.<ref>{{cite journal|last=Smyth|first=B.|title=Adaptive Information Access:: Personalization And Privacy |journal=International Journal Of Pattern Recognition & Artificial Intelligence |year=2007|pages=183205}}</ref>

An area of particular concern to some parts of the world is the use of personalized search as a form of control over the people utilizing the search by only giving them particular information. This can be used to give particular influence over highly talked about topics such as gun control or even gear people to side with a particular political regime in different countries.<ref>{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}</ref> While total control by a particular government just from personalized search is a stretch, control of the information readily available from searches can easily be controlled by the richest corporations. The biggest example of a corporation controlling the information is Google. Google is not only feeding you the information they want but they are at times using your personalized search to gear you towards their own companies or affiliates. This has led to a complete control of various parts of the web and a pushing out of their competitors such as how Google Maps took a major control over the online map and direction industry with MapQuest and others forced to take a backseat.<ref>http://{{cite web| title=Traffic Report: How Google is squeezing out competitors and muscling into new markets|url= http://www.consumerwatchdog.org/resources/TrafficStudy-Google.pdf|accessdate= 27 April 2014}}</ref>

Many search engines use concept-based user profiling strategies that derive only topics that users are highly interested in but for best results, according to researchers Wai-Tin and Dik Lun, both positive and negative preferences should be considered. Such profiles, applying negative and positive preferences, result in highest quality and most relevant results by separating alike queries from unalike queries. For example, typing in 'apple' could refer to either the fruit or the [[Macintosh]] computer and providing both preferences aids search engines' ability to learn which apple the user is really looking for based on the links clicked. One concept-strategy the researchers came up with to improve personalized search and yield both positive and negative preferences is the click-based method. This method captures a user's interests based on which links they click on in a results list, while downgrading unclicked links.<ref>{{cite journal|last=Wai-Tin|first=Kenneth|coauthors=Dik Lun, L|title=Deriving concept-based user profiles from search engine logs|journal=IEE transaction on knowledge and data engineering|year=2010|volume=22|issue=7|pages=969982|doi=10.1109/tkde.2009.144}}</ref>

The feature also has profound effects on the [[search engine optimization]] industry, due to the fact that search results will no longer be ranked the same way for every user.<ref>[http://www.networkworld.com/news/2009/120709-google-personalized-results-could-be.html "Google Personalized Results Could Be Bad for Search"]. ''Network World''. Retrieved July 12, 2010.</ref> An example of this is found in Eli Pariser's, The Filter Bubble, where he had two friends type in "BP" into Google's search bar. One friend found information on the BP oil spill in the Gulf of Mexico while the other retrieved investment information.<ref>{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}</ref>

Some have noted that personalized search results not only serve to customize a user's search results, but also [[Advertising|advertisements]].  This has been criticized as an [[Expectation of privacy|invasion on privacy]].<ref>{{cite web|url=http://www.seooptimizers.com/search-engines-and-customized-results-based-on-your-internet-history.html|title=Search Engines and Customized Results Based on Your Internet History|publisher=SEO Optimizers|accessdate=27 February 2013}}</ref>

==The Case of Google==
{{Main|Google Personalized Search}}

The perfect example of search personalization is [[Google]]. Google is not just a search engine, but a corporation that is entering every facet of our lives. Personalization with Google has gone far beyond just search. There are a host of new applications, all of which can be personalized and integrated with the help of a Google account. Personalizing search does not require an account. However, one is almost deprived a choice, since so many useful Google products are only accessible if one has a Google account. The Google Dashboard, introduced in 2009, covers more than 20 products and services, including Gmail, Calendar, Docs, YouTube, etc.<ref>{{cite journal|last=Mattison|first=D.|title=Time, Space, And Google: Toward A Real-Time, Synchronous, Personalized, Collaborative Web. |journal=Searcher|year=2010|pages=2031}}</ref> that keeps track of all the information directly under ones name. The free Google Custom Search is available for individuals and big companies alike, providing the Search facility for individual websites and powering corporate sites such as that of the [[New York Times]]. The high level of personalization that was available with Google played a significant part in helping remain the worlds most favorite search engine.

One large example of Googles ability to personalized search is in its use of Google News. Google has geared its news to show everyone a few similar articles that can be deemed as interesting, but as soon as the user scrolls down, it can be seen that the news articles begin to differ. Google takes into account past searches as well as the location of the user to make sure that local news gets to them first. This can lead to a much easier search and less time going through all of the news to find the information you want. The concern, however, is that the very important information can be held back because it does not match with the criteria that the program sets for the particular user. This can create the [[filter bubble]] as described earlier.<ref>{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}</ref>

An interesting point about personalization that often gets overlooked is the privacy vs personalization battle. While the two do not have to be mutually exclusive, it is often the case that as one becomes more prominent, it compromises the other. Google provides a host of services to people, and many of these services do not require information to be collected about a person to be customizable. Since there is no threat of privacy invasion with these services, the balance has been tipped to favor personalization over privacy, even when it comes to search. As people reap the rewards of convenience from customizing their other Google services, they desire better search results, even if it comes at the expense of private information. Where to draw the line between the information versus search results tradeoff, is new territory and Google gets to make that decision. Until people get the power to control the information that is being collected about them, Google is not truly protecting privacy.
Googles popularity as a search engine and Internet browser has allowed it to gain a lot of power. Their popularity has created millions of usernames, which have been used to collect vast amounts of information about individuals. Google can use multiple methods of personalization such as traditional, social, geographic, IP address, browser, cookies, time of day, year, behavioral, query history, bookmarks, and more. Although many people would say that having Google personalize your search results based on what you searched previously would be a good thing, there are negatives that come with it.<ref>{{cite web|last=Jackson|first=Mark|title=The Future of Google's Search Personalization|url=http://searchenginewatch.com/article/2067001/The-Future-of-Googles-Search-Personalization|accessdate=29 April 2014}}</ref><ref>{{cite web|last=Harry|first=David|title=Search Personalization and the User Experience|url=http://searchenginewatch.com/article/2118126/Search-Personalization-the-User-Experience|accessdate=29 April 2014}}</ref>
With the power from this information, Google has chosen to bully its way into other sectors it owned such as videos, document sharing, shopping, maps, and many more. Google has done this by steering searchers to their own services offered as opposed to others such as MapQuest.

Using Search Personalization, Google has doubled its video market share to about eighty percent. The legal definition of a monopoly is when a firm gains control of seventy to eighty percent of the market. Google has reinforced this monopoly by creating significant barriers of entry such as manipulating search results to show their own services. This can be clearly seen with Google Maps being the first thing displayed in most searches.

The analytical firm Experian Hitwise stated that since two thousand and seven, MapQuest has had its traffic cut in half because of this. Other statistics from around the same time include Photobucket going from twenty percent of market share to only three percent, Myspace going from twelve percent market share to less than one percent, and ESPN from eight percent to four percent market share. In terms of images, Photobucket went from thirty one percent in two thousand and seven to ten percent in two thousand and ten. Even Yahoo Images has gone from twelve percent to seven percent. It becomes very apparent that the decline of these companies has come because of Googles increase in market share from forty three percent in two thousand and seven to about fifty five percent in two thousand and nine.

It might be easy to say that all of this has come from Google being more dominant because they provide better services. However, Experian Hitwise has also created graphs to show the market share of about fifteen different companies at once. This has been done for every category for the market share of pictures, videos, product search, and more. The graph for product search is evidence enough for Googles bullying because their numbers went from one point three million unique visitors to eleven point nine unique visitors in one month. That kind of growth can only come with the change of a process.

In the end, there are two things in common theme with all of these graphs. The first is that Googles market share has a directly inverse relationship to the market share of the leading competitors. The second is that this directly inverse relationship began around two thousand and seven, which is around the time that Google began to use its Universal Search method.<ref>{{cite web|title=TRAFFIC REPORT:How Google is Squeezing out Competitors and Muscling into New Markets |url=https://courses.lis.illinois.edu/pluginfile.php/226148/mod_resource/content/1/TrafficStudy-Google.pdf|publisher=ConsumerWatchDog.org|accessdate=29 April 2014}}</ref>

==Benefits==

One of the most critical benefits personalized search has is to improve the quality of decisions consumers make. The internet has made the transaction cost of obtaining information significantly lower than ever. However, humans capability of processing information has not expanded much.<ref>Diehl, K. (2003). Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making. Advances In Consumer Research, 30(1), 166-169.</ref> When facing overwhelming amount of information, consumers need a sophisticated tool to help them make high quality decisions. Two studies examined the effects of personalized screening and ordering tools, and the results show positive correlation between personalized search and the quality of consumers decisions.

The first study was conducted by Kristin Diehl from University of South Carolina. Her research discovered that reducing search cost led to lower quality choices. The reason behind this discovery was that consumers make worse choices because lower search costs cause them to consider inferior options. It also showed that if consumers have a specific goal in mind, they would further their search, resulting in an even worse decision.<ref>Diehl, K. (2003). Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making. Advances In Consumer Research, 30(1), 166-169.</ref> The study by Gerald Haubl from University of Alberta and Benedict G.C. Dellaert from Maastricht University mainly focused on recommendation systems. Both studies concluded that a personalized search and recommendation system significantly improved consumers decision quality and reduced the number of products inspected.<ref>Diehl, K. (2003). Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making. Advances In Consumer Research, 30(1), 166-169.</ref>

==Models==

Personalized search gains popularity because of the demand for more relevant information. Research has indicated low success rates among major search engines in providing relevant results; in 52% of 20,000 queries, searchers did not find any relevant results within the documents that Google returned.<ref>Coyle, M., & Smyth, B. (2007). Information recovery and discovery in collaborative web search. Advances in Information Retrieval (pp. 356367).</ref> Personalized search can improve search quality significantly and there are mainly two ways to achieve this goal.

The first model available is based on the users historical searches and search locations. People are probably familiar with this model since they often find the results reflecting their current location and previous searches.

There is another way to personalize search results. In Bracha Shapira and Boaz Zabars Personalized Search: Integrating Collaboration and Social Networks, Shapira and Zabar focused on a model that utilizes a recommendation system.<ref>Shapira, B., & Zabar, B. (2011). Personalized search: Integrating collaboration and social networks. Journal Of The American Society For Information Science & Technology, 62(1), 146-160. doi:10.1002/asi.21446</ref> This model shows results of other users who have searched for similar keywords. The authors examined keyword search, the recommendation system, and the recommendation system with social network working separately and compares the results in terms of search quality. The results show that a personalized search engine with the recommendation system produces better quality results than the standard search engine, and that the recommendation system with social network even improves more.

==Disadvantages==

While there are documented benefits of the implementation of search personalization, there are also arguments against its use. The foundation of this argument against its use is because it confines internet users search engine results to material that aligns with the users interests and history. It limits the users ability to become exposed to material that would be relevant to the users search query but due to the fact that some of this material differs from the users interests and history, the material is not displayed to the user. Search personalization takes the objectivity out of the search engine and undermines the engine. Objectivity matters little when you know what you are looking for, but its lack is problematic when you do not.<ref>{{cite journal|last=Simpson|first=Thomas W.|title=Evaluating Google As An Epistemic Tool|journal=Metaphilosophy|date=2012|volume=43.4|pages=426445|doi=10.1111/j.1467-9973.2012.01759.x}}</ref>  One of the main functions of the internet is the collection and sharing of information. This is the criticism of search personalization. It limits a core function of the web. It helps prevent users from easily accessing all the possible information that is available for a specific search query.  Search personalization adds a bias to users search queries. If a user has a particular set of interests or internet history and uses the web to research a controversial issue. The users search results will reflect that. The user not be displayed both sides of the issue if the users interests lean to one side or another. The user may be missing out on information that could be important. A study done on search personalization and its effects on search results in Google News resulted in different orders of news stories being generated by different users even though each user entered the same search query. When I further distilled the results, I saw that only 12% of the searchers had the same three stories in the same order. This to me is prima facie evidence that there is filtering going on.<ref>{{cite journal|last=Bates|first=Mary Ellen|title=Is Google Hiding My News?|year=2011|volume=35.6}}</ref> If search personalization was not active, all the results in theory should have been the same stories in an identical order.

Another disadvantage of search personalization is that internet companies such as Google are gathering and potentially selling your internet interests and histories to other companies. This raises a privacy issue. The issue is if people are content with companies gather and selling their internet information without their consent or knowledge.  Many web users are unaware of the use of search personalization and even fewer have knowledge that user data is a valuable commodity for internet companies.

==Sites that use Personalized Search==

E. Pariser author of the Filter Bubble explains how there are differences that search personalization has on both Facebook and Google. Facebook implements personalization when it comes to the amount of things we share and also what pages we like. It also takes into consideration our social interactions, whose profile we visit the most, who we message or chat with are all indicators that are used when Facebook uses personalization. Rather than what we share being an indicator of what is filtered out, but Google takes into consideration what we click to filter out what comes up in our searches. In addition Facebook searches are not necessarily as private as the Google ones. Facebook draws on the more public self and we share what other people want to see. Even while tagging photographs, Facebook uses personalization and recognition that will automatically assign a name to face for you without you having to tag them. In terms of Google we are provided similar websites and resources based on what we initially click on. This doesn't just affect Google and Facebook. There are even other websites that use the filter tactic to better adhere to user preferences. For example, Netflix also judges from the users search history to suggest movies that they may be interested in for the future. There are cites like Amazon and personal shopping cites also use other peoples history in order to serve their interests better. Twitter also uses personalization by suggesting other people to follow. In addition, based on who we follow and who we tweet and retweet at Twitter filters out to peoples best interest for us.  Mark Zuckerberg, founder of Facebook, believed that we only have one identity. E. Pariser argues that is completely false and search personalization is just another way to prove that isnt true. Although personalized search may seem helpful it is not a very accurate representation of who we are as people. There are instances where people also search things and share things in order to make themselves look better. For example, someone may look up and share political articles and other intellectual articles in order to make themselves look better. Search personalization is not an ideal representation of any person. There are so many cites used for different purposes and that does not make up one persons identity at all that, but are in fact false representations of ourselves.<ref>http://www.sp.uconn.edu/~jbl00001/pariser_the%20filter%20bubble_introduction.pdf</ref>

==Personalized Search and Online Shopping==

Search engines, such as Google and Yahoo!, utilize personalized search to attract possible customers to products that fit their presumed desires. Based on a large amount of collected data aggregated from an individuals web clicks, search engines can use personalized search to put forth advertisements that may pique the interest of an individual. Utilizing personalized search can help consumers find what they want faster, as well as help match up products and services to individuals within more specialized and/or niche markets. Many of these products or services that are sold via personalized online results would struggle to sell in brick-and-mortar stores. These types of products and services are called long tail items.<ref>Badke, William. Personalization and Information Literacy. Online, 47. Feb. 2012.</ref> Using personalized search allows faster product and service discoveries for consumers, and reduces the amount of necessary advertisement money spent to reach those consumers. In addition, utilizing personalized search can help companies determine which individuals should be offered online coupon codes to their products and/or services. By tracking if an individual has perused their website, considered purchasing an item, or has previously made a purchase a company can post advertisements on other websites to reach that particular consumer in an attempt to have them make a purchase.

Aside from aiding consumers and businesses in finding one-another, the search engines that provide personalized search benefit greatly. The more data collected on an individual, the more personalized results will be. In turn, this allows search engines to sell more advertisements because companies understand that they will have a better opportunity to sell to high percentage matched individuals then medium and low percentage matched individuals. This aspect of personalized search angers many scholars, such as William Badke and Eli Pariser, because they believe personalized search is driven by the desire to increase advertisement revenues. In addition, they believe that personalized search results are frequently utilized to sway individuals into using products and services that are offered by the particular search engine company or any other company in partnered with them. For example, Google searching any company with at least one brick-and-mortar location will offer a map portraying the closest company location using the Google Maps service as the first result to the query.<ref>Inside Google. "Traffic Report: How Google Is Squeezing Out Competitors and Muscling Into New Markets." Consumer Watchdog. http://www.consumerwatchdog.org, 2 June 2010. Web.</ref> In order to use other mapping services, such as MapQuest, a user would have to dig deeper into the results. Another example pertains to more vague queries. Searching the word shoes using the Google search engine will offer several advertisements to shoe companies that pay Google to link their website as a first result to consumers queries.

==References==
{{reflist|30em}}

{{DEFAULTSORT:Personalized search}}
[[Category:Information retrieval]]
[[Category:Internet search engines| ]]
[[Category:Internet terminology]]
[[Category:Personalized search|*]]
>>EOP<<
119<|###|>Personalization
{{cleanup-reorganize|date=June 2008}}

'''Personalization''', also known as '''customization''', involves using technology to accommodate the differences between individuals.

==Web pages==
{{see also|Web pages|Adaptive hypermedia}}  
[[Web page]]s are personalized based on the characteristics (interests, social category, context, ...) of an individual. Personalization implies that the changes are based on implicit data, such as items purchased or pages viewed. The term ''customization'' is used instead when the site only uses explicit data such as ratings or preferences. 

On an [[intranet]] or [[B2E]] [[Web portal#Enterprise Web portals|Enterprise Web portals]], personalization is often based on user attributes such as department, functional area, or role.  The term '''customization''' in this context refers to the ability of users to modify the page layout or specify what content should be displayed.

There are three categories of personalization:
# Profile / Group based
# Behaviour based (also known as Wisdom of the Crowds)
# Collaboration based



There are three broad methods of personalization:
# Implicit
# Explicit
# Hybrid

With implicit personalization the personalization is performed by the web page (or information system) based on the different categories mentioned above. It can also be learned from interactions with the user directly.<ref>{{cite web|last1=Flynn|first1=Lawrence|title=5 Things To Know About Siri And Google Now's Growing Intelligence|url=http://www.forbes.com/sites/parmyolson/2014/07/08/5-things-to-know-about-siri-and-google-nows-growing-intelligence/|website=Forbes}}</ref> With explicit personalization, the web page (or information system) is changed by the user using the features provided by the system.
Hybrid personalization combines the above two approaches to leverage the ''best of both worlds''.

Many companies offer services for web recommendation and email recommendation that are based on personalization or anonymously collected user behaviors.<ref name=behaviors>[http://online.wsj.com/article/SB10001424052748703294904575385532109190198.html?mod=googlenews_wsj ''Wall Street Journal'', On the Web's Cutting Edge, Anonymity in Name Only], August 4, 2010</ref>  

Web personalization is closely linked to the notion of '''[[Adaptive hypermedia]]''' (AH). The main difference is that the former would usually work on what is considered an Open Corpus Hypermedia, whilst the latter would traditionally work on Closed Corpus Hypermedia. However, recent research directions in the AH domain take both closed and open corpus into account. Thus, the two fields are closely inter-related.

Personalization is also being considered for use in less overtly commercial applications to improve the user experience online.<ref>[[Jonathan Bowen|Bowen, J.P.]] and Filippini-Fantoni, S., [http://www.archimuse.com/mw2004/papers/bowen/bowen.html Personalization and the Web from a Museum Perspective]. In [[David Bearman]] and Jennifer Trant (eds.), ''[[Museums and the Web]] 2004: Selected Papers from an International Conference'', Arlington, Virginia, USA, 31 March  3 April 2004. Archives & Museum Informatics, pages 6378, 2004.</ref> [[Remote control]] manufacturer [[Ruwido]] developed an [[interactive]] [[IPTV]] platform in 2010 called Voco Media, which controls [[digital media]] in the [[living room]] using web personalization. It uses personalization as a tool that supports modern forms of [[TV]] usage, by allowing users to create different profiles for each family member, personalized menu structures and [[fingerprint recognition]].<ref>[http://www.digitaltveurope.net/news_articles/mar_10/23_mar_10/ruwido_wins_virgin_media_contract,_announces_new_voco_apps Ruwido Wins Virgin Media Contract, Announces New Voco App]{{dead link|date=January 2013}}</ref>

Internet activist [[Eli Pariser]] has documented that search engines like Google and Yahoo News give different results to different people (even when logged out).  He also points out social media site Facebook changes user's friend feeds based on what it thinks they want to see.  Pariser warns that these algorithms can create a "[[filter bubble]]" that prevents people from encountering a diversity of viewpoints beyond their own, or which only presents facts which confirm their existing views.

==Digital media==
Another aspect of personalization is the increasing prevalence of [[open data]] on the Web. Many companies make their data available on the Web via [[API]]s, web services, and [[open data]] standards.<ref>{{cite news| url=http://www.guardian.co.uk/news/datablog/2010/apr/02/ordnance-survey-open-data | location=London | work=The Guardian | first1=Chris | last1=Thorpe | first2=Simon | last2=Rogers | title=Ordnance Survey opendata maps: what does it actually include? | date=2 April 2010}}</ref> Ordnance Survey Open Data This data is structured to allow it to be inter-connected and re-used by third parties.<ref>{{cite web|url=http://www.cio.com/article/372363/Google_Opens_Up_Data_Center_For_Third_Party_Web_Applications |title=Google Opens Up Data Centre for Third Party Web Applications |publisher=Cio.com |date=2008-05-28 |accessdate=2013-01-16}}</ref>

Data available from a users personal [[social graph]] can be accessed by third-party [[application software]] to be suited to fit the personalized [[web page]] or [[information appliance]].

Current [[open data]] standards on the Web include:
# [[Attention Profiling Mark-up Language]] (APML)
# [[DataPortability]]
# [[OpenID]]
# [[OpenSocial]]

== Mobile phones ==

Over time mobile phones have seen an increased emphasis placed on user personalization. Far from the black and white screens and monophonic ringtones of the past, phones now offer interactive wallpapers and MP3 TruTones. In the UK and Asia, WeeMees have become popular. WeeMees are three-dimensional characters that are used as wallpaper and respond to the tendencies of the user. Video Graphics Array (VGA) picture quality allows people to change their background with ease without sacrificing quality. All of these services are downloaded through the provider with the goal to make the user feel connected to the phone.<ref>May, Harvey, and Greg Hearn. "The Mobile Phone as Media." International Journal of Cultural Studies 8.2 (2005): 195-211. Print.</ref>

==Print media==
{{main|Mail merge}}

In print media, ranging from [[magazine]]s to [[admail|promotional publication]]s, personalization uses databases of individual recipients information. Not only does the written document address itself by name to the reader, but the advertising is targeted to the recipients demographics or interests using fields within the database, such as "first name", "last name", "company", etc. 

The term "personalization" should not be confused with variable data, which is a much more granular method of marketing that leverages both images and text with the medium, not just fields within a database. Although personalized children's books are created by companies who are using and leveraging all the strengths of [[variable data printing| variable data printing (VDP)]]. This allows for full image and text variability within a printed book.
With the advent of online 3D printing services such as Shapeways and Ponoko we are seeing personalization enter into the realms of product design.

== Promotional merchandise ==
Promotional items ([[mug]]s, [[T-shirt]]s, [[keychain]]s, [[ball]]s etc.) are regularly personalized. Personalized childrens storybooks  wherein the child becomes the [[protagonist]], with the name and image of the child personalized  are also popular. Personalized CDs for children also exist. With the advent of [[digital printing]], personalized calendars that start in any month, birthday cards, cards, e-cards, posters and photo books can also be obtained. In addition, with the advent of [[3D printing]], personalised apparel and accessories, such as jewellery made by [[StyleRocks]], is also increasing in popularity.<ref>{{cite web|url=http://www.jewellermagazine.com/Article.aspx?id=2167&h=New-jewellery-website-targets-|title=New jewellery website targets 'customisers'|last=Weinman|first=Aaron|date=21 February 2012|publisher=Jeweller Magazine|language=|accessdate=6 January 2015|quote=StyleRocks founder and CEO, Pascale Helyar-Moray, said the site offers womens and mens rings, necklaces, bracelets, earrings and cufflinks. Working alongside an Australian jewellery wholesaler, Helyar-Moray said customers have access to a variety of different styles and designs in an attempt to widen the sites ability to personalise pieces.}}</ref><ref>{{cite web|url=http://www.ragtrader.com.au/news/style-first|title=Style first|date=15 August 2014|publisher=Ragtrader|language=|accessdate=6 January 2015|quote=Online retailer StyleRocks is about to introduce an Australian first for the jewellery sector. The customisable fine jewellery retailer has introduced 3D printing in conjunction with the launch of a new website.}}</ref>

== Mass personalization ==

{{tone|section|date=January 2011}}

Mass personalization is defined as custom tailoring by a company in accordance with its end users tastes and preferences.<ref>{{cite web|url=http://www.answers.com/personalization&r=67 |title=personalize: Definition, Synonyms from |publisher=Answers.com |date= |accessdate=2013-01-16}}</ref> From collaborative engineering perspective, mass customization can be viewed as collaborative efforts between customers and manufacturers, who have different sets of priorities and need to jointly search for solutions that best match customers individual specific needs with manufacturers customization capabilities. <ref>	Chen, S., Y. Wang and M. M. Tseng. 2009. Mass Customization as a Collaborative Engineering Effort. International Journal of Collaborative Engineering, 1(2): 152-167</ref> The main difference between mass customization and mass personalization is that customization is the ability for a company to give its customers an opportunity to create and choose product to certain specifications, but does have limits.<ref>Haag et al., ''Management Information Systems for the Information Age'', 3rd edition, 2006, page 331.</ref> Clothing industry has also adopted the mass customization paradigm and some footwear retailers are producing mass customized shoes.<ref>{{cite web|url=http://www.botisto.com/how.php?language=EN |title=Botisto |publisher=Botisto |date= |accessdate=2013-01-16}}</ref><ref>[http://www.promoline1.com/Custom-T-Shirts-s/1814.htm Clothing ]</ref> The gaming market is seeing personalization in the new custom controller industry. A new, and notable, company called "Experience Custom" gives customers the opportunity to order personalized gaming controllers.<ref>{{cite web|url=http://www.experiencecustom.com/|title=Custom Controllers |publisher=ExperienceCustom.com |date= |accessdate=2014-11-20}}</ref> 

A website knowing a user's location, and buying habits, will present offers and suggestions tailored to the user's demographics; this is an example of mass personalization. The personalization is not individual but rather the user is first classified and then the personalization is based on the group they belong to.<ref>{{cite news| url=http://www.telegraph.co.uk/foodanddrink/9808015/How-supermarkets-prop-up-our-class-system.html | location=London | work=The Daily Telegraph | first=Harry | last=Wallop | title=How supermarkets prop up our class system | date=2013-01-18}}</ref>

[[Behavioral targeting]] represents a concept that is similar to mass personalization.

== Predictive personalization ==

Predictive personalization is defined as the ability to predict customer behavior, needs or wants - and tailor offers and communications very precisely.<ref>{{cite web|url=http://www.slideshare.net/jwtintelligence/jwt-10-trends-for-2013-executive-summary|title=10 Trends for 2013 Executive Summary: Definition, Projected Trends |publisher=JWTIntelligence.com |date= |accessdate=2012-12-04}}</ref>  Social data is one source of providing this predictive analysis, particularly social data that is structured.  Predictive personalization is a much more recent means of personalization and can be used well to augment current personalization offerings.

==See also==
* [[Adaptation (computer science)]]
* [[Mass customization]]
* [[Adaptive hypermedia]]
* [[Behavioral targeting]]
* [[Bespoke]]
* [[Collaborative filtering]]
* [[Configurator]]
* [[Personalized learning]]
* [[Preorder economy]]
* [[Real-time marketing]]
* [[Recommendation system]]
* [[User modeling]]

==References==
{{reflist|2}}

==External links==
* [http://www.iimcp.org International Institute on Mass Customization & Personalization which organizes MCP, a biannual conference on customization and personalization]
* [http://www.umuai.org/ User Modeling and User-Adapted Interaction (UMUAI)] ''The Journal of Personalization Research''

[[Category:Humancomputer interaction]]
[[Category:World Wide Web]]
[[Category:User interface techniques]]
[[Category:Usability|Personas]]
[[Category:Types of marketing]]
[[Category:Information retrieval]]
>>EOP<<
125<|###|>Type-1 OWA operators
The [[Ordered weighted averaging aggregation operator|Yager's OWA (ordered weighted averaging) operators]]<ref name="yagerOWA">{{cite journal|last=Yager|first=R.R|title=On ordered weighted averaging aggregation operators in multi-criteria decision making|journal=IEEE Transactions on Systems, Man and Cybernetics|year=1988|volume=18|pages=183190|doi=10.1109/21.87068}}</ref>  have been widely used to aggregate the crisp values in decision making schemes (such as multi-criteria decision making, multi-expert decisin making, multi-criteria multi-expert decision making).<ref>{{cite book|last=Yager|first=R. R. and Kacprzyk, J|title=The Ordered Weighted Averaging Operators: Theory and Applications|year=1997|publisher=Kluwer: Norwell, MA}}</ref><ref>{{cite book|last=Yager|first=R.R, Kacprzyk, J. and Beliakov, G|title=Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|publisher=Springer}}</ref> It is widely accepted that fuzzy sets<ref>{{cite journal|last=Zadeh|first=L.A|title=Fuzzy sets|journal=Information and Control |year=1965|volume=8 |pages=338353|doi=10.1016/S0019-9958(65)90241-X}}</ref> are more suitable for representing preferences of criteria in decision making. But fuzzy sets are not crisp values, how can we aggregate fuzzy sets in OWA mechanism? 

The type-1 OWA operators<ref name="fssT1OWA">{{cite journal|last=Zhou|first=S. M.|coauthors=F. Chiclana, R. I. John and J. M. Garibaldi|title=Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers|journal=Fuzzy Sets and Systems|year=2008|volume=159|issue=24|pages=32813296|doi=10.1016/j.fss.2008.06.018}}</ref><ref name="kdeT1OWA">{{cite journal|last=Zhou|first=S. M.|coauthors=F. Chiclana, R. I. John and J. M. Garibaldi|title=Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments|journal=IEEE Transactions on Knowledge and Data Engineering|year=2011|volume=23|issue=10|pages=14551468|doi=10.1109/TKDE.2010.191}}</ref>  have been proposed for this purpose. So the type-1 OWA operators provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.

First, there are two definitions for type-1 OWA operators, one is based on Zadeh's Extension Principle, the other is based on <math>\alpha</math>-cuts of fuzzy sets. The two definitions lead to equivalent results.

==Definitions==

'''Definition 1.<ref name="fssT1OWA" /> '''
Let <math>F(X)</math> be the set of fuzzy sets with domain of discourse <math>X</math>, a type-1 OWA operator is defined as follows:

Given n linguistic weights <math>\left\{ {W^i} \right\}_{i = 1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,1]</math>, a type-1 OWA operator is a mapping, <math>\Phi</math>,

:<math>\Phi \colon F(X)\times \cdots \times F(X)  \longrightarrow  F(X)</math>
:<math>(A^1 , \cdots ,A^n)  \mapsto   Y</math>

such that

:<math>\mu _{Y} (y) =\displaystyle \sup_{\displaystyle \sum_{k =1}^n \bar {w}_i a_{\sigma (i)}  = y }\left({\begin{array}{*{1}l}\mu _{W^1 } (w_1 )\wedge \cdots \wedge \mu_{W^n } (w_n )\wedge \mu _{A^1 } (a_1 )\wedge \cdots \wedge \mu _{A^n } (a_n )\end{array}}\right)</math>

where <math>\bar {w}_i = \frac{w_i }{\sum_{i = 1}^n {w_i } }</math>,and <math>\sigma \colon \{1, \cdots ,n\} \longrightarrow \{1, \cdots ,n\}</math> is a permutation function such that <math>a_{\sigma (i)} \geq a_{\sigma (i + 1)},\ \forall i = 1, \cdots ,n - 1</math>, i.e., <math>a_{\sigma(i)} </math> is the <math>i</math>th highest element in the set <math>\left\{ {a_1 , \cdots ,a_n } \right\}</math>.

'''Definition 2.<ref name="kdeT1OWA" /> '''

The definition below is based on the alpha-cuts of fuzzy sets:

Given the n linguistic weights <math>\left\{ {W^i} \right\}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\;\;1]</math>, then for each <math>\alpha \in [0,\;1]</math>, an <math>\alpha </math>-level type-1 OWA operator with <math>\alpha </math>-level sets <math>\left\{ {W_\alpha ^i } \right\}_{i = 1}^n </math> to aggregate the <math>\alpha </math>-cuts of fuzzy sets <math>\left\{ {A^i} \right\}_{i =1}^n </math> is given as

: <math>
\Phi_\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right) =\left\{ {\frac{\sum\limits_{i = 1}^n {w_i a_{\sigma (i)} } }{\sum\limits_{i = 1}^n {w_i } }\left| {w_i \in W_\alpha ^i ,\;a_i } \right. \in A_\alpha ^i ,\;i = 1, \ldots ,n} \right\}</math>

where  <math>W_\alpha ^i= \{w| \mu_{W_i }(w) \geq \alpha \}, A_\alpha ^i=\{ x| \mu _{A_i }(x)\geq \alpha \}</math>, and <math>\sigma :\{\;1, \cdots ,n\;\} \to \{\;1, \cdots ,n\;\}</math> is a permutation function such that <math>a_{\sigma (i)} \ge a_{\sigma (i + 1)} ,\;\forall \;i = 1, \cdots ,n - 1</math>, i.e., <math>a_{\sigma (i)} </math> is the <math>i</math>th largest
element in the set <math>\left\{ {a_1 , \cdots ,a_n } \right\}</math>.

== Representation theorem of Type-1 OWA operators<ref name="kdeT1OWA" />==

Given the ''n'' linguistic weights <math>\left\{ {W^i} \right\}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\;\;1]</math>, and the fuzzy sets <math>A^1, \cdots ,A^n</math>, then we have that<ref name="kdeT1OWA" />
:<math>Y=G</math>

where <math>Y</math> is the aggregation result obtained by Definition 1, and <math>G</math> is the result obtained by in Definition 2.

==Programming problems for Type-1 OWA operators==

According to the '''''Representation Theorem of Type-1 OWA Operators''''',a general type-1 OWA operator can be decomposed into a series of <math>\alpha</math>-level type-1 OWA operators. In practice, these series of  <math>\alpha</math>-level type-1 OWA operators are used to construct the resulting aggregation fuzzy set. So we only need to compute the left end-points and right end-points of the intervals <math>\Phi _\alpha \left( {A_\alpha ^1 , \cdots ,A_\alpha ^n } \right)</math>. Then, the resulting aggregation fuzzy set is constructed with the membership function as follows:

:<math>\mu _{G} (x) = \mathop \vee \limits_{\alpha :x \in \Phi _\alpha \left( {A_\alpha ^1 , \cdots
,A_\alpha ^n } \right)_\alpha } \alpha </math>

For the left end-points, we need to solve the following programming problem:
:<math> \Phi _\alpha \left( {A_\alpha ^1 , \cdots ,A_\alpha ^n } \right)_{-} = \mathop {\min }\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i = 1}^n {w_i } } </math>

while for the right end-points, we need to solve the following programming problem:
:<math>\Phi _\alpha \left( {A_\alpha ^1 , \cdots , A_\alpha ^n } \right)_{+} = \mathop {\max }\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i  A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i =
1}^n {w_i } } </math>

A fast method has been presented to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently, for details, please see the paper.<ref name="kdeT1OWA" />

== Alpha-level approach to Type-1 OWA operation<ref name="kdeT1OWA" />==
* '''Step 1'''.To set up the <math>\alpha </math>- level resolution in [0, 1].
* '''Step 2'''. For each <math>\alpha \in [0,1]</math>,
''Step 2.1.'' To calculate <math>\rho _{\alpha +} ^{i_0^\ast } </math>
# Let <math>i_0 = 1</math>;
# If <math>\rho _{\alpha +} ^{i_0 } \ge A_{\alpha + }^{\sigma (i_0 )} </math>, stop, <math>\rho _{\alpha +} ^{i_0 } </math> is the solution; otherwise go to ''Step 2.1-3''.
# <math>i_0 \leftarrow i_0 + 1</math>, go to ''Step 2.1-2''.

''Step 2.2.'' To calculate<math>\rho _{\alpha -} ^{i_0^\ast } </math>
# Let <math>i_0 = 1</math>;
# If <math>\rho _{\alpha -} ^{i_0 } \ge A_{\alpha - }^{\sigma (i_0 )} </math>, stop, <math>\rho _{\alpha -} ^{i_0 } </math> is the solution; otherwise go to ''Step 2.2-3.''
#<math>i_0 \leftarrow i_0 + 1</math>, go to step ''Step 2.2-2.''

'''Step 3.'''To construct the aggregation resulting fuzzy set <math>G</math> based on all the available intervals <math>\left[ {\rho _{\alpha -} ^{i_0^\ast } ,\;\rho _{\alpha +} ^{i_0^\ast } } \right]</math>: 

:<math>\mu _{G} (x) = \mathop \vee \limits_{\alpha :x \in \left[ {\rho _{\alpha -} ^{i_0^\ast } ,\;\rho _{\alpha +} ^{i_0^\ast } } \right]} \alpha </math>

==Special cases of Type-1 OWA operators==
* Any OWA operators, like maximum, minimum, mean operators;<ref name="yagerOWA" />
* Join operators of (type-1) fuzzy sets,<ref name="MT">{{cite journal|last=Mizumoto|first=M.|author2=K. Tanaka |title=Some Properties of fuzzy sets of type 2|journal=Information and Control|year=1976|volume=31|pages=31240|doi=10.1016/s0019-9958(76)80011-3}}</ref><ref name="zadehJ">{{cite journal|last=Zadeh|first=L. A.|title=The concept of a linguistic variable and its application to approximate reasoning-1|journal=Information Sciences|year=1975|volume=8|pages=199249|doi=10.1016/0020-0255(75)90036-5}}</ref> i.e., fuzzy maximum operators;
* Meet operators of (type-1) fuzzy sets,<ref name="MT"/><ref name="zadehJ"/> i.e., fuzzy minimum operators;
* Join-like operators of (type-1) fuzzy sets;<ref name="kdeT1OWA"/><ref name="bookT1OWA">{{cite journal|last=Zhou|first=S. M.|author2=F. Chiclana |author3=R. I. John |author4=J. M. Garibaldi |title=Fuzzificcation of the OWA Operators in Aggregating Uncertain Information|journal=R. R. Yager, J. Kacprzyk and G. Beliakov (ed): Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|volume=Springer|pages=91109|doi=10.1007/978-3-642-17910-5_5}}</ref>
* Meet-like operators of (type-1) fuzzy sets.<ref name="kdeT1OWA"/><ref name="bookT1OWA"/>

==Generalizations==
Type-2 OWA operators<ref>{{cite journal|last=Zhou|first=S.M.|coauthors=R. I. John, F. Chiclana and J. M. Garibaldi|title=On aggregating uncertain information by type-2 OWA operators for soft decision making|journal=International Journal of Intelligent Systems|year=2010|volume=25|issue=6|pages=540558|doi=10.1002/int.20420}}</ref> have been suggested to aggregate the [[Type-2 fuzzy sets and systems|type-2 fuzzy sets]] for soft decision making.

== References ==
{{reflist}}

[[Category:Artificial intelligence]]
[[Category:Logic in computer science]]
[[Category:Fuzzy logic]]
[[Category:Information retrieval]]
>>EOP<<
131<|###|>Natural language user interface
'''Natural Language User Interfaces''' (LUI or NLUI) are a type of [[User interface|computer human interface]] where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications.

In [[interface design]] natural language interfaces are sought after for their speed and ease of use, but most suffer the challenges to [[natural language understanding|understanding]] wide varieties of ambiguous input.<ref>Hill, I. (1983). "Natural language versus computer language." In M. Sime and M. Coombs (Eds.) Designing for Human-Computer Communication. Academic Press.</ref>
Natural language interfaces are an active area of study in the field of [[natural language processing]] and [[computational linguistics]]. An intuitive general Natural language interface is one of the active goals of the [[Semantic Web]].

Text interfaces are 'natural' to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional [[keyword search]] engine could be described as a 'shallow' Natural language user interface.

==Overview==
A natural language search engine would in theory find targeted answers to user questions (as opposed to keyword search). For example, when confronted with a question of the form 'which [[United States|U.S.]] state has the highest [[income tax]]?', conventional search engines ignore the question and instead search on the [[index term|keywords]] 'state', 'income' and 'tax'. Natural language search, on the other hand, attempts to use natural language processing to understand the nature of the question and then to search and return a subset of the web that contains the answer to the question. If it works, results would have a higher relevance than results from a keyword search engine.

==History==

Prototype Nl interfaces had already appeared in the late sixties and early seventies.<ref name="edin">Natural Language Interfaces to Databases  An Introduction,
I. Androutsopoulos,
G.D. Ritchie,
P. Thanisch,
Department of Artificial Intelligence, University of Edinburgh</ref>

*[[SHRDLU]], a natural language interface that manipulates blocks in a virtual "blocks world"
*''Lunar'', a natural language interface to a database containing chemical analyses of Apollo-11 moon rocks by [http://parsecraft.com/ William A. Woods].
*''Chat-80'' transformed English questions into [[Prolog]] expressions, which were evaluated against the Prolog database.  The code of Chat-80 was circulated widely, and formed the basis of several other experimental Nl interfaces. An online demo is available on the LPA website.<ref>[http://www.lpa.co.uk/pws_dem5.htm Chat-80 demo]</ref>
*[[ELIZA]], written at MIT by Joseph Weizenbaum between 1964 and 1966, mimicked a psychotherapist and was operated by processing users' responses to scripts. Using almost no information about human thought or emotion, the DOCTOR script sometimes provided a startlingly human-like interaction. An online demo is available on the LPA website.<ref>[http://www.lpa.co.uk/pws_dem4.htm ELIZA demo]</ref>
* ''Janus'' is also one of the few systems to support temporal questions.
* ''Intellect'' from [[Trinzic]] (formed by the merger of AICorp and Aion).
* BBNs ''Parlance'' built on experience from the development of the ''Rus''  and ''Irus'' systems.
* [[IBM]] ''Languageaccess''
* [[Q&A (software)|Q&A]] from [[Symantec]].
* ''Datatalker'' from Natural Language Inc.
* ''Loqui''  from [[Bim]].
* ''English Wizard'' from [[Linguistic Technology Corporation]].
* ''iAskWeb'' from Anserity Inc. fully implemented in [[Prolog]] was providing interactive recommendations in NL to users in tax and investment domains in 1999-2001<ref>{{cite book | last = Galitsky
 | first = Boris
 | title = Natural Language Question Answering: technique of semantic headers
 | publisher = Advance Knowledge International
 | date = 2003
 | location = Adelaide, Australia
 | url = http://www.amazon.com/Natural-Language-Question-Answering-system/dp/0868039799
 | isbn = 0868039799
  }}</ref>

==Challenges==
Natural language interfaces have in the past led users to anthropomorphize the computer, or at least to attribute more intelligence to machines than is warranted. On the part of the user, this has led to unrealistic expectations of the capabilities of the system. Such expectations will make it difficult to learn the restrictions of the system if users attribute too much capability to it, and will ultimately lead to disappointment when the system fails to perform as expected as was the case in the [[AI winter]] of the 1970s and 80s.

A [http://arxiv.org/abs/cmp-lg/9503016 1995 paper] titled 'Natural Language Interfaces to Databases  An Introduction', describes some challenges:<ref name="edin"/>
* ''Modifier attachment''
The request List all employees in the company with a driving licence is ambiguous unless you know companies can't have drivers licences.

* ''Conjunction and disjunction''
List all applicants who live in California and Arizona is ambiguous unless you know that a person can't live in two places at once.
* ''[[Anaphora resolution]]''
- resolve what a user means by 'he', 'she' or 'it', in a self-referential query.

Other goals to consider more generally are the speed and efficiency of the interface, in all algorithms these two points are the main point that will determine if some methods are better than others and therefore have greater success in the market.

Finally, regarding the methods used, the main problem to be solved is creating a general algorithm that can recognize the entire spectrum of different voices, while disregarding nationality, gender or age. The significant differences between the extracted features - even from speakers who says the same word or phrase - must be successfully overcome.

==Uses and applications==

The natural language interface gives rise to technology used for many different applications. 

Some of the main uses are:

* ''Dictation'', is the most common use for [[automated speech recognition]] (ASR) systems today. This includes medical transcriptions, legal and business dictation, and general word processing. In some cases special vocabularies are used to increase the accuracy of the system.
* ''Command and control'', ASR systems that are designed to perform functions and actions on the system are defined as command and control systems. Utterances like "Open Netscape" and "Start a new xterm" will do just that.
* ''Telephony'', some PBX/[[Voice Mail]] systems allow callers to speak commands instead of pressing buttons to send specific tones.
* ''Wearables'', because inputs are limited for wearable devices, speaking is a natural possibility.
* ''Medical, disabilities'', many people have difficulty typing due to physical limitations such as repetitive strain injuries (RSI), muscular dystrophy, and many others. For example, people with difficulty hearing could use a system connected to their telephone to convert a caller's speech to text.
* ''Embedded applications'', some new cellular phones include C&C speech recognition that allow utterances such as "call home". This may be a major factor in the future of automatic speech recognition and [[Linux]].

Below are named and defined some of the applications that use natural language recognition, and so have integrated utilities listed above.

===Ubiquity===
{{main|Ubiquity (Firefox)}}
Ubiquity, an [[add-on (Mozilla)|add-on]] for [[Mozilla Firefox]], is a collection of quick and easy natural-language-derived commands that act as [[mashup (web application hybrid)|mashups]] of web services, thus allowing users to get information and relate it to current and other webpages.

===Wolfram Alpha===
{{main|Wolfram Alpha}}
Wolfram Alpha is an online service that answers factual queries directly by computing the answer from structured data, rather than providing a list of documents or web pages that might contain the answer as a [[search engine]] would.<ref>{{cite news |url=http://www.guardian.co.uk/technology/2009/mar/09/search-engine-google |title=British search engine 'could rival Google' |last=Johnson |first=Bobbie |date=2009-03-09 |work=[[The Guardian]] |accessdate=2009-03-09}}</ref> It was announced in March 2009 by [[Stephen Wolfram]], and was released to the public on May 15, 2009.<ref name="launch date">{{cite web|url=http://blog.wolframalpha.com/2009/05/08/so-much-for-a-quiet-launch/ |title=So Much for A Quiet Launch |publisher=Wolfram Alpha Blog |date=2009-05-08 |accessdate=2009-10-20}}</ref>

===Siri===
{{main|Siri (software)}}
Siri is a [[personal assistant]] application for the operating system [[iOS]]. The application uses [[natural language processing]] to answer questions and make recommendations. The iPhone app is the first public product by its makers, who are focused on [[artificial intelligence]] applications.

Siri's marketing claims include that it adapts to a user's individual preferences over time and personalizes results, and performs tasks such as making dinner reservations while trying to catch a cab.<ref>[http://www.apple.com/iphone/features/siri.html Siri webpage]</ref>

===Others===
* [[Anboto Group]] provides Web customer service and e-commerce technology based on semantics and natural language processing. The main offer of [http://www.anbotogroup.com/en/index.php Anboto Group] are the virtual sales agent and intelligent chat.
* [[Ask.com]] - The original idea behind Ask Jeeves (Ask.com) was traditional keyword searching with an ability to get answers to questions posed in everyday, natural language. The current Ask.com still supports this, with added support for math, dictionary, and conversion questions.
* [[Braina]]<ref>[http://www.brainasoft.com/braina/ Braina]</ref> - Braina is a natural language interface for [[Windows OS]] that allows to type or speak English language sentences to perform a certain action or find information.
* [http://www.cmantik.com/ CMANTIK] - CMANTIK is a semantic information search engine which is trying to answer user's questions by looking up relevant information in Wikipedia and some news sources.
* C-Phrase<ref>[http://code.google.com/p/c-phrase/ C-Phrase]</ref> - is a web-based natural language front end to relational databases. C-Phrase runs under Linux, connects with PostgreSQL databases via ODBC and supports both select queries and updates. Currently there is only support for English. C-Phrase is hosted on [[Google Code]] site.
* [http://devtools.korzh.com/easyquery/ EasyQuery] - is a component library (for .NET framework first of all) which allows you to implement natural language query builder in your application. Works both with relational databases or ORM solutions like Entity Framework.
[[File:GNOME Do Classic.png|thumb|Screenshot of GNOME DO classic interface.]]
* [[GNOME Do]] - Allows for quick finding miscellaneous artifacts of GNOME environment (applications, Evolution and Pidgin contacts, Firefox bookmarks, Rhythmbox artists and albums, and so on) and execute the basic actions on them (launch, open, email, chat, play, etc.).<ref>Ubuntu 10.04 Add/Remove Applications description for GNOME Do</ref>
* [[Invention Machine]] Goldfire - powered by a semantic research engine that has the capability to transform unstructured documents from various electronic sources into an index that, when searched, delivers answers to research questions. Goldfires Natural Language query interface enables the user to put a question in a free text format, which would be the same format as if the question were given to another person. And, once knowledge has been retrieved, Goldfire presents the results in a way that makes their meaning readily apparent.
* [[hakia]] - hakia is an Internet search engine. The company has invented an alternative new infrastructure to indexing that uses SemanticRank algorithm, a solution mix from the disciplines of ontological semantics, fuzzy logic, computational linguistics, and mathematics.
* [[Lexxe]] - Lexxe is an Internet search engine that uses natural language processing for queries (semantic search). Searches can be made with keywords, phrases, and questions, such as "How old is Wikipedia?" When it comes to facts, Lexxe is quite effective, though needs much improvement in natural language analysis in the area of facts and in other areas.
* [http://www.mnemoo.com/ Mnemoo] - Mnemoo is an answer engine that aimed to directly answer questions posed in plain text (Natural Language), which is accomplished using a database of facts and an inference engine.
* [http://www.naturaldateandtime.com/ Natural Date and Time] - Natural language date and time zone engine. It allows you to ask questions about time, daylight saving information and to do time zone conversions via plain English questions such as 'What is the time in Sao Paulo when it is 6pm on the 2nd of June in Detroit'.
* [http://www.linguasys.com/web_production/server-item/NLUI%20Server NLUI Server] - an enterprise-oriented multilingual application server by LinguaSys for natural language user interface scripts, supporting English, Spanish, Portuguese, German, Japanese, Chinese, Pashto, Thai, Russian, Vietnamese, Malay, with Arabic, French, and more languages in development.
* [[Pikimal]] - Pikimal uses natural language tied to user preference to make search recommendations by template.
* [[Powerset (company)|Powerset]]  On May 11, 2008, the company unveiled a tool for searching a fixed subset of [[Wikipedia]] using conversational phrases rather than keywords.<ref>{{cite news |url=http://bits.blogs.nytimes.com/2008/05/12/powerset-debuts-with-search-of-wikipedia/ |title=Powerset Debuts With Search of Wikipedia |publisher=The New York Times |first=Miguel |last=Helft |date=May 12, 2008}}</ref> On July 1, 2008, it was purchased by [[Microsoft]].<ref>{{cite web |url=http://www.powerset.com/blog/articles/2008/07/01/microsoft-to-acquire-powerset |archiveurl=http://web.archive.org/web/20090225064356/http://www.powerset.com/blog/articles/2008/07/01/microsoft-to-acquire-powerset |archivedate=February 25, 2009 |title=Microsoft to Acquire Powerset |publisher=Powerset Blog |first=Mark |last=Johnson |date=July 1, 2008}}</ref>
* [[Q-go]] - The Q-go technology provides relevant answers to users in response to queries on a companys internet website or corporate intranet, formulated in natural sentences or keyword input alike. Q-go was acquired by [[RightNow Technologies]] in 2011
* [[START (MIT project)]] - [http://start.csail.mit.edu/ START], Web-based question answering system. Unlike information retrieval systems such as search engines, START aims to supply users with "just the right information," instead of merely providing a list of hits. Currently, the system can answer millions of English questions about places, movies, people and dictionary definitions.
* [http://swingly.com/ Swingly] - Swingly is an answer engine designed to find exact answers to factual questions. Just ask a question in plain English - and Swingly will find you the answer (or answers) you're looking for (according to their site).
* [[Yebol]] - Yebol is a vertical "decision" search engine that had developed a knowledge-based, semantic search platform. Yebol's artificial intelligence human intelligence-infused algorithms automatically cluster and categorize search results, web sites, pages and content that it presents in a visually indexed format that is more aligned with initial human intent. Yebol uses association, ranking and clustering algorithms to analyze related keywords or web pages. Yebol integrates natural language processing, metasynthetic-engineered open complex systems, and machine algorithms with human knowledge for each query to establish a web directory that actually 'learns', using correlation, clustering and classification algorithms to automatically generate the knowledge query, which is retained and regenerated forward.<ref>Humphries, Matthew. [http://www.geek.com/articles/news/yebolcom-steps-into-the-search-market-20090731/ "Yebol.com steps into the search market"] ''Geek.com''. 31 July 2009.</ref>

==See also==
*[[Natural language programming]]
**[[xTalk]], a family of English-like programming languages
*[[Chatterbot]], a computer program that simulates human conversations
*[[Noisy text]]
*[[Question answering]]
*[[Selection-based search]]
*[[Semantic search]]
*[[Semantic Web]]

==References==
{{reflist}}

{{Internet search}}
{{Computable knowledge}}

{{DEFAULTSORT:Natural language user interface}}
[[Category:User interfaces]]
[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval]]
>>EOP<<
137<|###|>PolySpot
{{Infobox company
| name = PolySpot
| logo = [[File:PolySpot-Logo.jpg|300px]]
| type = [[Privately held company|Private]]
| foundation = [[Paris]] (2001)
| location = [[Paris]], [[London]]
| key_people = Guy Mounier, CEO
| industry = [[Information technology]] <br/> [[Unified Information Access]] <br/> [[Search Engine]]
| products = PolySpot Infowarehouse<br/>PolySpot Information At Work<br/>PolySpot Enterprise Search
| slogan = Open Search Solutions
| homepage = [http://www.polyspot.com/en/ www.polyspot.com]
}}
'''PolySpot''' is a subsidiary of CustomerMatrix, an [[enterprise search]] [[ISV|software company]].

Created in 2001, PolySpot has its headquarters in Paris, France. It also has offices in the United Kingdom.

In 2011, PolySpot raised EUR 2.5m from [[Newfund]].<ref>{{cite web|url=http://finance.yahoo.com/news/PolySpot-Raises-2-Million-prnews-4194799492.html| title=Yahoo News: PolySpot Raises 2 Million}}</ref> and True Global Ventures.

In 2013, CustomerMatrix (US) acquired PolySpot. In December 2013, PolySpot reduced its shareholders equity from 507,209 EUR to 206,120 EUR.

== Functionalities ==

PolySpot's infrastructure provide a unified information access to all data, through which users can instantly interact with all available information resources, both inside and outside the company, and regardless of whether or how the data are structured.

PolySpot's indexing capabilities are based on the [[Apache Software Foundation|Apache]] [[Lucene]] [[Solr]] Java-based open-source projects.

==References==
{{Portal|Software}}
{{Reflist|colwidth=30em}}

==External links==
*[http://www.polyspot.com/en/ Website]
*[http://www.customermatrix.com/ CustomerMatrix Website]
*[http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=38687243 Company profile in BusinessWeek]
*[http://www.arnoldit.com/search-wizards-speak/polyspot-2.html PolySpot in ArnoldIT]
*[http://arnoldit.com/wordpress/2010/01/13/polyspot-lands-crdit-agricole-sa/ PolySpot lands Credit Agricole in ArnoldIT]

{{DEFAULTSORT:Polyspot}}
[[Category:Searching]]
[[Category:Search engine software|*Enterprise search vendors]]
[[Category:Internet search engines]]
[[Category:Information retrieval]]
>>EOP<<
143<|###|>Artificial Solutions
{{Infobox company
|name= Artificial Solutions
|logo=[[Image:Artificial Solutions Logo.png]]
|type=[[Private company]]
|foundation=(2001)
|founder=Johan Ahlund, Johan Gustavsson and Michael Soderstrom 
|location=[[Stockholm]], [[Sweden]]
|locations=Offices worldwide with R&D centers in [[Barcelona]], [[Hamburg]], [[London]] and [[Stockholm]] 
|industry=[[Computer Software]], [[Natural language]], [[Intelligent software assistant]], 
|products= Teneo platform
|homepage=[http://www.artificial-solutions.com/ www.artificial-solutions.com]
}}

'''Artificial Solutions''' is a multinational [[software company]] that develops and sells natural language interaction products for enterprise and consumer use.<ref>{{cite web|last=Ion |first=Florence |url=http://arstechnica.com/gadgets/2013/06/review-indigo-brings-siri-like-conversation-to-the-android-platform/ |title=Review: Indigo wants to bring Siri-like conversation to the Android platform |publisher=Ars Technica |date=2013-06-05 |accessdate=2013-09-08}}</ref> The company's natural language solutions have been deployed in a wide range of industries including finance,<ref>{{cite web|last=Thompson|first=Scott|title=Agria working with Artificial Solutions|url=http://www.fstech.co.uk/fst/AgriaDjurf%C3%B6rs%C3%A4kring_ArtificialSolutions.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Savvas|first=Antony|title=Co-operative Bank uses Mia to speed up contact centre calls|url=http://www.computerworlduk.com/news/it-business/3316914/co-operative-bank-uses-mia-to-speed-up-contact-centre-calls/|work=Computerworld UK|publisher=IDG|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Thompson|first=Scott|title=2012 FStech Awards: winners announced|url=http://www.fstech.co.uk/fst/2012_FStechAwards_Winners.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}</ref> telecoms,<ref>{{cite web|last=Westerholm|first=Joel|title=Telenors elektroniska kundtjanst pressar kostnaderna|url=http://computersweden.idg.se/2.2683/1.143425|work=ComputerSweden|publisher=IDG|accessdate=12 September 2013}}</ref><ref>{{cite web|title=Artificial Solutions Powers Online IVA for Vodafone|url=http://langtechnews.hivefire.com/articles/262940/artificial-solutions-powers-online-iva-for-vodafon/|work=LangTechNews|accessdate=12 September 2013}}</ref> the public sector,<ref>{{cite web|last=Brax|first=Sofia|title=Digitala kolleger alltid till tjanst|url=http://www.publikt.se/artikel/digitala-kolleger-alltid-till-tjanst-38087|work=Publik|publisher=Fackforbundet ST|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Nilsson|first=Orjan|title=Cyber-damene husker deg|url=http://www.nettavisen.no/innenriks/ibergen/article1609734.ece|work=Nettavisen|publisher=iBergen}}</ref> retail<ref>{{cite web|author=Aaron Travis |url=http://techcrunch.com/2013/01/05/in-defense-of-the-humble-walkthrough/ |title=In Defense Of The Humble App Walkthrough |publisher=TechCrunch |date=2013-01-05 |accessdate=2013-09-08}}</ref> and travel.<ref>{{cite web|last=Fox|first=Linda|title=CWT brings virtual face to mobile service|url=http://www.tnooz.com/2013/04/16/news/cwt-brings-virtual-face-to-mobile-service/|work=Tnooz|accessdate=12 September 2013}}</ref>

==History==
Artificial Solutions was founded in Stockholm in 2001 by friends Johan Ahlund, Johan Gustavsson and Michael Soderstrom to create interactive web assistants using a combination of artificial intelligence and natural language processing. Though Ahlund initially took some persuading, he thought it sounded ridiculous to be talking to a virtual agent on the internet.<ref>{{cite web|url=http://it24.idg.se/2.2275/1.143922 |title=Lojlig affarside vinstlott for Artificial Solutions |publisher=IT24 |date= |accessdate=2013-09-08}}</ref>

The company expanded with the development of online customer service optimization products and by 2005 it had several offices throughout Europe supporting the development and sales of its online virtual assistants.<ref>{{cite web|url=http://www.elnuevolunes.es/historico/2008/1294/1294%20al%20grano.html |title=Al grano |publisher=Elnuevolunes.es |date= |accessdate=2013-09-08}}</ref> Artificial Solutions was placed as visionary in the latest Gartner Magic Quadrant for CRM Web Customer Service Applications.<ref>{{cite web|author=Barry Levine |url=http://www.cmswire.com/cms/customer-experience/gartner-mq-for-crm-web-customer-service-kana-moxie-software-oraclerightnow-among-leaders-019626.php |title=Gartner MQ for CRM Web Customer Service: Kana, Moxie Software, Oracle-RightNow Among Leaders |publisher=Cmswire.com |date= |accessdate=2013-09-08}}</ref>

In 2006 Artificial Solutions acquired Kiwilogic, a German software house creating its own virtual assistants.<ref>{{cite web|url=http://www.earlybird.com/en/companies/tech/exited/kiwilogic.html |title=Venture Capital: KIWILOGIC.COM AG |publisher=Earlybird |date= |accessdate=2013-09-08}}</ref>
[[Elbot]], Artificial Solutions test-bed to explore the psychology of human-machine communication, won the [[Loebner Prize]] in 2008 and is the closest contestant of the annual competition based on the [[Turing Test]] to reach the 30% threshold by fooling 25% of the human judges.<ref>[[Loebner Prize]]</ref><ref>{{cite web|url=http://news.bbc.co.uk/2/hi/uk_news/england/berkshire/7666246.stm |title=UK &#124; England &#124; Berkshire &#124; Test explores if robots can think |publisher=BBC News |date=2008-10-13 |accessdate=2013-09-08}}</ref><ref>{{cite web|last=Robson|first=David|title=Almost human: Interview with a chatbot|url=http://www.newscientist.com/article/dn14925-almost-human-interview-with-a-chatbot.html#.UjHKzTdBuM9|work=New Scientist|publisher=Reed Business Information Ltd}}</ref>

With a change in management in 2010 the company started to focus the basis of its technology on Natural Language Interaction and launched the Teneo Platform, which allows people to hold humanlike, intelligent conversations with applications and services running on electronic devices.<ref>{{cite web|author=[[Mike Elgan]] |url=http://www.computerworld.com/s/article/9237448/Smart_apps_think_so_you_don_t_have_to_ |title=Smart apps think (so you don't have to) |publisher=Computerworld |date=2013-03-09 |accessdate=2013-09-08}}</ref><ref>{{cite web|url=http://www.speechtechmag.com/Articles/News/Industry-News/Artificial-Solutions-Unveils-a-Software-Toolkit-for-Adding-Speech-to-Mobile-Apps-80015.aspx |title=Artificial Solutions Unveils a Software Toolkit for Adding Speech to Mobile Apps |publisher=SpeechTechMag.com |date=2012-01-17 |accessdate=2013-09-08}}</ref><ref>{{cite web|author= |url=http://www.computerworld.dk/art/220859/saa-effektiv-er-ikeas-chat-robot-har-vaeret-paa-efteruddannelse |title=Sa effektiv er Ikeas chat-robot: Har vret pa 'efteruddannelse' - Computerworld |publisher=Computerworld.dk |date= |accessdate=2013-09-08}}</ref>
In 2013 Artificial Solutions launched [[Indigo (virtual assistant)|Indigo]], a mobile personal assistant that is able to operate and remember the context of the conversation across different platforms and operating systems.<ref>{{cite web|last=Hoyle |first=Andrew |url=http://reviews.cnet.com/8301-13970_7-57570960-78/indigo-brings-siri-like-assistance-to-android-for-free-hands-on/ |title=Indigo brings Siri-like assistance to Android for free (hands-on) &#124; Mobile World Congress - CNET Reviews |publisher=Reviews.cnet.com |date=2013-02-24 |accessdate=2013-09-08}}</ref><ref>{{cite web|author= |url=http://lifehacker.com/indigo-wants-to-be-your-personal-assistant-across-devic-484924277 |title=Indigo Wants to Be Your Personal Assistant Across Devices |publisher=Lifehacker.com |date= |accessdate=2013-09-08}}</ref><ref>{{cite web|last=Wollman |first=Dana |url=http://www.engadget.com/2013/02/26/indigo-personal-assistant-hands-on/ |title=Indigo is a cloud-based, cross-platform personal assistant for Android and Windows Phone 8 (hands-on) |publisher=Engadget.com |date=2013-02-26 |accessdate=2013-09-08}}</ref>
A new round of funding was announced in June 2013. The $9.4m will be used to support expansion in the US market.<ref>{{cite web|url=http://www.altassets.net/private-equity-news/by-news-type/deal-news/artificial-solutions-raises-9-4m-in-scope-led-round-for-us-expansion.html |title=Artificial Solutions raises $9.4m in Scope-led round for US expansion &#124; AltAssets Private Equity News |publisher=Altassets.net |date=2013-06-25 |accessdate=2013-09-08}}</ref>

In February 2014 Artificial Solutions announced the Teneo Network of Knowledge, a patented intelligent framework that enables users to interact using natural language with private, shared and public ecosystem of devices, also known as the [[Internet of Things]].<ref>{{cite web|last1=Trenholm|first1=Rich|title=Next generation of personal assistant takes a step towards 'Her'-style super-Siri|url=http://www.cnet.com/news/next-generation-of-personal-assistant-takes-a-step-towards-her-style-super-siri/|website=Cnet|publisher=CBS Interactive}}</ref>

==References==
{{Reflist|30em}}

==External links==
*[http://www.hello-indigo.com Indigo]
*[http://www.elbot.com Elbot]

[[Category:Natural language processing software]]
[[Category:Intelligent software assistants]]
[[Category:User interfaces]]
[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval]]
>>EOP<<
149<|###|>Special Interest Group on Information Retrieval
{{Infobox organization
|name           = ACM Special Interest Group on Information Retrieval
|image          = sig-information-retrieval-logo.png
|size           = 140px
|alt            = ACM SIGIR
|parent_organization = [[Association for Computing Machinery]]
|website        = {{URL|sigir.org}}
}}

'''SIGIR''' is the [[Association for Computing Machinery]]'s Special Interest Group on [[Information Retrieval]]. The scope of the group's specialty is the theory and application of computers to the acquisition, organization, storage, retrieval and distribution of information; emphasis is placed on working with non-numeric information, ranging from natural language to highly structured data bases.

== Conferences ==
The annual international SIGIR conference, which began in 1978, is considered the most important in the field of information retrieval.  SIGIR also sponsors the annual [[Joint Conference on Digital Libraries]] (JCDL) in association with [[SIGWEB]], the [[Conference on Information and Knowledge Management]], and the [[International Conference on Web Search and Data Mining]] (WSDM) in association with [[SIGKDD]], [[SIGMOD]], and [[SIGWEB]].

=== SIGIR Conference Locations ===
{| class="wikitable" border="1"
|-
!  Number
!  Year
!  Location
|-
|  22
|  1999
|  [[Berkeley, California]]
|-
|  23
|  2000
|  [[Athens]]
|-
|  24
|  2001
|  [[New Orleans]]
|-
|  25
|  2002
|  [[Tampere]]
|-
|  26
|  2003
|  [[Toronto]]
|-
|  27
|  2004
|  [[Sheffield]]
|-
|  28
|  2005
|  [[Salvador, Bahia]]
|-
|  29
|  2006
|  [[Seattle]]
|-
|  30
|  2007
|  [[Amsterdam]]
|-
|  31
|  2008
|  [[Singapore]]
|-
|  32
|  2009
|  [[Boston]]
|-
|  33
|  2010
|  [[Geneva]]
|-
|  34
|  2011
|  [[Beijing]]
|-
|  35
|  2012
|  [[Portland, Oregon]]
|-
|  36
|  2013
|  [[Dublin]]
|-
|  37
|  2014
|  [[Gold Coast, Queensland]]
|-
|  38
|  2015
|  [[Santiago]]
|-
|  39
|  2016
|  [[Pisa]]
|-
|  40
|  2017
|  [[Tokyo]]
|}

== Awards ==
The group gives out several awards to contributions to the field of information retrieval. The most important award is the [[Gerard Salton Award]] (named after the computer scientist [[Gerard Salton]]), which is awarded every three years to an individual who has made "significant, sustained and continuing contributions to research in information retrieval". Additionally, SIGIR presents a Best Paper Award <ref>{{cite web | url=http://sigir.org/awards/awards.html#bestpaper | title=SIGIR Conference Best Paper Awards | accessdate=2012-08-29 }}</ref> to recognize the highest quality paper at each conference.

==See also==
* [[Conference on Information and Knowledge Management]]

==External links==
* [http://www.sigir.org/ SIGIR]

==References==

{{Reflist}}
{{Authority control}}

[[Category:Association for Computing Machinery Special Interest Groups]]
[[Category:Information retrieval]]
>>EOP<<
155<|###|>Hashtag
[[File:Global Summit to End Sexual Violence in Conflict (14203190979).jpg|thumb|A sign suggesting the usage of a #timetoact hashtag at a 2014 conference]]

A '''hashtag''' is a word or an unspaced phrase prefixed with the [[Number sign|hash character (or number sign), <code>#</code>]], to form a label.<ref>http://www.merriam-webster.com/dictionary/hashtag</ref> It is a type of [[Tag (metadata)|metadata tag]]. Words or phrases in messages on [[microblogging]] and [[social networking service]]s such as [[Facebook]], [[Google+]], [[Instagram]], [[Twitter]], or [[VK (social network)|VK]] may be tagged by entering # before them,<ref>{{cite web|url=http://support.twitter.com/articles/49309# |title=Using hashtags on Twitter|publisher=support.twitter.com |accessdate=2013-11-25}}</ref> either as they appear in a sentence, e.g., "New artists announced for #SXSW2014 Music Festival"<ref>{{cite web|url=https://dev.twitter.com/media/hashtags |title=Best Practices for Hashtags &#124; Twitter Developers |publisher=Dev.twitter.com |date=2011-07-19 |accessdate=2013-11-12}}</ref> or appended to it. The term hashtag can also refer to the hash symbol itself when used in the context of a hashtag.<ref>{{cite web |title=Oxford English Dictionary - Hash|url=http://www.oed.com/view/Entry/389023#eid301493073|work=Oxford English Dictionary|date=June de 2014}}</ref>

A hashtag allows grouping of similarly tagged messages, and also allows an electronic search to return all messages that contain it.

Due to its widespread use, 'hashtag' was added to the ''[[Oxford English Dictionary]]'' in June 2014.<ref>{{cite web |title='Hashtag' added to the OED  but # isn't a hash, pound, nor number sign|url=http://www.theregister.co.uk/2014/06/13/hashtag_added_to_the_oed/|work=The Register|date=13 June 2014}}</ref><ref>{{cite web |title=New words notes June 2014|url=http://public.oed.com/the-oed-today/recent-updates-to-the-oed/june-2014-update/new-words-notes-june-2014/|work=Oxford English Dictionary|date=June de 2014}}</ref>

==Origin==
The [[number sign]] was often used in [[information technology]] to highlight a special meaning. In 1970 for example, the number sign was used to denote ''immediate'' [[address mode]] in the assembly language of the [[PDP-11]]<ref>{{cite web|url=https://programmer209.wordpress.com/2011/08/03/the-pdp-11-assembly-language/ |title=PDP-11 assembly language |publisher=Programmer209.wordpress.com |date=2011-08-03 |accessdate=2014-08-25}}</ref> when placed next to a symbol or a number. In 1978, [[Brian Kernighan]] and [[Dennis Ritchie]] used ''#'' in the [[C (programming language)|C programming language]] for special keywords that had to be processed first by the [[C preprocessor]].<ref>{{cite book|title=[[The C Programming Language]]|authors=B.W.Kernighan &  d.Ritchie|publisher=Prentice Hall|year=1978|pages=86 and 207|isbn=0-13-110163-3}}</ref> Since before the invention of the hashtag, the number sign has been called the "hash symbol" in some countries outside of North America.<ref>{{cite book|last1=Bourke|first1=Jane|title=Communication Techonology Resource Book|date=2004|publisher=Ready-Ed Publications|pages=19|url=http://books.google.co.uk/books?id=gPNBTmxzpIIC&lpg=PA19&dq=hash%20key%20telephone&pg=PA19#v=onepage&q=hash&f=false|accessdate=7 November 2014|isbn=9781863975858}}</ref><ref>{{cite book|last1=Hargraves|first1=Orin|title=Mighty fine words and smashing expressions : making sense of transatlantic English|date=2003|publisher=Oxford Univ. Press|location=Oxford [u.a.]|isbn=9780195157048|pages=33, 260|url=http://books.google.co.uk/books?id=dUTdk93cq9UC&lpg=PA260&dq=hash%20telephone&pg=PA260#v=onepage&q=hash%20mark&f=false}}</ref>

The number sign then appeared and was used within [[Internet Relay Chat|IRC]] networks to label groups and topics.<ref>"Channel Scope". Section 2.2. RFC 2811</ref> Channels or topics that are available across an entire IRC network are prefixed with a hash symbol # (as opposed to those local to a server, which use an [[ampersand]] '&').<ref>{{cite IETF |title=Internet Relay Chat Protocol |rfc=1459 |sectionname=Channels |section=1.3 |page= |last1=Oikarinen |first1=Jarkko |authorlink1=Jarkko Oikarinen |last2=Reed |first2=Darren |authorlink2= |year=1993 |month=May |publisher=[[Internet Engineering Task Force|IETF]] |accessdate=3 June 2014}}</ref>

The use of the number sign in IRC inspired<ref>{{cite web|url=http://www.cmu.edu/homepage/computing/2014/summer/originstory.shtml |title=#OriginStory|publisher=Carnegie Mellon University|date=2014-08-29}}</ref> [[Chris Messina (open source advocate)|Chris Messina]] to propose a similar system to be used on Twitter to tag topics of interest on the microblogging network.<ref>{{cite news | url=http://www.nytimes.com/2011/06/12/fashion/hashtags-a-new-way-for-tweets-cultural-studies.html?_r=1&pagewanted=all | title=Twitters Secret Handshake | work=The New York Times | date=June 10, 2011 | accessdate=July 26, 2011 | author=Parker, Ashley}}</ref> He posted the first hashtag on Twitter: 
{{quote |1=how do you feel about using # (pound) for groups. As in #barcamp [msg]? |author = Chris Messina |source = ("factoryjoe"), August 23, 2007<ref>{{cite web|url = https://twitter.com/#!/factoryjoe/statuses/223115412|title = Twitter post|author = Chris Messina ("factoryjoe")|date = August 23, 2007<!-- 3:25 PM-->}}</ref> |width  = 50% |align  = center }}
Internationally, the hashtag became a practice of writing style for Twitter posts during the [[20092010 Iranian election protests]], as both English- and [[Persian language|Persian]]-language hashtags became useful for Twitter users inside and outside Iran.{{cite web|url=http://www.dw.de/%D8%AD%DA%A9%D8%A7%DB%8C%D8%AA-%D9%87%D8%B4%D8%AA%DA%AF%DB%8C-%DA%A9%D9%87-%D8%A7%DB%8C%D8%B1%D8%A7%D9%86%DB%8C%D8%A7%D9%86-%D8%A2%D8%BA%D8%A7%D8%B2-%DA%A9%D8%B1%D8%AF%D9%86%D8%AF/g-18012627|title = dw |date= 2009}}

The first use of the term "hash tag" was in a blog post by Stowe Boyd, "Hash Tags = Twitter Groupings,"<ref>{{cite web|url=http://stoweboyd.com/post/39877198249/hash-tags-twitter-groupings |title=Stowe Boyd, Hash Tags = Twitter Groupings |publisher=Stoweboyd.com |date= |accessdate=2013-09-19}}</ref> on 26 August 2007, according to lexicographer [[Ben Zimmer]], chair of the American Dialect Society's New Words Committee.

Beginning July 2, 2009,{{citation needed|date=November 2013}} Twitter began to hyperlink all hashtags in tweets to Twitter search results for the hashtagged word (and for the standard spelling of commonly misspelled words). In 2010, Twitter introduced "[[Twitter#Trending_topics|Trending Topics]]" on the Twitter front page, displaying hashtags that are rapidly becoming popular. Twitter has an algorithm to tackle attempts to [[spamming|spam]] the trending list and ensure that hashtags trend naturally.<ref>{{cite web|url=http://www.allisayis.com/the-secret-of-twitters-trending-hashtags-with-insight-and-tips/ |title=The Secret of Twitter's Trending Hashtags With Insight and Tips |publisher=AllISayIs.com |date= |accessdate=2014-12-03}}</ref>

==Style==
On microblogging or social networking sites, hashtags can be inserted anywhere within a sentence, either preceding it, following it as a [[postscript]], or being included as a word within the sentence (e.g. "It is #sunny today").

The quantity of hashtags used in a post or tweet is just as important as the type of hashtags used. It is currently considered acceptable to tag a post once when contributing to a specific conversation. Two hashtags are considered acceptable when adding a location to the conversation. Three hashtags are seen by some as the "absolute maximum", and any contribution exceeding this risks raising the ire of the community.<ref>{{cite web|title=What is a (#) Hashtag?|url=http://www.hashtags.org/how-to/history/what-is-a-hashtag/|publisher=Hashtags.org|accessdate=22 February 2014}}</ref>

As well as frustrating other users, the misuse of hashtags can lead to account suspensions. Twitter warns that adding hashtags to unrelated tweets, or repeated use of the same hashtag without adding to a conversation, could cause an account to be filtered from search, or even suspended.<ref>{{cite web|title=The Twitter Rules|url=https://support.twitter.com/groups/56-policies-violations/topics/236-twitter-rules-policies/articles/18311-the-twitter-rules|publisher=Twitter, Inc.|accessdate=22 February 2014}}</ref>{{failed verification|date=August 2014}}
 
[[Jimmy Fallon]] and [[Justin Timberlake]] performed a sketch parodying the often misused and misunderstood usage of hashtags on ''[[Late Night with Jimmy Fallon]]'' in September 2013.<ref>{{cite web|author=The Tonight Show Starring Jimmy Fallon |url=http://www.youtube.com/watch?v=57dzaMaouXA |title="#Hashtag" with Jimmy Fallon & Justin Timberlake (Late Night with Jimmy Fallon) |publisher=YouTube |date=2013-09-24 |accessdate=2014-08-25}}</ref>

==Function==
[[File:Seguir hashtags.png|300px|right|thumb|Search bar in the header of a social networking site, searching for most recent posts containing the hashtag "#science".]]
Hashtags are mostly used as unmoderated ad hoc discussion forums; any combination of characters led by a hash symbol is a hashtag, and any hashtag, if promoted by enough individuals, can "trend" and attract more individual users to discussion using the hashtag. On Twitter, when a hashtag becomes extremely popular, it will appear in the "Trending Topics" area of a user's homepage. The trending topics can be organized by geographic area or by all of Twitter. Hashtags are neither registered nor controlled by any one user or group of users, and neither can they be "retired" from public usage, meaning that hashtags can be used in theoretical perpetuity depending upon the longevity of the word or set of characters in a written language. They also do not contain any set definitions, meaning that a single hashtag can be used for any number of purposes as espoused by those who make use of them.

Hashtags intended for discussion of a particular event tend to use an obscure wording to avoid being caught up with generic conversations on similar subjects, such as a cake festival using "#cakefestival" rather than simply "#cake". However, this can also make it difficult for topics to become "trending topics" because people often use different spelling or words to refer to the same topic.  In order for topics to trend, there has to be a consensus, whether silent or stated, that the hashtag refers to that specific topic.

Hashtags also function as beacons in order for users to find and "follow" (subscribe) or "list" (organize into public contact lists) other users of similar interest.

Hashtags can be used on the social network [[Instagram]], by posting pictures and hashtagging it with its subject. As an example, a photo of oneself and a friend posted to the social network can be hashtagged #bffl or #friends. Instagram has banned certain hashtags, some because they are too generic like #photography #iPhone #iphoneography and therefore do not fulfil a purpose. They have also blocked hashtags that can be linked to illegal activities, such as drug use.<ref>{{cite web|url=http://www.bbc.co.uk/news/technology-24842750 |title=Instagram banned hashtags | date = 7 November 2013|publisher=BBC.co.uk |accessdate=2013-11-25}}</ref> The censorship and ban against certain hashtags has a consequential role in the way that particular subaltern communities are built and maintained on Instagram. Despite Instagrams content policies, users are finding creative ways of maintaining their practices and ultimately circumventing censorship.<ref>
Olszanowski, M. (2014). "Feminist Self-Imaging and Instagram: Tactics of Circumventing Sensorship". Visual Communication Quarterly, 21(1), 83-95. Retrieved February 8, 2015, from http://www.tandfonline.com/doi/abs/10.1080/15551393.2014.928154#.VNgGT7DF-7FF-7F</ref> 


Hashtags are also used informally to express context around a given message, with no intent to actually categorize the message for later searching, sharing, or other reasons.  This can help express humor, excitement, sadness or other contextual cues, for example "It's Monday!! #excited #sarcasm"

==Use outside of social networking websites==
The feature has been added to other, non-short-message-oriented services, such as the user comment systems on [[YouTube]] and [[Gawker Media]]; in the case of the latter, hashtags for blog comments and directly submitted comments are used to maintain a more constant rate of user activity even when paid employees are not logged into the website.<ref>{{cite web|url = http://gawker.com/5382267/anarchy-in-the-machine-welcome-to-gawkers-open-forums|title = Anarchy in the Machine: Welcome to Gawker's Open Forums|author = Gabriel Snyder|publisher = Gawker|date = Oct 15, 2009<!-- 3:25 PM-->}}</ref><ref>{{cite web|url = http://www.niemanlab.org/2009/10/got-a-tip-gawker-media-opens-tag-pages-to-masses-expecting-chaos/|title = Got a #tip? Gawker Media opens tag pages to masses, expecting "chaos"|author = Zachary M. Seward|publisher = Nieman Journalism Lab|date = Oct 15, 2009 <!-- 8 a.m. -->}}</ref> Real-time search aggregators such as the former [[Google Real-Time Search]] also support hashtags in syndicated posts, meaning that hashtags inserted into Twitter posts can be hyperlinked to incoming posts falling under that same hashtag; this has further enabled a view of the "river" of Twitter posts which can result from search terms or hashtags.{{citation needed|date=September 2014}}

==Websites that support hashtags==
{{Cleanup-list|section|date=May 2014}}
{{columns-list|2|
<!-- PLEASE RESPECT ALPHABETICAL ORDER -->
* [[App.net]]
* [[Diaspora (software)|Diaspora software]] and [[Diaspora (social network)|social network]]
* [[DeviantART]]
* [[Facebook]]
* [[Flickr]]
* [[FriendFeed]]
* [[Gawker Media]] websites
* [[GNU Social]]
* [[Google+]]
* [[Instagram]]
* [[Kickstarter]]
* [[Orkut]]<ref>{{cite web|url = http://en.blog.orkut.com/2012/02/hashtags-in-orkut-communities.html|title = Hashtags in Orkut communities|date = February 6, 2012 <!-- , 6:11 PM --> |publisher = Orkut|author = Marco Wisniewski}}</ref>
* [[Sina Weibo]]
* [[SoundCloud]]
* [[Tout (company)|Tout]]
* [[tsu]]
* [[Tumblr]]
* [[Twitter]]
* [[Vine (software)|Vine]]
* [[VK (social network)|VK]]
}}

==Usage==

===Mass broadcast media===

Since 2010, television series on various television channels promote themselves through "branded" hashtag [[digital on-screen graphic|bugs]].<ref>{{cite web|url = http://www.tvguide.com/News/New-TV-Screen-1032111.aspx|title = New to Your TV Screen: Twitter Hashtags|date = Apr 21, 2011<!-- 3:25 PM-->|author = Michael Schneider|publisher = TV Guide}}</ref> This is used as a means of promoting a [[backchannel]] of online side-discussion before, during and after an episode broadcast. Hashtag bugs appear on either corner of the screen, or they may appear at the end of an advertisement<ref>{{cite web|url = http://mashable.com/2012/12/03/mcdonalds-tv-ad-twitter-hashtag/|title = McDonald's Releases First TV Ad With Twitter Hashtag|date = Dec 3, 2012|author = Todd Wasserman|publisher = Mashable}}</ref> (for example, a motion picture trailer).

While personalities associated with broadcasts, such as hosts and correspondents, also promote their corporate or personal Twitter usernames in order to receive mentions and replies to posts, usage of related or "branded" hashtags alongside Twitter usernames (e.g., [[The Ed Show|#edshow]] as well as [[Ed Schultz|@edshow]]) is increasingly encouraged as a microblogging style in order to "trend" the hashtag (and, hence, the discussion topic) in Twitter and other search engines. Broadcasters also make use of such a style in order to index select posts for live broadcast. Chloe Sladden, Twitter's director of media partnerships, identified two types of television-formatted usage of hashtags: hashtags which identify a series being broadcast (i.e. [[It's Always Sunny in Philadelphia|#SunnyFX]]) and instantaneous, "temporary" hashtags issued by television personalities to gauge topical responses from viewers during broadcasts.<ref>{{cite web|url = http://www.fastcompany.com/1747437/twitter-tv-hashtag-tips-twitters-own-expert|title = Twitter TV Hashtag Tips From Twitter's Own Expert|author = Gregory Ferenstein|date = April 15, 2011|publisher = Fast Company}}</ref> Some have speculated that hashtags might take the place of (or co-exist with) the [[Nielsen ratings|Nielsen television ratings system]].<ref>{{cite web|url=http://www.ibtimes.com/twitter-chatter-correlates-tv-ratings-good-or-bad-news-nielsen-1144311 |title=Twitter Chatter Correlates With TV Ratings, But Is That Good Or Bad News For Nielsen? |publisher=Ibtimes.com |date=2013-03-22 |accessdate=2013-09-19}}</ref>

The increased usage of hashtags as brand promotion devices has been compared to the promotion of branded "[[Index term|keywords]]" by [[AOL]] in the late 1990s and early 2000s, as such keywords were also promoted at the end of commercials and series episodes.<ref>{{cite web|url = http://techcrunch.com/2012/06/10/twitter-hashtag-pages-aol-keywords/|title = Twitters Hashtag Pages Could Be The New AOL Keywords  But Better|author = Ryan Lawler|date = June 10, 2012|publisher = Techcrunch}}</ref>

===Purchasing===

Since February 2013 there is a collaboration between the social networking site Twitter and [[American Express]] that makes it possible to buy discounted goods online by tweeting a special hashtag.<ref>{{cite news | first = Kelly | last = Heather | title = Twitter and Amex let you pay with a hashtag | date = 12 February 2013 | url = http://edition.cnn.com/2013/02/11/tech/social-media/twitter-hashtag-purchases/| work = CNN | accessdate = 2013-11-25}}</ref> American Express members can sync their card with Twitter and use the offers by tweeting and look for a response in a tweet with the confirmation from American Express.<ref>{{cite web|url=https://sync.americanexpress.com/Twitter/Index |title=Sync with Twitter|publisher=Amex Sync |accessdate=2013-11-25}}</ref>

===Event promotion===

[[File:Occupy for Rights.JPG|thumb|[[Stencil graffiti]] promoting the hashtag #OccupyForRights]]
Organized real-world events have also made use of hashtags and ad hoc lists for discussion and promotion among participants. Hashtags are used as beacons by event participants in order to find each other on both Twitter and, in many cases, in real life during events.

Companies and advocacy organizations have taken advantage of hashtag-based discussions for promotion of their products, services or campaigns.

Political protests and campaigns in the early 2010s, such as [[Occupy Wall Street|#OccupyWallStreet]] and [[2011 Libyan civil war|#LibyaFeb17]], have been organized around hashtags or have made extensive usage of hashtags for the promotion of discussion.

===Consumer complaints===
Hashtags are often used by consumers on social media platforms in order to complain about the customer service experience with large companies.  The term "bashtag" has been created to describe situations in which a corporate social media hashtag is used to criticise the company or to tell others about poor customer service. For example, in January 2012, [[McDonald's]] created the #McDStories hashtag so customers could share positive experiences about the restaurant chain. The marketing effort was cancelled after just two hours when McDonald's received numerous complaint tweets rather than the positive stories they were expecting.<ref>{{cite news | first = Alexis | last = Akwagyiram | title = Are Twitter and Facebook changing the way we complain? | date = 17 May 2012 | url = http://www.bbc.co.uk/news/uk-18081651 | work = BBC News | accessdate = 2012-06-12}}</ref>

===Sentiment analysis===
The use of hashtags also reveals things about the sentiment an author attaches to a statement. This can range from the obvious, where a hashtag directly describes the state of mind, to the less obvious. For example, words in hashtags are the strongest predictor of whether or not a statement is [[sarcasm|sarcastic]]<ref>{{cite journal|last=Maynard|title=Who cares about sarcastic tweets? Investigating the impact of sarcasm on sentiment analysis|journal=Proceedings of the Conference on Language Resources and Evaluation|year=2014}}</ref>a difficult [[Artificial Intelligence|AI]] problem.{{citation needed|date=September 2014}}

==In popular culture==
During the [[2011 Canadian leaders debates|April 2011 Canadian party leader debate]], then-leader of the [[New Democratic Party of Canada|New Democratic Party]] [[Jack Layton]] referred to [[Conservative Party of Canada|Conservative]] Prime Minister [[Stephen Harper]]'s crime policies as "a hashtag fail" (presumably "#fail").<ref>{{cite news|url = http://www.theglobeandmail.com/news/politics/jack-laytons-debatable-hashtag-fail/article576224/|title = Jack Layton's debatable 'hashtag' #fail|author = Anna Mehler Paperny|publisher = The Globe and Mail|date = Apr 13, 2011 <!-- , 6:00 AM EDT --> }}</ref><ref>{{cite news|url = http://www.cbc.ca/news/politics/canadavotes2011/story/2011/04/13/cv-debate-twitter.html|title = Canadians atwitter throughout debate|date = Apr 13, 2011<!-- 3:25 PM-->|publisher = CBC News}}</ref>

The term "hashtag [[Hip hop music|rap]]", coined by [[Kanye West]],<ref>{{cite web |url = http://blogs.villagevoice.com/music/2010/11/the_ten_best_qu.php|title = The Ten Best Quotes From Kanye West's Epic Hot 97 Interview With Funkmaster Flex|author = Zach Baron|publisher = The Village Voice|date = November 3, 2010}}</ref> was developed in the 2010s to describe a style of rapping which, according to Rizoh of ''[[Houston Press]]'', uses "three main ingredients: a metaphor, a pause, and a one-word [[punch line]], often placed at the end of a rhyme".<ref>{{cite web|url = http://blogs.houstonpress.com/rocks/2011/07/a_brief_history_of_hashtag_rap.php|title = A Brief History Of Hashtag Rap|author = Rizoh|publisher = Houston Press|date = Jul 7, 2011 <!-- at 9:00 AM --> }}</ref> Rappers [[Nicki Minaj]], [[Big Sean]], [[Drake (rapper)|Drake]] and [[Lil Wayne]] are credited with the popularization of hashtag rap, while the style has been criticized by [[Ludacris]], [[The Lonely Island]]<ref>{{cite web|url = http://www.tucsonweekly.com/TheRange/archives/2013/05/22/the-lonely-island-puts-hashtag-rap-in-its-place-looking-at-you-drake|title = The Lonely Island Puts Hashtag Rap In Its Place (Looking at You, Drake)|author = David Mendez|date = May 22, 2013 <!-- AT 11:43 AM --> |publisher = Tucson Weekly}}</ref> and various music writers.<ref>{{cite web|url = http://www.joplinglobe.com/enjoy/x1666506743/Jeremiah-Tucker-Hashtag-rap-is-2010s-lamest-trend|title = Jeremiah Tucker: Hashtag rap is 2010's lamest trend|author = Jeremiah Tucker|date = December 17, 2010|publisher = Joplin Globe}}</ref>

On September 13, 2013, a hashtag, #TwitterIPO, appeared in the headline of a ''[[The New York Times|New York Times]]'' front page article regarding Twitter's [[initial public offering]].<ref>
{{cite web 
| title = Twitter / nickbilton: My first byline on A1 of the ... 
| url = https://twitter.com/nickbilton/status/378534272962793472/photo/1 
| accessdate = 2013-09-14 
 }}
</ref>

"Hashtag [[Heel (professional wrestling)|heel]]" is a moniker used by [[WWE]] wrestler [[Dolph Ziggler]].

[[Bird's Eye]] foods released in 2014 a shaped [[mashed potato]] food that included forms of @-symbols and hashtags, called "Mashtags".<ref>{{cite web|title=Birds Eye launches Mashtags - social media potato shapes|url=http://www.thegrocer.co.uk/fmcg/birds-eye-launches-mashtags-potato-shapes/354514.article|work=The Grocer}}</ref>

In May 2014, Twitter users began using the hashtag [[YesAllWomen|#YesAllWomen]] to raise awareness about personal experiences of [[sexism]] and [[violence against women]].<ref name="Nytimes">{{cite news |last=Medina| first=Jennifer | title = Campus Killings Set Off Anguished Conversation About the Treatment of Women | work = [[The New York Times]] | accessdate = September 23, 2014 | date = May 27, 2014 | url =http://www.nytimes.com/2014/05/27/us/campus-killings-set-off-anguished-conversation-about-the-treatment-of-women.html?ref=us&_r=0 }}</ref>

In September 2014, in response to the "[[blame the victim]]" public reactions to videotaped footage of [[NFL]] player [[Ray Rice]] assaulting his then-fiancee Janay Palmer in the elevator of an [[Atlantic City]] casino, Beverly Gooden shared on Twitter her own story of [[domestic abuse]], using the hashtag #WhyIStayed, and encouraged others to share theirs.<ref>{{cite news|work=Today|title=WhyIStayed: Woman behind Ray Rice-inspired hashtag writes to past self, other abuse victims|author=Gooden, Beverly|date=September 10, 2014| url= http://www.today.com/news/whyistayed-woman-behind-ray-rice-inspired-hashtag-writes-past-self-1D80139011}}</ref><ref>{{cite news|work=The Leonard Lopate Show|authors=Lopate, Leonard & Gooden, Beverly|title=#WhyIStayed|date=September 10, 2014}}</ref>

===Adaptations===
In 2010, Twitter introduced "hashflags" during the 2010 World Cup in South Africa.<ref>{{cite web|author=June 11, 2010 8:05 am |url=http://www.ryanseacrest.com/2010/06/11/twitter-supports-world-cup-fever-with-hashflags/ |title=Twitter Supports World Cup Fever with Hashflags |publisher=Ryanseacrest.com |date=2010-06-11 |accessdate=2014-08-25}}</ref> They reintroduced the feature on June 10, 2014, in time for the 2014 World Cup in Brazil.<ref>{{cite web|url=http://howto.digidefen.se/twitter/What-are-hashflags.php |title=What are Hashflags? |publisher=Howto.digidefen.se |date=2014-06-10 |accessdate=2014-08-25}}</ref><ref>{{cite web|author=Ben Woods |url=http://thenextweb.com/twitter/2014/06/10/twitter-brings-back-hashflags-just-time-world-cup-2014-kick/ |title=Twitter brings back hashflags just in time for World Cup 2014 kick-off |publisher=Thenextweb.com |date=2014-06-10 |accessdate=2014-08-25}}</ref> When a user tweets a hashtag consisting of the three letter country code of any of the 32 countries represented in the tournament, Twitter automatically embeds a flag emoticon for that country.

In July 2012, Twitter adapted the hashtag style to make company [[ticker symbol]]s preceded by the [[dollar sign]] clickable (as in [[Apple, Inc.|$AAPL]]), a method that Twitter dubbed the "cashtag".<ref>{{cite web|last=Kim |first=Erin |url=http://money.cnn.com/2012/07/31/technology/twitter-cashtag/ |title=Twitter unveils 'cashtags' to track stock symbols - Jul. 31, 2012 |publisher=Money.cnn.com |date=2012-07-31 |accessdate=2013-11-12}}</ref><ref>{{cite web|author= |url=http://www.theverge.com/2012/7/30/3205284/twitter-stock-ticker-cashtag-links-official |title=Twitter makes stock symbol $ 'cashtag' links official, following # and @ |publisher=The Verge |date=2012-07-30 |accessdate=2013-11-12}}</ref> This is intended to allow users to search posts discussing companies and their stocks.

In August 2012, British journalist Tom Meltzer reported in ''[[The Guardian]]'' about a new [[hand gesture]] that mimicked the hashtag, sometimes called the "finger hashtag", in which both hands form a [[Peace sign#The V sign|peace sign]], and then the fingers are crossed to form the symbol of a hashtag.<ref>{{cite web |url=http://www.theguardian.com/technology/shortcuts/2012/aug/01/how-to-say-hashtag-fingers |title=How to say 'hashtag' with your fingers |work=[[The Guardian]] |author=Tom Meltzer |date=1 August 2012 |accessdate=March 20, 2014}}</ref> The emerging gesture was reported about in ''[[Wired (magazine)|Wired]]'' by [[Nimrod Kamer]],<ref>{{cite web |url=http://www.wired.co.uk/news/archive/2013-03/06/hashtags |title=Finger-Hashtags |work=[[Wired (magazine)|Wired]] |author=[[Nimrod Kamer]] |date=March 2013 |accessdate=March 20, 2014}}</ref> and during 2013 it was seen on TV used by [[Jimmy Fallon]], and on ''[[The Colbert Report]]'' among other places.<ref>{{cite web |url=http://www.dailydot.com/lol/finger-hashtag-jimmy-fallon-twitter/ |title=I invented finger hashtagsand I regret nothing |work=[[The Daily Dot]] |author=[[Nimrod Kamer]] |date=February 26, 2014 |accessdate=March 20, 2014}}</ref>

==References==
{{Reflist|colwidth=30em}}
{{commons category|Hashtags}}

{{Microblogging}}
{{Online social networking}}
{{Web syndication}}

[[Category:Hashtags| ]]
[[Category:Collective intelligence]]
[[Category:Computer jargon]]
[[Category:Information retrieval]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Reference]]
[[Category:Web 2.0]]
[[Category:Social media]]
[[Category:2010s slang]]
>>EOP<<
161<|###|>Web search engine
{{Redirect|Search engine}}
{{selfref|For a tutorial on using search engines for researching Wikipedia articles, see [[Wikipedia:Search engine test]].}}
[[File:Internet Key Layers.png|thumb|400px|right|Finding information on the World Wide Web had been a difficult and frustrating task, but became much more usable with breakthroughs in search engine technology in the late 1990s.]]
A '''web search engine''' is a software system that is designed to search for information on the [[World Wide Web]].  The search results are generally presented in a line of results often referred to as [[search engine results pages]] (SERPs). The information may be a mix of [[web page]]s, images, and other types of files. Some search engines also [[data mining|mine data]] available in [[database]]s or [[web directory|open directories]].  Unlike [[web directories]], which are maintained only by human editors, search engines also maintain [[real-time computing|real-time]] information by running an [[algorithm]] on a [[web crawler]].

== History ==
{{further|Timeline of web search engines}}
<!-- Keep this list limited to notable engines (i.e. those that already have Wikipedia articles) to avoid link spam -->
{| class="bordered infobox"
|-
! colspan="3" | Timeline ([[List of search engines|full list]]) <!--Note:  "Launch" refers only to web availability of original crawl-based web search engine results.-->
|-
!Year
!Engine
!Current status
|-
| rowspan="4" |1993
||[[W3Catalog]]
|{{Site inactive}}
|-
||[[Aliweb]]
|{{Site inactive}}
|-
||[[JumpStation]]
|{{Site inactive}}
|-
||[[World-Wide Web Worm|WWW Worm]]
|{{Site inactive}}
|-
| rowspan="4" |1994
||[[WebCrawler]]
|{{Site active}}, Aggregator
|-
||[[Go.com]]
|{{Site active}}, Yahoo Search
|-
||[[Lycos]]
|{{Site active}}
|-
||[[Infoseek]]
|{{Site inactive}}
|-
| rowspan="6" |1995
||[[AltaVista]]
|{{Site inactive}}, redirected to Yahoo!
|-
|[[Daum Communications|Daum]]
|{{Site active}}
|-
||[[Magellan (search engine)|Magellan]]
|{{Site inactive}}
|-
||[[Excite]]
|{{Site active}}
|-
||[[SAPO (company)|SAPO]]
|{{Site active}}
|-
||[[Yahoo!]]
|{{Site active}}, Launched as a directory
|-
| rowspan="4" |1996
||[[Dogpile]]
|{{Site active}}, Aggregator
|-
||[[Inktomi (company)|Inktomi]]
|{{Site inactive}}, acquired by Yahoo!
|-
||[[HotBot]]
|{{Site active}}  (lycos.com)
|-
||[[Ask.com|Ask Jeeves]]
|{{Site active}}  (rebranded ask.com)
|-
| rowspan="2" |1997
||[[Northern Light Group|Northern Light]]
|{{Site inactive}}
|-
||[[Yandex]]
|{{Site active}}
|-
| rowspan="4" |1998
||[[Google Search|Google]]
|{{Site active}}
|-
||[[Ixquick]]
|{{Site active}}  also as Startpage
|-
||[[MSN Search]]
|{{Site active}}  as Bing
|-
||[[empas]]
|{{Site inactive}}  (merged with NATE)
|-
| rowspan="5" |1999
||[[AlltheWeb]]
|{{Site inactive}}  (URL redirected to Yahoo!)
|-
||[[GenieKnows]]
|{{Site active}}, rebranded Yellowee.com
|-
||[[Naver]]
|{{Site active}}
|-
||[[Teoma]]
|{{Site inactive}}, redirects to Ask.com
|-
||[[Vivisimo]]
|{{Site inactive}}
|-
| rowspan="3" |2000
||[[Baidu]]
|{{Site active}}
|-
||[[Exalead]]
|{{Site active}}
|-
||[[Gigablast]]
|{{Site active}}
|-
| rowspan="2" |2003
||[[Info.com]]
|{{Site active}}
|-
||[[Scroogle]]
|{{Site inactive}}
|-
| rowspan="3" |2004
||[[Yahoo! Search]]
|{{Site active}}, Launched own web search<br />(see Yahoo! Directory, 1995)
|-
||[[A9.com]]
|{{Site inactive}}
|-
||[[Sogou.com|Sogou]]
|{{Site active}}
|-
| rowspan="3" |2005
||[[AOL Search]]
|{{Site active}}
|-
||[[GoodSearch]]
|{{Site active}}
|-
||[[SearchMe]]
|{{Site inactive}}
|-
| rowspan="6" |2006
||[[Soso (search engine)]]
|{{Site active}}
|-
||[[Quaero]]
|{{Site inactive}}
|-
||[[Ask.com]]
|{{Site active}}
|-
||[[Live Search]]
|{{Site active}} as Bing, Launched as<br />rebranded MSN Search
|-
||[[ChaCha (search engine)|ChaCha]]
|{{Site active}}
|-
||[[Guruji.com]]
|{{Site inactive}}
|-
| rowspan="4" |2007
||[[wikiseek]]
|{{Site inactive}}
|-
||[[Sproose]]
|{{Site inactive}}
|-
||[[Wikia Search]]
|{{Site inactive}}
|-
||[[Blackle.com]]
|{{Site active}}, Google Search
|-
| rowspan="7" |2008
||[[Powerset (company)|Powerset]]
|{{Site inactive}} (redirects to Bing)
|-
||[[Picollator]]
|{{Site inactive}}
|-
||[[Viewzi]]
|{{Site inactive}}
|-
||[[Boogami]]
|{{Site inactive}}
|-
||[[LeapFish]]
|{{Site inactive}}
|-
||[[Forestle]]
|{{Site inactive}} (redirects to Ecosia)
|-
||[[DuckDuckGo]]
|{{Site active}}
|-
| rowspan="5" |2009
||[[Bing]]
|{{Site active}}, Launched as<br />rebranded Live Search
|-
||[[Yebol]]
|{{Site inactive}}
|-
||[[Mugurdy]]
|{{Site inactive}}  due to a lack of funding
|-
||[[Goby Inc.|Scout (Goby)]]
|{{Site active}}
|-
||[[Nate (web portal)|NATE]]
|{{Site active}}
|-
| rowspan="3" |2010
||[[Blekko]]
|{{Site active}}
|-
||[[Cuil]]
|{{Site inactive}}
|-
||[[Yandex]]
|{{Site active}}, Launched global<br />(English) search
|-
||2011
||[[YaCy]]
|{{Site active}}, [[Peer-to-peer|P2P]] web search engine
|-
| rowspan="1" |2012
||[[Volunia]]
|{{Site inactive}}
|-
| rowspan="1" |2013
||[[Halalgoogling]]
|{{Site active}}, Islamic / Halal<br />filter Search
|}

During early development of the web, there was a list of [[webserver]]s edited by [[Tim Berners-Lee]] and hosted on the [[CERN]] webserver. One historical snapshot of the list in 1992 remains,<ref>{{cite web|url=http://www.w3.org/History/19921103-hypertext/hypertext/DataSources/WWW/Servers.html |title=World-Wide Web Servers |publisher=W3.org |accessdate=2012-05-14}}</ref> but as more and more webservers went online the central list could no longer keep up. On the [[National Center for Supercomputing Applications|NCSA]]  site, new servers were announced under the title "What's New!"<ref>{{cite web|url=http://home.mcom.com/home/whatsnew/whats_new_0294.html |title=What's New! February 1994 |publisher=Home.mcom.com |accessdate=2012-05-14}}</ref>

The first tool used for searching on the [[Internet]] was [[Archie search engine|Archie]].<ref name=LeidenUnivSE>
     "Internet History - Search Engines" (from [[Search Engine Watch]]),
     Universiteit Leiden, Netherlands, September 2001, web:
     [http://www.internethistory.leidenuniv.nl/index.php3?c=7 LeidenU-Archie].
</ref>
The name stands for "archive" without the "v".  It was created in 1990 by [[Alan Emtage]], Bill Heelan and J. Peter Deutsch, computer science students at [[McGill University]]  in [[Montreal]]. The program downloaded the directory listings of all the files located on public anonymous FTP ([[File Transfer Protocol]]) sites, creating a searchable database of file names; however, Archie did not index the contents of these sites since the amount of data was so limited it could be readily searched manually.

The rise of [[Gopher (protocol)|Gopher]] (created in 1991 by [[Mark McCahill]]  at the [[University of Minnesota]]) led to two new search programs, [[Veronica (computer)|Veronica]]  and [[Jughead (computer)|Jughead]]. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (''V''ery ''E''asy ''R''odent-''O''riented ''N''et-wide ''I''ndex to ''C''omputerized ''A''rchives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (''J''onzy's ''U''niversal ''G''opher ''H''ierarchy ''E''xcavation ''A''nd ''D''isplay) was a tool for obtaining menu information from specific Gopher servers.  While the name of the search engine "Archie" was not a reference to the [[Archie Comics|Archie comic book]] series, "[[Veronica Lodge|Veronica]]" and "[[Jughead Jones|Jughead]]" are characters in the series, thus referencing their predecessor.

In the summer of 1993, no search engine existed for the web, though numerous specialized catalogues were maintained by hand. [[Oscar Nierstrasz]] at the [[University of Geneva]] wrote a series of [[Perl]] scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for [[W3Catalog]], the web's first primitive search engine, released on September 2, 1993.<ref name="Announcement html">{{cite web |url= http://groups.google.com/group/comp.infosystems.www/browse_thread/thread/2176526a36dc8bd3/2718fd17812937ac?hl=en&lnk=gst&q=Oscar+Nierstrasz#2718fd17812937ac|title=Searchable Catalog of WWW Resources (experimental)|author=[[Oscar Nierstrasz]]|date=2 September 1993}}</ref>

In June 1993, Matthew Gray, then at [[Massachusetts Institute of Technology|MIT]], produced what was probably the first [[web robot]], the [[Perl]]-based [[World Wide Web Wanderer]], and used it to generate an index called 'Wandex'.  The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995.  The web's second search engine [[Aliweb]] appeared in November 1993.  Aliweb did not use a [[web robot]], but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format.

[[JumpStation]] (created in December 1993<ref>{{cite web|url=http://archive.ncsa.uiuc.edu/SDG/Software/Mosaic/Docs/old-whats-new/whats-new-1293.html |archiveurl=//web.archive.org/web/20010620073530/http://archive.ncsa.uiuc.edu/SDG/Software/Mosaic/Docs/old-whats-new/whats-new-1293.html |archivedate=2001-06-20 |title=Archive of NCSA what's new in December 1993 page |publisher=Web.archive.org |date=2001-06-20 |accessdate=2012-05-14}}</ref> by [[Jonathon Fletcher]]) used a [[web crawler|web robot]] to find web pages and to build its index, and used a [[web form]] as the interface to its query program.  It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below.  Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered.

One of the first "all text" crawler-based search engines was [[WebCrawler]], which came out in 1994.  Unlike its predecessors, it allowed users to search for any word in any webpage, which has become the standard for all major search engines since. It was also the first one widely known by the public.  Also in 1994, [[Lycos]] (which started at [[Carnegie Mellon University]]) was launched and became a major commercial endeavor.

Soon after, many search engines appeared and vied for popularity. These included [[Magellan (search engine)|Magellan]], [[Excite]], [[Infoseek]], [[Inktomi (company)|Inktomi]], [[Northern Light Group|Northern Light]], and [[AltaVista]]. [[Yahoo!]] was among the most popular ways for people to find web pages of interest, but its search function operated on its [[web directory]], rather than its full-text copies of web pages. Information seekers could also browse the directory instead of doing a keyword-based search.

Google adopted the idea of selling search terms in 1998, from a small search engine company named [[goto.com]]. This move had a significant effect on the SE business, which went from struggling to one of the most profitable businesses in the internet.<ref>http://www.udacity.com/view#Course/cs101/CourseRev/apr2012/Unit/616074/Nugget/671097</ref>

In 1996, [[Netscape]] was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page.  The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.<ref>{{cite web|title=Yahoo! And Netscape Ink International Distribution Deal|url=http://files.shareholder.com/downloads/YHOO/701084386x0x27155/9a3b5ed8-9e84-4cba-a1e5-77a3dc606566/YHOO_News_1997_7_8_General.pdf|postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref><ref>{{Cite journal |date=1 April 1996|title=Browser Deals Push Netscape Stock Up 7.8% |publisher=Los Angeles Times |url=http://articles.latimes.com/1996-04-01/business/fi-53780_1_netscape-home |postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref>

Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.<ref>{{cite journal |last=Gandal |first=Neil |authorlink= |year=2001 |title=The dynamics of competition in the internet search engine market |journal=International Journal of Industrial Organization |volume=19 |issue=7 |pages=11031117 |doi=10.1016/S0167-7187(01)00065-0  |url= |accessdate=|quote= }}</ref> Several companies entered the market spectacularly, receiving record gains during their [[initial public offering]]s. Some have taken down their public search engine, and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the [[dot-com bubble]], a speculation-driven market boom that peaked in 1999 and ended in 2001.

Around 2000, [[Google Search|Google's search engine]] rose to prominence.<ref>{{cite web|url=http://www.google.com/about/company/history/ |title=Our History in depth |publisher=W3.org |accessdate=2012-10-31}}</ref>  The company achieved better results for many searches with an innovation called [[PageRank]], as was explained in the paper ''Anatomy of a Search Engine'' written by [[Sergey Brin]] and [[Larry Page]], the later founders of Google.<ref>{{cite web|url=http://ilpubs.stanford.edu:8090/361/1/1998-8.pdf|title=The Anatomy of a Large-Scale Hypertextual Web Search Engine|last1=Brin|first1=Sergey|last2=Page|first2=Larry}}</ref> This [[iterative algorithm]] ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a [[web portal]]. In fact, Google search engine became so popular that spoof engines emerged such as [[Mystery Seeker]].

By 2000, [[Yahoo!]] was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and [[Overture]] (which owned [[AlltheWeb]] and AltaVista) in 2003.  Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions.

[[Microsoft]] first launched MSN Search in the fall of 1998 using search results from Inktomi.  In early 1999 the site began to display listings from [[Looksmart]], blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista were instead.  In 2004, [[Microsoft]] began a transition to its own search technology, powered by its own [[web crawler]] (called [[msnbot]]).

Microsoft's rebranded search engine, [[Bing]], was launched on June 1, 2009.  On July 29, 2009, Yahoo! and Microsoft finalized a deal in which [[Yahoo! Search]] would be powered by Microsoft Bing technology.

== How web search engines work ==
{{Original research|section|date=October 2013
}}
{{Refimprove|date=July 2013}}
A search engine operates in the following order:
# [[Web crawling]]
# [[Index (search engine)|Indexing]]
# [[Web search query|Searching]]<ref name=Jawadekar2011>{{citation |year=2011 |author=Jawadekar, Waman S |title=Knowledge Management: Text & Cases |url=http://books.google.com/books?id=XmGx4J9daUMC&printsec=frontcover&dq=knowledge+management:+text&hl=en&sa=X&ei=ou6uUP-cNqWTiAe2oICoAw&sqi=2&ved=0CDIQ6AEwAA |chapter=8. Knowledge Management: Tools and Technology |chapter-url=http://books.google.com/books?id=XmGx4J9daUMC&pg=PA278&dq=%22search+engine+operates%22&hl=en&sa=X&ei=a-muUJ6UC4aeiAfI24GYAw&sqi=2&ved=0CDgQ6AEwBA |page=278 |place=New Delhi |publisher=Tata McGraw-Hill Education Private Ltd |isbn=978-0-07-07-0086-4 |accessdate=November 23, 2012 }}</ref>

Web search engines work by storing information about many web pages, which they retrieve from the [[HTML]] markup of the pages. These pages are retrieved by a [[Web crawler]] (sometimes also known as a spider)  an automated Web crawler which follows every link on the site. The site owner can exclude specific pages by using [[robots.txt]].

The search engine then analyzes the contents of each page to determine how it should be [[Search engine indexing|indexed]] (for example, words can be extracted from the titles, page content, headings, or special fields called [[meta tags]]). Data about web pages are stored in an index database for use in later queries. A query from a user can be a single word. The index helps find information relating to the query as quickly as possible.<ref name=Jawadekar2011/> Some search engines, such as [[Google]], store all or part of the source page (referred to as a [[web cache|cache]]) as well as information about the web pages, whereas others, such as [[AltaVista]], store every word of every page they find.{{Citation needed|date=November 2012}} This cached page always holds the actual search text since it is the one that was actually indexed, so it can be very useful when the content of the current page has been updated and the search terms are no longer in it.<ref name=Jawadekar2011/> This problem might be considered a mild form of [[linkrot]], and Google's handling of it increases [[usability]] by satisfying [[user expectations]] that the search terms will be on the returned webpage. This satisfies the [[principle of least astonishment]], since the user normally expects that the search terms will be on the returned pages. Increased search relevance makes these cached pages very useful as they may contain data that may no longer be available elsewhere.{{Citation needed|date=November 2012}}
[[File:WebCrawlerArchitecture.svg|thumb|High-level architecture of a standard Web crawler]]
When a user enters a [[web search query|query]] into a search engine (typically by using [[Keyword (Internet search)|keywords]]), the engine examines its [[inverted index|index]] and provides a listing of best-matching web pages according to its criteria, usually with a short summary containing the document's title and sometimes parts of the text. The index is built from the information stored with the data and the method by which the information is indexed.<ref name=Jawadekar2011/> From 2007 the Google.com search engine has allowed one to search by date by clicking "Show search tools" in the leftmost column of the initial search results page, and then selecting the desired date range.{{Citation needed|date=November 2012}} Most search engines support the use of the [[boolean operators]] AND, OR and NOT to further specify the [[web search query|search query]]. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered.  Some search engines provide an advanced feature called [[Proximity search (text)|proximity search]], which allows users to define the distance between keywords.<ref name=Jawadekar2011/>  There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases you search for. As well, natural language queries allow the user to type a question in the same form one would ask it to a human. A site like this would be ask.com.{{Citation needed|date=November 2012}}

The usefulness of a search engine depends on the [[relevance (information retrieval)|relevance]] of the '''result set''' it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to [[rank order|rank]] the results to provide the "best" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.<ref name=Jawadekar2011/> The methods also change over time as Internet usage changes and new techniques evolve.  There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an "[[inverted index]]" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work.

Most Web search engines are commercial ventures supported by [[advertising]] revenue and thus some of them allow advertisers to [[paid inclusion|have their listings ranked higher]] in search results for a fee. Search engines that do not accept money for their search results make money by running [[contextual advertising|search related ads]] alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.<ref>{{cite web|title=FAQ|url=http://www.rankstar.de/hilfe.html|publisher=RankStar|accessdate=19 June 2013}}</ref>

== Market share ==

[[Google Search|Google]] is the world's most popular search engine, with a marketshare of 66.44 percent as of December, 2014.<ref name="NMS">{{cite web|url=http://marketshare.hitslink.com/search-engine-market-share.aspx?qprid=4&qpcustomd=0&qpcustom=|title=Desktop Search Engine Market Share|publisher=NetMarketShare|accessdate=2014-06-04}}</ref> [[Baidu]] comes in at second place.<ref name="NMS" />

The world's most popular search engines are:<ref>{{cite web|title=FAQ|url=https://www.netmarketshare.com/search-engine-market-share.aspx?qprid=4&qpcustomd=0|publisher=NetMarketShare|accessdate=23 November 2014}}</ref>

{| class="wikitable sortable"
! Search engine !! colspan="2" |Market share in December 2014
|-
| [[Google Search|Google]] || style="text-align:right;"|{{bartable|66.44|%|2}}
|-
| [[Baidu]]  || style="text-align:right;"|{{bartable| 11.15|%|2}}
|-
| [[Bing]]   || style="text-align:right;"|{{bartable| 10.29|%|2}}
|-
| [[Yahoo!]]  || style="text-align:right;"|{{bartable| 9.31|%|2}}
|-
| [[AOL]]    || style="text-align:right;"|{{bartable| 0.53|%|2}}
|-
| [[Ask.com|Ask]]    || style="text-align:right;"|{{bartable| 0.21|%|2}}
|-
| [[Lycos]]   || style="text-align:right;"|{{bartable| 0.01|%|2}}
|}

{| class="wikitable sortable"
! Search engine !! colspan="2" |Market share in October 2014
|-
| [[Google Search|Google]] || style="text-align:right;"|{{bartable|58.01|%|2}}
|-
| [[Baidu]]  || style="text-align:right;"|{{bartable| 29.06|%|2}}
|-
| [[Bing]]   || style="text-align:right;"|{{bartable| 8.01|%|2}}
|-
| [[Yahoo!]]  || style="text-align:right;"|{{bartable| 4.01|%|2}}
|-
| [[AOL]]    || style="text-align:right;"|{{bartable| 0.21|%|2}}
|-
| [[Ask.com|Ask]]    || style="text-align:right;"|{{bartable| 0.10|%|2}}
|-
| [[Excite]]   || style="text-align:right;"|{{bartable| 0.00|%|2}}
|}

{| class="wikitable sortable"
! Search engine !! colspan="2" |Market share in July 2014<ref name="NMS" />
|-
| [[Google Search|Google]] || style="text-align:right;"|{{bartable|68.69|%|2}}
|-
| [[Baidu]]  || style="text-align:right;"|{{bartable| 17.17|%|2}}
|-
| [[Yahoo!]]  || style="text-align:right;"|{{bartable| 6.74|%|2}}
|-
| [[Bing]]   || style="text-align:right;"|{{bartable| 6.22|%|2}}
|-
| [[Excite]]   || style="text-align:right;"|{{bartable| 0.22|%|2}}
|-
| [[Ask.com|Ask]]    || style="text-align:right;"|{{bartable| 0.13|%|2}}
|-
| [[AOL]]    || style="text-align:right;"|{{bartable| 0.13|%|2}}
|}

=== East Asia and Russia ===

East Asian countries and Russia constitute a few places where Google is not the most popular search engine.

[[Yandex]] commands a marketshare of 61.9 per cent in Russia, compared to Google's 28.3 percent.<ref>{{cite web|url=http://www.liveinternet.ru/stat/ru/searches.html?slice=ru;period=week|title=Live Internet - Site Statistics|publisher=Live Internet|accessdate=2014-06-04}}</ref> In China, Baidu is the most popular search engine.<ref>{{cite news|url=http://www.theguardian.com/world/2014/jun/03/chinese-technology-companies-huawei-dominate-world|title=The Chinese technology companies poised to dominate the world|publisher=The Guardian|author=Arthur, Charles|date=2014-06-03|accessdate=2014-06-04}}</ref>  South Korea's homegrown search portal, [[Naver]], is used for 70 per cent online searches in the country.<ref>{{cite web|url=http://blogs.wsj.com/korearealtime/2014/05/21/how-naver-hurts-companies-productivity/|title=How Naver Hurts Companies Productivity|publisher=The Wall Street Journal|date=2014-05-21|accessdate=2014-06-04}}</ref> [[Yahoo! Japan]] and [[Yahoo! Search|Yahoo! Taiwan]] are the most popular avenues for internet search in Japan and Taiwan, respectively.<ref>{{cite web|url=http://geography.oii.ox.ac.uk/?page=age-of-internet-empires|title=Age of Internet Empires|publisher=Oxford Internet Institute|accessdate=2014-06-04}}</ref>

== Search engine bias ==
Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide.<ref>Segev, El (2010). Google and the Digital Divide: The Biases of Online Knowledge, Oxford: Chandos Publishing.</ref><ref name=vaughan-thelwall>{{cite journal|last=Vaughan|first=Liwen|author2=Mike Thelwall |title=Search engine coverage bias: evidence and possible causes|journal=Information Processing & Management|year=2004|volume=40|issue=4|pages=693707|doi=10.1016/S0306-4573(03)00063-3}}</ref> These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its [[organic search]] results), and political processes (e.g., the removal of search results to comply with local laws).<ref>Berkman Center for Internet & Society (2002), [http://cyber.law.harvard.edu/filtering/china/google-replacements/ Replacement of Google with Alternative Search Systems in China: Documentation and Screen Shots], Harvard Law School.</ref> For example, Google will not surface certain Neo-Nazi websites in France and Germany, where Holocaust denial is illegal.

Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more "popular" results.<ref>{{cite journal|last=Introna|first=Lucas|author2=[[Helen Nissenbaum]] |title=Shaping the Web: Why the Politics of Search Engines Matters|journal=The Information Society: An International Journal|year=2000|volume=16|issue=3|doi=10.1080/01972240050133634}}</ref> Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.<ref name=vaughan-thelwall />

[[Google Bombing]] is one example of an attempt to manipulate search results for political, social or commercial reasons.

== Customized results and filter bubbles ==

Many search engines such as Google and Bing provide customized results based on the user's activity history. This leads to an effect that has been called a [[filter bubble]]. The term describes a phenomenon in which websites use [[algorithm]]s to selectively guess what information a user would like to see, based on information about the user (such as location, past click behaviour and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint, effectively isolating the user in a bubble that tends to exclude contrary information. Prime examples are Google's personalized search results and [[Facebook]]'s personalized news stream. According to [[Eli Pariser]], who coined the term, users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Pariser related an example in which one user searched Google for "BP" and got investment news about [[British Petroleum]] while another searcher got information about the [[Deepwater Horizon oil spill]] and that the two search results pages were "strikingly different".<ref name=twsT43>{{cite news
 |first1= Lynn | last1= Parramore
 |title= The Filter Bubble
 |work= The Atlantic
 |quote= Since Dec. 4, 2009, Google has been personalized for everyone. So when I had two friends this spring Google "BP," one of them got a set of links that was about investment opportunities in BP. The other one got information about the oil spill....
 |date=  10 October 2010
 |url= http://www.theatlantic.com/daily-dish/archive/2010/10/the-filter-bubble/181427/
 |accessdate= 2011-04-20
}}</ref><ref name=twsO11>{{cite news
 |first= Jacob | last= Weisberg
 |title= Bubble Trouble: Is Web personalization turning us into solipsistic twits?
 |work= Slate
 |date= 10 June 2011
 |url= http://www.slate.com/id/2296633/
 |accessdate= 2011-08-15
}}</ref><ref name=twsO14>{{cite news
 |first= Doug | last= Gross
 |title= What the Internet is hiding from you
 |publisher= ''CNN''
 |quote= I had friends Google BP when the oil spill was happening. These are two women who were quite similar in a lot of ways. One got a lot of results about the environmental consequences of what was happening and the spill. The other one just got investment information and nothing about the spill at all.
 |date= May 19, 2011
 |url= http://edition.cnn.com/2011/TECH/web/05/19/online.privacy.pariser/
 |accessdate= 2011-08-15
}}</ref> The bubble effect may have negative implications for civic discourse, according to Pariser.<ref>{{cite journal| last1= Zhang | first1= Yuan Cao | first2= Diarmuid O |last2= Seaghdha | first3= Daniele | last3= Quercia | first4 =Tamas | last4 = Jambor |title=Auralist: Introducing Serendipity into Music Recommendation|journal=ACM WSDM |date=February 2012|url=http://www-typo3.cs.ucl.ac.uk/fileadmin/UCL-CS/research/Research_Notes/RN_11_21.pdf}}</ref>

Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or "bubbling" users.

== Faith-based search engines ==

The global growth of the Internet and popularity of electronic contents in the [[Arab]] and [[Muslim]] World during the last decade has encouraged faith adherents, notably in [[Middle East|the Middle East]] and [[Indian subcontinent|Asian sub-continent]], to "dream" of their own faith-based i.e. "[[Islamic]]" search engines or filtered search portals filters that would enable users to avoid accessing forbidden websites such as pornography and would only allow them to access sites that are compatible to the Islamic faith. Shortly before the Muslim only month of [[Ramadan]], [[Halalgoogling]] which collects results from other search engines like [[Google]] and [[Bing]] was introduced to the world July 2013 to presents the [[halal]] results to its users,<ref>{{cite web|url=http://news.msn.com/science-technology/new-islam-approved-search-engine-for-muslims |title=New Islam-approved search engine for Muslims |publisher=News.msn.com |date= |accessdate=2013-07-11}}</ref> nearly two years after ImHalal, another search engine initially (launched on September 2011) to serve Middle East Internet had to close its search service due to what its owner blamed on lack of funding.<ref>[http://blog.imhalal.com/ ImHalal - Islamic compliant search project launched September 2009 and shut down late 2011]</ref>

While lack of investment and slow pace in technologies in the Muslim World as the main consumers or targeted end users has hindered progress and thwarted success of serious Islamic search engine, the spectacular failure of heavily invested Muslim lifestyle web projects like [[Muxlim]], which received millions of dollars from investors like Rite Internet Ventures, has - according to ImHalal shutdown notice - made almost laughable the idea that the next [[Facebook]] or [[Google]] can only come from [[Middle East|the Middle East]] if you support your bright youth.<ref>[http://imhalal.com/ I'mHalal Blog]</ref> Yet Muslim internet experts have been determining for years what is or is not allowed according to [[Shariah|the "Law of Islam"]] and have been categorizing websites and such into being either "[[halal]]" or "[[haram]]". All the existing and past Islamic search engines are merely custom search indexed or monetized by web major search giants like [[Google]], [[Yahoo]] and [[Bing]] with only certain filtering systems applied to ensure that their users can't access Haram sites, which include such sites as nudity, gay, gambling or anything that is deemed to be anti-Islamic.<ref>[http://blog.imhalal.com/ I'mHalal Blog]</ref>

Another religiously-oriented search engine is Jewogle, which is the Jewish version of Google and yet another is SeekFind.org, which is a Christian website that includes filters preventing users from seeing anything on the internet that attacks or degrades their faith.<ref>[http://allchristiannews.com/halalgoogling-muslims-get-their-own-sin-free-google-should-christians-have-christian-google/ AllChristianNews]</ref>

== See also ==
*[[Most popular Internet search engines]]
* [[Comparison of web search engines]]
* [[List of search engines]]
* Answer engine ([[question answering]]) <!-- examples necessary here until article comprehensible to normal reader-->
** [[Quora]]
** [[True Knowledge]]
** [[Wolfram Alpha]]
* [[Google effect]]
* [[Internet Search Engines and Libraries]]
* [[Semantic Web]]
* [[Spell checker]]
* [[Web development tools]]

== References ==
{{Reflist|33em}}

== Further reading ==
* For a more detailed history of early search engines, see [http://searchenginewatch.com/showPage.html?page=3071951 Search Engine Birthdays] (from [[Search Engine Watch]]), Chris Sherman, September 2003.
* {{cite journal | quotes =| author =Steve Lawrence; C. Lee Giles | year =1999| title =Accessibility of information on the web | journal =[[Nature (journal)|Nature]] | volume =400 | issue =6740| doi =10.1038/21987 | pmid =10428673 | pages =1079 }}
* Bing Liu (2007), ''[http://www.cs.uic.edu/~liub/WebMiningBook.html Web Data Mining: Exploring Hyperlinks, Contents and Usage Data].'' Springer,ISBN 3-540-37881-2
* Bar-Ilan, J. (2004). The use of Web search engines in information science research. ARIST, 38, 231-288.
* {{cite book | first =Mark | last =Levene | year =2005 | title =An Introduction to Search Engines and Web Navigation | publisher =Pearson | location =| isbn =}}
* {{cite book | first =Randolph | last =Hock | year =2007 | title =The Extreme Searcher's Handbook}}ISBN 978-0-910965-76-7
* {{cite journal | quotes =| author =Javed Mostafa |date= February 2005 | title =Seeking Better Web Searches | journal =[[Scientific American]] | volume =| issue =| pages =| publisher =| pmid =| doi =| bibcode =| url =http://www.sciam.com/article.cfm?articleID=0006304A-37F4-11E8-B7F483414B7F0000 | language =}}<sup class="noprint Inline-Template"><span title="&nbsp;since September 2010" style="white-space: nowrap;">&#91;''&#93;</span></sup>
* {{cite journal |last=Ross |first=Nancy |authorlink=|author2=Wolfram, Dietmar  |year=2000 |title=End user searching on the Internet: An analysis of term pair topics submitted to the Excite search engine |journal=Journal of the American Society for Information Science |volume=51 |issue=10 |pages=949958 |doi=10.1002/1097-4571(2000)51:10<949::AID-ASI70>3.0.CO;2-5|url=|accessdate=|quote=}}
* {{cite journal |last=Xie |first=M. |authorlink=|year=1998 |title=Quality dimensions of Internet search engines |journal=Journal of Information Science |volume=24 |issue=5 |pages=365372 |doi=10.1177/016555159802400509 |url=|accessdate=|quote=|display-authors=1 |last2=Wang |first2=H. |last3=Goh |first3=T. N. }}
*{{cite book|title=Information Retrieval: Implementing and Evaluating Search Engines|url= http://www.ir.uwaterloo.ca/book/ | year=2010|publisher=MIT Press|author8=Stefan Buttcher, Charles L. A. Clarke, and Gordon V. Cormack}}

== External links ==
{{commons category|Internet search engines}}
* {{Dmoz|Computers/Internet/Searching/Search_Engines/|Search Engines}}

{{Internet search}}

{{DEFAULTSORT:Web Search Engine}}
[[Category:Internet search engines| ]]
[[Category:History of the Internet]]
[[Category:Information retrieval]]
[[Category:Internet terminology]]
>>EOP<<
167<|###|>Category:Search algorithms
{{Commons category|Search algorithms}}
{{Cat main|Search algorithms}}

[[Category:Algorithms]]
[[Category:Searching]]
>>EOP<<
173<|###|>Multimedia search
'''Multimedia search''' enables information [[Search engine technology|search]] using queries in multiple data types including text and other [[multimedia]] formats.
Multimedia search can be implemented through [[multimodal search]] interfaces, i.e., interfaces that allow to submit [[search queries]] not only as textual requests, but also through other media.
We can distinguish two methodologies in multimedia search:
*'''Metadata search''': the search is made on the layers of [[metadata]].
* '''[[Query by example]]''': The interaction consists in submitting a piece of information (e.g., a video, an image, or a piece of audio) at the purpose of finding similar multimedia items.


==Metadata search==

Search is made using the layers in metadata which contain information of the content of a multimedia file. Metadata search is easier, faster and effective because instead of working with complex material, such as an audio, a video or an image, it searches using text.

There are three processes which should be done in this method:
*'''[[Multimedia Information Retrieval#Feature Extraction Methods|Summarization of media content]]''' ([[feature extraction]]). The result of feature extraction is a description.
*'''[[ Multimedia Information Retrieval#Feature Extraction Methods |Filtering of media descriptions]]''' (for example, elimination of [[Redundancy (linguistics)|Redundancy]])
*'''[[ Multimedia Information Retrieval#Categorization Methods | Categorization of media descriptions ]]''' into classes.

==[[Query by Example]]==

In [[query by example]] the element used to search is a [[multimedia]] content (image, audio, video). In other words, the query is a media. Often its used [[Search engine indexing |audiovisual indexing]]. It will be necessary to choose the criteria we are going to use for creating metadata. The process of search can be divided in three parts:
*Generate descriptors for the media which we are going to use as query and the descriptors for the media in our [[database]].
*Compare descriptors of the query and our databases media.
*List the media sorted by maximum coincidence.

==Multimedia search engine==
There are two big search families, in function of the content:
* [[Visual search engine]]
*[[Audio search engine]]

===[[Visual search engine]]===
Inside this family we can distinguish two topics: [[image search]] and [[video search]]

*'''[[Image search]]''': Although usually its used simple metadata search, increasingly is being used indexing methods for making the results of users queries more accurate using [[query by example]]. For example [[QR codes]].
*'''[[Video search]]''': Videos can be searched for simple metadata or by complex metadata generated by indexing. The audio contained in the videos is usually scanned by audio search engines.

===[[Audio search engine]]===
There are different methods of audio searching:
*Voice search engine: Allows the user to search using speech instead of text. It uses algorithms of [[speech recognition]]. An example of this technology is [[Google Voice Search]].
*Music search engine: Although most of applications which searches music works on simple metadata (artist, name of track, album...) . There are some programs of [[music recognition]]. for example: [[Shazam (service)|Shazam]] or [[SoundHound]].

==See also==
*[[Search engine indexing]]
*[[Multimedia]]
*[[Multimedia Information Retrieval]]
*[[Streaming media]]
*[[Journal of Multimedia]]
*[[List of search engines#Multimedia|List of search engines]]
*[[Video search engine]]

==External links==

[[Category:Searching]]
[[Category:Multimedia]]
>>EOP<<
179<|###|>IBM Omnifind
'''IBM OmniFind''' was an [[enterprise search]] platform from [[IBM]].
It did come in several packages adapted to different business needs, including OmniFind Enterprise Edition, OmniFind Enterprise Starter Edition, and OmniFind Discovery Edition.<ref>[http://www-01.ibm.com/software/ecm/omnifind/library.html IBM - OmniFind - Library]</ref> IBM OmniFind as a standalone product was withdrawn in April 2011<ref>[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?subtype=ca&infotype=an&appname=iSource&supplier=897&letternum=ENUS911-075 IBM US Announcement Letter]</ref> and is now part of [[IBM Watson Content Analytics with Enterprise Search]].<ref>[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?infotype=AN&subtype=CA&htmlfid=897/ENUS211-133 IBM US Announcement Letter]</ref>

'''IBM OmniFind Yahoo! Edition''' was a free-of-charge version that could handle up to 500,000 documents in its index and was intended for small businesses. IBM OmniFind Yahoo! Edition was simple to install, provided a user friendly front end for administration, and incorporated technology from the open source [[Lucene]] project. IBM withdrew this product from marketing effective September 22, 2010 and withdrew support effective June 30, 2011.<ref>[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?subtype=ca&infotype=an&appname=iSource&supplier=897&letternum=ENUS910-115 IBM US Announcement Letter]</ref>

'''IBM OmniFind Personal E-mail Search''' was a research product launched in 2007 for doing [[semantic search]] over personal emails by extracting and organizing concepts and relationships (such as phone numbers and addresses). The project appears to have been silently abounded sometimes around 2010.

== See also ==
* [[Languageware]]
* [[UIMA]]
* [[Comparison of enterprise search software]]
* [[List of enterprise search vendors]]

==External links==
* [http://www.ibm.com/software/data/enterprise-search/ IBM OmniFind]
* [http://omnifind.ibm.yahoo.com/ IBM OmniFind Yahoo! Edition] {{Dead link|date=May 2012}}
* [http://www.alphaworks.ibm.com/tech/emailsearch IBM OmniFind Personal E-mail Search] {{Dead link|date=January 2012}}
* [http://www.opentestsearch.com/search-engines/ibm-omnifind-yahoo-edition-review/ Online demo and review of IBM OmniFind Yahoo! Edition]

==Notes==
{{reflist}}

[[Category:IBM software|OmniFind]]
[[Category:Searching]]
>>EOP<<
185<|###|>Concordance (publishing)
{{sister
|project=wiktionary
|text=See the [[Wiktionary:Wiktionary:Concordances|list of concordances]] in [[Wiktionary]], the free dictionary
}}

A '''concordance''' is an alphabetical list of the principal words used in a book or body of work, with their immediate [[context (language use)#Verbal context|context]]s.  Because of the time, difficulty, and expense involved in creating a concordance in the pre-[[computer]] era, only works of special importance, such as the [[Vedas]],<ref>{{cite book|first = Maurice | last = Bloomfield| authorlink = Maurice Bloomfield | title = A Vedic Concordance| year  = 1990| publisher = Motilal Banarsidass Publ| isbn = 81-208-0654-9}}</ref> [[Bible]], [[Qur'an]] or the works of [[William Shakespeare|Shakespeare]] or classical Latin and Greek authors,<ref>{{cite journal | first = Roy | last = Wisby | authorlink = Roy Wisby | title = Concordance Making by Electronic Computer: Some Experiences with the Wiener Genesis| journal = The Modern Language Review | publisher = Modern Humanities Research Association | volume = 57 | issue = 2 | pages = 161172 | date = April 1962 | doi=10.2307/3720960}}</ref> had concordances prepared for them. 
[[File:Mordechai nathan hebrew latin concordance.jpg|right|thumb|225px|Mordecai Nathan's Hebrew-Latin Concordance of the Bible]]
A concordance is more than an index; additional material, such as commentary, definitions, and topical cross-indexing make producing them a labor-intensive process, even when assisted by computers.

Although an automatically generated [[subject indexing|index]] lacks the richness of a published concordance, the ability to combine the result of queries concerning multiple terms (such as searching for words near other words) has reduced interest in concordance publishing.  In addition, mathematical techniques such as [[Latent Semantic Indexing]] have been proposed as a means of automatically identifying linguistic information based on word context.

A '''bilingual concordance''' is a concordance based on [[aligned parallel text]].

A '''topical concordance''' is a list of subjects that a book (usually The Bible) covers, with the immediate context of the coverage of those subjects. Unlike a traditional concordance, the indexed word does not have to appear in the verse. The most well known topical concordance is [[Nave's Topical Bible]].

The first concordance, to the [[Vulgate]] Bible, was compiled by [[Hugh of St Cher]] (d.1262), who employed 500 monks to assist him. In 1448 Rabbi Mordecai Nathan completed a concordance to the Hebrew Bible. It took him ten years. 1599 saw a concordance to the Greek New Testament published by Henry Stephens and the Septuagint was done a couple of years later by Conrad Kircher in 1602. The first concordance to the English bible was published in 1550 by Mr Marbeck. According to Cruden it did not employ the verse numbers devised by Robert Stephens in 1545 but "the pretty large concordance" of Mr Cotton did. Then followed [[Cruden's Concordance]] and [[Strong's Concordance]].

==Use in linguistics==
Concordances are frequently used in [[linguistics]], when studying a text. For example:    
* comparing different usages of the same word
* analysing keywords
* analysing [[word frequencies]]
* finding and analysing phrases and [[idioms]]
* finding [[translation]]s of subsentential elements, e.g. [[terminology]], in [[Bitext#Bitexts and translation memories|bitexts and translation memories]]
* creating indexes and word lists (also useful for publishing)

Concordancing techniques are widely used in national corpora such as [[American National Corpus]], [[British National Corpus]], and [[Corpus of Contemporary American English]] available on-line.  Stand-alone applications that employ concordancing techniques are known as concordancers.<ref>[http://www.lexically.net/wordsmith/introduction.htm?gclid=COjFnvGKhakCFVJX4Qod-RqjjQ Introduction to WordSmith]</ref> Some of them have integrated part-of-speech taggers and enable the user to create his/her own pos-annotated corpora to conduct various type of searches adopted in corpus linguistics.<ref>[http://yatsko.zohosites.com/linguistic-toobox-a-concordancer.html Linguistic Toolbox]</ref>

==Inversion==

The reconstruction of the text of some of the [[Dead Sea Scrolls]] involved a concordance.

Access to some of the scrolls was governed by a "secrecy rule" that allowed only the original International Team or their designates to view the original materials. After the death of [[Roland de Vaux]] in 1971, his successors repeatedly refused to even allow the publication of photographs to other scholars. This restriction was circumvented by [[Martin Abegg]] in 1991, who used a computer to "invert" a concordance of the missing documents made in the 1950s which had come into the hands of scholars outside of the International Team, to obtain an approximate reconstruction of the original text of 17 of the documents.<ref>{{cite web |last= Hawrysch |first= George |title= Dr. George Hawrysch's speech on concordance book launch |work= The Ukrainian Weekly, No. 31, Vol. LXX |publisher= Ukrainian National Association |date= 2002-08-04 |url= http://www.ukrweekly.com/old/archive/2002/310217.shtml |accessdate= 2008-06-19}}</ref><ref>{{cite web |last= Jillette |first= Penn |title= You May Already be a "Computer Expert" |url= http://pennandteller.com/sincity/penn-n-teller/pcc/deadsea.html |accessdate= 2008-06-14}}</ref> This was soon followed by the release of the original text of the scrolls.

== See also ==
* [[Back-of-the-book index]]
* [[A Vedic Word Concordance]]
* [[Bible concordance]]
* [[Bitext]]
* [[Concordancer]]
* [[Cross-reference]]
* [[Index (publishing)|Index]]
* [[Key Word in Context|KWIC]]
* [[Text mining]]

== References ==
{{Reflist}}

== External links ==
* [http://www.opensourceshakespeare.org/concordance/ Shakespeare concordance] - A concordance of Shakespeare's complete works (from Open Source Shakespeare)
* [http://www.arts.ualberta.ca/~ukr/skovoroda/NEW/ Online Concordance to the Complete Works of Hryhorii Skovoroda] - A concordance to Hryhorii Skovoroda's complete works (University of Alberta, Edmonton, Canada)
* [http://infomotions.com/alex/ Alex Catalogue of Electronic Texts] - The Alex Catalogue is a collection of public domain electronic texts from American and English literature as well as Western philosophy. Each of the 14,000 items in the Catalogue are available as full-text but they are also complete with a concordance. Consequently, you are able to count the number of times a particular word is used in a text or list the most common (10, 25, 50, etc.) words.
* [http://victorian.lang.nagoya-u.ac.jp/concordance/ Hyper-Concordance] - The Hyper-Concordance is written in C++, a program that scans and displays lines based on a command entered by the user. The main advantage of the C++ program is that it not only identifies the concordance lines but the words occurring to the left and the right of the word or phrase searched. It also reports the total number of text lines, the total word count and the number of occurrences of the word or phrase searched. The full text of the book is displayed in a box at the bottom of the screen. Each line of the text is numbered, and the line number and the term(s) searched provide a link to the full text.
* [http://cherry.conncoll.edu/cohar/Programs.htm Concord] - Page includes link to Concord, an on-the-fly KWIC concordance generator.  Works with at least some non-Latin scripts (modern Greek, for instance).  Multiple choices for sorting results; multi-platform; Open Source.
* [http://buschmeier.org/bh/study/ccd/ ConcorDance] - A concordance interface to the WorldWideWeb, it uses Google's or Yahoo's search engine to find concordances and can be used directly from the browser.
* [http://ctext.org/tools/concordance Chinese Text Project Concordance Tool] - Concordance lookup and discussion of the continued importance of printed concordances in [[Sinology]] - [[Chinese Text Project]]
* [http://khc.sourceforge.net/en/ KH Coder] - A free software for KWIC concordance and collocation stats generation. Various statistical analysis functions are also available such as co-occurrence network, multidimensional scaling, hierarchical cluster analysis, and correspondence analysis of words.

{{DEFAULTSORT:Concordance (Publishing)}}
[[Category:Concordances (publishing)| ]]
[[Category:Indexing]]
[[Category:Searching]]
[[Category:Library science]]
[[Category:Information science]]
[[Category:Reference works]]
>>EOP<<
191<|###|>Variable neighborhood search
'''Variable neighborhood search''' (VNS),<ref>{{cite journal |pages=367407 |last1 = Hansen  |first1 = P.|last2 = Mladenovic|first2 = N.|last3 = Perez|first3 = J.A.M.|title=Variable neighbourhood search: methods and applications
|volume=175 |journal= Annals of Operations Research |year=2010 |doi=10.1007/s10479-009-0657-6}}</ref> proposed by [[Mladenovic, Hansen]], 1997,<ref name=".....">{{cite journal
 | author = Nenad Mladenovi c, Pierre Hansen
 | year = 1997
 | title = Variable neighborhood search
 | journal = Computers and Operations Research
 | volume = 24
 | issue= 11
 | pages = 10971100
 | doi=10.1016/s0305-0548(97)00031-2
 }}
</ref> is a [[metaheuristic]] method for solving a set of [[combinatorial optimization (mathematics)|combinatorial optimization]] and global optimization problems.
It explores distant neighborhoods of the current incumbent solution, and moves from there to a new one if and only if an improvement was made. The local search method is applied repeatedly to get from solutions in the neighborhood to local optima.
VNS was designed for approximating solutions of discrete and continuous optimization problems and according to these, it is aimed for solving [[linear programming|linear program]] problems, [[linear programming|integer program]] problems, mixed integer program problems, [[nonlinear programming|nonlinear program]] problems, etc.

== Introduction ==
VNS systematically changes the neighborhood in two phases: firstly, descent to find a [[local optimum]] and finally, a perturbation phase to get out of the corresponding valley.

Applications are rapidly increasing in number and pertain to many fields: [[location theory]], [[cluster analysis]], [[scheduling]], [[Vehicle routing problem|vehicle routing]], [[Network planning and design|network design]], lot-sizing, [[artificial intelligence]], engineering, pooling problems, biology, [[Phylogenetics|phylogeny]], [[wikt:reliability|reliability]], geometry, telecommunication design, etc.

There are several books important for understanding VNS, such as: ''Handbook of Metaheuristics'', 2010,<ref>{{cite journal |last1=Gendreau|  first1=M.|last2= Potvin|first2=J-Y.|title=Handbook of Metaheuristics|publisher =Springer|year=2010 }}</ref> Handbook of Metaheuristics, 2003<ref>{{cite journal|last1=Glover|  first1=F.|last2= Kochenberger|first2=G.A.|title=Handbook of Metaheuristics|publisher = Kluwer Academic Publishers |year=2003}}</ref> and Search methodologies, 2005.<ref>{{cite journal |last1=Burke|first1=EK.|last2= Kendall | first2=G.| title=Search methodologies. Introductory tutorials in optimization and decision support techniques |journal = Springer|year=2005}}</ref>
Earlier work that motivated this approach can be found in
# Davidson, W.C.,<ref>{{cite journal |last1=Davidson  |first1=W.C.|title=Variable metric algorithm for minimization  |journal= Argonne National Laboratory Report ANL-5990 |year=1959 }}</ref>
# Fletcher, R., Powell, M.J.D.,<ref>{{cite journal |pages=163168 |last1=Fletcher |first1=R. |last2=Powell |first2=M.J.D. |title=Rapidly convergent descent method for minimization|volume=6 |journal=Comput.J. |year=1963 |doi=10.1093/comjnl/6.2.163}}</ref>
# Mladenovi c, N.<ref>{{cite journal |pages= 112 |last1=Mladenovi c |first1=N. |title=A variable neighborhood algorithma new metaheuristic for combinatorial optimization | journal=Abstracts of papers presented at Optimization Days, Montr eal |year=1995 }}
</ref> and 4. Brimberg, J., Mladenovi c, N.<ref>{{cite journal |pages=112 |last1=Brimberg |first1=J. |last2 = Mladenovi c |first2=N. |title=A variable neighborhood algorithm for solving the continuous location-allocation problem |volume=10 |journal=Stud. Locat. Anal. |year=1996}}</ref> Recent surveys on VNS  methodology as well as numerous applications can be found in 4OR, 2008.<ref>{{cite journal |pages=319360 |last1=Hansen |first1=P. |last2 = Mladenovi c |first2=N. |last3= Perez| first3=J.A.M|title=Variable neighbourhood search: methods and applications|volume=6 |journal=4OR |year=2008 |doi=10.1007/s10288-008-0089-1}}</ref> and Annals of OR, 2010.

== Basic description ==
Define one deterministic [[optimization problem]] with

<math> \min {\{f (x)|x \in X, X \subseteq S\}} </math>, (1)

where ''S'', ''X'', ''x'', and ''f''  are the solution space, the feasible set, a feasible solution, and a real-valued [[mathematical optimization|objective function]], respectively. If ''S'' is a finite but large set, a combinatorial optimization problem is defined. If <math>{S = R^{n}}</math>, there is continuous optimization model.

A solution <math>{x^* \in X}</math> is optimal if

<math> {f (x^{*}) \leq f (x), \qquad \forall{x}\, \in X} </math>.

Exact algorithm for problem (1) is to be found an optimal solution ''x*'', with the validation of its optimal structure, or if it is unrealizable, in procedure have to be shown that there is no  achievable solution, i.e., <math>X =\varnothing</math>, or the solution is unbounded. CPU time has to be finite and short. For continuous optimization, it is reasonable to allow for some degree of tolerance, i.e., to stop when a feasible solution <math>x^{*}</math> has been found such that

<math> {f (x^{*}) \leq f (x) + \epsilon, \qquad \forall{x}\, \in X} </math> or
<math> {(f (x^{*})- f (x))/ f (x^{*})  <  \epsilon  , \qquad \forall{x}\, \in X} </math>

Some heuristics speedily accept an approximate solution, or optimal solution but one with no validation of its optimality.
Some of them have an incorrect certificate, i.e., the solution <math>x_h</math> obtained satisfies

<math> {(f (x_{h})- f (x))/ f (x_{h})  \leq  \epsilon  , \qquad \forall{x}\, \in X} </math>
for some <math>\epsilon</math>, though this is rarely small.

Heuristics are faced with the problem of local optima as a result of avoiding boundless computing time.
A local optimum <math>x_L</math> of problem is such that

<math> {f (x_{L}) \leq f (x), \qquad \forall{x}\, \in N(x_{L}) \cap X} </math>

where <math> N(x_{L})</math>  denotes a neighborhood of <math> x_{L} </math>

== Description ==
According to (Mladenovic, 1995), VNS is a metaheuristic which systematically performs the procedure of neighborhood change, both in descent to local minima and in escape from the valleys which contain them.

VNS is built upon the following perceptions:

# A local minimum with respect to one neighbourhood structure is not necessarily a local minimum for another neighbourhood structure.
# A global minimum is a local minimum with respect to all possible neighborhood structures.
# For many problems, local minima with respect to one or several neighborhoods are relatively close to each other.

Unlike many other metaheuristics, the basic schemes of VNS and its extensions are simple and require few, and sometimes no parameters. Therefore, in addition to providing very good solutions, often in simpler ways than other methods, VNS gives insight into the reasons for such a performance, which, in turn, can lead to more efficient and sophisticated implementations.

There are several papers where it could be studied among recently mentioned, such as (Hansen and Mladenovi c 1999, 2001a, 2003, 2005; Moreno-Perez et al.;<ref>{{cite journal||last1=Moreno-Perez|first1=JA.|last2=Hansen|first2=P. |last3=Mladenovic|first3=N.| title = Parallel variable neighborhood search|journal=Alba E (ed) Parallel metaheuristics: a new class of algorithms|year=2005}}</ref>)

==[[Local search (optimization)|Local search]]==

A local search heuristic is performed through choosing an initial solution x, discovering a direction of descent from x, within a neighbourhood N(x), and proceeding to the minimum of f(x) within N(x) in the same direction. If there is no direction of descent, the heuristic stops; otherwise, it is iterated. Usually the highest direction of descent, also related to as best improvement, is used. This set of rules is summarized in Algorithm 1, where we assume that an initial solution x is given. The output consists of a local minimum, also denoted by x, and its value. Observe that a neighbourhood structure N(x) is defined for all x  X. At each step, the neighbourhood N(x) of x is explored completely. As this may be timeconsuming, an alternative is to use the first descent heuristic. Vectors <math>x^i \in N(x)</math> are then enumerated systematically and a move is made as soon as a direction for the descent is found. This is summarized in Algorithm 2.

Algorithm 1 Best improvement (highest descent) heuristic

Function BestImprovement(x)

  1: repeat
  2:     x'  x
  3:     xargmin_{f (y)}, yN(x)
  4: until ( f (x)  f (x'))
  5: return x

Algorithm 2 First improvement (first descent) heuristic

Function FirstImprovement(x)

  1: repeat
  2:    x'  x; i0
  3:    repeat
  4:       ii+1
  5:       xargmin{ f (x), f (x^i)}, x^i   N(x)
  6:    until ( f (x) < f (x^i) or i = |N(x)|)
  7: until ( f (x)  f (x'))
  8: return x

Let one denote <math> \mathcal{ N}_k(k=1, . . . ,k_{max}) </math>, a finite set of pre-selected neighborhood structures, and with <math>\mathcal{N}_k(x)</math> the set of solutions in the ''kth'' neighborhood of ''x''.

One will also use the notation <math>\mathcal{N'}_k(x), k = 1, . . . , k'_{max} </math> when describing local descent. Neighborhoods <math>\mathcal{N}_k(x)</math> or <math>\mathcal{N'}_k(x)</math> may be induced from one or more [[metric (mathematics)|metric]] (or quasi-metric) functions introduced into a solution space ''S''.
An optimal solution <math>x_{opt}</math> (or [[maxima and minima|global minimum]]) is a feasible solution where a minimum of problem ( is reached. We call ''x'  X'' a local minimum of problem with respect to <math>\mathcal{N}_k(x) </math>, if there is no solution <math> x \in \mathcal{N'}_k(x) \subseteq X </math> such that <math>f (x) < f (x')</math>.

In order to solve problem by using several neighbourhoods, facts 13 can be used in three different ways: (i) deterministic; (ii) [[stochastic]]; (iii) both deterministic and stochastic. We first give in Algorithm 3 the steps of the neighbourhood change function which will be used later. Function NeighbourhoodChange() compares the new value f(x') with the incumbent value f(x) obtained in the neighbourhood k (line 1). If an improvement is obtained, k is returned to its initial value and the new incumbent updated (line 2). Otherwise, the next neighbourhood is considered (line 3).

Algorithm 3&nbsp; Neighborhood change

Function NeighborhoodChange (x, x', k)

<code>
 1: if f (x') < f(x) then
 2:    x  x' // Make a move
 3:    k  1 // Initial neighborhood
 4: else
 5:    k  k+1 // Next neighborhood

</code>

When VNS does not render good solution, there are several steps which could be helped in process, such as comparing first and best improvement strategies in local search, reducing neighborhood, intensifying shaking, adopting VND, adopting FSS, and experimenting with parameter settings.

The Basic VNS (BVNS) method (Mladenovic and Hansen 1997) combines deterministic and stochastic changes of neighbourhood. Its steps are given in Algorithm 4. Often successive neighbourhoods <math> \mathcal{N}_k</math> will be nested. Observe that point x' is generated at random in Step 4 in order to avoid cycling, which might occur if a deterministic rule were applied. In Step 5, the first improvement local search (Algorithm 2) is usually
adopted. However, it can be replaced with best improvement (Algorithm 1).

Algorithm 4: Basic VNS

Function VNS (x, kmax, tmax );

<code>

 1: repeat
 2:    k  1;
 3:    repeat
 4:       x' Shake(x, k) /* Shaking */;
 5:       x''  FirstImprovement(x' ) /* Local search */;
 6:       NeighbourhoodChange(x, x', k) /* Change neighbourhood */;
 7:    until k = k_max ;
 8:    t CpuTime()
 9: until t > t_max ;

</code>

The basic VNS is a first improvement [[method of steepest descent|descent method]] with randomization. Without much additional effort, it can be transformed into a descent-ascent method: in NeighbourhoodChange() function, replace also x by x" with some probability, even if the solution is worse than the incumbent. It can also be changed into a best improvement method: make a move to the best neighbourhood k* among all k_max of them.
Another variant of the basic VNS can be to find a solution x' in the Shaking step as the best among b (a parameter) randomly generated solutions from the ''k''th neighbourhood. There are two possible variants of this extension: (1) to perform only one local search from the best among b points; (2) to perform all b local searches and then choose the best. In paper (Fleszar and Hindi<ref>{{cite journal|last1=Fleszar|first1=K|last2=Hindi|first2=KS|title=Solving the resource-constrained project scheduling problem by a variable neighborhood search|journal=Eur J Oper Res|year=2004|volume=155|issue=2|pages=402413|doi=10.1016/s0377-2217(02)00884-6}}</ref>) could be found algorithm.

== Extensions ==
* VND<ref>{{cite journal|last1=Brimberg|first1=J.|last2=Hansen|first2=P.|last3=Mladenovic|first3=N.|last4=Taillard |first4=E. |title=Improvements and comparison of heuristics for solving the multisource Weber problem|journal=Oper. Res.|year=2000|volume=48 |pages=444460 |doi=10.1287/opre.48.3.444.12431}}</ref>
:The variable neighborhood descent (VND) method is obtained if a change of neighborhoods is performed in a deterministic way. In the descriptions :of its algorithms, we assume that an initial solution x is given. Most local search heuristics in their descent phase use very few :neighbourhoods. The final solution should be a local minimum with respect to all <math>k_{max}</math> neighbourhoods; hence the chances to reach :a global one are larger when using VND than with a single neighbourhood structure.
* RVNS<ref>{{cite journal|last1=Mladenovic|first1=N.|last2=Petrovic|first2=J.|last3=Kovacevic-Vujcic|first3=V.|last4=Cangalovic |first4=M. |title=Solving spread spectrum radar polyphase code design problem by tabu search and variable neighborhood search|journal=Eur. J. Oper. Res.|year=2003b|volume=151 |pages=389399 |doi=10.1016/s0377-2217(02)00833-0}}</ref>

:The reduced VNS (RVNS) method is obtained if random points are selected from <math>\mathcal{N}_k(x)</math> and no descent is made. Rather, the :values of these new points are compared with that of the incumbent and an update takes place in case of improvement. It is assumed that a :stopping condition has been chosen like the maximum [[CPU time]] allowed <math>t_{max}</math> or the maximum number of iterations :between two improvements.
:To simplify the description of the algorithms it is used <math>t_{max}</math> below. Therefore, RVNS uses two parameters: <math>t_{max}</math> :and <math>k_{max}</math>. RVNS is useful in very large instances, for which local search is costly. It has been observed that the best value for :the parameter k_max is often 2. In addition, the maximum number of iterations between two improvements is usually used as a stopping condition. :RVNS is akin to a [[Monte-Carlo method]], but is more systematic.
* Skewed VNS
:The skewed VNS (SVNS) method (Hansen et al.)<ref>{{cite journal|last1=Hansen|first1=P.|last2=Jaumard|first2=B|last3=Mladenovi c|first3=N|last4=Parreira |first4=A |title=Variable neighborhood search :for weighted maximum satisfiability problem|journal=Les Cahiers du GERAD G200062, HEC Montreal, Canada|year=2000}}</ref> addresses the :problem of exploring valleys far from the incumbent solution. Indeed, once the best solution in a large region has been found, it is necessary to :go some way to obtain an improved one. Solutions drawn at random in distant neighbourhoods may differ substantially from the incumbent and VNS :can then degenerate, to some extent, into the Multistart heuristic (in which descents are made iteratively from solutions generated at random, a :heuristic which is known not to be very efficient). Consequently, some compensation for distance from the incumbent must be made.
* Variable Neighbourhood Decomposition Search
:The variable neighbourhood decomposition search (VNDS) method (Hansen et al.)<ref>{{cite journal|last1=Hansen|first1=P|last2=Mladenovi c|first2=N|last3=Perez-Brito|first3=D |title=Variable neighborhood decomposition :search|journal=J Heuristics|year=2001|volume=7|issue=4|pages=335350}}</ref> extends the basic VNS into a two-level VNS scheme based upon :decomposition of the problem. For ease of presentation, but without loss of generality, it is assumed that the solution x represents the set of :some elements.
* Parallel VNS
:Several ways of parallelizing VNS have recently been proposed for solving the p-Median problem. In Garcia-Lopez et al.:<ref>{{cite journal|last1=Garcia-Lopez|first1=F|last2=Melian-Batista|first2=B|last3= Moreno-Perez|first3= JA|last4= |first4=JM :|title=The parallel :variable neighborhood search for the p-median problem|journal=J Heuristics|year=2002|volume=8|issue=3|pages=375388}}</ref>&nbsp; three of them :are tested: (i) parallelize local search; (ii) augment the number of solutions drawn from the current neighbourhood and make a :local search in :parallel from each of them and (iii) do the same as (ii) but update the information about the best solution found. Three Parallel :VNS strategies :are also suggested for solving the [[Travelling purchaser problem]] in Ochi et al.<ref>{{cite journal|last1=Ochi|first1=LS|last2=Silva|first2=MB|last3= Drummond|first3= L|title=Metaheuristics based on GRASP and VNS for solving traveling purchaser :problem|journal=MIC2001, Porto|year=2001|pages=489494}}</ref>
* Primal-dual VNS
:For most modern heuristics, the difference in value between the optimal solution and the obtained one is completely unknown. Guaranteed :performance of the primal heuristic may be determined if a [[upper and lower bounds|lower bound]] on the objective function value is known. To :this end, the standard approach is to relax the integrality condition on the primal variables, based on a mathematical programming formulation of :the problem.
:However, when the dimension of the problem is large, even the relaxed problem may be impossible to solve exactly by standard :commercial solvers. :Therefore, it seems a good idea to solve dual relaxed problems heuristically as well. It was obtained guaranteed bounds on :the primal heuristics :performance.  In Primal-dual VNS (PD-VNS) (Hansen et al.)<ref>{{cite journal|last1=Hansen|first1=P|last2=Brimberg|first2=J|last3=Urosevi c|first3=D|last4=Mladenovi c|first4=N|title=Primal-dual variable neighborhood search for the simple plant location problem|journal=INFORMS J Comput|year=2007a|volume=19|issue=4|pages=552564|doi=10.1287/ijoc.1060.0196}}</ref> one :possible general way to attain both the guaranteed bounds and the exact solution is proposed.
* Variable Neighborhood Branching.)<ref>{{cite journal|last1=Hansen|first1=P.|last2=Mladenovic|first2=N.|last3=Urosevic|first3=D.|title=Variable neighborhood search and local branching|journal=Computers and Operations Research|year=2006|volume=33|pages=30343045|doi=10.1016/j.cor.2005.02.033}}</ref>
:The mixed integer linear programming (MILP) problem consists of maximizing or minimizing a linear function, subject to equality or inequality :constraints, and integrality restrictions on some of the variables.
* Variable Neighborhood Formulation Space Search .)<ref>{{cite journal|last1=Mladenovic|first1=N.|last2=Plastria|first2=F.|author2-link=Frank Plastria|last3=Urosevic|first3=D.|title=Reformulation descent applied to circle packing problems|journal=Computers and Operations Research|year=2006|volume=32|pages=24192434|doi=10.1016/j.cor.2004.03.010}}</ref>
:FSS is method which is very useful because, one problem could be defined in addition formulations and moving through formulations is legitimate. :It is proved that local search works within formulations, implying a final solution when started from some initial solution in first formulation. :Local search systematically alternates between different formulations which was investigated for [[Circle packing in a circle|circle packing]] :problem (CPP) where [[stationary point]] for a [[nonlinear programming]] formulation of CPP in [[Cartesian coordinate system|Cartesian coordinates]] is not strictly a stationary point in [[Polar coordinate system|polar coordinates]].

== Development ==
In order to make a simple version of VNS, here is the list of steps which should be made. Most of it is very similar with steps in other metaheuristics.
# It is necessary to be involved in problem, give some examples and try to solve them
# Study books, surveys and scientific papers
# Try to test some benchmarks
# Choose appropriate data structure for representing in memory
# Find initial solution
# Calculate objective function
# Design a procedure for Shaking
# Choose an local search heuristic with some moves as drop, add, swap, interchange, etc.
# Compare VNS with other methods from the literature

== Applications ==
Applications of VNS, or of varieties of VNS are very abundant and numerous. Some fields where it could be found collections of scientific papers:
* Industrial applications
* Design problems in communication
* Location problems
* [[Data mining]]
* [[Graph theory|Graph problems]]
* [[Knapsack problem|Knapsack]] and packing problems
* Mixed integer problems
* Time tabling
* [[Scheduling]]
* [[Vehicle routing problem]]s
* [[Arc routing]] and waste collection
* Fleet sheet problems
* Extended vehicle routing problems
* Problems in biosciences and chemistry
* Continuous optimization
* Other optimization problems
* Discovery science

== Conclusion ==
VNS implies several features which are presented in Hansen and Mladenovic<ref>{{cite journal|last1=Hansen|first1=P|last2=Mladenovi c|first2=N|title=Variable neighborhood search|journal=Glover F, Kochenberger G (eds) Handbook
of Metaheuristics|year=2003|issue=Kluwer, Dordrecht|pages=145184}}</ref> and some are presented here:

(i) Simplicity: VNS is simple a simple and clear which is universally applicable;

(ii) Precision: VNS is formulated in precise mathematical definitions;

(iii) Coherence: all actions of the heuristics for solving problems follow from the VNS principles;

(iv) Effectiveness: VNS supplies optimal or near-optimal solutions for all or at least most realistic instances;

(v) Efficiency: VNS takes a moderate computing time to generate optimal or near-optimal solutions;

(vi) Robustness: the functioning of the VNS is coherent over a variety of instances;

(vii) User friendliness: VNS has no parameters, so it is easy for understanding, expressing and using;

(viii) Innovation: VNS is generating new types of application.

(ix) Generality: VNS is inducing to good results for a wide variety of
problems;

(x) Interactivity: VNS allows the user to incorporate his knowledge to improve the resolution process;

(xi) Multiplicity: VNS is able to produce a certain near-optimal solutions from which the user can choose;

Interest in VNS is growing quickly, evidenced by the increasing number of papers published each year on this topic (10 years ago, only a few; 5 years ago, about a dozen; and about 50 in 2007).
Moreover, the 18th EURO mini-conference held in Tenerife in November 2005 was entirely devoted to VNS. It led to special issues of [[Institute of Mathematics and its Applications|IMA Journal of Management Mathematics]] in 2007, European Journal of Operational Research (http://www.journals.elsevier.com/european-journal-of-operational-research/), and Journal of Heuristics (http://www.springer.com/mathematics/applications/journal/10732/) in 2008.

== References ==
{{Reflist}}

== External links ==
* [http://toledo.mi.sanu.ac.rs/~grujicic/vnsconference EURO Mini Conference XXVIII on Variable Neighbourhood Search]

[[Category:Searching]]
>>EOP<<
197<|###|>Daffodil (software)
{{Orphan|date=February 2009}}
The '''Daffodil''' system is a virtual [[digital library]] system for strategic support of users during the information search process. It implements mainly high-level search functions, so-called stratagems, which provide functionality beyond today's digital libraries.  The Daffodil system was developed as a research project starting as a collaboration between the University of Dortmund (Germany) and the IZ Bonn (Germany), funded by the [[Deutsche Forschungsgemeinschaft]] (DFG) (20002004). 

Currently the Daffodil framework is extended to become an experimental evaluation platform for digital library evaluation at the [[University of Duisburg-Essen]].

== External links ==
* [http://www.dlib.org/dlib/june04/kriewel/06kriewel.html A description of functions and services]
* [http://www.is.informatik.uni-duisburg.de/projects/daffodil/index.html Project description]

[[Category:Library science]]
[[Category:Searching]]


{{Compu-library-stub}}
>>EOP<<
203<|###|>String metric
{{redirect|String distance|the distance between strings and the fingerboard in musical instruments|Action (music)}}

In [[mathematics]] and [[computer science]], a '''string metric''' (also known as a '''string similarity metric''' or '''string distance function''') is a [[metric (mathematics)|metric]] that measures [[distance]] ("inverse similarity") between two [[string (computer science)|text strings]] for [[approximate string  matching]] or comparison and in [[approximate string  matching|fuzzy string searching]]. Necessary requirement for a string ''metric'' (e.g. in contrast to [[string matching]]) is fulfillment of the [[triangle inequality]]. For example the strings "Sam" and "Samuel" can be considered to be close. A string metric provides a number indicating an algorithm-specific indication of distance.

The most widely known string metric is a rudimentary one called the [[Levenshtein distance|Levenshtein Distance]] (also known as Edit Distance).  It operates between two input strings, returning a number equivalent to the number of substitutions and deletions needed in order to transform one input string into another. Simplistic string metrics such as [[Levenshtein distance]] have expanded to include phonetic, [[token (parser)|token]], grammatical and character-based methods of statistical comparisons.

A widespread example of a string metric is [[DNA]] [[sequence analysis]] and RNA analysis, which are performed by optimized string metrics to identify matching sequences.

String metrics are used heavily in [[information integration]] and are currently used in areas including [[Data analysis techniques for fraud detection|fraud detection]], [[fingerprint analysis]], [[plagiarism detection]], [[ontology merging]], [[DNA analysis]], RNA analysis, [[image analysis]], evidence-based machine learning, [[database]] [[data deduplication]], [[data mining]], Web interfaces, e.g. [[Ajax (programming)|Ajax]]-style suggestions as you type, [[data integration]], and semantic [[knowledge integration]].

==List of string metrics==

<!-- This can be a separate article, someday. -->
* [[SrensenDice coefficient]]
* [[Hamming distance]]
* [[Levenshtein distance]] and [[DamerauLevenshtein distance]]
* [[Block distance]] or [[L1 distance]] or [[City block distance]]
* [[Simple matching coefficient]] (SMC)
* [[Jaccard similarity]] or [[Jaccard coefficient]] or [[Tanimoto coefficient]]
* [[Most frequent k characters]]
* [[Tversky index]]
* [[Overlap coefficient]]
* [[Variational distance]]
* [[Hellinger distance]] or [[Bhattacharyya distance]]
* [[Information radius]] ([[JensenShannon divergence]])
* [[Skew divergence]]
* [[Confusion probability]]
* [[Kendall_tau_distance|Tau metric]], an approximation of the [[KullbackLeibler divergence]]
* [[Fellegi and Sunters metric]] (SFS)
* [[Maximal matches]]
* [[Lee distance]]

==Selected string measures examples==

{| class="wikitable"
|-
! Name
! Example
|-
|[[Hamming distance]]
| "'''</span>ka<span style="color:#0082ff">rol</span>in</span>'''" and "'''</span>ka<span style="color:red;">thr</span>in</span>'''" is 3.
|-
|[[Levenshtein distance]] and [[DamerauLevenshtein distance]]
| 
# '''k'''itten  '''s'''itten (substitution of "s" for "k")
# sitt'''e'''n  sitt'''i'''n (substitution of "i" for "e")
# sittin  sittin'''g''' (insertion of "g" at the end).
<!--|-
|[[Simple matching coefficient]] (SMC)
|-->
<!--|-
|-
|[[Jaccard similarity]] or [[Jaccard coefficient]] or [[Tanimoto coefficient]]
|-->
|-
|[[Most frequent k characters]]
|MostFreqKeySimilarity('<span style="color:red;">r</span><span style="color:#0082ff">e</span>s<span style="color:#0082ff">e</span>a<span style="color:red;">r</span>ch', 's<span style="color:#0082ff">ee</span>king', 2) = 2
<!--|-
|[[Tversky index]]
|-->
<!--|-
|[[Overlap coefficient]]
|-->
<!--|-
|[[Variational distance]]
|-->
<!--|-
|[[Hellinger distance]] or [[Bhattacharyya distance]]
|-->
<!--|-
|[[Information radius]] ([[JensenShannon divergence]])
|-->
<!--|-
|[[Skew divergence]]
|-->
<!--|-
|[[Confusion probability]]
|-->
<!--|-
|[[Tau metric]], an approximation of the [[KullbackLeibler divergence]]
|-->
<!--|-
|[[Fellegi and Sunters metric]] (SFS)
|-->
<!--|-
|[[Maximal matches]]
|-->
|}

==See also==
* [[approximate string  matching]]
* [[String matching]]
* [http://www.speech.cs.cmu.edu/ Carnegie Mellon University open source library]
* [http://rockymadden.com/stringmetric/ StringMetric project] a [[Scala programming language|Scala]] library of string metrics and phonetic algorithms
* [https://github.com/NaturalNode/natural Natural project] a [[JavaScript]] natural language processing library which includes implementations of popular string metrics

==External links==
*http://www.dcs.shef.ac.uk/~sam/stringmetrics.html {{Dead link|date=July 2011}} A fairly complete overview {{wayback|url=http://www.dcs.shef.ac.uk/~sam/stringmetrics.html#ukkonen}}

{{DEFAULTSORT:String Metric}}
[[Category:String similarity measures| ]]
[[Category:Metrics]]

[[de:Ahnlichkeitsanalyse]]
>>EOP<<
209<|###|>String kernel
In [[machine learning]] and [[data mining]], a '''string kernel''' is a [[Positive-definite kernel|kernel function]] that operates on [[String (computer science)|strings]], i.e. finite sequences of symbols that need not be of the same length. String kernels can be intuitively understood as functions measuring the similarity of pairs of strings: the more similar two strings ''a'' and ''b'' are, the higher the value of a string kernel ''K''(''a'', ''b'') will be.

Using string kernels with [[Kernel trick|kernelized]] learning algorithms such as [[support vector machine]]s allow such algorithms to work with strings, without having to translate these to fixed-length, real-valued [[feature vector]]s.<ref name="Lodhi"/> String kernels are used in domains where sequence data are to be [[Cluster analysis|clustered]] or [[statistical classification|classified]], e.g. in [[text mining]] and [[bioinformatics|gene analysis]].<ref>
{{Citation
  | title = The spectrum kernel: A string kernel for SVM protein classification
  | last = Leslie
  | first = C.
  | last2 = Eskin
  | first2 = E.
  | last3 = Noble
  | first3 = W.S.
  | booktitle = Proceedings of the Pacific Symposium on Biocomputing
  | volume = 7
  | pages = 566575
  | year = 2002
}}</ref>

==Informal introduction==

Suppose one wants to compare some text passages automatically and indicate their relative similarity.
For many applications, it might be sufficient to find some keywords which match exactly.
One example where exact matching is not always enough is found in [[Spam (electronic)|spam]] detection.<ref>
{{Citation
  | title = Improved Online Support Vector Machines Spam Filtering Using String Kernels
  | last = Amayri
  | first = O.
}}</ref>
Another would be in computational gene analysis, where [[Homology (biology)|homologous]] [[genes]] have [[mutated]], resulting in common subsequences along with deleted, inserted or replaced symbols.
<!--- TODO insert a picture here --->

==Motivation==

Since several well-proven data clustering, classification and information retrieval
<!--- and other ... see manifold learning --->
methods (for example support vector machines) are designed to work on vectors
(i.e. data are elements of a vector space), using a string kernel allows the extension of these methods to handle sequence data.

The string kernel method is to be contrasted with earlier approaches for text classification where feature vectors only indicated
the presence or absence of a word.
Not only does it improve on these approaches, but it is an example for a whole class of kernels adapted to data structures, which
began to appear at the turn of the 21st century. A survey of such methods has been compiled by Gartner.<ref>
{{Citation
  | last = Gartner
  | first = T.
  | title = A survey of kernels for structured data
  | journal = CM SIGKDD Explorations Newsletter
  | publisher = [[Association for Computing Machinery|ACM]]
  | year = 2003
  | volume = 5
  | number = 1
  | page = 58}}
</ref>

==Definition==

A [[Kernel trick|kernel]] on a domain <math>D</math> is a function <math>K: D \times D \rightarrow \mathbb{R}</math>
satisfying some conditions (being [[symmetric]] in the arguments, [[continuous function|continuous]] and [[Positive-semidefinite function|positive semidefinite]] in a certain sense).

[[Mercer's theorem]] asserts that <math>K</math> can then be expressed as <math>K(x,y)=\varphi(x)\cdot \varphi(y)</math> with <math>\varphi</math> mapping the arguments into an [[inner product space]].

We can now reproduce the definition of a '''string subsequence kernel'''<ref name="Lodhi">{{Cite journal
  | last = Lodhi
  | first = Huma
  | last2 = Saunders
  | first2 = Craig
  | last3 = Shawe-Taylor
  | first3 = John
  | last4 = Cristianini
  | first4 = Nello
  | last5 = Watkins
  | first5 = Chris
  | title = Text classification using string kernels
  | journal = [[Journal of Machine Learning Research]]
  | year = 2002
  | pages = 419444}}</ref>
on strings over an [[Alphabet (computer science)|alphabet]] <math>\Sigma</math>. Coordinate-wise, the mapping is defined as follows:

:<math>\varphi_u :
\left\{
\begin{array}{l}
\Sigma^n \rightarrow \mathbb{R}^{\Sigma^n} \\
 s \mapsto \sum_{\mathbf{i} : u=s_{\mathbf{i}}} \lambda^{l(\mathbf{i})}
\end{array}
\right.
</math>

The <math>\mathbf{i}</math> are [[multiindices]] and <math>u</math> is a string of length <math>n</math>:
subsequences can occur in a non-contiguous manner, but gaps are penalized.
The parameter <math>\lambda</math> may be set to any value between <math>0</math> (gaps are not allowed) and <math>1</math>
(even widely-spread "occurrences" are weighted the same as appearances as a contiguous substring).

<!--- TODO put an example here !!! --->

For several relevant algorithms, data enters into the algorithm only in expressions involving an inner product of feature vectors,
hence the name [[kernel methods]]. A desirable consequence of this is that one does not need to explicitly calculate the transformation <math>\phi(x)</math>, only the inner product via the kernel, which may be a lot quicker, especially when [[approximation|approximated]].<ref name=Lodhi/>
<!--- ==Efficitent Computation== --->
<!--- == See also == --->

==References==
<!--- cite "alignment kernels", precursor --->
{{Reflist}}

[[Category:Algorithms on strings]]
[[Category:Kernel methods for machine learning]]
[[Category:Natural language processing]]
[[Category:String similarity measures]]
>>EOP<<
215<|###|>Simple matching coefficient
The '''Simple Matching Coefficient (SMC)''' is a [[statistic]] used for comparing the [[Similarity measure|similarity]] and [[diversity index|diversity]] of [[Sample (statistics)|sample]] sets.<ref>http://mines.humanoriented.com/classes/2010/fall/csci568/portfolio_exports/sdaugherty/similarity.html</ref>

Given two objects, A and B, each with n binary attributes, SMC is defined as:
:<math> SMC = {\text{Number of Matching Attributes}\over \text{Number of Attributes}} = {{M_{00}+M_{11}}\over{M_{00}+M_{01}+M_{10}+M_{11}}}</Math>

Where:
:<math>M_{11}</math> represents the total number of attributes where ''A'' and ''B'' both have a value of 1.
:<math>M_{01}</math> represents the total number of attributes where the attribute of ''A'' is 0 and the attribute of ''B'' is 1.
:<math>M_{10}</math> represents the total number of attributes where the attribute of ''A'' is 1 and the attribute of ''B'' is 0.
:<math>M_{00}</math> represents the total number of attributes where ''A'' and ''B'' both have a value of 0.

== See also ==
* [[Jaccard index]]

==Notes==
{{reflist}}

[[Category:Index numbers]]
[[Category:Measure theory]]
[[Category:Clustering criteria]]
[[Category:String similarity measures]]
>>EOP<<
221<|###|>Islamic World Science Citation Database
'''Islamic World Science Citation Database''' (ISC) is a [[citation index]] established by the Iranian [[Ministry of Science, Research and Technology]] after it was approved by the [[Organisation of the Islamic Conference]].  It only indexes journals from the [[Islamic world]].

It was announced in [[Baku]], Azerbaijan during the Fourth Islamic Conference of the Ministers of Higher Education and Scientific Research held in October 2008.<ref>{{cite news | url = http://www.scidev.net/en/science-communication/science-publishing/news/islamic-countries-to-get-own-science-citation-inde.html | title = Islamic countries to get own science citation index | author = Wagdy Sawahel | date = 17 October 2008 | publisher = [[SciDev.Net]] }}</ref>  It is managed by the Islamic World Science Citation Center, located in [[Shiraz]].

In 2009, ISC partnered with [[Scopus]] that allows ISC's publications to be indexed in Scopus.<ref>{{cite journal | journal = [[Library Connect]] | title = The Islamic World Science Citation Database partnership with Scopus brings greater visibility to Islamic researchers | url = http://libraryconnect.elsevier.com/lcn/0703/lcn070319.html | author = Ahmed Rostom | volume = 7 | issue = 3 | date = August 2009 | issn = 1549-3725 }}</ref>

== References ==
{{Reflist}}

==See also==
* [[Academic publishing]]
* [[List of academic databases and search engines]]
* [[Impact factor]]

== External links ==
* {{Official website|http://www.isc.gov.ir/isce.htm}}

[[Category:Bibliographic databases]]
[[Category:Online databases]]
[[Category:Citation indices]]
[[Category:Research management]]
[[Category:Databases in Iran]]


{{science-journal-stub}}
{{islam-stub}}
>>EOP<<
227<|###|>Russian Science Citation Index
{{primary sources|date=March 2012}}
'''Russian Science Citation Index''' is a [[bibliographic database]] of [[scientific publication]]s in Russian. It accumulates more than 2 million publications of Russian authors, as well as information about citing these publications from more than 2000 Russian journals. The Russian Science Citation Index has been developed since 2009 by the Scientific Electronic Library. The information-analytical system Science Index is a search engine of this database; it offers a wide range of services for authors, research institutions and scientific publishers. It is designed not only for operational search for relevant bibliographic information, but is also as a powerful tool to assess the impact and effectiveness of research organizations, scientists, and the level of scientific journals, etc.

== Purpose ==
From 3000 Russian scientific journals only about 150 are presented in foreign databases (i.e. not more than 5%). Those are mainly translated journals. So far, the vast majority of Russian scientific publications remain "invisible" and not available online.  Russian Science Citation Index makes it real to objectively compare Russian journals with  the best international journals and brings them closer to researchers all over the world.

== Functionality ==
In Russia, this database is one of the main sources of information for evaluating the effectiveness of organizations involved in research. It allows to appraise: 
* Scientific capacity and effectiveness of research, and
* Publication activity
through the following indicators:
* The number of publications (including foreign scientific and technical journals, and local publications from the list of [[Higher Attestation Commission]]) of researchers from a particular scientific organization, divided by the number of researchers,
* The number of publications (registered in the Russian Science Citation Index) of researchers from a particular scientific organization, divided by the number of researchers, and
* Citation of researchers (registered in the Russian Science Citation Index) from a particular scientific organization, divided by the number of researchers.

== See also ==
*[[List of academic databases and search engines]]
*[[Science Citation Index]]
*[[Scopus]]

==External links==
* [http://elibrary.ru/ Scientific Electronic Library]


[[Category:Citation indices]]
>>EOP<<
233<|###|>Tunebot
'''Tunebot''' is a music search engine developed by the Interactive Audio Lab at [[Northwestern University]]. Users can search the database by humming or singing a melody into a microphone, playing the melody on a virtual keyboard, or by typing some of the lyrics. This allows users to finally identify that song that was stuck in their head.

==Searching Techniques==

Tunebot is a [[Query by humming]] system. It compares a sung query to a database of musical themes by using the intervals between each note. This allows a user to sing in a different key than the target recording and still produce a match. The intervals are also unquantized to allow for other tunings besides the standard A=440Hz, since not many people in the world have [[perfect pitch]].

In addition to note intervals, Tunebot compares a query with potential targets by using rhythmic ratios between notes. Since ratios between note lengths are used, the tempo of the performance does not affect the rhythmic similarity measure. 

Queries and targets are then matched by a weighted string alignment algorithm between the note intervals and rhythmic ratios.
<!--Note segmentation, then to pitches and then use Pitch intervals (instead of melodic contour - measured frequency at given times). Pitch intervals are relative (unquantized) to adjust for singing in the wrong key or wrong tempo. Faster and more reliable search.

Model singer error: gaussian distribution because wider interval and lower intervals seem to be more prone to singer error. Combination of gaussians with 5 parameters to tweak: pitch weight, rhythm weight, sensitivity to distance for pitch, sensisitivity to distance for rhythm, octave decay

Do we use rhythmic ratios? (LIR)

Genetic algorithm to tune system parameters-->

==The Database==
The database consists of unaccompanied melodies sung by contributors (a capella). Contributors log into the website and sing their examples to the system. Each of these recordings is associated with a corresponding song on [[Amazon.com|Amazon]]. A sung query is compared to these examples. A capella sung examples are used as search keys because it is much easier to compare one unaccompanied vocal (the sung query) to another (an example search key) than it is to compare an unaccompanied vocal to a full band recording, which may contain guitar, drums, other singers, sound effects, etc.

==Distinguishing Features==

Tunebot learns from user input, and it improve its results as each user submits more queries. Since no human can sing perfectly in tune every time they sing, the search engine must take that into account. By choosing a song from a list of ranked results, users tell Tunebot which song was correct. Tunebot then pairs that song with the user's query, analyzes the differences, and runs a [[Genetic Algorithm]]. This process tweaks the parameters that control how the system compares the user's query to the targets. For instance, if a user has no sense of rhythm, that factor of the comparison is lowered for future queries.

==References==

* B. Pardo. [http://127.0.0.1/publications/pardo-IEEE-signal-processing-mag-06.pdf Finding Structure in Audio for Music Information Retrieval.] IEEE Signal Processing Magazine. vol. 49 (8), pp. 49-52, 2006
* D. Little, D. Raffensperger, B. Pardo. [http://music.cs.northwestern.edu/files/ISMIR%202007%20v2.pdf A Query by Humming System that Learns from Experience.] Proceedings of the 8th International Conference on Music Information Retrieval, Vienna, Austria, September 23-27, 2007.
* D. Little, D. Raffensperger and B. Pardo.[http://www.eecs.northwestern.edu/docs/techreports/2007_TR/NWU-EECS-07-03.pdf Online Training of a Music Search Engine.] Northwestern University, Evanston, IL, NWU-EECS-07-03, 2007

==External links==
*[http://tunebot.cs.northwestern.edu Tunebot @ Northwestern]


[[Category:Music search engines]]
[[Category:Acoustic fingerprinting]]
>>EOP<<
239<|###|>Songza
{{use mdy dates|date=July 2014}}
{{use American English|date=July 2014}}
{{Infobox website
|name           = Songza
|logo           = [[File:Songza Logo.jpg|frameless|150px]]
|screenshot     = 
|caption        = 
|url            = {{URL|songza.com}}
|alexa          = {{Loss}} 9,279 ({{as of|2014|4|1|alt=April 2014}})<ref name="alexa">{{cite web|url= http://www.alexa.com/siteinfo/songza.com |title= Songza.com Site Info | publisher= [[Alexa Internet]] |accessdate= April 1, 2014 }}</ref><!--Updated monthly by OKBot.-->
|commercial     = 
|type           = Free [[internet radio]]
|language       = [[English language|English]]
|location       = [[Long Island City, New York|Long Island City]], [[Queens]], [[New York City]], [[New York]], United States
|registration   = 
|owner          = [[Google Inc.]]
|author         = [[Aza Raskin]] and Scott Robbin
|launch date    = {{start date and age|2007|11|08|paren=yes}}
|current status = Active
|revenue        = 
|slogan         = Good music makes good times.<ref>{{cite web|url= http://songza.com |title= Songza.com Site Info | publisher= Songza Media, Inc. |accessdate= August 15, 2012 }}</ref>
}}

'''Songza''' is a free [[music streaming]] and [[Recommender system|recommendation]] service for Internet users in the United States and Canada. 

Stating that its playlists are made by music experts, the service recommends various playlists based on time of day and mood or activity.<ref name="The New York Times">{{cite news| first= Ben| last= Sisaro| work = [[The New York Times]] |title= Pandora Faces Rivals for Ears and Ads| accessdate = June 20, 2012| url= http://www.nytimes.com/2012/06/21/business/songza-and-spotify-challenge-pandora-for-ears-and-ads.html?_r=3| date= June 20, 2012}}</ref><ref name=PandoDaily>{{cite web| first= Erin|last= Griffith| publisher= [[PandoDaily]]|title= Songza's Founders Realized They Weren't Thinking Radically Enough{{spaced ndash}} Here's How They Changed That| accessdate = August 15, 2012|url= http://pandodaily.com/2012/08/15/songzas-founders-realized-they-werent-thinking-radically-enough-heres-how-they-changed-that/}}</ref> Songza offers playlists for activities such as waking up, working out, commuting, concentrating, unwinding, entertaining, and sleeping.<ref name="The Washington Post" >{{cite news| first= Hayley| last= Tsukayama| work = [[The Washington Post]] |title=TechBits: Songza adapts the music to your mood| accessdate = June 23, 2012| url = http://www.washingtonpost.com/techbits-songza-adapts-the-music-to-your-mood/2012/06/23/gJQAYRzKyV_story.html| date= June 25, 2012}}</ref>  Users can vote songs up or down, and the service will adapt to the user's personal music preferences.<ref name="The Washington Post" /> Users can find playlists not just based on artists, songs, or genres, but also based on themes, interests, and eras, such as "[[List of 1990s one-hit wonders in the United States|90s One-Hit Wonders]]", or "Music of [[Fashion Week]]".<ref name=SongzaAbout>{{cite web| publisher= Songza|title= About Us| accessdate = March 25, 2011| url = http://songza.com/page/about/}}</ref>

Songza is headquartered in the [[Long Island City]] neighborhood of the [[Queens]] [[borough (New York City)|borough]] of [[New York City]], [[New York]].<ref name="NY Daily News">{{cite news| first= Clare | last= Trapasso| work = [[Daily News (New York)|Daily News]] |title= Songza music service streams for success| accessdate = July 27, 2012| url= http://articles.nydailynews.com/2012-07-27/news/32874462_1_spotify-apps-music-download}}</ref>

== History ==
[[Amie Street]] acquired Songza, a product created by [[Aza Raskin]] and Scott Robbin, in October 2008.<ref>{{cite web| first= Kristen| last=Nicole| publisher= bub.blicio.us |title= Interview with Amie Street: Why Keep Acquisition of Songza a Secret?| accessdate = March 25, 2011| url = http://bub.blicio.us/interview-with-amie-street-why-keep-acquisition-of-songza-a-secret/}}</ref> In August 2010, Amie Street was sold to Amazon for an undisclosed amount.<ref>{{cite web| first= Michael | last= Arrington| publisher= [[TechCrunch]]|title= Amazon Acquires Amie Street, But Not in a Good Way| accessdate = September 8, 2010| url= http://techcrunch.com/2010/09/08/amazon-acquires-amie-street-but-not-in-a-good-way/}}</ref>  Shortly after this the co-founders{{spaced ndash}} CEO Elias Roman, COO Peter Asbill, CPO Elliott Breece and CCO Eric Davich{{spaced ndash}} refocused their efforts on Songza.<ref name="The New York Times" /><ref>{{cite web| publisher= [[Internships.com]]|title= 5 in 5! with Eric Davich, Chief Content Officer and Co-Founder of Songza| accessdate = August 6, 2012| url= http://www.internships.com/eyeoftheintern/applying-2/employers-applying-2/5-5-eric-davichchief-content-officer-cofounder-songza/?cid=SO_ST_TW_080612_5IN5_SONGZA}}</ref>  The team discontinued the original version and relaunched a new alpha version of Songza, keeping nothing of the original product but the name.<ref name=Upstart>{{cite news| first= Michael| last= del Castillo| work =  [[American City Business Journals|Upstart Business Journal]] |title= Downtime: The birth of Songza| accessdate = June 15, 2012| url= http://upstart.bizjournals.com/entrepreneurs/hot-shots/2012/06/15/songza-minigolfs-to-no-1-app.html?page=2}}</ref>

Over the next year the founders experimented with various iterations, when the app originally launched in 2010 "it was like a pre-Turntable.fm.  A function called Social Radio allowed users to be DJs for their friends" stated PandoDaily.<ref name="PandoDaily" />  This version of the app allowed it to be social and crowdsourced; the problem with it was that the service as it stood was not sufficiently differentiated from other services on the market and the quality of the crowd sourced playlists was low.<ref name=PandoDaily/>  Following a year of testing various iterations of the alpha version of the app, Songza relaunched in beta on iPhone and Android apps on September 13, 2011, armed with a team of 25 expert music curators.<ref name="The New York Times" /><ref name="PandoDaily" /><ref name=TechCrunch>{{cite web| first= Rip| last= Empson| publisher= [[TechCrunch]]|title= Songza Raises Seven Figure Round; Launches Mobile, Sharable Music Collections in the Cloud| accessdate = September 13, 2011| url= http://techcrunch.com/2011/09/13/songza-raises-seven-figure-round-launches-mobile-sharable-music-collections-in-the-cloud/}}</ref><ref>[http://www.ad60.com/2011/09/19/songza-launches-iphone-android-apps-digitize-mix-tape/ "Songza launches iPhone and Android apps to digitize the mix tape"].</ref>

In March 2012, Songza released its Music Concierge feature, on iPhone and the web.<ref name="The New York Times" /><ref name = TechCrunch>{{cite web| first= Jordan| last= Crook| publisher= [[TechCrunch.com]]|title= Songza, the Music Streaming Service That Does All Work for You, Launches an iPad App| accessdate = June 7, 2012| url= http://techcrunch.com/2012/06/07/songza-the-music-streaming-service-that-does-all-work-for-you-launches-an-ipad-app/}}</ref>  The concierge presents users with up to six situations based on time of day, with filters for whatever mood they might be in.  For example, on a Wednesday morning a user might be presented with situations for "Waking Up", "Singing in the Shower", "Working Out" and so on.  This feature was rolled out to iPad on June 7, 2012; during the first ten days following the iPad app launch, Songza saw over 1.15 million downloads.<ref>{{cite news| first= Stephanie| last= Mlot| work = [[PC Magazine]]|title= Songza Hits 1.15 Million iOS Downloads in 10 Days| accessdate = June 18, 2012| url= http://www.pcmag.com/article2/0,2817,2405952,00.asp}}</ref>

On June 12, 2012, Songza was listed as the top free app on iTunes for the iPad and the number two free app for the iPhone.<ref>{{cite web| first= Glenn| last= Peoples| publisher= [[Billboard.biz]]|title= Songza Reaches One Million iOs Downloads in Ten Days, But Is It the Next Big Thing?| accessdate = June 19, 2012| url= http://www.billboard.biz/bbbiz/others/songza-reaches-one-million-ios-downloads-1007360352.story}}</ref>  Concierge was released on Android on July 10, 2012, and for Android tablets on August 14, 2012.<ref>{{cite web| first= Andrew| last= Kameka| publisher= Androinica.com |title= Songza re-ups with expert Music Concierge playlists, lockscreen controls, and new Holo-like design| accessdate = July 10, 2012| url= http://androinica.com/2012/07/songza-android-app/}}</ref><ref>{{cite news| first= Stephanie| last= Mlot| work = [[PC Magazine]]|title= Songza App Now Available on Android Tablets| accessdate = August 14, 2012| url= http://www.pcmag.com/article2/0,2817,2408435,00.asp}}</ref>  The app expanded to Canada on August 7, 2012, and became the number-one overall free app in Canada on August 13, 2012.<ref name=PandoDaily/><ref>{{cite web| first= Anand| last= Ram| publisher= o.canada.com |title= Songza's Elias Roman wants to provide the music for every mood | accessdate = August 7, 2012| url= http://o.canada.com/2012/08/05/songzas-elias-roman-wants-to-provide-the-music-for-every-mood/}}</ref> Within the week of Microsoft's Build developer event in June 2013, Songza snuck in its official Windows 8 App.<ref>[http://www.wpcentral.com/songza-sneaks-windows-store-wins-our-hearts]. WP Central. June 27, 2013.</ref>

Songza launched in Canada on August 7, 2012, and reached the one million download mark after 70 days.<ref>Dobby, Christine (August 23, 2012).  [http://business.financialpost.com/2012/08/23/songza-startup-singing-a-canadian-tune/ "Songza startup singing a Canadian tune"].  ''[[Financial Post]]''. August 23, 2012.</ref><ref>Crook, Jordan (October 18, 2012). [http://techcrunch.com/2012/10/18/songzas-canada-launch-nabs-1-million-new-users-in-70-days/ "Songza's Canada Launch Nabs 1 Million New Users in 70 Days"]. [[TechCrunch]].</ref>

Starting October 2013, Songza began inserting pop-up audio/video ads when initiating a playlist so it is no longer "audio-ad free". Songza reported having 5.5 million regular users at the end of 2013.<ref>{{Cite news|url = http://www.nytimes.com/2014/07/02/business/media/google-buys-songza-a-playlist-app-for-any-occasion.html|title = Google in Deal for Songza, a Music Playlist Service|last = Sisario|first = Ben|date = July 1, 2014|work = New York Times|accessdate = }}</ref>

Songza was acquired by Google on July 1, 2014.<ref>{{cite web | url=http://techcrunch.com/2014/07/01/google-buys-songza/ | title=Google Buys Songza | publisher= [[TechCrunch]] | accessdate= July 1, 2014}}</ref> No terms were disclosed but speculation put the price at somewhere between $15 million and $39 million. Both companies issued statements saying they were "thrilled" to be doing the deal.<ref name="GoogleSongza">{{cite news|title=Google acquires music app start-up Songza|url=http://www.businesssun.com/index.php/sid/223470673/scat/3a8a80d6f705f8cc/ht/Google-acquires-music-app-start-up-Songza|accessdate= July 3, 2014|publisher=''Business Sun''}}</ref> In October 2014, following the acquisition, the [[Google Play Music|Google Play Music All Access]] service was updated to include functionality adapted from Songza's Concierge system.<ref name=verge-songzagpm>{{cite web|title=Google brings Songza's best feature to Play Music|url=http://www.theverge.com/2014/10/21/7027707/google-brings-best-songza-feature-to-play-music|website=The Verge|accessdate=21 October 2014}}</ref>

==Similar organizations==
{{div col|colwidth=30em}}
* [[8tracks]]
* [[AccuRadio]]
* [[Deezer]]
* [[Digitally Imported]]
* [[FIT Radio]]
* [[Google Play Music]]
* [[Grooveshark]]
* [[Guvera]]
* [[iHeartRadio]]
* [[Live365]]
* [[MOG (online music)|MOG]]
* [[Musicovery]]
* [[Pandora Radio]]
* [[Rara.com]]
* [[Rdio]]
* [[Rhapsody (online music service)|Rhapsody]]
* [[Slacker Radio]]
* [[Spotify]]
* [[Soundtracker (music streaming)]]
* [[WiMP]]
* [[Xbox Music]]
{{div col end}}


{{portal|Companies|Music}}

==References==
{{reflist|30em}}
==External links==
*{{official website|songza.com}}

{{Digital distribution platforms}}
{{Google Inc.}}

[[Category:American companies established in 2007]]
[[Category:Community websites]]
[[Category:Companies based in Queens, New York]]
[[Category:Domain-specific search engines]]
[[Category:Free music]]
[[Category:Google acquisitions]]
[[Category:Internet advertising]]
[[Category:Internet companies of the United States]]
[[Category:Internet properties established in 2007]]
[[Category:Internet radio in the United States]]
[[Category:Long Island City]]
[[Category:Media companies based in New York City]]
[[Category:Music companies of the United States]]
[[Category:Music search engines]]
[[Category:Recommender systems]]
[[Category:Technology companies established in 2007]]
>>EOP<<
