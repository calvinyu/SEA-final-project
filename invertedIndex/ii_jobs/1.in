1<|###|>Proximity search (text)
In [[natural language processing|text processing]], a '''proximity search''' looks for documents where two or more separately matching term occurrences are within a specified [[string distance|distance]], where distance is the number of intermediate words or characters. In addition to proximity, some implementations may also impose a constraint on the word order, in that the order in the searched text must be identical to the order of the search query. Proximity searching goes beyond the simple matching of words by adding the constraint of proximity and is generally regarded as a form of advanced search.

For example, a search could be used to find "red brick house", and match phrases such as "red house of brick" or "house made of red brick". By limiting the proximity, these phrases can be matched while avoiding documents where the words are scattered or spread across a page or in unrelated articles in an anthology.

== Rationale ==
The basic linguistic assumption of proximity searching is that the proximity of the words in a document implies a [[semantic relation|relationship]] between the words. Given that authors of documents try to formulate sentences which contain a single idea, or cluster of related ideas within neighboring sentences or organized into paragraphs, there is an inherent, relatively high, probability within the document structure that words used together are related. On the other hand, when two words are on the opposite ends of a book, the probability of a relationship between the words is relatively weak. By limiting search results to only include matches where the words are within the specified maximum proximity, or distance, the search results are assumed to be of higher relevance than the matches where the words are scattered.

Commercial internet search engines tend to produce too many matches (known as recall) for the average search query. Proximity searching is one method of reducing the number of pages matches, and to improve the relevance of the matched pages by using word proximity to assist in ranking. As an added benefit, proximity searching helps combat [[spamdexing]] by avoiding webpages which contain dictionary lists or shotgun lists of thousands of words, which would otherwise rank highly if the search engine was heavily biased toward [[word frequency]].

== Boolean syntax and operators ==
Note that a proximity search can designate that only some keywords must be within a specified distance. Proximity searching can be used with other search syntax and/or controls to allow more articulate search queries. Sometimes query operators like NEAR, NOT NEAR, FOLLOWED BY, NOT FOLLOWED BY, SENTENCE or FAR are used to indicate a proximity-search limit between specified keywords: for example, "brick NEAR house".

== Usage in commercial search engines ==
In regards to implicit/automatic versus explicit proximity search, as of November 2008, most Internet [[search engine]]s only implement an implicit proximity search functionality. That is, they automatically rank those search results higher where the user keywords have a good "overall proximity score" in such results. If only two keywords are in the search query, this has no difference from an explicit proximity search which puts a NEAR operator between the two keywords. However, if three or more than three keywords are present, it is often important for the user to specify which subsets of these keywords expect a proximity in search results. This is useful if the user wants to do a [[prior art]] search (e.g. finding an existing approach to complete a specific task, finding a document that discloses a system that exhibits a procedural behavior collaboratively conducted by several components and links between these components).

[[Web search engine]]s which support proximity search via an explicit proximity operator in their query language include  [[Walhello]], [[Exalead]], [[Yandex]], [[Yahoo!]] and [[Altavista]]:
* When using the [[Walhello]] search-engine, the proximity can be defined by the number of characters between the keywords.<ref>[http://www.walhello.com/aboutgl.html "About Walhello"], visited 23 December 2009</ref>
* The search engine Exalead allows the user to specify the required proximity, as the maximum number of words between keywords. The syntax is <tt>(keyword1 NEAR/n keyword2)</tt> where n is the number of words.<ref>[http://www.exalead.com/search/web/search-syntax/#proximity_search "Web Search Syntax"], visited 23 December 2009</ref>
* [[Yandex]] uses the syntax <tt>keyword1 /n keyword2</tt> to search for two keywords separated by at most <math>n - 1</math> words, and supports a few other variations of this syntax.<ref>[http://help.yandex.ru/search/?id=481939 Yandex help page on query language] (in Russian)</ref>
* [[Yahoo!]] and [[Altavista]] both support an undocumented NEAR operator.<ref>[http://search.yahoo.com/search?p=site%3Awww.rfc-editor.org+inurl%3Arfc2606+guidance+NEAR+additional "Successful Yahoo! proximity query"] (22 Feb 2010)</ref><ref>[http://search.yahoo.com/search?p=site%3Awww.rfc-editor.org+inurl%3Arfc2606+guidance+NEAR+unused "Unsuccessful Yahoo! proximity query"] (22 Feb 2010)</ref> The syntax is <tt>keyword1 NEAR keyword2</tt>.
* Google supports AROUND(#).<ref>[http://www.guidingtech.com/16116/google-search-little-known-around-operator/ "GuidingTech: Meet Google Search's Little Known AROUND Operator"]</ref>

Ordered search within the [[Google]] and [[Yahoo!]] search engines is possible using the asterisk (*) full-word [[Wildcard character|wildcard]]s: in Google this matches one or more words,<ref>[http://www.google.com/support/websearch/bin/answer.py?answer=136861 "More Google Search Help" visited 23 December 2009]</ref> and an in Yahoo! Search this matches exactly one word.<ref>[http://www.searchengineshowdown.com/features/yahoo/review.html "Review of Yahoo! Search", by Search Engine Showdown, visited 23 December 2009]</ref>  (This is easily verified by searching for the following phrase in both Google and Yahoo!: "addictive * of biblioscopy".)

To emulate unordered search of the NEAR operator can be done using a combination of ordered searches.  For example, to specify a close co-occurrence of "house" and "dog", the following search-expression could be specified: "house dog" OR "dog house" OR "house * dog" OR "dog * house" OR "house * * dog" OR "dog * * house".

== See also ==
* [[Compound term processing]]
* [[Edit distance]]
* [[Information retrieval]]
* [[Search engine]]
* [[Search engine indexing]] - how texts are indexed to support proximity search
* [[Semantic proximity]]

== Notes ==
{{Reflist}}

[[Category:Information retrieval]]
[[Category:Internet search algorithms]]
>>EOP<<
7<|###|>European Conference on Information Retrieval
The '''European Conference on Information Retrieval''' (ECIR) is the main 
European research conference for the presentation of new results in the field of [[information retrieval]] (IR).
It is organized by the [[Information Retrieval Specialist Group]] of the [[British Computer Society]] (BCS-IRSG).
      
The event started its life as the ''Annual Colloquium on Information Retrieval Research'' in 1978 and was 
held in the UK each year until 1998 when it was hosted in Grenoble, France. Since then the venue has
alternated between the United Kingdom and continental Europe. To mark the metamorphosis
from a small informal colloquium to a major event in the IR research calendar, the 
BCS-IRSG later renamed the event to ''European Conference on Information Retrieval''. In recent years,
ECIR has continued to grow and has become the major European forum for the discussion
of research in the field of Information Retrieval. 

Some of the topics dealt with include:
* IR models, techniques, and algorithms
* IR applications
* IR system architectures
* Test and evaluation methods for IR
* [[Natural Language Processing]] for IR
* Distributed IR
* Multimedia and cross-media IR

==Time and Location==

Traditionally, the ECIR is held in Spring, near the Easter weekend. Previous locations include
the following:

* [[Amsterdam, Netherlands]], 2014 [http://ecir2014.org/]
* [[Moscow, Russia]], 2013 [http://ecir2013.org/]
* [[Barcelona, Spain]], 2012 [http://ecir2012.upf.edu/]
* [[Dublin, Ireland]], 2011 [http://www.ecir2011.dcu.ie/]
* [[Milton Keynes]], 2010 [http://kmi.open.ac.uk/events/ecir2010/]
* [[Toulouse]], 2009 [http://ecir09.irit.fr/]
* [[Glasgow]], 2008 [http://ecir2008.dcs.gla.ac.uk/]
* [[Rome]], 2007 [http://ecir2007.fub.it/]
* [[London]], 2006 [http://ecir2006.soi.city.ac.uk/]
* [[Santiago de Compostela|Santiago]], 2005 [http://www-gsi.dec.usc.es/ecir05/]
* [[Sunderland, Tyne and Wear|Sunderland]], 2004 [http://ecir04.sunderland.ac.uk/]
* [[Pisa]], 2003 [http://ecir03.isti.cnr.it/]
* [[Glasgow]], 2002 [http://irsg.bcs.org/past_ecir.php]*
* [[Darmstadt]], 2001* (organized by GMD)
* [[Cambridge]], 2000* (organized by Microsoft Research)
* [[Glasgow]], 1999*
* [[Grenoble]], 1998*
* [[Aberdeen, Scotland|Aberdeen]], 1997*
* [[Manchester]], 1996*
* [[Crewe]], 1995* (organized by Manchester Metropolitan University)
* [[Drymen]], Scotland, 1994* (organized by Strathclyde University)
* [[Glasgow]], 1993* (organized by Strathclyde University)
* [[Lancaster, Lancashire|Lancaster]], 1992*
* [[Lancaster, Lancashire|Lancaster]], 1991*
* [[Huddersfield]], 1990*
* [[Huddersfield]], 1989*
* [[Huddersfield]], 1988*
* [[Glasgow]], 1987*
* [[Glasgow]], 1986*
* [[Bradford]], 1985*
* [[Bradford]], 1984*
* [[Sheffield]], 1983*
* [[Sheffield]], 1982*
* [[Birmingham]], 1981*
* [[Leeds]], 1980*
* [[Leeds]], 1979*

<br /> *as the Annual Colloquium on Information Retrieval Research

Future locations include:
* [[Vienna, Austria]], 2015 [http://www.ecir2015.org/]

==External links==
* [http://irsg.bcs.org/ecir.php Official page at the website of the British Computer Society]

[[Category:Information retrieval]]
[[Category:Computer science conferences]]
>>EOP<<
13<|###|>Mooers' law
{{For|the observation regarding integrated circuits|Moore's law}}
{{Refimprove|date=September 2011}}

'''Mooers' law''' is an empirical observation of behavior made by American [[computer scientist]] [[Calvin Mooers]] in 1959. The observation is made in relation to [[information retrieval]] and the interpretation of the observation is used commonly throughout the information profession both within and outside its original context.

{{quote|An information retrieval system will tend not to be used whenever it is more painful and troublesome for a customer to have information than for him not to have it.|[[Calvin Mooers]]<ref name="morville">{{cite book|url=http://books.google.com/books?id=xJNLJXXbhusC&printsec=frontcover&dq=isbn:9780596007652&hl=en&sa=X&ei=qvWhT5DfHITs2QX1rNzPCA&ved=0CDAQ6AEwAA#v=onepage&q=mooers'%20law&f=false |title= Ambient findability |series= O'Reilly Series. Marketing/Technology & Society |author= Peter Morville |edition= illustrated |publisher= O'Reilly Media |year= 2005 |page= 44|isbn= 978-0-596-00765-2}}</ref>}}

==Original interpretation==

Mooers argued that information is at risk of languishing unused due not only on the effort required to assimilate it but also to any fallout that could arise from the discovery of information that conflicts with the users personal, academic or corporate interests. In interacting with new information, a user runs the risk of proving their work incorrect or even irrelevant. Instead, Mooers argued, users prefer to remain in a state of safety in which new arguments are ignored in an attempt to save potential embarrassment or reprisal from supervisors.<ref>{{cite web|last=Mooers|first=Calvin|title=Mooers Law, or Why some Retrieval Systems are Used and Others Are not|url=http://findarticles.com/p/articles/mi_qa3633/is_199610/ai_n8749122/|work=Business Library|accessdate=25 October 2011}}</ref>

==Out-of-context interpretation==

The more commonly used interpretation of Mooers' law is considered to be a derivation of the [[principle of least effort]] first stated by [[George Kingsley Zipf]]. This interpretation focuses on the amount of effort that will be expended to use and understand a particular information retrieval system before the information seeker 'gives up', and the Law is often paraphrased to increase the focus on the retrieval system:

{{quote|The more difficult and time consuming it is for a customer to use an information system, the less likely it is that he will use that information system.|J. Michael Pemberton}}
{{quote|Mooers' Law tells us that information will be used in direct proportion to how easy it is to obtain.|Roger K. Summit <ref name="morville"/>}}

In this interpretation, "painful and troublesome" comes from ''using'' the retrieval system.

==References==
{{reflist}}

*{{cite journal |last=Austin |first=Brice |date=June 2001 |title=Mooers' Law: In and out of Context |journal=Journal of the American Society for Information Science and Technology |volume=25 |issue=8 |pages=pp 607609 |url=http://spot.colorado.edu/~norcirc/Mooers.html |accessdate=2007-05-23 |doi=10.1002/asi.1114}}

==External links==
* [http://special.lib.umn.edu/findaid/xml/cbi00081.xml Calvin N. Mooers Papers, 1930-1992] at the [[Charles Babbage Institute]], University of Minnesota.
* [http://purl.umn.edu/107510 Oral history interview with Calvin N. Mooers and Charlotte D. Mooers] at the [[Charles Babbage Institute]].  Interview discusses information retrieval and programming language research from World War II through the early 1990s.
* [http://www.phillyimc.org/en/gasoline-7-17-moors-law-kent-moors-authority Another empirical observation with a similar-sounding name is Moors' law], named for Kent Moors of Duquesne University, which states crude oil prices double every five years. 
[[Category:Empirical laws]]
[[Category:Library science]]
[[Category:Information retrieval]]
>>EOP<<
19<|###|>Information retrieval applications
Areas where [[information retrieval]] techniques are employed include (the entries are in alphabetical order within each category):

==General applications of information retrieval==
* [[Digital libraries]]
*  [[Information filtering]]
** [[Recommender systems]]
*  Media search
** Blog search
** [[Image retrieval]]
** [[Music information retrieval|Music retrieval]]
** News search
** Speech retrieval
** Video retrieval
* [[Search engines]]
** [[Desktop search]]
** [[Enterprise search]]
** [[Federated search]]
** [[Mobile search]]
** [[Social search]]
** [[Web search engine|Web search]]

==Domain specific applications of information retrieval==
* Expert search finding
* Genomic information retrieval
* [[Geographic information retrieval]]
*  Information retrieval for chemical structures
* Information retrieval in [[software engineering]]
* [[Legal information retrieval]]
* [[Vertical search]]

==Other retrieval methods==
Methods/Techniques in which [[information retrieval]] techniques are employed include:
* [[Adversarial information retrieval]]
* [[Automatic summarization]]
**[[Multi-document summarization]]
* [[Compound term processing]]
* [[Cross-language information retrieval|Cross-lingual retrieval]]
* [[Document classification]]
* [[Spam filtering]]
* [[Question answering]]

== See also ==
* [[Information retrieval]]

{{DEFAULTSORT:Information Retrieval Applications}}
[[Category:Information retrieval|*]]
>>EOP<<
25<|###|>Precision and recall
[[File:Precisionrecall.svg|thumb|350px|Precision and recall]]
In [[pattern recognition]] and [[information retrieval]] with [[binary classification]], '''precision''' (also called [[positive predictive value]]) is the fraction of retrieved instances that are relevant, while '''recall''' (also known as [[Sensitivity and specificity|sensitivity]]) is the fraction of relevant instances that are retrieved. Both precision and recall are therefore based on an understanding and measure of [[relevance]]. Suppose a program for recognizing dogs in scenes from a video identifies 7 dogs in a scene containing 9 dogs and some cats. If 4 of the identifications are correct, but 3 are actually cats, the program's precision is 4/7 while its recall is 4/9.  When a search engine returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3.

In [[statistics]], if the [[null hypothesis]] is that all and only the relevant items are retrieved, absence of [[type I and type II errors]] corresponds respectively to maximum precision (no false positive) and maximum recall (no false negative).  The above pattern recognition example contained 7 &minus; 4 = 3 type I errors and 9 &minus; 4 = 5 type II errors.  Precision can be seen as a measure of exactness or ''quality'', whereas recall is a measure of completeness or ''quantity''.

In simple terms, high '''precision''' means that an algorithm returned substantially more relevant results than irrelevant, while high '''recall''' means that an algorithm returned most of the relevant results.

==Introduction==
As an example, in an [[information retrieval]] scenario, the instances are documents and the task is to return a set of relevant documents given a search term; or equivalently, to assign each document to one of two categories, "relevant" and "not relevant".  In this case, the "relevant" documents are simply those that belong to the "relevant" category.  Recall is defined as the ''number of relevant documents'' retrieved by a search ''divided by the total number of existing relevant documents'', while precision is defined as the ''number of relevant documents'' retrieved by a search ''divided by the total number of documents retrieved'' by that search.

In a [[classification (machine learning)|classification]] task, the precision for a class is the ''number of '''true positives''''' (i.e. the ''number of items correctly labeled as belonging to the positive class'') ''divided by the total number of elements labeled as belonging to the positive class'' (i.e. the sum of true positives and '''[[Type I and type II errors|false positives]]''', which are items incorrectly labeled as belonging to the class).  Recall in this context is defined as the ''number of true positives'' ''divided by the total number of elements that actually belong to the positive class'' (i.e. the sum of true positives and '''[[Type I and type II errors|false negatives]]''', which are items which were not labeled as belonging to the positive class but should have been).

In information retrieval, a perfect precision score of 1.0 means that every result retrieved by a search was relevant (but says nothing about whether all relevant documents were retrieved) whereas a perfect recall score of 1.0 means that all relevant documents were retrieved by the search (but says nothing about how many irrelevant documents were also retrieved).

In a classification task, a precision score of 1.0 for a class C means that every item labeled as belonging to class C does indeed belong to class C (but says nothing about the number of items from class C that were not labeled correctly) whereas a recall of 1.0 means that every item from class C was labeled as belonging to class C (but says nothing about how many other items were incorrectly also labeled as belonging to class C).

Often, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other. Brain surgery provides an obvious example of the tradeoff.  Consider a brain surgeon tasked with removing a cancerous tumor from a patients brain. The surgeon needs to remove all of the tumor cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain she removes to ensure she has extracted all the cancer cells. This decision increases recall but reduces precision.  On the other hand, the surgeon may be more conservative in the brain she removes to ensure she extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome).  Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome).

Usually, precision and recall scores are not discussed in isolation. Instead, either values for one measure are compared for a fixed level at the other measure (e.g. ''precision at a recall level of 0.75'') or both are combined into a single measure. Examples for measures that are a combination of precision and recall are the [[Precision and recall#F-measure|F-measure]] (the weighted [[harmonic mean]] of precision and recall), or the [[Matthews correlation coefficient]], which is a [[geometric mean]] of the chance-corrected variants: the [[regression coefficient]]s Informedness (DeltaP') and Markedness (DeltaP).<ref name="Powers2007">{{cite journal |first=David M W |last=Powers |date=2007/2011 |title=Evaluation: From Precision, Recall and F-Factor  to ROC, Informedness, Markedness & Correlation |journal=Journal of Machine Learning Technologies |volume=2 |issue=1 |pages=3763 |url=http://www.bioinfo.in/uploadfiles/13031311552_1_1_JMLT.pdf}}</ref><ref>{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97119 |doi=10.1016/s0911-6044(03)00059-9}}</ref> [[Accuracy and precision#In binary classification|Accuracy]] is a weighted arithmetic mean of Precision and Inverse Precision (weighted by Bias) as well as a weighted arithmetic mean of Recall and Inverse Recall (weighted by Prevalence).<ref name="Powers2007"/> Inverse Precision and Recall are simply the Precision and Recall of the inverse problem where positive and negative labels are exchanged (for both real classes and prediction labels).  Recall and Inverse Recall, or equivalently true positive rate and false positive rate, are frequently plotted against each other as [[Receiver operating characteristic|ROC]] curves and provide a principled mechanism to explore operating point tradeoffs. Outside of Information Retrieval, the application of Recall, Precision and F-measure are argued to be flawed as they ignore the true negative cell of the contingency table, and they are easily manipulated by biasing the predictions.<ref name="Powers2007"/>  The first problem is 'solved' by using [[Accuracy and precision#In binary classification|Accuracy]] and the second problem is 'solved' by discounting the chance component and renormalizing to [[Cohen's kappa]], but this no longer affords the opportunity to explore tradeoffs graphically. However, Informedness and Markedness are Kappa-like renormalizations of Recall and Precision,<ref>{{cite conference |first=David M. W. |last=Powers |date=2012 |title=The Problem with Kappa |booktitle=Conference of the European Chapter of the Association for Computational Linguistics (EACL2012) Joint ROBUS-UNSUP Workshop}}</ref> and their geometric mean [[Matthews correlation coefficient]] thus acts like a debiased F-measure.

== Definition (information retrieval context) ==

In [[information retrieval]] contexts, precision and recall are defined in terms of a set of '''retrieved documents''' (e.g. the list of documents produced by a [[web search engine]] for a query) and a set of '''relevant documents''' (e.g. the list of all documents on the internet that are relevant for a certain topic), cf. [[relevance]].

===[[Positive predictive value | Precision]]===

In the field of [[information retrieval]], '''precision''' is the fraction of retrieved documents that are [[Relevance (information retrieval)|relevant]] to the find:

:<math> \text{precision}=\frac{|\{\text{relevant documents}\}\cap\{\text{retrieved documents}\}|}{|\{\text{retrieved documents}\}|} </math>

Precision takes all retrieved documents into account, but it can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called '''precision at n''' or '''P@n'''.

For example for a text search on a set of documents precision is the number of correct results divided by the number of all returned results.

Precision is also used with [[recall (information retrieval)|recall]], the percent of ''all'' relevant documents that is returned by the search. The two measures are sometimes used together in the [[F1 Score]] (or f-measure) to provide a single measurement for a system.

Note that the meaning and usage of "precision" in the field of Information Retrieval differs from the definition of [[accuracy and precision]] within other branches of science and technology.

===Recall===

Recall in information retrieval is the fraction of the documents that are relevant to the query that are successfully retrieved.

:<math> \text{recall}=\frac{|\{\text{relevant documents}\}\cap\{\text{retrieved documents}\}|}{|\{\text{relevant documents}\}|} </math>

For example for text search on a set of documents recall is the number of correct results divided by the number of results that should have been returned

In binary classification, recall is called [[Sensitivity_and_specificity#Sensitivity|sensitivity]]. So it can be looked at as the probability that a relevant document is retrieved by the query.

It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.

== Definition (classification context) ==
{| class="wikitable" align="right" width=35% style="font-size:98%; margin-left:0.5em; padding:0.25em; background:#f1f5fc;"
|+ Terminology and derivations<br 
/>from a confusion matrix
|- valign=top
|
; true positive (TP)
:eqv. with hit
; true negative (TN)
:eqv. with correct rejection
; false positive (FP)
:eqv. with [[false alarm]], [[Type I error]]
; false negative (FN)
:eqv. with miss, [[Type II error]]
--------------------------------------------------------
; [[sensitivity (test)|sensitivity]] or true positive rate (TPR)
:eqv. with [[hit rate]], [[Information retrieval#Recall|recall]]
:<math>\mathit{TPR} = \mathit{TP} / P = \mathit{TP} / (\mathit{TP}+\mathit{FN})</math>
; [[Specificity (tests)|specificity]] (SPC) or True Negative Rate
:<math>\mathit{SPC} = \mathit{TN} / N = \mathit{TN} / (\mathit{FP} + \mathit{TN}) </math>
; [[Information retrieval#Precision|precision]] or [[positive predictive value]] (PPV)
:<math>\mathit{PPV} = \mathit{TP} / (\mathit{TP} + \mathit{FP})</math>
; [[negative predictive value]] (NPV)
:<math>\mathit{NPV} = \mathit{TN} / (\mathit{TN} + \mathit{FN})</math>
; [[Information retrieval#Fall-out|fall-out]] or false positive rate (FPR)
:<math>\mathit{FPR} = \mathit{FP} / N = \mathit{FP} / (\mathit{FP} + \mathit{TN})</math>
; [[false discovery rate]] (FDR)
:<math>\mathit{FDR} = \mathit{FP} / (\mathit{FP} + \mathit{TP}) = 1 - \mathit{PPV} </math>
; [[false negative rate]] (FNR)
:<math>\mathit{FNR} = \mathit{FN} / (\mathit{FN} + \mathit{TP}) = 1 - \mathit{TPR} </math>
------------------------------------------------
; [[accuracy]] (ACC)
:<math>\mathit{ACC} = (\mathit{TP} + \mathit{TN}) / (P + N)</math>
;[[F1 score]]
: is the [[Harmonic mean#Harmonic mean of two numbers|harmonic mean]] of [[Information retrieval#Precision|precision]] and [[sensitivity (test)|sensitivity]]
:<math>\mathit{F1} = 2 \mathit{TP} / (2 \mathit{TP} + \mathit{FP} + \mathit{FN})</math>
; [[Matthews correlation coefficient]] (MCC)
:<math> \frac{ TP \times TN - FP \times FN } {\sqrt{ (TP+FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }
</math>
;
<span style="font-size:90%;">''Source: Fawcett (2006).''<ref name=Fawcelt2006>{{cite journal|last=Fawcelt|first=Tom|title=An Introduction to ROC Analysis|journal=Pattern Recognition Letters|date=2006|volume=27|issue=8|pages=861 - 874|doi=10.1016/j.patrec.2005.10.010}}</ref></span>
|}

For classification tasks, the terms '''true positives''', '''true negatives''', '''false positives''', and '''false negatives''' (see also [[Type I and type II errors]]) compare the results of the classifier under test with trusted external judgments.  The terms ''positive'' and ''negative'' refer to the classifier's prediction (sometimes known as the ''expectation''), and the terms ''true'' and ''false'' refer to whether that prediction corresponds to the external judgment (sometimes known as the ''observation''). 

Let us define an experiment from '''P''' positive instances and '''N''' negative instances for some condition. The four outcomes can be formulated in a 22 ''[[contingency table]]'' or ''[[confusion matrix]]'', as follows:

{{DiagnosticTesting_Diagram}}

<!--
{| border="0" align="center" style="text-align: center; background: #FFFFFF;"
|+
!
! colspan="2" style="background: #ddffdd;"|actual class <br/> (observation)
|-
!
|-----
|+
! rowspan="2" style="background: #ffdddd;"|predicted class <br/> (expectation)
| '''tp''' <br> (true positive) <br/> Correct result
| '''fp''' <br> (false positive) <br/> Unexpected result
|-bgcolor="#EFEFEF"
| '''fn''' <br> (false negative) <br/> Missing result
| '''tn''' <br> (true negative) <br/> Correct absence of result
|+
|}

-->



Precision and recall are then defined as:<ref name="OlsonDelen">Olson, David L.; and Delen, Dursun (2008); ''Advanced Data Mining Techniques'', Springer, 1st edition (February 1, 2008), page 138, ISBN 3-540-76916-1</ref>

: <math>\text{Precision}=\frac{tp}{tp+fp} \, </math>

: <math>\text{Recall}=\frac{tp}{tp+fn} \, </math>

Recall in this context is also referred to as the true positive rate or [[Sensitivity and specificity|sensitivity]], and precision is also referred to as [[positive predictive value]] (PPV); other related measures used in classification include true negative rate and [[Accuracy_and_precision#In_binary_classification|accuracy]].<ref name="OlsonDelen" /> True negative rate is also called [[Specificity_(tests)#Specificity|specificity]].

: <math>\text{True negative rate}=\frac{tn}{tn+fp} \, </math>

: <math>\text{Accuracy}=\frac{tp+tn}{tp+tn+fp+fn} \, </math>

== Probabilistic interpretation ==

It is possible to interpret precision and recall not as ratios but as probabilities:

* '''Precision''' is the probability that a (randomly selected) retrieved document is relevant.

* '''Recall''' is the probability that a (randomly selected) relevant document is retrieved in a search.

Note that the random selection refers to a uniform distribution over the appropriate pool of documents; i.e. by '''randomly selected retrieved document''', we mean selecting a document from the set of retrieved documents in a random fashion. The random selection should be such that all documents in the set are equally likely to be selected. 

Note that, in a typical classification system, the probability that a retrieved document is relevant depends on the document. The above interpretation extends to that scenario also (needs explanation). 

Another interpretation for precision and recall is as follows. Precision is the average probability of relevant retrieval. Recall is the average probability of complete retrieval. Here we average over multiple retrieval queries.

== F-measure ==
{{main|F1 score}}
A measure that combines precision and recall is the [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score:

: <math>F = 2 \cdot \frac{\mathrm{precision} \cdot \mathrm{recall}}{ \mathrm{precision} + \mathrm{recall}}</math>

There are several reasons that the F-score can be criticized in particular circumstances due to its bias as an evaluation metric. <ref>{{cite journal|last=POWERS|first=D.M.W.|title=EVALUATION: FROM PRECISION, RECALL AND F-MEASURE TO ROC, INFORMEDNESS, MARKEDNESS & CORRELATION|journal=Journal of Machine Learning Technologies|date=February 27, 2011|volume=2|issue=1|pages=37-63|url=http://www.bioinfo.in/contents.php?id=51}}</ref> This is also known as the <math>F_1</math> measure, because recall and precision are evenly weighted.

It is a special case of the general <math>F_\beta</math> measure (for non-negative real values of&nbsp;<math>\beta</math>):

:<math>F_\beta = (1 + \beta^2) \cdot \frac{\mathrm{precision} \cdot \mathrm{recall} }{ \beta^2 \cdot \mathrm{precision} + \mathrm{recall}}</math>

Two other commonly used <math>F</math> measures are the <math>F_2</math> measure, which weights recall higher than precision, and the <math>F_{0.5}</math> measure, which puts more emphasis on precision than recall.

The F-measure was derived by van Rijsbergen (1979) so that <math>F_\beta</math> "measures the effectiveness of retrieval with respect to a user who attaches <math>\beta</math> times as much importance to recall as precision".  It is based on van Rijsbergen's effectiveness measure <math>E = 1 - \frac{1}{\frac{\alpha}{P} + \frac{1-\alpha}{R}}</math>.  Their relationship is <math>F_\beta = 1 - E</math> where <math>\alpha=\frac{1}{1 + \beta^2}</math>.

==Limitations as goals==
There are other parameters and strategies for performance metric of information retrieval system, such as the area under the precision-recall curve (AUC).<ref>Zygmunt Zajac. What you wanted to know about AUC.  http://fastml.com/what-you-wanted-to-know-about-auc/</ref> 

For [[web document]] retrieval, if the user's objectives are not clear, the  precision and recall can't be optimized. As summarized by Lopresti,<ref>Lopresti, Daniel (2001); [http://www.csc.liv.ac.uk/~wda2001/Panel_Presentations/Lopresti/Lopresti_files/v3_document.htm ''WDA 2001 panel'']</ref>
:''"[[Browsing]] is a comfortable and powerful paradigm (the [[Serendipity|serendipity effect]]).''
:* ''Search results don't have to be very good.''
:* ''Recall?    Not important (as long as you get at least some good hits).''
:* ''Precision? Not important (as long as at least some of the hits on the first page you return are good)."''

==See also==
* [[Binary classification]]
* [[Information retrieval]]
* [[Receiver operating characteristic]]
* [[Relevance]]
* [[Sensitivity and specificity]]
* [[Type I and type II errors]], where ''false positives'' and ''false negatives'' are defined
* [[Uncertainty coefficient]], aka Proficiency

== Sources ==
<references>
* Baeza-Yates, Ricardo; Ribeiro-Neto, Berthier (1999). ''Modern Information Retrieval''. New York, NY: ACM Press, Addison-Wesley, Seiten 75 ff. ISBN 0-201-39829-X
* Hjrland, Birger (2010); ''The foundation of the concept of relevance'', Journal of the American Society for Information Science and Technology, 61(2), 217-237
* Makhoul, John; Kubala, Francis; Schwartz, Richard; and Weischedel, Ralph (1999); [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.4637 ''Performance measures for information extraction''], in ''Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February 1999''
* van Rijsbergen, Cornelis Joost "Keith" (1979); ''Information Retrieval'', London, GB; Boston, MA: Butterworth, 2nd Edition, ISBN 0-408-70929-4
</references>

== External links ==
* [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval  C. J. van Rijsbergen 1979]
* [http://www.text-analytics101.com/2014/10/computing-precision-and-recall-for.html Computing Precision and Recall for a Multi-class Classification Problem]

[[Category:Information retrieval]]
[[Category:Information science]]
[[Category:Bioinformatics]]
[[Category:Summary statistics for contingency tables]]

[[de:Beurteilung eines Klassifikators#Anwendung im Information Retrieval]]
>>EOP<<
31<|###|>Term Discrimination
'''Term Discrimination''' is a way to rank keywords in how useful they are for [[Information Retrieval]].

== Overview ==

This is a method similar to [[tf-idf]] but it deals with finding keywords suitable for [[information retrieval]] and ones that are not.  Please refer to [[Vector Space Model]] first.

This method uses the concept of ''Vector Space Density'' that the less dense an [[occurrence matrix]] is, the better an information retrieval query will be.

An optimal index term is one that can distinguish two different documents from each other and relate two similar documents.  On the other hand, a sub-optimal index term can not distinguish two different document from two similar documents.  

The discrimination value is the difference in the occurrence matrix's vector-space density versus the same matrix's vector-space without the index term's density.

 Let:
 <math>A</math> be the occurrence matrix
 <math>A_k</math> be the occurrence matrix without the index term <math>k</math>
 and <math>Q(A)</math> be density of <math>A</math>.
 Then:
 The discrimination value of the index term <math>k</math> is: 
 <math>DV_k = Q(A) - Q(A_k)</math>

== How to compute ==

Given an [[occurrency matrix]]: <math>A</math> and one keyword: <math>k</math>
* Find the global document [[centroid]]: <math>C</math> (this is just the average document vector)
* Find the average [[euclidean distance]] from every document vector, <math>D_i</math> to <math>C</math>
* Find the average euclidean distance from every document vector, <math>D_i</math> to <math>C</math> ''IGNORING'' <math>k</math>
* The difference between the two values in the above step is the ''discrimination value'' for keyword <math>K</math>

A higher value is better because including the keyword will result in better information retrieval.

== Qualitative Observations ==
Keywords that are ''[[Sparse matrix|sparse]]'' should be poor discriminators because they have poor ''[[Precision and recall|recall]],''
whereas
keywords that are ''frequent'' should be poor discriminators because they have poor ''[[Precision and recall|precision]].''

== References ==
* [[Gerard Salton|G. Salton]], A. Wong, and C. S. Yang (1975), "[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing]," ''Communications of the ACM'', vol. 18, nr. 11, pages 613620. ''(The article in which the vector space model was first presented)''

* Can, F., Ozkarahan, E. A (1987), "Computation of term/document discrimination values by use of the cover coefficient concept." ''Journal of the American Society for Information Science'', vol. 38, nr. 3, pages 171-183.

[[Category:Information retrieval]]
>>EOP<<
37<|###|>Concept Searching Limited
{{Infobox company |
  name   = Concept Searching Limited |
  logo = [[Image:conceptSearching.jpg]] |
  company_slogan = "Retrieval Just Got Smarter" |
  type   =  [[Privately held company|Private]] |
  foundation     = 2002|
  location       = [[UK]], [[USA]] |
  area_served    = Global |
  industry       = [[Information retrieval]] |
  products       = conceptSearch<br/>conceptClassifier<br/>conceptClassifier for SharePoint<br/>conceptClassifier for SharePoint Online<br/>Taxonomy Manager<br/>Taxonomy Workflow |
  homepage       = [http://www.conceptsearching.com/ www.conceptsearching.com]
}}

'''Concept Searching Limited''' is a [[software company]] which specializes in [[information retrieval]] software. It has products for [[Enterprise search]], Taxonomy Management and  [[Statistical classification]].

==History==
Concept Searching was founded in 2002 in the UK and now has offices in the USA and South Africa. In August 2003 the company introduced the idea of using [[Compound term processing]].<ref>[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&ETOC=RN Lateral thinking in information retrieval] ''Information Management and Technology.'' 2003. vol 36; part 4, pp 169-173</ref><ref>[http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf] Lateral Thinking in Information Retrieval</ref>

Compound term processing allows statistical information retrieval applications to perform matching using multi-word concepts. This can improve the quality of search results and also allows unstructured information to be automatically classified with semantic metadata.<ref>[http://airforcemedicine.afms.mil/711hswom/InterSymp2008/AFMS%20-%20InterSymp%202008.html] US Air Force Medical Service presentation at InterSymp-2008</ref>

The company's products run on the Microsoft [[.NET Framework|.NET]] platform. The products integrate with Microsoft [[SharePoint]] and many other platforms.<ref>[http://pinpoint.microsoft.com/en-US/partners/Concept-Searching-Inc-4297066101] Microsoft Partner Profile</ref>

Concept Searching has developed the '''Smart Content Framework''', which is a toolset that provides an enterprise framework to mitigate risk, automate processes, manage information, protect privacy, and address compliance issues. The Smart Content Framework is used by many large organizations including 23,000 users at the [[NASA]] Safety Center <ref>[http://www.aiim.org/About/News/CS-NASA-Safety] NASA Safety Center using Smart Content Framework</ref>

== Awards ==
* 100 Companies that Matter in Knowledge Management 2009/2010/2011/2012/2013/2014 <ref>{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-100-Companies-That-Matter-in-Knowledge-Management-94933.aspx |title=KMWorld Magazine}}</ref>
* KMWorld Trend-Setting Products of 2009/2010/2011/2012/2013/2014 <ref>{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-Trend-Setting-Products-of-2014-98792.aspx |title=Trend-Setting Products}}</ref>

==See also==
* [[Compound term processing]]
* [[Enterprise search]]
* [[Full text search]]
* [[Information retrieval]]
* [[Concept Search]]

==References==
{{Reflist}}

==External links==
*[http://www.conceptsearching.com/ Company Website]

[[Category:Information retrieval]]
[[Category:Privately held companies of the United Kingdom]]
>>EOP<<
43<|###|>SimRank
'''SimRank''' is a general [[Semantic similarity|similarity measure]], based on a simple and intuitive [[Graph theory|graph-theoretic model]].
SimRank is applicable in any [[Domain model|domain]] with object-to-object [[Relation (mathematics)|relationships]], that measures similarity of the structural context in which objects occur, based on their relationships with other objects.
Effectively, SimRank is a measure that says "'''two objects are considered to be similar if they are referenced by similar objects'''."

== Introduction ==

Many [[Application software|applications]] require a measure of "similarity" between objects.
One obvious example is the "find-similar-document" query,
on traditional text corpora or the [[World Wide Web|World-Wide Web]].
More generally, a similarity measure can be used to [[Cluster analysis|cluster objects]], such as for [[collaborative filtering]] in a [[recommender system]], in which similar users and items are grouped based on the users preferences.

Various aspects of objects can be used to determine similarity, usually depending on the domain and the appropriate definition of similarity for that domain.
In a [[Text corpus|document corpus]], matching text may be used, and for collaborative filtering, similar users may be identified by common preferences.
SimRank is a general approach that exploits the object-to-object relationships found in many domains of interest.
On the [[World Wide Web|Web]], for example, two pages are related if there are [[hyperlink]]s between them.
A similar approach can be applied to scientific papers and their citations, or to any other document corpus with [[cross-reference]] information.
In the case of recommender systems, a users preference for an item constitutes a relationship between the user and the item.
Such domains are naturally modeled as [[Graph (mathematics)|graphs]], with [[Vertex (graph theory)|nodes]] representing objects and [[Edge (graph theory)#Graph|edges]] representing relationships.

The intuition behind the SimRank algorithm is that, in many domains, '''similar objects are referenced by similar objects'''.
More precisely, objects <math>a</math> and <math>b</math> are considered to be similar if they are pointed from objects <math>c</math> and <math>d</math>, respectively, and <math>c</math> and <math>d</math> are themselves similar.
The [[Recursion (computer science)#Recursive programming|base case]] is that objects are maximally similar to themselves
.<ref name=jeh_widom>G. Jeh and J. Widom. SimRank: A Measure of Structural-Context Similarity. In [[SIGKDD|KDD'02]]: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 538-543. [[Association for Computing Machinery|ACM Press]], 2002. [http://www-cs-students.stanford.edu/~glenj/simrank.pdf]</ref>

It is important to note that SimRank is a general algorithm that determines only the similarity of structural context.
SimRank applies to any domain where there are enough relevant relationships between objects to base at least some notion of similarity on relationships.
Obviously, similarity of other domain-specific aspects are important as well; these can  and should be combined with relational structural-context similarity for an overall similarity measure.
For example, for [[Web page]]s SimRank can be combined with traditional textual similarity; the same idea applies to scientific papers or other document corpora.
For recommendation systems, there may be built-in known similarities between items (e.g., both computers, both clothing, etc.), as well as similarities between users (e.g., same gender, same spending level).
Again, these similarities can be combined with the similarity scores that are computed based on preference patterns, in order to produce an overall similarity measure.

== Basic SimRank equation ==

For a node <math>v</math> in a directed graph, we denote by <math>I(v)</math> and <math>O(v)</math> the set of in-neighbors and out-neighbors of <math>v</math>, respectively.
Individual in-neighbors are denoted as <math>I_i(v)</math>, for <math>1 \le i \le \left|I(v)\right|</math>, and individual
out-neighbors are denoted as <math>O_i(v)</math>, for <math>1 \le i \le \left|O(v)\right|</math>.

Let us denote the similarity between objects <math>a</math> and <math>b</math> by <math>s(a, b) \in [0, 1]</math>. 
Following the earlier motivation, a recursive equation is written for <math>s(a, b)</math>.
If <math>a = b</math> then <math>s(a, b)</math> is defined to be <math>1</math>.
Otherwise,
:<math>s(a, b) = \frac{C}{\left|I(a)\right| \left|I(b)\right|}
 \sum_{i=1}^{\left|I(a)\right|}\sum_{j=1}^{\left|I(b)\right|}
 s(I_i(a), I_j(b))</math>
where <math>C</math> is a constant between <math>0</math> and <math>1</math>.
A slight technicality here is that either <math>a</math> or <math>b</math> may not have any in-neighbors.
Since there is no way to infer any similarity between <math>a</math> and <math>b</math> in this case, similarity is set to <math>s(a, b) = 0</math>, so the summation in the above equation is defined to be <math>0</math> when <math>I(a) = \emptyset</math> or <math>I(b) = \emptyset</math>.

== Matrix representation of SimRank ==

Let <math>\mathbf{S}</math> be the similarity matrix whose entry <math>[\mathbf{S}]_{a,b}</math> denotes the similarity score <math>s(a,b)</math>, and <math>\mathbf{A}</math> be the column normalized adjacency matrix whose entry <math>[\mathbf{A}]_{a,b}=\tfrac{1}{|\mathcal{I}(b)|}</math> if there is an edge from <math>a</math> to <math>b</math>, and 0 otherwise. Then, in matrix notations, SimRank can be formulated as

:<math>
   {{\mathbf{S}}}= \max\{C\cdot (\mathbf{A}^{T} \cdot {{\mathbf{S}}}\cdot {{\mathbf{A}}} ) , {{\mathbf{I}}}\},</math>

where <math>\mathbf{I}</math> is an identity matrix.

== Computing SimRank ==

A solution to the SimRank equations for a graph <math>G</math> can be reached by [[Iterative method|iteration]] to a [[Fixed point (mathematics)|fixed-point]].
Let <math>n</math> be the number of nodes in <math>G</math>.
For each iteration <math>k</math>, we can keep <math>n^2</math> entries <math>s_k(*, *)</math>, where <math>s_k(a, b)</math> gives the score between <math>a</math> and <math>b</math> on iteration <math>k</math>.
We successively compute <math>s_{k+1}(*, *)</math> based on <math>s_k(*, *)</math>.
We start with <math>s_0(*, *)</math> where each <math>s_0(a, b)</math> is a lower bound on the actual SimRank score <math>s(a, b)</math>:
:<math> s_0(a, b) =
 \begin{cases}
  1 \mbox{  } , \mbox{    } \mbox{if } a = b  \mbox{  } , \\
  0 \mbox{  } , \mbox{    } \mbox{if } a \neq b \mbox{  } .
 \end{cases}</math>

To compute <math>s_{k+1}(a, b)</math> from <math>s_k(*, *)</math>, we use the basic SimRank equation to get:
:<math>s_{k + 1}(a, b) = 
 \frac{C}{\left|I(a)\right| \left|I(b)\right|}
 \sum_{i=1}^{\left|I(a)\right|}\sum_{j=1}^{\left|I(b)\right|}
  s_k(I_i(a), I_j(b))</math>
for <math>a \ne b</math>, and <math>s_{k+1}(a, b) = 1</math> for <math>a = b</math>.
That is, on each iteration <math>k + 1</math>, we update the similarity of <math>(a, b)</math> using the similarity scores of the neighbours of <math>(a, b)</math> from the previous iteration <math>k</math> according to the basic SimRank equation.
The values <math>s_k(*, *)</math> are [[Monotonic function|nondecreasing]] as <math>k</math> increases.
It was shown in <ref name="jeh_widom"/> that the values [[Limit of a sequence|converge]] to [[Limit of a sequence|limits]] satisfying the basic SimRank equation, the SimRank scores <math>s(*, *)</math>, i.e., for all <math>a, b \in V</math>, <math>\lim_{k \to \infty} s_k(a, b) = s(a, b)</math>.

The original SimRank proposal suggested choosing the decay factor <math>C = 0.8</math> and a fixed number <math>K = 5</math> of iterations to perform.
However, the recent research <ref name="lizorkin">D. Lizorkin, P. Velikhov, M. Grinev and D. Turdakov. Accuracy Estimate and Optimization Techniques for
SimRank Computation. In [[Very large database|VLDB '08]]: Proceedings of the 34th International Conference on Very Large Data Bases, pages 422--433. [http://modis.ispras.ru/Lizorkin/Publications/simrank_accuracy.pdf]</ref> showed that the given values for <math>C</math> and <math>K</math> generally imply relatively low [[Accuracy and precision|accuracy]] of iteratively computed SimRank scores.
For guaranteeing more accurate computation results, the latter paper suggests either using a smaller decay factor (in particular, <math>C = 0.6</math>) or taking more iterations.

== Partial Sums Memoization ==

The recent work of Lizorkin et al.<ref name="lizorkin"/> proposed three optimization techniques for speeding up the computation of SimRank:

(1) Essential nodes selection may eliminate the computation of a fraction of node pairs with a-priori zero scores.

(2) Partial sums memoization can effectively reduce repeated calculations of the similarity among different node pairs by caching part of similarity summations for later reuse.

(3) A threshold setting on the similarity enables a further reduction in the number of node pairs to be computed. 

In particular, the second observation of partial sums memoization plays a paramount role in greatly speeding up the computation of SimRank from <math>O(Kd^2n^2)</math> to <math>O(Kdn^2)</math>, where <math>K</math> is the number of iterations, <math>d</math> is average degree of a graph, and <math>n</math> is the number of nodes in a graph. The central idea of partial sums memoization consists of two steps:

First, the partial sums over <math>{\mathcal I}(a)</math> are memoized as
:<math>
Partial_{{\mathcal I}(a)}^{s_{k}}(j)=\sum_{i\in{\mathcal I}(a)}s_{k}(i,j), \qquad (\forall j \in {\mathcal I}(b))
</math>
and then <math>s_{k+1} (a,b)</math> is iteratively computed from <math>Partial_{{\mathcal I}(a)}^{s_{k}}(j)</math> as
:<math>
s_{k+1}( a,b )=\tfrac{C}{| \mathsf{\mathcal{I}}( a ) | | \mathsf{\mathcal{I}}( b ) |}\sum_{j \in \mathsf{\mathcal{I}}( b ) } Partial_{{\mathcal I}(a)}^{s_{k}}(j).
</math>
Consequently, the results of <math>Partial_{{\mathcal I}(a)}^{s_{k}}(j)</math>, <math>\forall j \in {\mathcal I}(b)</math>,
can be reused later when we compute the similarities <math>s_{k+1}(a,*)</math> for a given vertex <math>a</math> as the first argument.

== Further research on SimRank ==

* Fogaras and Racz <ref name="fogaras_racz">D. Fogaras and B. Racz. Scaling link-based similarity search. In [[World Wide Web Conference|WWW '05]]: Proceedings of the 14th international conference on World Wide Web, pages 641--650, New York, NY, USA, 2005. [[Association for Computing Machinery|ACM]]. [http://www2005.org/docs/p641.pdf]</ref> suggested speeding up SimRank computation through [[Probability theory|probabilistic]] computation using the [[Monte Carlo method]].

* Antonellis et al.<ref name="simrank_plusplus">I. Antonellis, H. Garcia-Molina and C.-C. Chang. Simrank++: Query Rewriting through Link Analysis of the Click Graph. In [[Very large database|VLDB '08]]: Proceedings of the 34th International Conference on Very Large Data Bases, pages 408--421. [http://dbpubs.stanford.edu/pub/showDoc.Fulltext?lang=en&doc=2008-17&format=pdf&compression=&name=2008-17.pdf]</ref> extended SimRank equations to take into consideration (i) evidence factor for [[Graph (mathematics)#Properties of graphs|incident nodes]] and (ii) link weights.

* Lizorkin et al.<ref name="lizorkin"/> proposed several [[Optimization (computer science)|optimization]] techniques for speeding up SimRank iterative computation.

* Yu et al.<ref name="yu_icde13">W. Yu, X. Lin, W. Zhang. Towards Efficient SimRank Computation on Large Networks. In [[International Conference on Data Engineering|ICDE '13]]: Proceedings of the 29th IEEE International Conference on Data Engineering, pages 601--612. [http://www.cse.unsw.edu.au/~weirenyu/pubs/icde13.pdf]</ref> further improved SimRank computation via a fine-grained [[memoization]] method to share small common parts among different partial sums.

== See also ==

* [[PageRank]]

== Citations ==
{{reflist|colwidth=30em}}

[[Category:Information retrieval]]
>>EOP<<
49<|###|>Subject indexing
'''Subject indexing''' is the act of describing or [[Document classification|classifying]] a [[document]] by [[keyword (search)|index terms]] or other symbols in order to indicate what the document is '''[[Aboutness|about]],''' to summarize its [[content (media and publishing)|content]] or to increase its [[findability]].  In other words, it is about identifying and describing the '''[[Subject (documents)|subject]]''' of documents.  Indexes are constructed, separately, on three distinct levels:  terms in a document such as a book;  objects in a collection such as a library;  and documents (such as books and articles) within a field of knowledge.

Subject indexing is used in [[information retrieval]] especially to create [[bibliographic database]]s to retrieve documents on a particular subject. Examples of academic indexing services are [[Zentralblatt MATH]], [[Chemical Abstracts]] and [[PubMed]]. The index terms were mostly assigned by experts but author keywords are also common.

The process of indexing begins with any analysis of the subject of the document. The indexer must then identify terms which appropriately identify the subject either by extracting words directly from the document or assigning words from a [[controlled vocabulary]].<ref name="Lancaster2003a">F. W. Lancaster (2003): "Indexing and abstracting in theory and practise". Third edition. London, Facet ISBN 1-85604-482-3. page 6</ref> The terms in the index are then presented in a systematic order.

Indexers must decide how many terms to include and how specific the terms should be. Together this gives a depth of indexing.

== Subject analysis ==
The first step in indexing is to decide on the subject matter of the document. In manual indexing, the indexer would consider the subject matter in terms of answer to a set of questions such as "Does the document deal with a specific product, condition or phenomenon?".<ref name="Chowdhury2004">G.G. Chowdhury (2004): "Introduction to modern information retrieval". Third Edition. London, Facet. ISBN 1-85604-480-7. page 71</ref> As the analysis is influenced by the knowledge and experience of the indexer, it follows that two indexers may analyse the content differently and so come up with different index terms. This will impact on the success of retrieval.

=== Automatic vs. manual subject analysis ===
Automatic indexing follows set processes of analysing frequencies of word patterns and comparing results to other documents in order to assign to subject categories. This requires no understanding of the material being indexed therefore leads to more uniform indexing but this is at the expense of the true meaning being interpreted. A computer program will not understand the meaning of statements and may therefore fail to assign some relevant terms or assign incorrectly. Human indexers focus their attention on certain parts of the document such as the title, abstract, summary and conclusions, as analysing the full text in depth is costly and time consuming <ref name="Lancaster2003b">F. W. Lancaster (2003): "Indexing and abstracting in theory and practise". Third edition. London, Facet ISBN 1-85604-482-3. page 24</ref> An automated system takes away the time limit and allows the entire document to be analysed, but also has the option to be directed to particular parts of the document.

== Term selection ==
The second stage of indexing involves the translation of the subject analysis into a set of [[keyword (search)|index terms]]. This can involve extracting from the document or assigning from a [[controlled vocabulary]]. With the ability to conduct a [[full text search]] widely available, many people have come to rely on their own expertise in conducting information searches and [[full text search]] has become very popular.  Subject indexing and its experts, professional indexers, [[catalogers]], and [[librarians]], remains crucial to information organization and retrieval.  These experts understand [[controlled vocabularies]] and are able to find information that cannot be located by [[full text search]].  The cost of expert analysis to create subject indexing is not easily compared to the cost of hardware, software and labor to manufacture a comparable set of full-text, fully searchable materials.  With new web applications that allow every user to annotate documents, [[social tagging]] has gained popularity especially in the Web.<ref name="Voss2007">
{{cite conference
  | first= Jakob | last = Voss
  | title = Tagging, Folksonomy & Co - Renaissance of Manual Indexing?
  | booktitle = Proceedings of the International Symposium of Information Science
  | pages = 234254
  | year = 2007
  | arxiv = cs/0701072
}}</ref>

One application of indexing, the [[Index (publishing)|book index]], remains relatively unchanged despite the information revolution.

=== Extraction/Derived indexing ===
Extraction indexing involves taking words directly from the document. It uses [[natural language]] and lends itself well to automated techniques where word frequencies are calculated and those with a frequency over a pre-determined threshold are used as index terms. A stop-list containing common words such as the, and would be referred to and such [[stop words]] would be excluded as index terms. Automated extraction indexing may lead to loss of meaning of terms by indexing single words as opposed to phrases. Although it is possible to extract commonly occurring phrases, it becomes more difficult if key concepts are inconsistently worded in phrases.
Automated extraction indexing also has the problem that even with use of a stop-list to remove common words such as the, some frequent words may not be useful for allowing discrimination between documents. For example, the term glucose is likely to occur frequently in any document related to diabetes. Therefore use of this term would likely return most or all the documents in the database. Post-co-ordinated indexing where terms are combined at the time of searching would reduce this effect but the onus would be on the searcher to link appropriate terms as opposed to the information professional. In addition terms that occur infrequently may be highly significant for example a new drug may be mentioned infrequently but the novelty of the subject makes any reference significant. One method for allowing rarer terms to be included and common words to be excluded by automated techniques  would be a relative frequency approach where frequency of a word in a document is compared to frequency in the database as a whole. Therefore a term that occurs more often in a document than might be expected based on the rest of the database could then be used as an index term, and terms that occur equally frequently throughout will be excluded. Another problem with automated extraction is that it does not recognise when a concept is discussed but is not identified in the text by an indexable keyword.<ref name="Lamb2008">J. Lamb (2008): ''[http://www.indexers.org.uk/index.php?id=463 Human or computer produced indexes?]'' [online] Sheffield, Society of Indexers. Accessed 15 January 2009.</ref>

=== Assignment indexing ===
An alternative is assignment indexing where index terms are taken from a controlled vocabulary. This has the advantage of controlling for [[synonym]]s as the preferred term is indexed and synonyms or related terms direct the user to the preferred term. This means the user can find articles regardless of the specific term used by the author and saves the user from having to know and check all possible synonyms.<ref name="Tenopir">C. Tenopir (1999): "Human or automated, indexing is important". ''Library Journal'' '''124'''(18) pages 34-38.</ref> It also removes any confusion caused by [[homograph]]s by inclusion of a qualifying term. A third advantage is that it allows the linking of related terms whether they are linked by hierarchy or association, e.g. an index entry for an oral medication may list other oral medications as related terms on the same level of the hierarchy but would also link to broader terms such as treatment. Assignment indexing is used in manual indexing to improve inter-indexer consistency as different indexers will have a controlled set of terms to choose from. Controlled vocabularies do not completely remove inconsistencies as two indexers may still interpret the subject differently.<ref name="Chowdhury2004" />

== Index presentation ==
The final phase of indexing is to present the entries in a systematic order. This may involve linking entries. In a pre-coordinated index the indexer determines the order in which terms are linked in an entry by considering how a user may formulate their search. In a post-coordinated index, the entries are presented singly and the user can link the entries through searches, most commonly carried out by computer software. Post-coordination results in a loss of precision in comparison to pre-coordination <ref name="Bodoff1998">D. Bodoff and A. Kambil, (1998): "Partial coordination. I. The best of pre-coordination and post-coordination." ''Journal of the American Society for Information Science'', '''49'''(14), 1254-1269.</ref>

== Depth of Indexing ==
Indexers must make decisions about what entries should be included and how many entries an index should incorporate. The depth of indexing describes the thoroughness of the indexing process with reference to exhaustivity and specificity <ref name="Cleveland2001">D.B. Cleveland and A.D. Cleveland (2001): "Introduction to indexing and abstracting". 3rd Ed. Englewood, libraries Unlimited, Inc. ISBN 1-56308-641-7. page 105</ref>

=== Exhaustivity ===
An exhaustive index is one which lists all possible index terms. Greater exhaustivity gives a higher [[Recall (information retrieval)|recall]], or more likelihood of all the relevant articles being retrieved, however, this occurs at the expense of [[Precision (information retrieval)|precision]]. This means that the user may retrieve a larger number of irrelevant documents or documents which only deal with the subject in little depth. In a manual system a greater level of exhaustivity brings with it a greater cost as more man hours are required. The additional time taken in an automated system would be much less significant. At the other end of the scale, in a selective index only the most important aspects are covered.<ref name="Weinberg1999">B.H. Weinberg (1990): "Exhaustivity of indexes: Books, journals, and electronic full texts; Summary of a workshop presented at the 1999 ASI Annual Conference". ''Key Words'', '''7'''(5), pages 1+.</ref> Recall is reduced in a selective index as if an indexer does not include enough terms, a highly relevant article may be overlooked. Therefore indexers should strive for a balance and consider what the document may be used. They may also have to consider the implications of time and expense.

=== Specificity ===
The specificity describes how closely the index terms match the topics they represent <ref name="Anderson1997">J.D. Anderson (1997): ''[http://www.niso.org/publications/tr/ Guidelines for indexes and related information retrieval devices]'' [online]. Bethesda, Maryland, Niso Press. 10 December 2008.</ref> An index is said to be specific if the indexer uses parallel descriptors to the concept of the document and reflects the concepts precisely.<ref name="Cleveland2001b">D.B. Cleveland and A.D. Cleveland (2001): "Introduction to indexing and abstracting". 3rd Ed. Englewood, libraries Unlimited, Inc. ISBN 1-56308-641-7. page 106</ref> Specificity tends to increase with exhaustivity as the more terms you include, the narrower those terms will be.

==Indexing theory==
[[Birger Hjrland|Hjrland]] (2011)<ref>Hjrland, Birger (2011). The Importance of Theories of Knowledge: Indexing and Information retrieval as an example. ''Journal of the American Society for Information Science and Technology'', 62(1,), 72-77.</ref> found that theories of indexing is at the deepest level connected to different theories of knowledge:

'''Rationalist theories of indexing''' (such as Ranganathan's theory) suggest that subjects are constructed logically from a fundamental set of categories. The basic method of subject analysis is then "analytic-synthetic", to isolate a set of basic categories (=analysis) and then to construct the subject of any given document by combining those categories according to some rules (=synthesis). '''Empiricist theories of indexing''' are based on selecting similar documents based on their properties, in particular by applying numerical statistical techniques.  '''Historicist and hermeneutical theories of indexing''' suggest that the subject of a given document is relative to a given discourse or domain, why the indexing should reflect the need of a particular discourse or domain. According to hermeneutics is a document always written and interpreted from particular horizon. The same is the case with systems of knowledge organization and with all users searching such systems. Any question put to such a system is put from a particular horizon. All those horizons may be more or less in consensus or in conflict. To index a document is to try to contribute to the retrieval of relevant documents by knowing about those different horizons. '''Pragmatic and critical theories of indexing''' (such as Hjrland, 1997)<ref>Hjrland, B. (1997). Information Seeking and Subject Representation. An Activity-theoretical approach to Information Science. Westport & London: Greenwood Press.</ref> is in agreement with the historicist point of view that subjects are relative to specific discourses but emphasizes that subject analysis should support given goals and values and should consider the consequences of indexing one way or another. These theories believe that indexing cannot be neutral and that it is a wrong goal to try to index in a neutral way. Indexing is an act (and computer based indexing is acting according to the programmers intentions). Acts serve human goals. Libraries and information services also serve human goals, why their indexing should be done in a way that supports these goals as much as possible. At a first glance this looks strange because the goals of libraries and information services is to identify any document or piece of information. Nonetheless is any specific way of indexing always supporting some kind of uses at the expense of other. The documents to be indexed intend to serve some specific purposes in a community. Basically the indexing should intend serving the same purposes. Primary and secondary documents and information services are parts of the same overall social system. In such a system different theories, epistemologies, worldviews etc. may be at play and users need to be able to orient themselves and to navigate among those different views. This calls for a mapping of the different epistemologies in the field and classification of the single document into such a map. Excellent examples of such different paradigms and their consequences for indexing and classification systems are provided in the domain of art by rom (2003)<ref>rom, Anders (2003). Knowledge Organization in the domain of Art Studies - History, Transition and Conceptual Changes. Knowledge Organization. 30(3/4), 128-143.</ref> and in music by Abrahamsen (2003).<ref>Abrahamsen, Knut T. (2003). Indexing of Musical Genres. An Epistemological Perspective. Knowledge Organization, 30(3/4), 144-169. 
</ref>

The core of indexing is, as stated by Rowley & Farrow<ref name=rowley2000>Rowley, J. E. & Farrow, J. (2000). Organizing Knowledge: An Introduction to Managing Access to Information. 3rd. Alderstot: Gower Publishing Company</ref> to evaluate a papers contribution to knowledge and index it accordingly. Or, with the words of Hjrland (1992,<ref>Hjrland, Birger (1992). The Concept of "Subject" in Information Science. Journal of Documentation. 48(2), 172-200. http://iva.dk/bh/Core%20Concepts%20in%20LIS/1992JDOC%5FSubject.PDF</ref> 1997) to index its informative potentials.

"In order to achieve good consistent indexing, the indexer must have a thorough appreciation of the structure of the subject  and the nature of the contribution that the document is making to the advancement of knowledge." (Rowley & Farrow, 2000,<ref name=rowley2000/> p.&nbsp;99).

== See also ==
* [[Indexing and abstracting service]]
* [[Document classification]]
* [[Metadata]]
* [[Overcategorization]]
* [[Thomas of Ireland]], a medieval pioneer in subject indexing

== References ==
<references/>
*{{cite book|author=Fugman, Robert|year=1993|title=Subject analysis and indexing. Theoretical foundation and practical advice|place=Frankfurt/Main|publisher=Index Verlag}}
*{{cite journal|author=Frohmann, B.|year=1990|title=Rules of Indexing: A Critique of [[Mentalism]] in Information Retrieval Theory|journal=Journal of Documentation|volume=46|issue=2|pages=81101|doi=10.1108/eb026855}}

[[Category:Library science]]
[[Category:Information science]]
[[Category:Information retrieval]]
>>EOP<<
55<|###|>Compound term processing
{{copy edit|for=Use of references (both inline, and ref tags)|date=February 2015}}

'''Compound term processing''' refers to a category of techniques used in [[information retrieval]] applications that perform matching on the basis of [[compound term]]s. Compound terms are built by combining two or more simple terms; for example, "triple" is a single word term, but "triple heart bypass" is a compound term.

In August 2003, [[Concept Searching Limited]] introduced the idea of using statistical Compound Term Processing <ref>{{cite journal|url=http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf|title=Lateral Thinking in Information Retrieval|journal=INFORMATION MANAGEMENT AND TECHNOLOGY|volume=36 PART 4}} British Library Direct catalogue entry can be found here:[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&ETOC=RN]</ref>

CLAMOUR<ref>[http://www.statistics.gov.uk/methods_quality/clamour/coordination/wp03.asp] National Statistics CLAMOUR project</ref><ref>[http://www.statistics.gov.uk/methods_quality/clamour/downloads/Clamour_march2002_final_reportAO.pdf] CLAMOUR Final Report</ref> is a European collaborative project which aims to find a better way to classify when collecting and disseminating industrial information & statistics. In contrast to the techniques discussed by Concept Searching Limited, CLAMOUR appears to use a linguistic approach, rather than one based on statistical modelling.

Compound Term Processing allows information retrieval applications, such as search engines, to perform their matching on the basis of multi-word concepts, rather than on single words in isolation which can be highly ambiguous.

Most [[search engine]]s simply look for documents containing the words entered by the user into the search box . These are known as [[keyword search]] engines. [[Boolean search]] engines add a degree of sophistication by allowing the user to specify additional requirements. For example, "Tiger NEAR Woods AND (golf OR golfing) NOT Volkswagen" uses the operators "NEAR", "AND", "OR" and "NOT" to specify that these words must follow certain requirements. [[Phrase search]] is simpler to use, but requires that the exact phrase specified appear in the results.

Techniques for probabilistic weighting of single word terms dates back to at least 1976 in the landmark publication by [[Stephen Robertson (computer scientist)|Stephen E. Robertson]] and [[Karen Sparck Jones]] entitled "Relevance weighting of search terms", originally published in the ''Journal of the American Society for Information Science''.<ref>{{cite doi | 10.1002/asi.4630270302}}</ref>  Robertson stated that the assumption of word independence is not justified and exists simply as a matter of mathematical convenience. His objection to term independence is not a new idea, dating back to at least 1964 when H. H. Williams expressed that "[t]he assumption of independence of words in a document is usually made as a matter of mathematical convenience".<ref>WILLIAMS, J.H., 'Results of classifying documents with multiple discriminant functions', In : Statistical Association Methods for Mechanized Documentation, National Bureau of Standards, Washington, 217-224 (1965).</ref>

Compound term processing is a new approach to an old problem: how can one improve the relevance of search results while maintaining ease of use? By forming compound terms and placing these terms in a search engine's index, searches can be performed with a higher degree of accuracy, as the ambiguity inherent in single words is no longer a problem. Using this technique, a search for ''survival rates following a triple heart bypass in elderly people'' will locate documents about this topic even if this precise phrase is not contained in any document. This can be performed by a [[concept search]], which itself uses compound term processing. This will extract the key concepts automatically (in this case "survival rates", "triple heart bypass" and "elderly people") and use these concepts to select the most relevant documents.

In 2004, Anna Lynn Patterson filed a number of patents on "phrase-based searching in an information retrieval system"<ref>[http://appft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&d=PG01&p=1&u=%2Fnetahtml%2FPTO%2Fsrchnum.html&r=1&f=G&l=50&s1=%2220060031195%22.PGNR.&OS=DN/20060031195&RS=DN/20060031195] US Patent: 20060031195</ref> to which Google subsequently acquired the rights. A full discussion of the patents can be found at [http://www.webmasterwoman.com/search-engines/phrase-based-indexing.html Webmaster Woman]{{dead link|date=February 2015}}.

Statistical compound term processing is a method more adaptive than the process described by Patterson in her patent applications. Her process is targeted at searching the World Wide Web where an extensive statistical knowledge of common searches can be used to identify candidate phrases. Statistical compound term processing is more suited to [[enterprise search]] applications where such [[A priori and a posteriori|a priori]] knowledge is not available.

Statistical compound term processing is also more adaptive than the linguistic approach taken by the CLAMOUR project, which must take into consideration the syntactic properties of the terms (i.e. part of speech, gender, number, etc.) and their combinations. CLAMOUR is highly language-dependent, whereas the statistical approach is language-independent.

==See also==
* [[Enterprise search]]
* [[Information retrieval]]

== References ==
{{Reflist}}

==  External links ==
*[http://www.conceptsearching.com/ Concept Searching Limited]
*[http://www.webmasterwoman.com/search-engines/phrase-based-indexing.html Webmaster Woman]

{{Natural Language Processing}}

{{DEFAULTSORT:Compound Term Processing}}
[[Category:Information retrieval]]
>>EOP<<
61<|###|>Matthews correlation coefficient
The '''Matthews correlation coefficient''' is used in [[machine learning]] as a measure of the quality of binary (two-class) [[Binary classification|classifications]]. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between &minus;1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and &minus;1 indicates total disagreement between prediction and observation. The statistic is also known as the [[phi coefficient]]. MCC is related to the [[Pearson's chi-square test|chi-square statistic]] for a 22 [[contingency table]]

: <math>|\text{MCC}| = \sqrt{\frac{\chi^2}{n}}</math>

where ''n'' is the total number of observations.

While there is no perfect way of describing the [[confusion matrix]] of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures{{Citation needed|reason=Source needed for being 'best'|date=December 2014}}. Other measures, such as the proportion of correct predictions (also termed [[accuracy]]), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification.

The MCC can be calculated directly from the [[confusion matrix]] using the formula:

: <math>
\text{MCC} = \frac{ TP \times TN - FP \times FN } {\sqrt{ (TP + FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }
</math>

In this equation, ''TP'' is the number of [[true positive]]s, ''TN'' the number of [[true negative]]s, ''FP'' the number of [[false positive]]s and ''FN'' the number of [[false negative]]s. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.

The measure was introduced in 1975 by Matthews.<ref>{{cite journal|last=Matthews|first=B. W.|title=Comparison of the predicted and observed secondary structure of T4 phage lysozyme|journal=Biochimica et Biophysica Acta (BBA) - Protein Structure|date=1975|volume=405|issue=2|pages=442-451|doi=10.1016/0005-2795(75)90109-9}}</ref> The original formula equal to above was:
: <math>
\text{N} = TN + TP + FN + FP
</math>
: <math>
\text{S} = \frac{ TP + FN } { N }
</math>
: <math>
\text{P} = \frac{ TP + FP } { N }
</math>
: <math>
\text{MCC} = \frac{ TP / N - S \times P } {\sqrt{ P S  ( 1 - S)  ( 1 - P ) } }
</math>

As a [[Correlation and dependence|correlation coefficient]], the Matthews correlation coefficient is the [[geometric mean]] of the [[regression coefficient]]s of the problem and its [[Dual (mathematics)|dual]]. The component regression coefficients of the Matthews correlation coefficient are [[markedness]] (deltap) and informedness (deltap').<ref name="Perruchet2004">{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97119 |doi=10.1016/s0911-6044(03)00059-9}}</ref><ref name="Powers2007">{{cite journal |first=David M W |last=Powers |date=2007/2011 |title=Evaluation: From Precision, Recall and F-Measure  to ROC, Informedness, Markedness & Correlation |journal=Journal of Machine Learning Technologies |volume=2 |issue=1 |pages=3763 |url=http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf}}</ref>

== Confusion Matrix ==
{{main|Confusion matrix}}

{| class="wikitable" align="right" width=35% style="font-size:98%; margin-left:0.5em; padding:0.25em; background:#f1f5fc;"
|+ Terminology and derivations<br 
/>from a confusion matrix
|- valign=top
|
; true positive (TP)
:eqv. with hit
; true negative (TN)
:eqv. with correct rejection
; false positive (FP)
:eqv. with [[false alarm]], [[Type I error]]
; false negative (FN)
:eqv. with miss, [[Type II error]]
----
; [[sensitivity (test)|sensitivity]] or true positive rate (TPR)
:eqv. with [[hit rate]], [[Information retrieval#Recall|recall]]
:<math>\mathit{TPR} = \mathit{TP} / P = \mathit{TP} / (\mathit{TP}+\mathit{FN})</math>
; [[Specificity (tests)|specificity]] (SPC) or True Negative Rate
:<math>\mathit{SPC} = \mathit{TN} / N = \mathit{TN} / (\mathit{FP} + \mathit{TN}) </math>
; [[Information retrieval#Precision|precision]] or [[positive predictive value]] (PPV)
:<math>\mathit{PPV} = \mathit{TP} / (\mathit{TP} + \mathit{FP})</math>
; [[negative predictive value]] (NPV)
:<math>\mathit{NPV} = \mathit{TN} / (\mathit{TN} + \mathit{FN})</math>
; [[Information retrieval#Fall-out|fall-out]] or false positive rate (FPR)
:<math>\mathit{FPR} = \mathit{FP} / N = \mathit{FP} / (\mathit{FP} + \mathit{TN})</math>
; [[false discovery rate]] (FDR)
:<math>\mathit{FDR} = \mathit{FP} / (\mathit{FP} + \mathit{TP}) = 1 - \mathit{PPV} </math>
; Miss Rate or [[Type I and type II errors#False positive and false negative rates|False Negative Rate]] (FNR)
:<math>\mathit{FNR} = \mathit{FN} / (\mathit{FN} + \mathit{TP}) </math>
----
; [[accuracy]] (ACC)
:<math>\mathit{ACC} = (\mathit{TP} + \mathit{TN}) / (P + N)</math>
;[[F1 score]]
: is the [[Harmonic mean#Harmonic mean of two numbers|harmonic mean]] of [[Information retrieval#Precision|precision]] and [[sensitivity (test)|sensitivity]]
:<math>\mathit{F1} = 2 \mathit{TP} / (2 \mathit{TP} + \mathit{FP} + \mathit{FN})</math>
; Matthews correlation coefficient (MCC)
:<math> \frac{ TP \times TN - FP \times FN } {\sqrt{ (TP+FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }
</math>

;Informedness
:<math>TPR + SPC - 1</math>
;Markedness
:<math>PPV + NPV - 1</math>
;
<span style="font-size:90%;">''Source: Fawcett (2006).''<ref name=Fawcelt2006>{{cite journal|last=Fawcelt|first=Tom|title=An Introduction to ROC Analysis|journal=Pattern Recognition Letters|date=2006|volume=27|issue=8|pages=861 - 874|doi=10.1016/j.patrec.2005.10.010}}</ref></span>
|}

Let us define an experiment from '''P''' positive instances and '''N''' negative instances for some condition. The four outcomes can be formulated in a 22 ''[[contingency table]]'' or ''[[confusion matrix]]'', as follows:

{{DiagnosticTesting_Diagram}}

== See also ==
* [[Phi coefficient]]
* [[F1 score]]
* [[Cramer's V (statistics)|Cramer's V]], a similar measure of association between nominal variables.
* [[Cohen's kappa]]

== References ==

{{Reflist}}

=== General References ===
* [[Pierre Baldi|Baldi, P.]]; Brunak, S.; Chauvin, Y.; Andersen, C. A. F.; Nielsen, H. Assessing the accuracy of prediction algorithms for classification: an overview" ''Bioinformatics'' 2000, 16, 412&ndash;424. [http://bioinformatics.oxfordjournals.org/cgi/content/abstract/16/5/412]
* Matthews, B.W., Comparison of the predicted and observed secondary structure of T4 phage lysozyme" ''Biochim. Biophys. Acta'' 1975, 405, 442&ndash;451
* Carugo, O., Detailed estimation of bioinformatics prediction reliability through the Fragmented Prediction Performance Plots. BMC Bioinformatics 2007. [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2148069/]

{{DEFAULTSORT:Matthews Correlation Coefficient}}
[[Category:Machine learning]]
[[Category:Information retrieval]]
[[Category:Statistical classification]]
[[Category:Computational chemistry]]
[[Category:Cheminformatics]]
[[Category:Bioinformatics]]
[[Category:Statistical ratios]]
[[Category:Summary statistics for contingency tables]]
>>EOP<<
67<|###|>Full text search
{{Multiple issues|
{{refimprove|date=August 2012}}
{{cleanup|date=September 2009}}
}}

In [[text retrieval]], '''full-text search''' refers to techniques for searching a single [[computer]]-stored [[document]] or a collection in a [[full text database]]. Full-text search is distinguished from searches based on [[metadata]] or on parts of the original texts represented in databases (such as titles, abstracts, selected sections, or bibliographical references).

In a full-text search, a [[search engine]] examines all of the words in every stored document as it tries to match search criteria (text specified by a user). Full-text-searching techniques became common in online [[bibliographic databases]] in the 1990s.{{Verify source|date=October 2008}} Many websites and application programs (such as [[word processing]] software) provide full-text-search capabilities. Some web search engines, such as [[AltaVista]], employ full-text-search techniques, while others index only a portion of the web pages examined by their indexing systems.<ref>In practice, it may be difficult to determine how a given search engine works. The [[search algorithms]] actually employed by web-search services are seldom fully disclosed out of fear that web entrepreneurs will use [[search engine optimization]] techniques to improve their prominence in retrieval lists.</ref>

==Indexing==
When dealing with a small number of documents, it is possible for the full-text-search engine to directly scan the contents of the documents with each [[Information retrieval|query]], a strategy called "serial scanning." This is what some tools, such as [[grep]], do when searching.

However, when the number of documents to search is potentially large, or the quantity of search queries to perform is substantial, the problem of full-text search is often divided into two tasks: indexing and searching. The indexing stage will scan the text of all the documents and build a list of search terms (often called an [[Search index|index]], but more correctly named a [[concordance (publishing)|concordance]]). In the search stage, when performing a specific query, only the index is referenced, rather than the text of the original documents.<ref name="Capabilities of Full Text Search System ">[http://www.lucidimagination.com/full-text-search Capabilities of Full Text Search System] {{Dead link |date=October 2012}}</ref>

The indexer will make an entry in the index for each term or word found in a document, and possibly note its relative position within the document. Usually the indexer will ignore [[stop words]] (such as "the" and "and") that are both common and insufficiently meaningful to be useful in searching. Some indexers also employ language-specific [[stemming]] on the words being indexed. For example, the words "drives", "drove", and "driven" will be recorded in the index under the single concept word "drive."

==The precision vs. recall tradeoff==
[[Image:Full-text-search-results.png|150px|thumb|right|This diagram represents a low-precision, low-recall search as described in the text.]]
Recall measures the quantity of relevant results returned by a search, while precision is the measure of the quality of the results returned. Recall is the ratio of relevant results returned divided by all relevant results. Precision is the number of relevant results returned divided by the total number of results returned.

The diagram at right represents a low-precision, low-recall search. In the diagram the red and green dots represent the total population of potential search results for a given search. Red dots represent irrelevant results, and green dots represent relevant results. Relevancy is indicated by the proximity of search results to the center of the inner circle. Of all possible results shown, those that were actually returned by the search are shown on a light-blue background. In the example only one relevant result of three possible relevant results was returned, so the recall is a very low ratio of 1/3 or 33%. The precision for the example is a very low 1/4 or 25%, since only one of the four results returned was relevant.<ref name="isbn1430215941">{{cite book|last=Coles|first=Michael|year=2008|title=Pro Full-Text Search in SQL Server 2008|edition=Version 1|publisher=[[Apress|Apress Publishing Company]]|isbn=1-4302-1594-1}}</ref>

Due to the ambiguities of [[natural language]], full text search systems typically includes options like [[stop words]] to increase precision and [[stemming]] to increase recall. [[Controlled vocabulary|Controlled-vocabulary]] searching also helps alleviate low-precision issues by [[tag (metadata)|tagging]] documents in such a way that ambiguities are eliminated. The trade-off between precision and recall is simple: an increase in precision can lower overall recall while an increase in recall lowers precision.<ref name="YuwonoLee">{{Cite conference | first = Yuwono | last = B. |author2=Lee, D.L. | title = Search and ranking algorithms for locating resources on the World Wide Web | pages = 164 | publisher = 12th International Conference on Data Engineering (ICDE'96) | year = 1996}}</ref>

{{See also|Precision and recall}}

==False-positive problem==

Free text searching is likely to retrieve many documents that are not [[relevance|relevant]] to the ''intended'' search question. Such documents are called ''false positives'' (see [[Type I and type II errors#Type I error|Type I error]]). The retrieval of irrelevant documents is often caused by the inherent ambiguity of [[natural language]]. In the sample diagram at right, false positives are represented by the irrelevant results (red dots) that were returned by the search (on a light-blue background).

Clustering techniques based on [[Bayesian inference|Bayesian]] algorithms can help reduce false positives. For a search term of "football", clustering can be used to categorize the document/data universe into "American football", "corporate football", etc. Depending on the occurrences of words relevant to the categories, search terms a search result can be placed in one or more of the categories. This technique is being extensively deployed in the e-discovery domain.{{clarify|date=January 2012}}

==Performance improvements==

The deficiencies of free text searching have been addressed in two ways: By providing users with tools that enable them to express their search questions more precisely, and by developing new search algorithms that improve retrieval precision.

===Improved querying tools===

*[[Index term|Keyword]]s. Document creators (or trained indexers) are asked to supply a list of words that describe the subject of the text, including synonyms of words that describe this subject. Keywords improve recall, particularly if the keyword list includes a search word that is not in the document text.
* [[Field-restricted search]]. Some search engines enable users to limit free text searches to a particular [[field (computer science)|field]] within a stored [[Record (computer science)|data record]], such as "Title" or "Author."
* [[Boolean query|Boolean queries]]. Searches that use [[Boolean logic|Boolean]] operators (for example, "encyclopedia" AND "online" NOT "Encarta") can dramatically increase the precision of a free text search. The AND operator says, in effect, "Do not retrieve any document unless it contains both of these terms." The NOT operator says, in effect, "Do not retrieve any document that contains this word." If the retrieval list retrieves too few documents, the OR operator can be used to increase [[recall (information retrieval)|recall]]; consider, for example, "encyclopedia" AND "online" OR "Internet" NOT "Encarta". This search will retrieve documents about online encyclopedias that use the term "Internet" instead of "online." This increase in precision is very commonly counter-productive since it usually comes with a dramatic loss of recall.<ref>Studies have repeatedly shown that most users do not understand the negative impacts of boolean queries.[http://eprints.cs.vt.edu/archive/00000112/]</ref>
* [[Phrase search]]. A phrase search matches only those documents that contain a specified phrase, such as "Wikipedia, the free encyclopedia."
* [[Concept search]]. A search that is based on multi-word concepts, for example [[Compound term processing]]. This type of search is becoming popular in many e-Discovery solutions.
* [[Concordance search]]. A concordance search produces an alphabetical list of all principal words that occur in a [[Plain text|text]] with their immediate context.
* [[Proximity search (text)|Proximity search]]. A phrase search matches only those documents that contain two or more words that are separated by a specified number of words; a search for "Wikipedia" WITHIN2 "free" would retrieve only those documents in which the words "Wikipedia" and "free" occur within two words of each other.
* [[Regular expression]]. A regular expression employs a complex but powerful querying [[syntax]] that can be used to specify retrieval conditions with precision.
* [[Fuzzy search]] will search for document that match the given terms and some variation around them (using for instance [[edit distance]] to threshold the multiple variation)
* [[Wildcard character|Wildcard search]]. A search that substitutes one or more characters in a search query for a wildcard character such as an [[asterisk]]. For example using the asterisk in a search query "s*n" will find "sin", "son", "sun", etc. in a text.

===Improved search algorithms===
The [[PageRank]] algorithm developed by [[Google]] gives more prominence to documents to which other [[Web page]]s have linked.<ref>{{Cite patent | inventor-last = Page | inventor-first = Lawrence | publication-date = 1/9/1998 | issue-date = 9/4/2001 | title = Method for node ranking in a linked database | country-code = US | description = A method assigns importance ranks to nodes in a linked database, such as any database of documents containing citations, the world wide web or any other hypermedia database. The rank assigned to a document is calculated from the ranks of documents citing it. In addition, the rank of a document is... | patent-number = 6285999 | postscript = <!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref> See [[Search engine]] for additional examples.

==Software==

The following is a partial list of available software products whose predominant purpose is to perform full text indexing and searching. Some of these are accompanied with detailed descriptions of their theory of operation or internal algorithms, which can provide additional insight into how full text search may be accomplished.

=== Free and open source software ===
<!--

Please do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.

-->
* [[Apache Solr]]
* [[BaseX]]
* [[DataparkSearch]]
* [[ElasticSearch]]
* [[Ht-//Dig|ht://Dig]]
* [[KinoSearch]]
* [[Lemur Project|Lemur/Indri]]
* [[Lucene]]
* [[mnoGoSearch]]
* [[Searchdaimon]]
* [[Sphinx (search engine)|Sphinx]]
* [[Swish-e]]
* [[Xapian]]

=== Proprietary software ===
<!--

Please do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.

-->
* [[Attivio]]
* [[Autonomy Corporation]]
* [[Bar Ilan Responsa Project]]
* [[Brainware]]
* [[BRS/Search]] 
* [[Clusterpoint|Clusterpoint Server]]
* [[Concept Searching Limited]]
* [[Dieselpoint]]
* [[dtSearch]]
* [[Endeca]]
* [[Exalead]]
* [[Fast Search & Transfer]]
* [[Inktomi (company)|Inktomi]]
* [[Dan Wagner#Locayta|Locayta]](rebranded to [[ATTRAQT]] in 2014)
* [[Lookeen]]
* [[Lucid Imagination]]
* [[MarkLogic]]
* [[Swiftype]]
* [[Thunderstone Software LLC.]]
* [[Vivisimo]]

==Notes==
{{Reflist}}

==See also==
*[[Pattern matching]] and [[string matching]]
*[[Compound term processing]]
*[[Controlled vocabulary]]
*[[Enterprise search]]
*[[Information Extraction]]
*[[Information retrieval]]
*[[Faceted search]]
*[[Full text database]]
*[[List of enterprise search vendors]]
*[[Search engine]]
*[[WebCrawler]], first FTS engine
*[[Search engine indexing]] - how search engines generate indices to support full text searching
*[[SQL Server Full Text Search|SQL Server Full Text Search (implementation of)]]

{{DEFAULTSORT:Full Text Search}}
[[Category:Searching]]
[[Category:Text editor features]]
[[Category:Information retrieval]]
>>EOP<<
73<|###|>Cognitive models of information retrieval
{{Orphan|date=September 2012}}

'''Cognitive models of information retrieval''' rest on the mix of areas such as [[cognitive science]], [[human-computer interaction]], [[information retrieval]], and  [[library science]]. They describe the relationship between a person's cognitive model of the information sought and the organization of this information in an information system.  These models attempt to understand how a person is searching for information so that the database and the search of this database can be designed in such a way as to best serve the user. [[Information retrieval]] may incorporate multiple tasks and cognitive problems, particularly because different people may have different methods for attempting to find this information and expect the information to be in different forms.  Cognitive models of information retrieval may be attempts at something as apparently prosaic as improving search results or may be something more complex, such as attempting to create a database which can be queried with natural language search.

==Berrypicking==
One way of understanding how users search for information has been described by [[Marcia Bates]]<ref>[[Marcia Bates]] (1989). "The Design of Browsing and Berrypicking Techniques for the Online Search Interface." http://www.gseis.ucla.edu/faculty/bates/berrypicking.html</ref> at the [[University of California at Los Angeles]]. Bates argues that "berrypicking" better reflects how users search for information than previous models of information retrieval.  This may be because previous models were strictly linear and did not incorporate cognitive questions.  For instance, one typical model is of a simple linear match between a query and a document.  However, Bates points out that there are simple modifications that can be made to this process.  For instance, Salton has argued that user feedback may help improve the search results.<ref>[[Gerard Salton]] (1968). ''Automatic Information and Retrieval'' (Computer Science). Dubuque, Iowa: Mcgraw-Hill Inc.</ref>

Bates argues that searches are evolving and occur bit by bit.  That is to say, a person constantly changes his or her search terms in response to the results returned from the information retrieval system.  Thus, a simple linear model does not capture the nature of information retrieval because the very act of searching causes feedback which causes the user to modify his or her [[cognitive model]] of the information being searched for.  In addition, information retrieval can be bit by bit.  Bates gives a number of examples.  For instance, a user may look through footnotes and follow these sources.  Or, a user may scan through recent journal articles on the topic.  In each case, the user's question may change and thus the search evolves.

==Exploratory Search==
Researchers in the areas of [[human-computer interaction]] and [[cognitive science]] focus on how people explore for information when interacting with the WWW. This kind of search, sometimes called [[exploratory search]], focuses on how people iteratively refine their search activities and update their internal representations of the search problems.<ref>Qu, Yan & Furnas, George. "Model-driven formative evaluation of exploratory search: A study under a sensemaking framework"</ref> Existing search engines were designed based on traditional library science theories related to retrieval basic facts and simple information through an interface. However, exploratory information retrieval often involves ill-defined search goals and evolving criteria for evaluation of relevance. The interactions between humans and the information system will therefore involve more cognitive activity, and systems that support exploratory search will therefore need to take into account the cognitive complexities involved during the dynamic information retrieval process.

==Natural language searching==

Another way in which cognitive models of information may help in information retrieval is with natural language searching.  For instance, How Stuff Works imagines a world in which, rather than searching for local movies, reading the reviews, then searching for local Mexican restaurants, and reading their reviews, you will simply type ""I want to see a funny movie and then eat at a good Mexican restaurant. What are my options?" into your browser, and you will receive a useful and relevant response.<ref>Strickland, J. (n.d.). HowStuffWorks "How Web 3.0 Will Work". Howstuffworks "Computer". Retrieved November 4, 2009, from http://computer.howstuffworks.com/web-30.htm</ref>  Although such a thing is not possible today, it represents a holy grail for researchers into cognitive models of information retrieval.  The goal is to somehow program information retrieval programs to respond to natural language searches.  This would require a fuller understanding of how people structure queries.

==Notes==
{{Reflist}}

[[Category:Information retrieval]]
[[Category:Cognitive modeling]]
>>EOP<<
79<|###|>Query expansion
'''Query expansion''' ('''QE''') is the process of reformulating a seed query to improve retrieval performance in [[information retrieval]] operations.<ref>{{cite journal
 | last = Vectomova | first = Olga |author2=Wang, Ying  | year = 2006
 | title = A study of the effect of term proximity on query expansion | journal = [[Journal of Information Science]]
 | volume = 32 | issue = 4 | pages = 324&ndash;333
 | doi = 10.1177/0165551506065787 | id =  | url = http://jis.sagepub.com/cgi/content/abstract/32/4/324
 | format = Abstract | accessdate = 2006-12-09
 }}</ref>
In the context of web [[search engine]]s, query expansion involves evaluating a user's input (what words were typed into the search query area, and sometimes other types of [[data]]) and expanding the search query to match additional documents.  Query expansion involves techniques such as:

* Finding [[synonym]]s of words, and searching for the synonyms as well
* Finding all the various [[Morphology (linguistics)|morphological]] forms of words by [[stemming]] each word in the [[search query]]
* Fixing [[Typographical error|spelling errors]] and automatically searching for the corrected form or suggesting it in the results
* Re-weighting the terms in the original query

Query expansion is a methodology studied in the field of [[computer science]], particularly within the realm of [[natural language processing]] and [[information retrieval]].

== Precision and recall tradeoffs ==

Search engines invoke query expansion to increase the quality of user search results.  It is assumed that users do not always formulate search queries using the best terms. Best in this case may be because the database does not contain the user entered terms.  

By [[stemming]] a user-entered term, more documents are matched, as the alternate word forms for a user entered term are matched as well, increasing the total [[recall (information retrieval)|recall]]. This comes at the expense of reducing the [[precision (information retrieval)|precision]].  By expanding a search query to search for the synonyms of a user entered term, the recall is also increased at the expense of precision.  This is due to the nature of the equation of how precision is calculated, in that a larger recall implicitly causes a decrease in precision, given that factors of recall are part of the denominator. It is also inferred that a larger recall negatively impacts overall search result quality, given that many users do not want more results to comb through, regardless of the precision.

The goal of query expansion in this regard is by increasing recall, precision can potentially increase (rather than decrease as mathematically equated), by including in the result set pages which are more relevant (of higher quality), or at least equally relevant. Pages which would not be included in the result set, which have the potential to be more relevant to the user's desired query, are included, and without query expansion would not have, regardless of relevance.  At the same time, many of the current commercial search engines use word frequency ([[Tf-idf]]) to assist in ranking.  By ranking the occurrences of both the user entered words and synonyms and alternate morphological forms, documents with a higher density (high frequency and close proximity) tend to migrate higher up in the search results, leading to a higher quality of the search results near the top of the results, despite the larger recall.

This tradeoff is one of the defining problems in query expansion, regarding whether it is worthwhile to perform given the questionable effects on precision and recall. Critics{{Who|date=March 2009}} state one of the problems is that the dictionaries and [[thesauri]], and the stemming algorithm, are driven by human bias and while this is implicitly handled by the query expansion algorithm, this explicitly affects the results in a non-automated manner (similar to how statisticians can 'lie' with statistics){{Citation needed|date=July 2013}}. Other critics{{Who|date=March 2009}} point out potential for corporate influence on the dictionaries, promoting advertising of online web pages in the case of [[web search engine]]s. {{Citation needed|date=December 2007}}

==See also==

* [[Search engine]]
* [[Search engine indexing]]
* [[Information retrieval]]
* [[Document retrieval]]
* [[Linguistics]]
* [[Natural language processing]]
* [[Stemming]]
* [[Morphology (linguistics)]]

== Software libraries ==
*[http://qtanalyzer.codeplex.com/ QueryTermAnalyzer] open-source, C#. Machine learning based query term weight and synonym analyzer for query expansion.
*[http://lucene-qe.sourceforge.net/ LucQE] - open-source, Java.  Provides a framework along with several implementations that allow to perform query expansion with the use of Apache [[Lucene]].
*[[Xapian]] is an open-source search library which includes support for query expansion

== References ==

* D. Abberley, D. Kirby, S. Renals, and T. Robinson, The THISL broadcast news  retrieval system. In ''Proc. ESCA ETRW Workshop Accessing Information in Spoken Audio'', (Cambridge), pp.&nbsp;1419, 1999. Section on [http://homepages.inf.ed.ac.uk/srenals/pubs/1999/esca99-thisl/node6.html Query Expansion] - Concise, mathematical overview.
* R. Navigli, P. Velardi. [http://www.dcs.shef.ac.uk/~fabio/ATEM03/navigli-ecml03-atem.pdf An Analysis of Ontology-based Query Expansion Strategies]. ''Proc. of Workshop on Adaptive Text Extraction and Mining (ATEM 2003)'', in the ''14th European Conference on Machine Learning (ECML 2003)'', Cavtat-Dubrovnik, Croatia, September 22-26th, 2003, pp.&nbsp;4249 - An analysis of query expansion methods relying on WordNet as the reference ontology.
* Y. Qiu and H.P. Frei. [http://citeseer.ist.psu.edu/qiu93concept.html Concept Based Query Expansion]. In ''Proceedings of SIGIR-93, 16th ACM International Conference on Research and Development in Information Retrieval'', Pittsburgh, SIGIR Forum, ACM Press, June 1993 - Academic document on a specific method of query expansion
* Efthimis N. Efthimiadis. [http://faculty.washington.edu/efthimis/pubs/Pubs/qe-arist/QE-arist.html Query Expansion]. In: Martha E. Williams (ed.), ''Annual Review of Information Systems and Technology (ARIST)'', v31, pp 121187, 1996 - An introduction for less-technical viewers.

=== Notes ===
{{reflist}}

{{DEFAULTSORT:Query Expansion}}
[[Category:Information retrieval]]
[[Category:Searching]]
>>EOP<<
85<|###|>Database search engine
There are several categories of search engine software:  Web search or full-text search (example: [[Lucene]]), database or structured data search (example: [[Dieselpoint]]), and mixed or [[enterprise search]] (example: [[Google Search Appliance]]).  The largest web search engines such as [[Google]] and [[Yahoo!]] utilize tens or hundreds of thousands of computers to process billions of web pages and return results for thousands of searches per second. High volume of queries and text processing requires the software to run in highly distributed environment with high degree of redundancy. Modern search engines have the following main components:

Searching for text-based content in [[databases]] or other [[structured data]] formats ([[XML]], [[Comma-separated values|CSV]], etc.) presents some special challenges and opportunities which a number of specialized search engines resolve.  Databases are slow when solving complex queries (with multiple logical or [[string matching]] arguments.  Databases allow logical queries which full-text search doesn't (use of multi-field boolean logic for instance).  There is no crawling necessary for a database since the data is already structured but it is often necessary to index the data in a more compact form designed to allow for faster search.

Database search engines were initially (and still usually are) included with major database software products.  As such, they are usually called indexing engines.  However, these indexing engines are relatively limited in their ability to customize indexing formats (compounding, normalization, transformation, [[transliteration]], etc.)   Usually they do not provide sophisticated data matching technology ([[string matching]], [[boolean logic]], algorithmic methods, search scripting, etc.).

In more advanced Database search systems relational databases are indexed by compounding multiple tables into a single table containing only the fields that need to be queried (or displayed in search results).  The actual data matching engines can include any functions from basic string matching, normalization, transformation,  Database search technology is heavily used by government database services, e-commerce companies, web advertising platforms, telecommunications service providers, etc.

==See also==

*[[Search engine]]
*[[Web crawler]]
*[[Search engine indexing]]
*[[Enterprise search]]

==External links==
* [http://www.searchtools.com/info/database-search.html Searching for Text Information in Databases]

{{DEFAULTSORT:Search Engine Technology}}
[[Category:Internet search engines]]
[[Category:Information retrieval]]
>>EOP<<
91<|###|>Fuzzy retrieval
'''Fuzzy retrieval''' techniques are based on the [[Extended Boolean model]] and the [[Fuzzy set]] theory. There are two classical fuzzy retrieval models: Mixed Min and Max (MMM) and the Paice model. Both models do not provide a way of evaluating query weights, however this is considered by the [[Extended Boolean model|P-norms]] algorithm.

==Mixed Min and Max model (MMM)==

In fuzzy-set theory, an element has a varying degree of membership, say ''d<sub>A</sub>'', to a given set ''A'' instead of the traditional membership choice (is an element/is not an element).<br />
In MMM<ref>{{citation | last=Fox | first=E. A. | coauthors=S. Sharat | year=1986 | title=A Comparison of Two Methods for Soft Boolean Interpretation in Information Retrieval | publisher=Technical Report TR-86-1, Virginia Tech, Department of Computer Science}}</ref> each index term has a fuzzy set associated with it. A document's weight with respect to an index term ''A'' is considered to be the degree of membership of the document in the fuzzy set associated with ''A''. The degree of membership for union and intersection are defined as follows in Fuzzy set theory:<br/>
:<math>d_{A\cap B}= min(d_A, d_B)</math>
:<math>d_{A\cup B}= max(d_A,d_B)</math>

According to this, documents that should be retrieved for a query of the form ''A or B'', should be in the fuzzy set associated with the union of the two sets ''A'' and ''B''. Similarly, the documents that should be retrieved for a query of the form ''A and B'', should be in the fuzzy set associated with the intersection of the two sets. Hence, it is possible to define the similarity of a document to the ''or'' query to be ''max(d<sub>A</sub>, d<sub>B</sub>)'' and the similarity of the document to the ''and'' query to be ''min(d<sub>A</sub>, d<sub>B</sub>)''. The MMM model tries to soften the Boolean operators by considering the query-document similarity to be a linear combination of the ''min'' and ''max'' document weights.

Given a document ''D'' with index-term weights ''d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>'' for terms ''A<sub>1</sub>, A<sub>2</sub>, ..., A<sub>n</sub>'', and the queries:

''Q<sub>or</sub> = (A<sub>1</sub> or A<sub>2</sub> or ... or A<sub>n</sub>)''<br />
''Q<sub>and</sub> = (A<sub>1</sub> and A<sub>2</sub> and ... and A<sub>n</sub>)''

the query-document similarity in the MMM model is computed as follows:

''SlM(Q<sub>or</sub>, D) = C<sub>or1</sub> * max(d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>) + C<sub>or2</sub> * min(d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>)''<br />
''SlM(Q<sub>and</sub>, D) = C<sub>and1</sub> * min(d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>) + C<sub>and2</sub> * max(d<sub>A1</sub>, d<sub>A2</sub> ..., d<sub>An</sub>)''

where ''C<sub>or1</sub>, C<sub>or2</sub>'' are "softness" coefficients for the ''or'' operator, and ''C<sub>and1</sub>, C<sub>and2</sub>'' are softness coefficients for the ''and'' operator. Since we would like to give the maximum of the document weights more importance while considering an ''or'' query and the minimum more importance while considering an ''and'' query, generally we have ''C<sub>or1</sub> > C<sub>or2</sub> and C<sub>and1</sub> > C<sub>and2</sub>''. For simplicity it is generally assumed that ''C<sub>or1</sub> = 1 - C<sub>or2</sub>'' and ''C<sub>and1</sub> = 1 - C<sub>and2</sub>''.

Lee and Fox<ref name="leefox">{{citation | last=Lee | first=W. C. | coauthors=E. A. Fox | year=1988 | title=Experimental Comparison of Schemes for Interpreting Boolean Queries}}</ref> experiments indicate that the best performance usually occurs with ''C<sub>and1</sub>'' in the range [0.5, 0.8] and with ''C<sub>or1</sub>'' > 0.2. In general, the computational cost of MMM is low, and retrieval effectiveness is much better than with the [[Standard Boolean model]].

==Paice model==

The Paice model<ref>{{citation | last=Paice | first=C. P. | year=1984 | title=Soft Evaluation of Boolean Search Queries in Information Retrieval Systems | publisher=Information Technology, Res. Dev. Applications, 3(1), 33-42 }}</ref> is a general extension to the MMM model. In comparison to the MMM model that considers only the minimum and maximum weights for the index terms, the Paice model incorporates all of the term weights when calculating the similarity:

:<math>S(D,Q) = \sum_{i=1}^n\frac{r^{i-1}*w_{di}}{\sum_{j=1}^n r^{j-1}}</math>

where ''r'' is a constant coefficient and ''w<sub>di</sub>'' is arranged in ascending order for ''and'' queries and descending order for ''or'' queries. When n = 2 the Paice model shows the same behavior as the MMM model.

The experiments of Lee and Fox<ref name="leefox"/> have shown that setting the ''r'' to 1.0 for ''and'' queries and 0.7 for ''or'' queries gives good retrieval effectiveness. The computational cost for this model is higher than that for the MMM model. This is because the MMM model only requires the determination of ''min'' or ''max'' of a set of term weights each time an ''and'' or ''or'' clause is considered, which can be done in ''O(n)''. The Paice model requires the term weights to be sorted in ascending or descending order, depending on whether an ''and'' clause or an ''or'' clause is being considered. This requires at least an ''0(n log n)'' sorting algorithm. A good deal of floating point calculation is needed too.

==Improvements over the Standard Boolean model==
Lee and Fox<ref name="leefox"/> compared the Standard Boolean model with MMM and Paice models with three test collections, CISI, CACM and INSPEC. These are the reported results for average mean precision improvement:
{| class="wikitable"
|-
!
! CISI
! CACM
! INSPEC
|-
! MMM
| 68%
| 109%
| 195%
|-
! Paice
| 77%
| 104%
| 206%
|}

These are very good improvements over the Standard model. MMM is very close to Paice and P-norm results which indicates that it can be a very good technique, and is the most efficient of the three.

==Recent work==

Recently '''Kang ''et al.'''.<ref>{{citation | title=Fuzzy Information Retrieval Indexed by Concept Identification | url=http://www.springerlink.com/content/ac96v4qf4f8adatp/ | last=Kang | first=Bo-Yeong | coauthors=Dae-Won Kim, Hae-Jung Kim | publisher=Springer Berlin / Heidelberg | year=2005}}</ref> have devised a fuzzy retrieval system indexed by concept identification.

If we look at documents on a pure [[Tf-idf]] approach, even eliminating stop words, there will be words more relevant to the topic of the document than others and they will have the same weight because they have the same term frequency. If we take into account the user intent on a query we can better weight the terms of a document. Each term can be identified as a concept in a certain lexical chain that translates the importance of that concept for that document.<br />
They report improvements over Paice and P-norm on the average precision and recall for the Top-5 retrieved documents.

Zadrozny<ref>{{citation | title=Fuzzy information retrieval model revisited | doi=10.1016/j.fss.2009.02.012 | first=Sawomir | last=Zadrozny | coauthors=Nowacka, Katarzyna | year=2009 | publisher=Elsevier North-Holland, Inc.}}</ref> revisited the fuzzy information retrieval model. He further extends the fuzzy extended Boolean model by:
* assuming linguistic terms as importance weights of keywords also in documents
* taking into account the uncertainty concerning the representation of documents and queries
* interpreting the linguistic terms in the representation of documents and queries as well as their matching in terms of the Zadehs fuzzy logic (calculus of linguistic statements)
* addressing some pragmatic aspects of the proposed model, notably the techniques of indexing documents and queries

The proposed model makes it possible to grasp both imprecision and uncertainty concerning the textual information representation and retrieval.

==See also==
*[[Information retrieval]]

==Further reading==
* {{citation | title=Information Retrieval: Algorithms and Data structures; Extended Boolean model | last=Fox | first=E. | coauthors=S. Betrabet , M. Koushik , W. Lee | year=1992 | publisher=Prentice-Hall, Inc. | url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes}}

==References==
{{reflist}}

{{DEFAULTSORT:Fuzzy Retrieval}}
[[Category:Information retrieval]]
>>EOP<<
97<|###|>Search suggest drop-down list
{{Refimprove|date=June 2009}}

A '''search [[Suggestion|suggest]] [[drop-down list]]''' is a [[Query language|query]] feature used in [[computing]]. A quick system to show the searcher [[Computer shortcut|shortcut]]s, while the query is typed. Before the query has been typed, a drop-down list with the suggested complete search queries, is given as options to select and access. The suggested queries then enable the searcher to complete the required search quickly.

It is a form of [[Autocomplete|autocompletion]] while typing into a query [[text box]], before a detailed search result is entered. Lists can be based on popular searches or other options. The [[Computer science|computing science]] of [[syntax]] and [[algorithm]]s are used to form search results from data or a [[database]], with search suggested drop-down lists being a common industry standard for an instant search.

Search suggested lists are used by [[internet browsers]], [[website]]s and [[search engine]]s, local [[operating system]]s and [[database]]s.

[[Content management system]]s and frequent searches can assist [[Software engineering|software engineers]] in [[Optimization (computer science)|optimizing]] more refined queries with methods of parameters and subroutines. Suggestions can be results for the current query or related queries by words, time and dates, categories and [[Tag (metadata)|tags]]. The suggestion list may be reordered by other options, as [[Enumeration|enumerative]], [[Hierarchical organization|hierarchical]] or [[Faceted classification|faceted]].

==See also ==
*[[Autocomplete]]
*[[Search engine (computing)]]
*[[Search box]]
*[[Search algorithm]]
*[[Censorship by Google#Search suggestions]]


{{DEFAULTSORT:Search Suggest Drop-Down List}}
[[Category:Data search engines]]
[[Category:Information retrieval]]
[[Category:Search algorithms]]
>>EOP<<
103<|###|>Exploratory search
'''Exploratory search''' is a specialization of information exploration which represents the activities carried out by searchers who are either:<nowiki>[3]</nowiki>
* a) unfamiliar with the domain of their goal (i.e. need to learn about the topic in order to understand how to achieve their goal)
* b) unsure about the ways to achieve their goals (either the technology or the process)
* c) or even unsure about their goals in the first place.

Consequently, exploratory search covers a broader class of activities than typical [[information retrieval]], such as investigating, evaluating, comparing, and synthesizing, where new information is sought in a defined conceptual area; [[exploratory data analysis]] is another example of an information exploration activity. Typically, therefore, such users generally combine querying and browsing strategies to foster learning and investigation.

==History==
Exploratory search is a topic that has grown from the fields of [[information retrieval]] and [[information seeking]] but has become more concerned with alternatives to the kind of search that has received the majority of focus (returning the most relevant documents to a [[Google]]-like keyword search). The research is motivated by questions like "what if the user doesn't know which keywords to use?" or "what if the user isn't looking for a single answer?" Consequently, research has begun to focus on defining the broader set of ''information behaviors'' in order to learn about the situations when a user is, or feels, limited by only having the ability to perform a keyword search.

In the last few years, a series of workshops have been held at various related and key events. In 2005, the [http://research.microsoft.com/~ryenw/xsi/index.html Exploratory Search Interfaces workshop] focused on beginning to define some of the key challenges in the field. Since then a series of other workshops have been held at related conferences: [http://research.microsoft.com/~ryenw/eess/index.html Evaluating Exploratory Search] at [http://www.sigir2006.org SIGIR06] and [http://research.microsoft.com/~ryenw/esi/index.html Exploratory Search and HCI] at [http://www.chi2007.org CHI07] (in order to meet with the experts in [[humancomputer interaction]]).

In March 2008, an [http://www.sciencedirect.com/science/journal/03064573 ''Information Processing and Management'' special issue]<nowiki>[2]</nowiki> has focused particularly on the challenges of evaluating exploratory search, given the reduced assumptions that can be made about scenarios of use.

In June 2008, the [[National Science Foundation]] sponsored an [http://www.ils.unc.edu/ISSS_workshop/ invitational workshop] to identify a research agenda for exploratory search and similar fields for the coming years.

==Research challenges==

===Important scenarios===
With the majority of research in the [[information retrieval]] community focusing on typical keyword search scenarios, one challenge for exploratory search is to further understand the scenarios of use for when keyword search is not sufficient. An example scenario, often used to motivate the research by [http://mspace.fm mSpace] states: if a user does not know much about classical music, how should they even begin to find a piece that they might like.

===Designing new interfaces===
With one of the motivations being to support users when keyword search is not enough, some research has focused on identifying alternative user interfaces and interaction models that support the user in different ways. An example is [[Faceted classification|faceted search]] which presents diverse category-style options to the user, so that they can choose from a list instead of guess a possible keyword query.

Many of the [[humancomputer information retrieval|interactive forms of search]], including [[faceted browser]]s, are being considered for their support of exploratory search conditions.

Computational cognitive models of exploratory search have been developed to capture the cognitive complexities involved in exploratory search. Model-based dynamic presentation of information cues are proposed to facilitate exploratory search performance.<ref>Fu, W.-T., Kannampalill, T. G., & Kang, R. (2010). Facilitating exploratory search by model-based navigational cues. In Proceedings of the ACM International conference on Intelligent User Interface. 199-208.  http://portal.acm.org/citation.cfm?id=1719970.1719998</ref>

===Evaluating interfaces===
As the tasks and goals involved with exploratory search are largely undefined or unpredictable, it is very hard to evaluate systems with the measures often used in information retrieval. Accuracy was typically used to show that a user had found a correct answer, but when the user is trying to summarize a domain of information, the ''correct'' answer is near impossible to identify, if not entirely subjective (for example: possible hotels to stay in Paris). In exploration, it is also arguable that spending more time (where time efficiency is typically desirable) researching a topic shows that a system provides increased support for investigation. Finally, and perhaps most importantly, giving study participants a well specified task could immediately prevent them from exhibiting exploratory behavior.

===Models of exploratory search behavior===
There has been recent attempts to develop process model of exploratory search behavior, especially in social information system (e.g., see [[models of collaborative tagging]].<ref>{{Citation
  | doi = 10.1145/1460563.1460600
  | last1 = Fu  | first1 = Wai-Tat
  | title = The Microstructures of Social Tagging: A Rational Model
  | journal = Proceedings of the ACM 2008 conference on Computer Supported Cooperative Work. 
  | pages = 6672
  | date = April 2008
  | url = http://portal.acm.org/citation.cfm?id=1460600
  | isbn = 978-1-60558-007-4 }}
</ref>
.<ref>{{Citation
  | last1 = Fu  | first1 = Wai-Tat
  | title = A Semantic Imitation Model of Social Tagging
  | journal = Proceedings of the IEEE conference on Social Computing
  | pages = 6672
  | date = Aug 2009
  | url = http://www.humanfactors.illinois.edu/Reports&PapersPDFs/IEEESocialcom09/A%20Semantic%20Imitation%20Model%20of%20Social%20Tag%20Choices%20(2).pdf }}</ref> The process model assumes that user-generated information cues, such as social tags, can act as navigational cues that facilitate exploration of information that others have found and shared with other users on a social information system (such as [[social bookmarking]] system). These models provided extension to existing process model of information search that characterizes information-seeking behavior in traditional fact-retrievals using search engines.<ref>
{{Citation
  | last1 = Fu  | first1 = Wai-Tat
  | last2 = Pirolli  | first2 = Peter
  | title = SNIF-ACT: a cognitive model of user navigation on the world wide web
  | journal = Human-Computer Interaction
  | pages = 335412
  | year = 2007
  | url = http://portal.acm.org/citation.cfm?id=1466608
  | volume = 22}}</ref><ref>Kitajima, M., Blackmon, M. H., & Polson, P. G. (2000). A comprehension-based
model of Web navigation and its application to Web usability analysis. In S. Mc-
Donald, Y. Waern, & G. Cockton (Eds.), People and computers XIVUsability or else!
New York: Springer-Verlag.</ref><ref>Miller, C. S., & Remington, R.W. (2004). Modeling information navigation: Implications
for information architecture. Human Computer Interaction, 19, 225271.</ref>
Recent development in exploratory search is often concentrated in predicting user's search intents in interaction with the user.<ref>
{{Citation
  | last1 = Ruotsalo  | first1 = Tuukka
  | last2 = Athukorala  | first2 = Kumaripaba
  | last3 = Glowacka  | first3 = Dorota
  | last4 = Konuyshkova  | first4 = Ksenia
  | last5 = Oulasvrita  | first5 = Antti
  | last6 = Kaipiainen  | first6 = Samuli
  | last7 = Kaski  | first7 = Samuel
  | last8 = Jacucci  | first8 = Giulio
  | title = Supporting exploratory search tasks with interactive user modeling
  | journal = Proceedings of the 76th Annual Meeting of the American Society for Information Science and Technology ASIS&T
  | year = 2013}}
</ref>
Such predictive user modeling, also referred as intent modeling, can help users to get accustomed to a body of domain knowledge and help users to make sense of the potential directions to be explored around their initial, often vague, expression of information needs
<ref>
{{Citation
  | last1 = Ruotsalo  | first1 = Tuukka
  | last2 = Peltonen  | first2 = Jaakko
  | last3 = Eugster | first3 = Manuel J.A.
  | last4 = Glowacka  | first4 = Dorota
  | last5 = Konuyshkova  | first5 = Ksenia
  | last6 = Athukorala  | first6 = Kumaripaba
  | last7 = Kosunen | first7 = Ilkka    
  | last8 = Reijonen  | first8 = Aki
  | last9 = Myllymaki | first9 = Petri
  | last10 = Kaski  | first10 = Samuel
  | last11 = Jacucci  | first11 = Giulio
  | title = Directing Exploratory Search with Interactive Intent Modeling
  | journal = Proceedings of the ACM Conference of Information and Knowledge Management CIKM
  | year = 2013}}
</ref>
.<ref>
{{Citation
  | last1 = Glowacka  | first1 = Dorota
  | last2 = Ruotsalo  | first2 = Tuukka
  | last3 = Konuyshkova  | first3 = Ksenia
  | last4 = Athukorala  | first4 = Kumaripaba
  | last5 = Kaski  | first5 = Samuel
  | last6 = Jacucci  | first6 = Giulio
  | title = Directing exploratory search: Reinforcement learning from user interactions with keywords
  | journal = Proceedings of the ACM Conference of Intelligent User Interfaces IUI
  | url = http://dl.acm.org/citation.cfm?id=2449413
  | pages = 117128 
  | year = 2013}}
</ref>

==Major figures==

Key figures, including experts from both [[information seeking]] and [[humancomputer interaction]], are:
*[http://research.microsoft.com/~ryenw Ryen White]
*[http://ils.unc.edu/~march Gary Marchionini]
*[http://comminfo.rutgers.edu/~belkin/belkin.html Nicholas Belkin]
*[http://users.ecs.soton.ac.uk/mc m.c. schraefel]
*[[Marcia Bates]]

==References==
<References/>
#White, R.W., Kules, B., Drucker, S.M., and schraefel, m.c. (2006). ''Supporting Exploratory Search'', Introduction to Special Section of Communications of the ACM, Vol. 49, Issue 4, (2006), pp.&nbsp;3639.
#Ryen W. White, Gary Marchionini, Gheorghe Muresan (2008). ''Evaluating exploratory search systems: Introduction to special topic issue of information processing and management'' Vol. 44, Issue 2, (2008), pp.&nbsp;433436
#Ryen W. White and Resa A. Roth (2009). ''Exploratory Search: Beyond the Query-Response Paradigm'', San Rafael, CA: Morgan and Claypool.
#P. Papadakos, S. Kopidaki, N. Armenatzoglou and Y. Tzitzikas (2009). ''Exploratory Web Searching with Dynamic Taxonomies and Results Clustering'',13th European Conference on Digital Libraries (ECDL'09), Corfu, Greece, Sep-Oct 2009

{{DEFAULTSORT:Exploratory Search}}
[[Category:Humancomputer interaction]]
[[Category:Information retrieval]]
[[Category:Information science]]
>>EOP<<
109<|###|>Stemming
{{Expert-subject|date=October 2010}}
{{for|the skiing technique|Stem (skiing)}}
'''Stemming''' is the term used in [[linguistic morphology]] and [[information retrieval]] to describe the process for reducing inflected (or sometimes derived) words to their [[word stem]], base or [[root (linguistics)|root]] formgenerally a written word form. The stem needs not to be identical to the [[morphological root]] of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. [[Algorithm]]s for stemming have been studied in [[computer science]] since the 1960s. Many [[search engine]]s treat words with the same stem as [[synonym]]s as a kind of [[query expansion]], a process called conflation.

Stemming programs are commonly referred to as stemming algorithms or stemmers.

==Examples==
A stemmer for English, for example, should identify the [[string literal|string]] "cats" (and possibly "catlike", "catty" etc.) as based on the root "cat", and "stemmer", "stemming", "stemmed" as based on "stem". A stemming algorithm reduces the words "fishing", "fished", and "fisher" to the root word, "fish". On the other hand, "argue", "argued", "argues", "arguing", and "argus" reduce to the stem "argu" (illustrating the case where the stem is not itself a word or root) but "argument" and "arguments" reduce to the stem "argument".<!-- using the Porter algorithm -->

==History==
The first published stemmer was written by [[Julie Beth Lovins]] in 1968.<ref>{{cite journal |first=Julie Beth |last=Lovins |year=1968 |title=Development of a Stemming Algorithm |journal=Mechanical Translation and Computational Linguistics |volume=11 |pages=2231 }}</ref> This paper was remarkable for its early date and had great influence on later work in this area.

A later stemmer was written by [[Martin Porter]] and was published in the July 1980 issue of the journal ''Program''. This stemmer was very widely used and became the de facto standard algorithm used for English stemming. Dr. Porter received the [[Tony Kent Strix award]] in 2000 for his work on stemming and information retrieval.

Many implementations of the Porter stemming algorithm were written and freely distributed; however, many of these implementations contained subtle flaws. As a result, these stemmers did not match their potential. To eliminate this source of error, Martin Porter released an official [http://tartarus.org/~martin/PorterStemmer/ free-software implementation] of the algorithm around the year 2000. He extended this work over the next few years by building [[Snowball programming language|Snowball]], a framework for writing stemming algorithms, and implemented an improved English stemmer together with stemmers for several other languages.

==Algorithms==
There are several types of stemming algorithms which differ in respect to performance and accuracy and how certain stemming obstacles are overcome.

===Lookup algorithms===
A simple stemmer looks up the inflected form in a [[lookup table]]. The advantages of this approach is that it is simple, fast, and easily handles exceptions. The disadvantages are that all inflected forms must be explicitly listed in the table: new or unfamiliar words are not handled, even if they are perfectly regular (e.g. iPads ~ iPad), and the table may be large. For languages with simple morphology, like English, table sizes are modest, but highly inflected languages like Turkish may have hundreds of potential inflected forms for each root.

A lookup approach may use preliminary part-of-speech tagging to avoid overstemming.<ref>Yatsko, V. A.; [http://yatsko.zohosites.com/y-stemmer.html ''Y-stemmer'']</ref>

====The production technique====
The lookup table used by a stemmer is generally produced semi-automatically. For example, if the word is "run", then the inverted algorithm might automatically generate the forms "running", "runs", "runned", and "runly". The last two forms are valid constructions, but they are unlikely.

===Suffix-stripping algorithms===
Suffix stripping algorithms do not rely on a lookup table that consists of inflected forms and root form relations. Instead, a typically smaller list of "rules" is stored which provides a path for the algorithm, given an input word form, to find its root form. Some examples of the rules include:
* if the word ends in 'ed', remove the 'ed'
* if the word ends in 'ing', remove the 'ing'
* if the word ends in 'ly', remove the 'ly'

Suffix stripping approaches enjoy the benefit of being much simpler to maintain than brute force algorithms, assuming the maintainer is sufficiently knowledgeable in the challenges of linguistics and morphology and encoding suffix stripping rules. Suffix stripping algorithms are sometimes regarded as crude given the poor performance when dealing with exceptional relations (like 'ran' and 'run'). The solutions produced by suffix stripping algorithms are limited to those [[lexical category|lexical categories]] which have well known suffixes with few exceptions. This, however, is a problem, as not all parts of speech have such a well formulated set of rules. Lemmatisation attempts to improve upon this challenge.

Prefix stripping may also be implemented. Of course, not all languages use prefixing or suffixing.

====Additional algorithm criteria====
Suffix stripping algorithms may differ in results for a variety of reasons. One such reason is whether the algorithm constrains whether the output word must be a real word in the given language. Some approaches do not require the word to actually exist in the language lexicon (the set of all words in the language). Alternatively, some suffix stripping approaches maintain a database (a large list) of all known morphological word roots that exist as real words. These approaches check the list for the existence of the term prior to making a decision. Typically, if the term does not exist, alternate action is taken. This alternate action may involve several other criteria. The non-existence of an output term may serve to cause the algorithm to try alternate suffix stripping rules.

It can be the case that two or more suffix stripping rules apply to the same input term, which creates an ambiguity as to which rule to apply. The algorithm may assign (by human hand or stochastically) a priority to one rule or another. Or the algorithm may reject one rule application because it results in a non-existent term whereas the other overlapping rule does not. For example, given the English term ''friendlies'', the algorithm may identify the ''ies'' suffix and apply the appropriate rule and achieve the result of ''friendl''. ''friendl'' is likely not found in the lexicon, and therefore the rule is rejected.

One improvement upon basic suffix stripping is the use of suffix substitution. Similar to a stripping rule, a substitution rule replaces a suffix with an alternate suffix. For example, there could exist a rule that replaces ''ies'' with ''y''. How this affects the algorithm varies on the algorithm's design. To illustrate, the algorithm may identify that both the ''ies'' suffix stripping rule as well as the suffix substitution rule apply. Since the stripping rule results in a non-existent term in the lexicon, but the substitution rule does not, the substitution rule is applied instead. In this example, ''friendlies'' becomes ''friendly'' instead of ''friendl''.

Diving further into the details, a common technique is to apply rules in a cyclical fashion (recursively, as computer scientists would say). After applying the suffix substitution rule in this example scenario, a second pass is made to identify matching rules on the term ''friendly'', where the ''ly'' stripping rule is likely identified and accepted. In summary, ''friendlies'' becomes (via substitution) ''friendly'' which becomes (via stripping) ''friend''.

This example also helps illustrate the difference between a rule-based approach and a brute force approach. In a brute force approach, the algorithm would search for ''friendlies'' in the set of hundreds of thousands of inflected word forms and ideally find the corresponding root form ''friend''. In the rule-based approach, the three rules mentioned above would be applied in succession to converge on the same solution. Chances are that the rule-based approach would be slower, as lookup algorithms have a direct access to the solution, while rule-based should try several options, and combinations of them, and then choose which result seems to be the best.

===Lemmatisation algorithms===
A more complex approach to the problem of determining a stem of a word is [[lemmatisation]]. This process involves first determining the [[part of speech]] of a word, and applying different normalization rules for each part of speech. The part of speech is first detected prior to attempting to find the root since for some languages, the stemming rules change depending on a word's part of speech.

This approach is highly conditional upon obtaining the correct lexical category (part of speech). While there is overlap between the normalization rules for certain categories, identifying the wrong category or being unable to produce the right category limits the added benefit of this approach over suffix stripping algorithms. The basic idea is that, if the stemmer is able to grasp more information about the word being stemmed, then it can apply more accurate normalization rules (which unlike suffix stripping rules can also modify the stem).

===Stochastic algorithms===
[[Stochastic]] algorithms involve using probability to identify the root form of a word. Stochastic algorithms are trained (they "learn") on a table of root form to inflected form relations to develop a probabilistic model. This model is typically expressed in the form of complex linguistic rules, similar in nature to those in suffix stripping or lemmatisation. Stemming is performed by inputting an inflected form to the trained model and having the model produce the root form according to its internal ruleset, which again is similar to suffix stripping and lemmatisation, except that the decisions involved in applying the most appropriate rule, or whether or not to stem the word and just return the same word, or whether to apply two different rules sequentially, are applied on the grounds that the output word will have the highest probability of being correct (which is to say, the smallest probability of being incorrect, which is how it is typically measured).

Some lemmatisation algorithms are stochastic in that, given a word which may belong to multiple parts of speech, a probability is assigned to each possible part. This may take into account the surrounding words, called the context, or not. Context-free grammars do not take into account any additional information. In either case, after assigning the probabilities to each possible part of speech, the most likely part of speech is chosen, and from there the appropriate normalization rules are applied to the input word to produce the normalized (root) form.

===''n''-gram analysis===
Some stemming techniques use the [[n-gram]] context of a word to choose the correct stem for a word.<ref name="Workshop2006">{{cite book|author=Cross-Language Evaluation Forum. Workshop|title=Accessing Multilingual Information Repositories: 6th Workshop of the Cross-Language Evaluation Forum, CLEF 2005, Vienna, Austria, 21-23 September, 2005, Revised Selected Papers|url=http://books.google.com/books?id=jWKzNGr1H0AC&pg=PA159|date=20 September 2006|publisher=Springer|isbn=978-3-540-45697-1|pages=159}}</ref>

===Hybrid approaches===
Hybrid approaches use two or more of the approaches described above in unison. A simple example is a [[probabilistic suffix tree|suffix tree]] algorithm which first consults a lookup table using brute force. However, instead of trying to store the entire set of relations between words in a given language, the lookup table is kept small and is only used to store a minute amount of "frequent exceptions" like "ran => run". If the word is not in the exception list, apply suffix stripping or lemmatisation and output the result.

===Affix stemmers===
In [[linguistics]], the term [[affix]] refers to either a [[prefix]] or a [[suffix]]. In addition to dealing with suffixes, several approaches also attempt to remove common prefixes. For example, given the word ''indefinitely'', identify that the leading "in" is a prefix that can be removed. Many of the same approaches mentioned earlier apply, but go by the name '''affix stripping'''. A study of affix stemming for several European languages can be found here.<ref>Jongejan, B.; and Dalianis, H.; ''Automatic Training of Lemmatization Rules that Handle Morphological Changes in pre-, in- and Suffixes Alike'', in the ''Proceeding of the ACL-2009, Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Singapore, August 27, 2009'', pp. 145-153
[http://www.aclweb.org/anthology/P/P09/P09-1017.pdf]</ref>

===Matching algorithms===
Such algorithms use a stem database (for example a set of documents that contain stem words). These stems, as mentioned above, are not necessarily valid words themselves (but rather common sub-strings, as the "brows" in "browse" and in "browsing"). In order to stem a word the algorithm tries to match it with stems from the database, applying various constraints, such as on the relative length of the candidate stem within the word (so that, for example, the short prefix "be", which is the stem of such words as "be", "been" and "being", would not be considered as the stem of the word "beside").

==Language challenges==
While much of the early academic work in this area was focused on the English language (with significant use of the Porter Stemmer algorithm), many other languages have been investigated.<ref>Dolamic, Ljiljana; and Savoy, Jacques; [http://clef.isti.cnr.it/2007/working_notes/DolamicCLEF2007.pdf ''Stemming Approaches for East European Languages (CLEF 2007)'']</ref><ref>Savoy, Jacques; [http://portal.acm.org/citation.cfm?doid=1141277.1141523 ''Light Stemming Approaches for the French, Portuguese, German and Hungarian Languages''], ACM Symposium on Applied Computing, SAC 2006, ISBN 1-59593-108-2</ref><ref>Popovic, Mirko; and Willett, Peter (1992); [http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199206%2943:5%3C384::AID-ASI6%3E3.0.CO;2-L/abstract ''The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data''], Journal of the [[American Society for Information Science]], Volume 43, Issue 5 (June), pp. 384390</ref><ref>[http://staff.science.uva.nl/~mdr/Publications/Files/clef2005-proc-adhoc.pdf ''Stemming in Hungarian at CLEF 2005'']</ref><ref>Viera, A. F. G. & Virgil, J. (2007); [http://InformationR.net/ir/12-3/paper315.html ''Uma revisao dos algoritmos de radicalizacao em lingua portuguesa''], Information Research, 12(3), paper 315</ref>

Hebrew and Arabic are still considered difficult research languages for stemming. English stemmers are fairly trivial (with only occasional problems, such as "dries" being the third-person singular present form of the verb "dry", "axes" being the plural of "axe" as well as "axis"); but stemmers become harder to design as the morphology, orthography, and character encoding of the target language becomes more complex. For example, an Italian stemmer is more complex than an English one (because of a greater number of verb inflections), a Russian one is more complex (more noun [[declension]]s), a Hebrew one is even more complex (due to [[nonconcatenative morphology]], a writing system without vowels, and the requirement of prefix stripping: Hebrew stems can be two, three or four characters, but not more), and so on.

===Multilingual stemming===
Multilingual stemming applies morphological rules of two or more languages simultaneously instead of rules for only a single language when interpreting a search query. Commercial systems using multilingual stemming exist{{CN|date=October 2013}}.

==Error metrics==
There are two error measurements in stemming algorithms, overstemming and understemming. Overstemming is an error where two separate inflected words are stemmed to the same root, but should not have beena [[false positive]]. Understemming is an error where two separate inflected words should be stemmed to the same root, but are nota [[false negative]]. Stemming algorithms attempt to minimize each type of error, although reducing one type can lead to increasing the other.

For example, the widely used Porter stemmer stems "universal", "university", and "universe" to "univers". This is a case of overstemming: though these three words are etymologically related, their modern meanings are in widely different domains, so treating them as synonyms in a search engine will likely reduce the relevance of the search results.

An example of understemming in the Porter stemmer is "alumnus"  "alumnu", "alumni"  "alumni", "alumna"/"alumnae"  "alumna".  This English word keeps Latin morphology, and so these near-synonyms are not conflated.

==Applications==
Stemming is used as an approximate method for grouping words with a similar basic meaning together. For example, a text mentioning "daffodils" is probably closely related to a text mentioning "daffodil" (without the s). But in some cases, words with the same morphological stem have [[idiom]]atic meanings which are not closely related: a user searching for "marketing" will not be satisfied by most documents mentioning "markets" but not "marketing".

===Information retrieval===
Stemmers are common elements in [[Information Retrieval|query systems]] such as [[World Wide Web|Web]] [[search engine]]s. The effectiveness of stemming for English query systems were soon found to be rather limited, however, and this has led early [[information retrieval]] researchers to deem stemming irrelevant in general.<ref>Baeza-Yates, Ricardo; and Ribeiro-Neto, Berthier (1999); ''Modern Information Retrieval'', ACM Press/Addison Wesley</ref> An alternative approach, based on searching for [[n-gram]]s rather than stems, may be used instead. Also, recent research has shown greater benefits for retrieval in other languages.<ref>Kamps, Jaap; Monz, Christof; de Rijke, Maarten; and Sigurbjornsson, Borkur (2004); ''Language-Dependent and Language-Independent Approaches to Cross-Lingual Text Retrieval'', in Peters, C.; Gonzalo, J.; Braschler, M.; and Kluck, M. (eds.); ''Comparative Evaluation of Multilingual Information Access Systems'', Springer Verlag, pp. 152165</ref><ref>Airio, Eija (2006); ''Word Normalization and Decompounding in Mono- and Bilingual IR'', Information Retrieval '''9''':249271</ref>

===Domain Analysis===
Stemming is used to determine domain vocabularies in [[domain analysis]].
<ref>Frakes, W.; Prieto-Diaz, R.; & Fox, C. (1998); ''DARE: Domain Analysis and Reuse Environment'', Annals of Software Engineering (5), pp. 125-141</ref>

===Use in commercial products===
Many commercial companies have been using stemming since at least the 1980s and have produced algorithmic and lexical stemmers in many languages.<ref>[http://www.dtsearch.co.uk/language.htm ''Language Extension Packs''], dtSearch</ref><ref>[http://technet2.microsoft.com/Office/en-us/library/87065c9d-d39d-479d-909b-02160ec6d7791033.mspx?mfr=true ''Building Multilingual Solutions by using Sharepoint Products and Technologies''], Microsoft Technet</ref>

The Snowball stemmers have been compared with commercial lexical stemmers with varying results.<ref>[http://clef.isti.cnr.it/2003/WN_web/19.pdf CLEF 2003: Stephen Tomlinson compared the Snowball stemmers with the Hummingbird lexical stemming (lemmatization) system]</ref><ref>[http://clef.isti.cnr.it/2004/working_notes/WorkingNotes2004/21.pdf CLEF 2004: Stephen Tomlinson "Finnish, Portuguese and Russian Retrieval with Hummingbird SearchServer"]</ref>

[[Google search]] adopted word stemming in 2003.<ref>[http://www.google.com/support/bin/static.py?page=searchguides.html&ctx=basics#stemming ''The Essentials of Google Search''], Web Search Help Center, [[Google|Google Inc.]]</ref> Previously a search for "fish" would not have returned "fishing". Other software search algorithms vary in their use of word stemming. Programs that simply search for substrings obviously will find "fish" in "fishing" but when searching for "fishes" will not find occurrences of the word "fish".

==See also==
* [[Root (linguistics)]] - linguistic definition of the term "root"
* [[Stem (linguistics)]] - linguistic definition of the term "stem"
* [[Morphology (linguistics)]]
* [[Lemma (morphology)]] - linguistic definition
* [[Lemmatization]]
* [[Lexeme]]
* [[Inflection]]
* [[Derivation (linguistics)|Derivation]] - stemming is a form of reverse derivation
* [[Natural language processing]] - stemming is generally regarded as a form of NLP
* [[Text mining]] - stemming algorithms play a major role in commercial NLP software
* [[Computational linguistics]]

{{Natural Language Processing}}

==References==
{{reflist|2}}

==Further reading==
{{refbegin|2}}
* Dawson, J. L. (1974); ''Suffix Removal for Word Conflation'', Bulletin of the Association for Literary and Linguistic Computing, 2(3): 3346
* Frakes, W. B. (1984); ''Term Conflation for Information Retrieval'', Cambridge University Press
* Frakes, W. B. & Fox, C. J. (2003); ''Strength and Similarity of Affix Removal Stemming Algorithms'', SIGIR Forum, 37: 2630
* Frakes, W. B. (1992); ''Stemming algorithms, Information retrieval: data structures and algorithms'', Upper Saddle River, NJ: Prentice-Hall, Inc.
* Hafer, M. A. & Weiss, S. F. (1974); ''Word segmentation by letter successor varieties'', Information Processing & Management 10 (11/12), 371386
* Harman, D. (1991); ''How Effective is Suffixing?'', Journal of the American Society for Information Science 42 (1), 715
* Hull, D. A. (1996); ''Stemming Algorithms&nbsp; A Case Study for Detailed Evaluation'', JASIS, 47(1): 7084
* Hull, D. A. & Grefenstette, G. (1996); ''A Detailed Analysis of English Stemming Algorithms'', Xerox Technical Report
* Kraaij, W. & Pohlmann, R. (1996); ''Viewing Stemming as Recall Enhancement'', in Frei, H.-P.; Harman, D.; Schauble, P.; and Wilkinson, R. (eds.); ''Proceedings of the 17th ACM SIGIR conference held at Zurich, August 1822'', pp.&nbsp;4048
* Krovetz, R. (1993); ''Viewing Morphology as an Inference Process'', in ''Proceedings of ACM-SIGIR93'', pp.&nbsp;191203
* Lennon, M.; Pierce, D. S.; Tarry, B. D.; & Willett, P. (1981); ''An Evaluation of some Conflation Algorithms for Information Retrieval'', Journal of Information Science, 3: 177183
* Lovins, J. (1971); ''[http://www.eric.ed.gov/sitemap/html_0900000b800c571a.html Error Evaluation for Stemming Algorithms as Clustering Algorithms]'', JASIS, 22: 2840
* Lovins, J. B. (1968); ''Development of a Stemming Algorithm'', Mechanical Translation and Computational Linguistics, 11, 2231
* Jenkins, Marie-Claire; and Smith, Dan (2005); [http://www.uea.ac.uk/polopoly_fs/1.85493!stemmer25feb.pdf ''Conservative Stemming for Search and Indexing'']
* Paice, C. D. (1990); ''[http://www.comp.lancs.ac.uk/computing/research/stemming/paice/article.htm Another Stemmer]'', SIGIR Forum, 24: 5661
* Paice, C. D. (1996) ''[http://www3.interscience.wiley.com/cgi-bin/abstract/57804/ABSTRACT Method for Evaluation of Stemming Algorithms based on Error Counting]'', JASIS, 47(8): 632649
* Popovic, Mirko; and Willett, Peter (1992); [http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199206%2943:5%3C384::AID-ASI6%3E3.0.CO;2-L/abstract ''The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data''], Journal of the [[American Society for Information Science]], Volume 43, Issue 5 (June), pp.&nbsp;384390
* Porter, Martin F. (1980); ''[http://telemat.det.unifi.it/book/2001/wchange/download/stem_porter.html An Algorithm for Suffix Stripping]'', Program, 14(3): 130137
* Savoy, J. (1993); ''[http://www3.interscience.wiley.com/cgi-bin/abstract/10049824/ABSTRACT?CRETRY=1&SRETRY=0 Stemming of French Words Based on Grammatical Categories]'' Journal of the American Society for Information Science, 44(1), 19
* Ulmschneider, John E.; & Doszkocs, Tamas (1983); ''[http://www.eric.ed.gov/sitemap/html_0900000b8007ea83.html A Practical Stemming Algorithm for Online Search Assistance]'', Online Review, 7(4), 301318
* Xu, J.; & Croft, W. B. (1998); ''[http://portal.acm.org/citation.cfm?doid=267954.267957 Corpus-Based Stemming Using Coocurrence of Word Variants]'', ACM Transactions on Information Systems, 16(1), 6181
{{refend}}

==External links==
*[http://opennlp.apache.org/index.html Apache OpenNLP] includes Porter and Snowball stemmers
* [http://smile-stemmer.appspot.com SMILE Stemmer] - free online service, includes Porter and Paice/Husk' Lancaster stemmers (Java API)
* [http://code.google.com/p/ir-themis/ Themis] - open source IR framework, includes Porter stemmer implementation (PostgreSQL, Java API)
* [http://snowball.tartarus.org Snowball] - free stemming algorithms for many languages, includes source code, including stemmers for five romance languages
* [http://www.iveonik.com/blog/2011/08/snowball-stemmers-on-csharp-free-download/ Snowball on C#] - port of Snowball stemmers for C# (14 languages)
* [http://snowball.tartarus.org/wrappers/guide.html Python bindings to Snowball API]
* [http://locknet.ro/archive/2009-10-29-ann-ruby-stemmer.html Ruby-Stemmer] - Ruby extension to Snowball API
* [http://pecl.php.net/package/stem/ PECL] - PHP extension to the Snowball API
* [http://www.oleandersolutions.com/stemming.html Oleander Porter's algorithm] - stemming library in C++ released under BSD
* [http://www.cs.waikato.ac.nz/~eibe/stemmers/index.html Unofficial home page of the Lovins stemming algorithm] - with source code in a couple of languages
* [http://www.tartarus.org/~martin/PorterStemmer/index.html Official home page of the Porter stemming algorithm] - including source code in several languages
* [http://www.comp.lancs.ac.uk/computing/research/stemming/index.htm Official home page of the Lancaster stemming algorithm] - Lancaster University, UK
* [http://www.cmp.uea.ac.uk/Research/stemmer/ Official home page of the UEA-Lite Stemmer ] - University of East Anglia, UK
* [http://www.comp.lancs.ac.uk/computing/research/stemming/general/index.htm Overview of stemming algorithms]
* [http://code.google.com/p/ptstemmer/ PTStemmer] - A Java/Python/.Net stemming toolkit for the Portuguese language
* [http://mazko.github.com/jssnowball/ jsSnowball] - open source JavaScript implementation of Snowball stemming algorithms for many languages
* [http://trimc-nlp.blogspot.com/2013/08/snowball-stemmer-for-java.html Snowball Stemmer] - implementation for Java
* [http://hlt.di.fct.unl.pt/luis/hindi_stemmer/ hindi_stemmer] - open source stemmer for Hindi
* [http://hlt.di.fct.unl.pt/luis/czech_stemmer/ czech_stemmer] - open source stemmer for Czech
* [http://www.comp.leeds.ac.uk/eric/sawalha08coling.pdf Comparative Evaluation of Arabic Language Morphological Analysers and Stemmers]
* [https://github.com/rdamodharan/tamil-stemmer Tamil Stemmer]

{{FOLDOC}}

[[Category:Linguistic morphology]]
[[Category:Natural language processing]]
[[Category:Tasks of natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval]]
>>EOP<<
115<|###|>Latent semantic analysis
{{mergefrom|Latent semantic indexing|date=July 2012}}
{{semantics}}
'''Latent semantic analysis''' ('''LSA''') is a technique in [[natural language processing]], in particular in [[vectorial semantics]], of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.  LSA assumes that words that are close in meaning will occur in similar pieces of text.  A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called [[singular value decomposition]] (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.  Words are then compared by taking the cosine of the angle between the two vectors (or the [[dot product]] between the [[Unit vector|normalizations]] of the two vectors) formed by any two rows.  Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.<ref>{{cite journal | title=Latent Semantic Analysis | author=Susan T. Dumais |year=2005 | doi=10.1002/aris.1440380105 | journal=Annual Review of Information Science and Technology | volume=38 | pages=188}}</ref>

An information retrieval method using latent semantic structure was patented in 1988 ([http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=4839853 US Patent 4,839,853]) by [[Scott Deerwester]], [[Susan Dumais]], [[George Furnas]], [[Richard Harshman]], [[Thomas Landauer]], [[Karen Lochbaum]] and [[Lynn Streeter]]. In the context of its application to [[information retrieval]], it is sometimes called [[Latent semantic indexing|Latent Semantic Indexing '''(LSI)''']].<ref>{{cite web | url=http://lsa.colorado.edu/ | title=The Latent Semantic Indexing home page}}</ref>

== Overview ==

=== Occurrence matrix ===
LSA can use a [[term-document matrix]] which describes the occurrences of terms in documents; it is a [[sparse matrix]] whose rows correspond to [[terminology|terms]] and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is [[tf-idf]] (term frequencyinverse document frequency): the element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.

This matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.

=== Rank lowering ===
After the construction of the occurrence matrix, LSA finds a [[low-rank approximation]]<ref>Markovsky I. (2012) Low-Rank Approximation: Algorithms, Implementation, Applications, Springer, 2012, ISBN 978-1-4471-2226-5 {{page needed|date=January 2012}}</ref> to the [[term-document matrix]]. There could be various reasons for these approximations:

* The original term-document matrix is presumed too large for the computing resources; in this case, the approximated low rank  matrix is interpreted as an ''approximation'' (a "least and necessary evil").
* The original term-document matrix is presumed ''noisy'': for example, anecdotal instances of terms are to be eliminated. From this point of view, the approximated matrix is interpreted as a ''de-noisified matrix'' (a better matrix than the original).
* The original term-document matrix is presumed overly [[Sparse matrix|sparse]] relative to the "true" term-document matrix.  That is, the original matrix lists only the words actually ''in'' each document, whereas we might be interested in all words ''related to'' each documentgenerally a much larger set due to [[synonymy]].

The consequence of the rank lowering is that some dimensions are combined and depend on more than one term:

:: {(car), (truck), (flower)} -->  {(1.3452 * car + 0.2828 * truck), (flower)}

This mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also mitigates the problem with [[polysemy]], since components of polysemous words that point in the "right" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.

=== Derivation ===
Let <math>X</math> be a matrix where element <math>(i,j)</math> describes the occurrence of term <math>i</math> in document <math>j</math> (this can be, for example, the frequency). <math>X</math> will look like this:

:<math>
\begin{matrix} 
 & \textbf{d}_j \\
 & \downarrow \\
\textbf{t}_i^T \rightarrow &
\begin{bmatrix} 
x_{1,1} & \dots & x_{1,n} \\
\vdots & \ddots & \vdots \\
x_{m,1} & \dots & x_{m,n} \\
\end{bmatrix}
\end{matrix}
</math>

Now a row in this matrix will be a vector corresponding to a term, giving its relation to each document:

:<math>\textbf{t}_i^T = \begin{bmatrix} x_{i,1} & \dots & x_{i,n} \end{bmatrix}</math>

Likewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term:

:<math>\textbf{d}_j = \begin{bmatrix} x_{1,j} \\ \vdots \\ x_{m,j} \end{bmatrix}</math>

Now the [[dot product]] <math>\textbf{t}_i^T \textbf{t}_p</math> between two term vectors gives the [[correlation]] between the terms over the documents. The [[matrix product]] <math>X X^T</math> contains all these dot products. Element <math>(i,p)</math> (which is equal to element <math>(p,i)</math>) contains the dot product <math>\textbf{t}_i^T \textbf{t}_p</math> (<math> = \textbf{t}_p^T \textbf{t}_i</math>). Likewise, the matrix <math>X^T X</math> contains the dot products between all the document vectors, giving their correlation over the terms: <math>\textbf{d}_j^T \textbf{d}_q = \textbf{d}_q^T \textbf{d}_j</math>.

Now, from the theory of linear algebra, there exists a decomposition of <math>X</math> such that <math>U</math> and <math>V</math> are [[orthogonal matrix|orthogonal matrices]] and <math>\Sigma</math> is a [[diagonal matrix]]. This is called a [[singular value decomposition]] (SVD):

:<math>
\begin{matrix}
X = U \Sigma V^T
\end{matrix}
</math>

The matrix products giving us the term and document correlations then become

:<math>
\begin{matrix}
X X^T &=& (U \Sigma V^T) (U \Sigma V^T)^T = (U \Sigma V^T) (V^{T^T} \Sigma^T U^T) = U \Sigma V^T V \Sigma^T U^T = U \Sigma \Sigma^T U^T \\
X^T X &=& (U \Sigma V^T)^T (U \Sigma V^T) = (V^{T^T} \Sigma^T U^T) (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T = V \Sigma^T \Sigma V^T
\end{matrix}
</math>

Since <math>\Sigma \Sigma^T</math> and <math>\Sigma^T \Sigma</math> are diagonal we see that <math>U</math> must contain the [[eigenvector]]s of <math>X X^T</math>, while <math>V</math> must be the eigenvectors of <math>X^T X</math>. Both products have the same non-zero eigenvalues, given by the non-zero entries of <math>\Sigma \Sigma^T</math>, or equally, by the non-zero entries of <math>\Sigma^T\Sigma</math>. Now the decomposition looks like this:

:<math>
\begin{matrix} 
 & X & & & U & & \Sigma & & V^T \\
 & (\textbf{d}_j) & & & & & & & (\hat{\textbf{d}}_j) \\
 & \downarrow & & & & & & & \downarrow \\
(\textbf{t}_i^T) \rightarrow 
&
\begin{bmatrix} 
x_{1,1} & \dots & x_{1,n} \\
\\
\vdots & \ddots & \vdots \\
\\
x_{m,1} & \dots & x_{m,n} \\
\end{bmatrix}
&
=
&
(\hat{\textbf{t}}_i^T) \rightarrow
&
\begin{bmatrix} 
\begin{bmatrix} \, \\ \, \\ \textbf{u}_1 \\ \, \\ \,\end{bmatrix} 
\dots
\begin{bmatrix} \, \\ \, \\ \textbf{u}_l \\ \, \\ \, \end{bmatrix}
\end{bmatrix}
&
\cdot
&
\begin{bmatrix} 
\sigma_1 & \dots & 0 \\
\vdots & \ddots & \vdots \\
0 & \dots & \sigma_l \\
\end{bmatrix}
&
\cdot
&
\begin{bmatrix} 
\begin{bmatrix} & & \textbf{v}_1 & & \end{bmatrix} \\
\vdots \\
\begin{bmatrix} & & \textbf{v}_l & & \end{bmatrix}
\end{bmatrix}
\end{matrix}
</math>

The values <math>\sigma_1, \dots, \sigma_l</math> are called the singular values, and <math>u_1, \dots, u_l</math> and <math>v_1, \dots, v_l</math> the left and right singular vectors.
Notice the only part of <math>U</math> that contributes to <math>\textbf{t}_i</math> is the <math>i\textrm{'th}</math> row.
Let this row vector be called <math>\hat{\textrm{t}}_i</math>.
Likewise, the only part of <math>V^T</math> that contributes to <math>\textbf{d}_j</math> is the <math>j\textrm{'th}</math> column, <math>\hat{ \textrm{d}}_j</math>.
These are ''not'' the eigenvectors, but ''depend'' on ''all'' the eigenvectors.

It turns out that when you select the <math>k</math> largest singular values, and their corresponding singular vectors from <math>U</math> and <math>V</math>, you get the rank <math>k</math> approximation to <math>X</math> with the smallest error ([[Frobenius norm]]). This approximation has a minimal error.  But more importantly we can now treat the term and document vectors as a "semantic space". The vector <math>\hat{\textbf{t}}_i</math> then has <math>k</math> entries mapping it to a lower-dimensional space dimensions. These new dimensions do not relate to any comprehensible concepts. They are a lower-dimensional approximation of the higher-dimensional space. Likewise, the vector <math>\hat{\textbf{d}}_j</math> is an approximation in this lower-dimensional space. We write this approximation as

:<math>X_k = U_k \Sigma_k V_k^T</math>

You can now do the following:
* See how related documents <math>j</math> and <math>q</math> are in the low-dimensional space by comparing the vectors <math>\Sigma_k \hat{\textbf{d}}_j </math> and <math>\Sigma_k \hat{\textbf{d}}_q </math> (typically by [[vector space model|cosine similarity]]).
* Comparing terms <math>i</math> and <math>p</math> by comparing the vectors <math>\Sigma_k \hat{\textbf{t}}_i^T </math> and <math>\Sigma_k \hat{\textbf{t}}_p^T </math>.
* Documents and term vector representations can be clustered using traditional clustering algorithms like k-means using similarity measures like cosine.
* Given a query, view this as a mini document, and compare it to your documents in the low-dimensional space.

To do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents:

:<math>\hat{\textbf{d}}_j = \Sigma_k^{-1} U_k^T \textbf{d}_j</math>

Note here that the inverse of the diagonal matrix <math>\Sigma_k</math> may be found by inverting each nonzero value within the matrix.

This means that if you have a query vector <math>q</math>, you must do the translation <math>\hat{\textbf{q}} = \Sigma_k^{-1} U_k^T \textbf{q}</math> before you compare it with the document vectors in the low-dimensional space. You can do the same for pseudo term vectors:

:<math>\textbf{t}_i^T = \hat{\textbf{t}}_i^T \Sigma_k V_k^T</math>

:<math>\hat{\textbf{t}}_i^T = \textbf{t}_i^T V_k^{-T} \Sigma_k^{-1} = \textbf{t}_i^T V_k \Sigma_k^{-1}</math>

:<math>\hat{\textbf{t}}_i = \Sigma_k^{-1}  V_k^T \textbf{t}_i</math>

== Applications ==

The new low-dimensional space typically can be used to:
* Compare the documents in the low-dimensional space ([[data clustering]], [[document classification]]).
* Find similar documents across languages, after analyzing a base set of translated documents ([[cross language retrieval]]).
* Find relations between terms ([[synonymy]] and [[polysemy]]).
* Given a query of terms, translate it into the low-dimensional space, and find matching documents ([[information retrieval]]).
* Find the best similarity between small groups of terms, in a semantic way (i.e. in a context of a knowledge corpus), as for example in multi choice questions [[Multiple choice question|MCQ]] answering model.<ref name="Alain2009">{{cite journal | url=http://hal.archives-ouvertes.fr/docs/00/38/41/43/PDF/eLSA1-brm20.pdf |format=PDF| title=Effect of tuned parameters on an LSA multiple choice questions answering model | author=Alain Lifchitz, Sandra Jhean-Larose, Guy Denhiere | journal=Behavior Research Methods | volume=41 | issue=4 | pages=12011209 | year=2009  | doi=10.3758/BRM.41.4.1201 | pmid=19897829 }}</ref>

Synonymy and polysemy are fundamental problems in [[natural language processing]]: 
* Synonymy is the phenomenon where different words describe the same idea. Thus, a query in a search engine may fail to retrieve a relevant document that does not contain the words which appeared in the query. For example, a search for "doctors" may not return a document containing the word "[[physicians]]", even though the words have the same meaning.
* Polysemy is the phenomenon where the same word has multiple meanings. So a search may retrieve irrelevant documents containing the desired words in the wrong meaning. For example, a botanist and a computer scientist looking for the word "tree" probably desire different sets of documents.

=== Commercial applications ===

LSA has been used to assist in performing [[prior art]] searches for [[patents]].<ref name="Gerry2007">{{Cite journal | author=Gerry J. Elman | title=Automated Patent Examination Support - A proposal | journal=Biotechnology Law Report | date=October 2007 | doi=10.1089/blr.2007.9896 | volume=26 | issue=5 | pages=435 | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref>

=== Applications in human memory ===

The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of [[free recall]] and memory search.  There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words.  These findings are referred to as the [[Semantic Proximity Effect]].<ref>{{cite journal | url=http://psycnet.apa.org/journals/xlm/25/4/923.pdf |format=PDF| title=Contextual Variability and Serial Position Effects in Free Recall | author=Marc W. Howard and Michael J. Kahana |year=1999}}</ref>

When participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list.  These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall.<ref>{{cite journal | url=https://memory.psych.upenn.edu/files/pubs/ZaroEtal06.pdf |format=PDF| title=Temporal Associations and Prior-List Intrusions in Free Recall | author=Franklin M. Zaromb et al. | booktitle=Interspeech'2005|year=2006}}</ref>

Another model, termed [[Word Association Spaces]] (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs.<ref>{{cite web|last=Nelson|first=Douglas|title=The University of South Florida Word Association, Rhyme and Word Fragment Norms|url=http://w3.usf.edu/FreeAssociation/Intro.html|accessdate=5/8/2011}}</ref>

== Implementation ==

The [[Singular Value Decomposition|SVD]] is typically computed using large matrix methods (for example, [[Lanczos method]]s) but may also be computed incrementally and with greatly reduced resources via a [[neural network]]-like approach, which does not require the large, full-rank matrix to be held in memory.<ref name="Genevi2005">{{cite conference | url=http://www.dcs.shef.ac.uk/~genevieve/gorrell_webb.pdf |format=PDF| title=Generalized Hebbian Algorithm for Latent Semantic Analysis | author=Genevieve Gorrell and Brandyn Webb | booktitle=Interspeech'2005 |year=2005}}</ref>
A fast, incremental, low-memory, large-matrix SVD algorithm has recently been developed.<ref name="brand2006">{{cite journal | url=http://www.merl.com/reports/docs/TR2006-059.pdf |format=PDF| title=Fast Low-Rank Modifications of the Thin Singular Value Decomposition | author=Matthew Brand | journal=Linear Algebra and Its Applications | volume=415 | pages=2030 | year=2006 | doi=10.1016/j.laa.2005.07.021 }}</ref> [http://web.mit.edu/~wingated/www/resources.html MATLAB] and [http://radimrehurek.com/gensim Python] implementations of these fast algorithms are available. Unlike Gorrell and Webb's (2005) stochastic approximation, Brand's algorithm (2003) provides an exact solution.
In recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality.<ref>doi: 10.1109/ICCSNT.2011.6182070</ref>

== Limitations ==
Some of LSA's drawbacks include:

* The resulting dimensions might be difficult to interpret. For instance, in
:: {(car), (truck), (flower)}   {(1.3452 * car + 0.2828 * truck), (flower)}
:the (1.3452 * car + 0.2828 * truck) component could be interpreted as "vehicle". However, it is very likely that cases close to
:: {(car), (bottle), (flower)}   {(1.3452 * car + 0.2828 * '''bottle'''), (flower)}
:will occur. This leads to results which can be justified on the mathematical level, but have no interpretable meaning in natural language.

* LSA cannot capture [[polysemy]] (i.e., multiple meanings of a word){{Citation needed|date=October 2013}}.  Each occurrence of a word is treated as having the same meaning due to the word being represented as a single point in space.  For example, the occurrence of "chair" in a document containing "The Chair of the Board" and in a separate document containing "the chair maker" are considered the same.  The behavior results in the vector representation being an ''average'' of all the word's different meanings in the corpus, which can make it difficult for comparison.  However, the effect is often lessened due to words having a [[word sense disambiguation|predominant sense]] throughout a corpus (i.e. not all meanings are equally likely).

* Limitations of [[bag of words model]] (BOW), where a text is represented as an unordered collection of words.

* To address some of the limitation of [[bag of words model]] (BOW), [[N-gram|multi-gram]] dictionary can be used to find direct and indirect association as well as [[Higher-order statistics|higher-order]] [[co-occurrence]]s among terms.<ref>[http://www.translational-medicine.com/content/12/1/324 J Transl Med. 2014 Nov 27;12(1):324.]</ref>

* The [[probabilistic model]] of LSA does not match observed data: LSA assumes that words and documents form a joint [[normal distribution|Gaussian]] model ([[ergodic hypothesis]]), while a [[Poisson distribution]] has been observed.  Thus, a newer alternative is [[probabilistic latent semantic analysis]], based on a [[multinomial distribution|multinomial]] model, which is reported to give better results than standard LSA.<ref name="Thomas1999">{{cite conference | url=http://www.cs.brown.edu/people/th/papers/Hofmann-UAI99.pdf |format=PDF| title=Probabilistic Latent Semantic Analysis | author=Thomas Hofmann | booktitle=Uncertainty in Artificial Intelligence |year=1999}}</ref>

== See also ==
* [[Compound term processing]]
* [[Explicit semantic analysis]]
* [[Latent semantic mapping]]
* [[Latent Semantic Structure Indexing]]
* [[Principal components analysis]]
* [[Probabilistic latent semantic analysis]]
* [[Spamdexing]]
* [[Topic model]]
** [[Latent Dirichlet allocation]]
* [[Vectorial semantics]]
* [[Coh-Metrix]]

== References ==
{{Reflist}}
* {{cite journal
 | url=http://lsa.colorado.edu/papers/dp1.LSAintro.pdf
 |format=PDF| title=Introduction to Latent Semantic Analysis
 | author=[[Thomas Landauer]], Peter W. Foltz, & Darrell Laham
 | journal=Discourse Processes
 | volume=25
 | pages=259284
 |year=1998
 | doi=10.1080/01638539809545028
 | issue=23
}}
* {{cite journal
 | url=http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf 
 |format=PDF| title=Indexing by Latent Semantic Analysis
 | author=[[Scott Deerwester]], [[Susan Dumais|Susan T. Dumais]], [[George Furnas|George W. Furnas]], [[Thomas Landauer|Thomas K. Landauer]], [[Richard Harshman]]
 | journal=Journal of the American Society for Information Science
 | volume=41
 | issue=6
 | pages=391407
 | year=1990 
 | doi=10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
}} Original article where the model was first exposed.
* {{cite journal
 | url=http://citeseer.ist.psu.edu/berry95using.html
 | title=Using Linear Algebra for Intelligent Information Retrieval
 | author=Michael Berry, [[Susan Dumais|Susan T. Dumais]], Gavin W. O'Brien
 |year=1995
}} [http://lsirwww.epfl.ch/courses/dis/2003ws/papers/ut-cs-94-270.pdf (PDF)]. Illustration of the application of LSA to document retrieval.
* {{cite web
 | url=http://iv.slis.indiana.edu/sw/lsa.html
 | title=Latent Semantic Analysis
 | publisher=InfoVis
}}
* {{cite web
 | url=http://cran.at.r-project.org/web/packages/lsa/index.html
 | title=An Open Source LSA Package for R
 | publisher=CRAN
 | author=Fridolin Wild
 | date=November 23, 2005
 | accessdate=2006-11-20
}}
* {{ cite web
 | url=http://www.welchco.com/02/14/01/60/96/02/2901.HTM
 | title=A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge
 | author=[[Thomas Landauer]], [[Susan Dumais|Susan T. Dumais]]
 | accessdate=2007-07-02
}}

==External links==

===Articles on LSA===
* [http://www.scholarpedia.org/article/Latent_semantic_analysis Latent Semantic Analysis], a scholarpedia article on LSA written by Tom Landauer, one of the creators of LSA.

===Talks and demonstrations===
* [http://videolectures.net/slsfs05_hofmann_lsvm/ LSA Overview], talk by Prof. [http://www.cs.brown.edu/~th/ Thomas Hofmann] describing LSA, its applications in Information Retrieval, and its connections to [[probabilistic latent semantic analysis]].
* [http://www.semanticsearchart.com/researchLSA.html Complete LSA sample code in C# for Windows]. The demo code includes enumeration of text files, filtering stop words, stemming, making a document-term matrix and SVD.

===Implementations===

Due to its cross-domain applications in [[Information Retrieval]], [[Natural Language Processing]] (NLP), [[Cognitive Science]] and [[Computational Linguistics]], LSA has been implemented to support many different kinds of applications.
* [http://www.d.umn.edu/~tpederse/senseclusters.html Sense Clusters], an Information Retrieval-oriented perl implementation of LSA
* [http://code.google.com/p/airhead-research/ S-Space Package], a Computational Linguistics and Cognitive Science-oriented Java implementation of LSA
* [http://code.google.com/p/semanticvectors/ Semantic Vectors] applies Random Projection, LSA, and Reflective Random Indexing to [[Lucene]] term-document matrices
* [http://infomap-nlp.sourceforge.net/ Infomap Project], an NLP-oriented C implementation of LSA (superseded by semanticvectors project)
* [http://scgroup20.ceid.upatras.gr:8000/tmg/index.php/Main_Page Text to Matrix Generator], A MATLAB Toolbox for generating term-document matrices from text collections, with support for LSA
* [[Gensim]] contains a fast, online Python implementation of LSA for matrices larger than RAM.

{{DEFAULTSORT:Latent Semantic Analysis}}
[[Category:Information retrieval]]
[[Category:Natural language processing]]
[[Category:Latent variable models]]

[[fa:   ]]
>>EOP<<
121<|###|>Ness Computing
{{Notability|Companies|date=July 2011}}

'''Ness Computing''' is a personal search company. It was acquired by OpenTable in 2014 and is being shut down in April.<ref>{{cite web|last=Lunden|first=Ingrid|title=OpenTable Buys Ness For $17.3M|url=http://techcrunch.com/2014/02/06/opentable-ness/|work=TechCrunch|accessdate=26 March 2014}}</ref> 

It was founded in October 2009 by Corey Reese,<ref>http://www.linkedin.com/in/coreyreese</ref> Paul Twohey,<ref>http://www.linkedin.com/in/twohey</ref> Nikhil Raghavan,<ref>http://www.linkedin.com/in/nikhilraghavan</ref> and Steven Schlansker.<ref>http://www.linkedin.com/in/stevenschlansker</ref> The company is headquartered in Los Altos, California.

The company, whose mission is to make search personal, is sometimes referred to as the "Palantir for fun". It aims to help people make decisions about dining, nightlife, entertainment, shopping, music, travel and more. 

Ness' mission is to make search personal. The company refers to its technology as the "Likeness Engine", a combination of a [[recommendation engine]] that uses [[machine learning]] to look at data from diverse sources and a traditional [[search engine]] that serves up results based on these signals. 

The free Ness Dining App (for iPhone) has been referred to as the [[Netflix]] <ref>http://eater.com/archives/2011/08/26/ness-iphone-app-recommends-restaurants-using-likeness-score.php</ref> or [[Pandora]] <ref>http://gigaom.com/2011/08/25/ness-restaurant-app/</ref> for restaurants. Based on a user's ratings and preferences, the service will deliver recommendations for a particular time, location, price range, and cuisine preference. Users may view the menu for a place via SinglePlatform,<ref>http://www.singleplatform.com/</ref> browse [[Instagram]] photos tagged at the restaurant, and make reservations in the app via [[OpenTable]]. The app is free and available in the [[App Store (iOS)]].

==References==
{{Reflist}}

[[Category:Information retrieval]]
[[Category:Software companies based in California]]
>>EOP<<
127<|###|>Macroglossa Visual Search
{{Infobox Website
| name           = Macroglossa
| logo           = [[File:Macroglossa Visual Search Engine Logo, 2012.gif]]
| screenshot     = 
| caption        = Macroglossa logo
| url            = [http://www.macroglossa.com macroglossa.com]
| type           = [[Visual search engine|Visual]] [[Search engine|Search Engine]]
| language       = English
| registration   = optional
| author         = MVE
| launch date    = 2010
| current status = beta 0.1
| slogan         = search is visual
| alexa          = {{IncreaseNegative}} 2,828,096 ({{as of|2014|4|1|alt=April 2014}})<ref name="alexa">{{cite web|url= http://www.alexa.com/siteinfo/macroglossa.com |title= Macroglossa.com Site Info | publisher= [[Alexa Internet]] |accessdate= 2014-04-01 }}</ref><!--Updated monthly by OKBot.-->
}}
'''Macroglossa''' is a [[visual search engine]] based on the comparison of images,<ref>Nicola Mattina. "[http://blog.wired.it/startupcloud/2010/12/29/macroglossa-usare-le-immagini-per-effettuare-ricerche-sul-web.html Macroglossa: usare le immagini per effettuare ricerche sul web]", Wired.it, Retrieved December 29, 2010.</ref><ref>GreatStartups.com . "[http://greatstartups.com/2010/10/13/macroglossa-com-whats-in-the-picture/ Macroglossa.com-Whats In The Picture ]", greatstartups.com, Retrieved October 13, 2010.</ref> coming from an Italian Group. The development of the project began in 2009. In April 2010 is released the first public [[Alpha stage#Alpha|alpha]].<ref>Liva Judic. "[http://searchenginewatch.com/article/2050950/Macroglossas-Visual-Search-Engine-fails-to-meet-basic-expectations Macroglossa's Visual Search Engine fails to meet basic expectations ]", SEW - searchenginewatch, Retrieved April 26, 2010.</ref>
Users can upload photos or images that they aren't sure what they are to determine what the images contain. Macroglossa compares images to return search results based on specific search categories. 
The engine does not use technologies and solutions such as [[Optical character recognition|OCR]], [[Tag (metadata)|tags]], vocabulary trees. The comparison is directly based on the contents of the image which the user wants to know more.

Interesting features are the categorization of the elements, the ability to search specific portions of the image or start a search from a video file,<ref>Mve. "[http://www.macroglossa.com/press_macrog_eng_a2dot0.pdf - Macroglossa PR]",  - Retrieved 2011.</ref> but the main function is to simulate a digital eye on trying to find similarities of an unknown subject. This feature makes the engine unique.

This technology has several advantages. First, it allows users to pull results from collections of visual content<ref>Make Use OF . "[http://www.makeuseof.com/dir/macroglossa-identify-objects-in-image/ - MacroGlossa: Find Similar Images & Identify Objects In Image ]",  - Makeuseof.com. 2010.</ref> without using tags for search. Second, the visuals can be [[Crowdsourcing|crowd sourced]]. In fact by being a search engine, rather than simply a tool, Macroglossa should be able to crowdsourced and scale its recognition vocabulary faster than anyone else and a technology like this would increase the cognitive and spatial skills in [[humanoid]] robotics.<ref>J. Sturm, A. Visser. "[http://cvpr.in.tum.de/old/pub/pub/sturm09ras.pdf An appearance-based visual compass for mobile robots ]", Appearance-based, mobile robot localization, active vision, machine learning. 2000.</ref> In addition Macroglosssa can also be used as a Reverse Image Search to find [[orphan works]] and possible violations of copyright of images.

Macroglossa supports all popular image extensions such [[Jpg|jpeg]], [[Portable Network Graphics|png]], [[BMP file format|bmp]], [[gif]] and video formats such [[Audio Video Interleave|avi]], [[.mov|mov]], [[mp4]], [[m4v]], [[3gp]], [[wmv]], [[mpeg]].

Macroglossa enters [[Beta stage#Beta beta|beta]] stage in September 2011<ref>Mve. "[http://www.macroglossa.com/disclaimer.html - macroglossa.com]",  - Releases and Features. 2011.</ref> and at the same time open to the public the opportunity to use the developed [[Interface (object-oriented programming)|interfaces]] ( Api for web and mobile applications ) in order to expand the use of the engine in the [[Business-to-business|B2B]] and [[Business-to-consumer|B2C]] fields. Macroglossa becomes a [[Software as a service|SaaS]].

[[Api|API]] are distributed on three levels : free, basic, and premium. The free API has limited use, but basic and premium do not. The premium API also offers custom services allowing customers to extend and mold the features offered by computer vision.<ref>J. R. Martinez-de Dios, C. Serna y A. Ollero. "[http://grvc.us.es/publica/revistas/documentos/FishFarms.pdf Computer vision and robotics techniques in fish farms ]", Robotica. Vo. 21. No. 3. Editor Cambridge University Press. June 2003.</ref>

==References==
{{reflist}}

==Notes==
* ''Wired.it, Retrieved December 29, 2010 :'' Macroglossa is an Italian project born from a passion for research and innovation by the MVE group of independent developers. The startup has developed a visual search engine based on the comparison of the subjects in the images. The owners of the project define it as "a sort of digital eye can capture, compare and draw conclusions." The purpose of this service is to provide a new type of research within the network. The search engine allows you to upload a picture on the platform and look for similar images on the web. The engine is not based on text tags and does not use OCR to extract strings from images to locate the target. Everything focuses on the key points of the image uploaded by the user. The aim is to give as much information as possible on the results obtained. Each image has a direct result of the source.

==External links==
* [http://www.macroglossa.com Macroglossa] home page
* Macroglossa [http://www.macroglossa.com/api.html Api program]
* Macroglossa on [http://www.killerstartups.com/Search/macroglossa-com-carry-out-visual-searches Killer Startups]
* [http://yourstory.in/2011/07/macroglossa-reaches-alpha-version-4-0-a-picture-search-engine/ Yourstory.in] talks about Macroglossa

[[Category:Information retrieval]]
[[Category:Internet search engines]]
[[Category:Data search engines]]
[[Category:Multimedia]]
[[Category:Image search]]
>>EOP<<
133<|###|>Temporal information retrieval
'''Temporal Information Retrieval (T-IR)''' is an emerging area of research related to the field of [[information retrieval]] (IR) and a considerable number of sub-areas, positioning itself, as an important dimension in the context of the user information needs.

According to [[information theory]] science (Metzger, 2007),<ref name="Metzger2007">{{cite journal |last=Metzger |first=Miriam |title=Making Sense of Credibility on the Web: Models for Evaluating Online Information and Recommendations for Future Research |journal=Journal of the American Society for Information Science and Technology |volume=58 |issue=13 |pages=20782091 |year =2007 |url=http://dl.acm.org/citation.cfm?id=1315940 |doi=10.1002/asi.20672 }}</ref> timeliness or currency is one of the key five aspects that determine a documents credibility besides relevance, accuracy, objectivity and coverage. One can provide many examples when the returned search results are of little value due to temporal problems such as obsolete data on weather, outdated information about a given companys earnings or information on already-happened or invalid predictions.

T-IR, in general, aims at satisfying these temporal needs and at combining traditional notions of document relevance with the so-called temporal relevance. This will enable the return of temporally relevant documents, thus providing a temporal overview of the results in the form of timeliness or similar structures. It also shows to be very useful for query understanding, query disambiguation, query classification, result diversification and so on.

This page contains a list of the most important research in temporal information retrieval (T-IR) and its related sub-areas. As several of the referred works are related with different research areas a single article can be found in more than one different table. For ease of reading the articles are categorized in a number of different sub-areas referring to its main scope, in detail.

== Temporal dynamics (T-dynamics) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Baeza, Y.''' (2002). [http://www.dcs.bbk.ac.uk/webDyn2/proceedings/baeza_yates_web_strucutre.pdf/ Web Structure, Dynamics and Page Quality]. In A. Laendar & A. Oliveira (Eds.), ''In Lecture Notes in Computer Science - SPIRE2002: 9th International Symposium on String Processing and Information Retrieval'' (Vol. 2476/2002, pp.&nbsp;117  130). Lisbon, Portugal. September 1113: Springer Berlin / Heidelberg. || 2002 || SPIRE || T-Dynamics ||
|-
|'''Cho, J., & Garcia-Molina, H.''' (2003). [http://dl.acm.org/citation.cfm?id=857170 Estimating Frequency of Change]. ''In [http://toit.acm.org TOIT: ACM Transactions on Internet Technology]'', 3(3), 256 - 290.|| 2003 || TOIT || T-Dynamics ||
|-
| '''Fetterly, D., Manasse, M., Najork, M., & Wiener, J.''' (2003). [http://dl.acm.org/citation.cfm?id=775246|A Large-Scale Study of the Evolution of Web Pages]]. ''In [http://www2003.org/ WWW2003]: Proceedings of the 12th International World Wide Web Conference'' (pp.&nbsp;669  678). Budapest, Hungary. May 2024: ACM Press. || 2003 || WWW || T-Dynamics ||
|-
| '''Ntoulas, A., Cho, J., & Olston, C.''' (2004). [http://dl.acm.org/citation.cfm?id=988674 What's New on the Web?: the Evolution of the Web from a Search Engine Perspective]. In [http://www2004.org WWW2004]: Proceedings of the 13th International World Wide Web Conference (pp.&nbsp;1  12). New York, NY, United States. May 1722: ACM Press. || 2004 || WWW || T-Dynamics ||
|-
| '''Vlachos, M., Meek, C., Vagena, Z., & Gunopulos, D.''' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&nbsp;131  142). Paris, France. June 1318: ACM Press. || 2004 || SIGMOD || T-Dynamics ||
|-
| '''Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., & Grossman, D.''' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]. ''In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology'', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||
|-
| '''Jones, R., & Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||
|-
| '''Bordino, I., Boldi, P., Donato, D., Santini, M., & Vigna, S.''' (2008). [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4734022&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F4733896%2F4733897%2F04734022.pdf%3Farnumber%3D4734022 Temporal Evolution of the UK Web]. In [http://compbio.cs.uic.edu/adn-icdm08/ ADN2008]: Proceedings of the 1st International Workshop on Analysis of Dynamic Networks associated to [http://icdm08.isti.cnr.it/ ICDM2008]: IEEE International Conference on Data Mining (pp.&nbsp;909  918). Pisa, Italy. December 19: IEEE Computer Society Press. || 2008 || ICDM - ADN || T-Dynamics ||
|-
| '''Adar, E., Teevan, J., Dumais, S. T., & Elsas, J. L.''' (2009). [http://portal.acm.org/citation.cfm?id=1498837 The Web Changes Everything: Understanding the Dynamics of Web Content]. ''In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;282  291). Barcelona, Spain. February 912: ACM Press. || 2009 || WSDM || T-Dynamics ||
|-
| '''Metzler, D., Jones, R., Peng, F., & Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;700  701). Boston, MA, United States. July 1923: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''Elsas, J. L., & Dumais, S. T.''' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. ''In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;1  10). New York, United States. February 306: ACM Press. || 2010 || WSDM || T-Dynamics ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&nbsp;1123  1124). Raleigh, United States. April 2630: ACM Press. || 2010 || WWW || F-IRetrieval ||
|-
| '''Aji, A., Agichtein, E.''' (2010). [http://dl.acm.org/citation.cfm?id=2175298.2175332 Deconstructing Interaction Dynamics in Knowledge Sharing Communities]. ''In [http://sbp.asu.edu/sbp2010/sbp10.html]: Third International Conference on Social Computing, Behavioral-Cultural Modeling, & Prediction'' (pp.&nbsp;273  281). Washington DC, United States. March 3031: Springer-Verlag. || 2010 || SBP || T-Dynamics ||
|-
| '''Kulkarni, A., Teevan, J., Svore, K. M., & Dumais, S. T.''' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]. ''In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;167  176). Hong Kong, China. February 912: ACM Press. || 2011 || WSDM || T-Dynamics ||
|-
| '''Campos, R., Dias, G., & Jorge, A. M.''' (2011). [http://ceur-ws.org/Vol-707/TWAW2011.pdf What is the Temporal Value of Web Snippets?] ''In [http://temporalweb.net/page3/page3.html TWAW 2011]: Proceedings of the 1st International Temporal Web Analytics Workshop associated to [http://www.www2011india.com/ WWW2011]: 20th International World Wide Web Conference''. Hyderabad, India. March 28.: CEUR Workshop Proceedings. || 2011 || WWW - TWAW || T-Dynamics ||
|-
| '''Campos, R., Jorge, A., & Dias, G.''' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. ''In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2011 Conference on Research and Development in Information Retrieval'', (pp.&nbsp;13  16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||
|-
| '''Shokouhi, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. In [http://www.sigir2011.org/ SIGIR2011]: ''In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information'' (pp.&nbsp;1171  1172). Beijing, China. July 2428: ACM Press. || 2011 || SIGIR || T-Dynamics ||
|-
| '''Dias, G., Campos, R., & Jorge, A.''' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] ''In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||
|-
| '''Campos, R., Dias, G., & Jorge, A. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. ''In L. Antunes, & H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'' (Vol. 7026/2011, pp.&nbsp;581  596). Lisboa, Portugal. October 1013: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||
|-
| '''Jatowt, A., & Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1259  1264). Glasgow, Scotland, UK. October 24 - 28: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Yeung, C.-m. A., & Jatowt, A.''' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1231  1240). Glasgow, Scotland, UK. October 2428: ACM Press. || 2011 || CIKM || C-Memory ||
|-
| '''Costa, M., & Silva, M. J., & Couto, F. M.''' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. ''In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference'' (pp.&nbsp;757--766). Gold Coast, Australia. July 611: ACM Press. || 2014 || SIGIR || T-RModels ||
|}

== Temporal markup languages (T-MLanguages) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Setzer, A., & Gaizauskas, R.''' (2000). [ftp://ftp.dcs.shef.ac.uk/home/robertg/papers/lrec00-tempann.pdf Annotating Events and Temporal Information in Newswire Texts]. ''In [http://www.xanthi.ilsp.gr/lrec/ LREC2000]: Proceedings of the 2nd International Conference on Language Resources and Evaluation''. Athens, Greece. May 31 - June 2: ELDA. || 2000 || LREC || T-MLanguages ||
|-
| '''Setzer, A.''' (2001). [http://www.andrea-setzer.org.uk/PAPERS/thesis.pdf Temporal Information in Newswire Articles: An Annotation Scheme and Corpus Study]. Sheffield, UK: University of Sheffield. || 2001 || Phd Thesis || T-MLanguages ||
|-
| '''Ferro, L., Mani, I., Sundheim, B., & Wilson, G.''' (2001). [http://www.timeml.org/site/terqas/readings/MTRAnnotationGuide_v1_02.pdf TIDES Temporal Annotation Guidelines]. Version 1.0.2. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2001 || Technical Report || T-MLanguages ||
|-
| '''Pustejovsky, J., Castano, J., Ingria, R., Sauri, R., Gaizauskas, R., Setzer, A., et al.''' (2003). TimeML: Robust Specification of Event and Temporal Expression in Text. ''In [http://iwcs.uvt.nl/iwcs5/index.htm IWCS2003]: Proceedings of the 5th International Workshop on Computational Semantics'', (pp.&nbsp;28  34). Tilburg, Netherlands. January 1517. || 2003 || IWCS || T-MLanguages ||
|-
| '''Ferro, L., Gerber, L., Mani, I., Sundheim, B., & Wilson, G.''' (2005). [http://projects.ldc.upenn.edu/ace/docs/English-TIMEX2-Guidelines_v0.1.pdf TIDES 2005 Standard for the Annotation of Temporal Expressions]. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2005 || Technical Report || T-MLanguages ||
|}

== Temporal taggers (T-taggers) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| [http://www.timeml.org/site/tarsqi/toolkit/manual/ TempEx Module] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] - '''Mani, I., & Wilson, G.''' (2000). [[dl.acm.org/citation.cfm?id=1075228|Robust Temporal Processing of News]]. ''In [http://www.cse.ust.hk/acl2000/ ACL2000]: Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics'' (pp.&nbsp;69  76). Hong Kong, China. October 18: Association for Computational Linguistics. || 2000 || ACL || T-Taggers ||
|-
| [http://www.aktors.org/technologies/annie/ Annie] - [http://gate.ac.uk/download/index.html GATE distribution] - '''Cunningham, H., Maynard, D., Bontcheva, K., & Tablan, V.''' (2002). [http://eprints.aktors.org/90/01/acl-main.pdf GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applications]. ''In [http://www.aclweb.org/mirror/acl2002/ ACL2002]: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics'' (pp.&nbsp;168  175). Philadelphia, PA, United States. July 612: Association for Computational Linguistics. || 2002 || ACL || T-Taggers ||
|-
| [http://www.timeml.org/site/tarsqi/modules/gutime/download.html GUTime] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] || 2002 ||  || T-Taggers ||
|-
| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - '''Strotgen, J., & Gertz, M.''' (2010). [http://delivery.acm.org/10.1145/1860000/1859735/p321-strotgen.pdf?ip=188.80.124.88&acc=OPEN&CFID=82473711&CFTOKEN=13661527&__acm__=1337002719_1b05141ffc83e798f400c972756d43ad HeidelTime: High Quality Rule-based Extraction and Normalization of Temporal Expressions]. ''In [http://semeval2.fbk.eu/semeval2.php SemEval2010]: Proceedings of the 5th International Workshop on Semantic Evaluation associated to [http://acl2010.org/ ACL2010]: 41st Annual Meeting of the Association for Computational Linguistics'', (pp.&nbsp;321  324). Uppsala, Sweden. July 1116.|| 2010 || ACL - SemEval || T-Taggers ||
|-
| [http://www.timen.org/ TIMEN] '''Llorens, H., Derczynski, L., Gaizauskas, R. & Saquete, E.''' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/128_Paper.pdf TIMEN: An Open Temporal Expression Normalisation Resource]. ''In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation''. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||
|-
| '''Chang, A., & Manning, C.''' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/284_Paper.pdf SUTIME: A Library for Recognizing and Normalizing Time Expressions]. ''In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation''. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||
|-
| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - '''Strotgen, J., & Gertz, M.''' (2012). [http://www.springerlink.com/content/64767752451075k8/ Multilingual and cross-domain temporal tagging]. ''In [http://www.springerlink.com/content/1574-020x/ LRE]: Language Resources and Evaluation'', 1 - 30.|| 2012 || LRE || T-Taggers ||
|-
| [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ ManTIME] - '''Filannino, M., Brown, G. & Nenadic G.''' (2013). [http://www.aclweb.org/anthology/S/S13/S13-2.pdf#page=89 ManTIME: Temporal expression identification and normalization in the TempEval-3 challenge]. ''In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013)'', 53 - 57, Atlanta, Georgia, June 14-15, 2013.|| 2013 || ACL - SemEval || T-Taggers || [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ online demo]
|}

== Temporal indexing (T-indexing) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Alonso, O., & Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;597  598). Seattle, Washington, United States. August 611: ACM Press. || 2006 || SIGIR || T-Clustering ||
|-
| '''Berberich, K., Bedathur, S., Neumann, T., & Weikum, G.''' (2007). [http://dl.acm.org/citation.cfm?id=1277831 A Time Machine for Text Search]. ''In [http://www.sigir.org/sigir2007 SIGIR 2007]: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;519  526). Amsterdam, Netherlands. July 2327: ACM Press. || 2007 || SIGIR || W-Archives ||
|-
| '''Jin, P., Lian, J., Zhao, X., & Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&nbsp;220  224). Shanghai, China. December 2122: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||
|-
| '''Song, S., & JaJa, J.''' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||
|-
| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&nbsp;1117  1121). Fortaleza, Ceara, Brazil. March 1620: ACM Press. || 2008 || SAC || T-QAnswering ||
|-
| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 26: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Arikan, I., Bedathur, S., & Berberich, K.''' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. ''In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining''. Barcelona, Spain. February 912: ACM Press. || 2009 || WSDM ||| T-RModels ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41  44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.''' (2010). [http://dl.acm.org/citation.cfm?id=1871437.1871528 Efficient temporal keyword search over versioned text]. ''In [http://www.yorku.ca/cikm10/ CIKM2010]: Proceedings of the 19th ACM international conference on Information and knowledge management'', (pp.&nbsp;699-708). Toronto, Canada. October 26-30. ACM Press. || 2010 || CIKM || W-Archives||
|-
| '''Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.''' (2011). [http://dl.acm.org/citation.cfm?id=2009991 Temporal index sharding for space-time efficiency in archive search]. ''In [http://www.sigir.org/sigir2011/ SIGIR2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&nbsp;545-554). Beijing, China. July 24-28. ACM Press. || 2011 || SIGIR || T-Indexing||
|-
| '''Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.''' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&nbsp;235  243). Portland, United States. August 12-16. ACM Press. || 2012 || SIGIR || W-Archives ||
|}

== Temporal query understanding (TQ-understanding) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Vlachos, M., Meek, C., Vagena, Z., & Gunopulos, D.''' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&nbsp;131  142). Paris, France. June 1318: ACM Press. || 2004 || SIGMOD || T-Dynamics ||
|-
| '''Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., & Grossman, D.''' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]]. ''In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology'', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||
|-
| '''Jones, R., & Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||
|-
| '''Dakka, W., Gravano, L., & Ipeirotis, P. G.''' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]]. ''In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1437  1438). Napa Valley, California, United States. October 2630: ACM Press. || 2008 || CIKM || TQ-Understanding ||
|-
| '''Diaz, F.''' (2009). [http://dl.acm.org/citation.cfm?id=1498825 Integration of News Content into Web Results]. ''In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;182  191). Barcelona, Spain. February 912: ACM Press. || 2009 || WSDM || TQ-Understanding ||
|-
| '''Metzler, D., Jones, R., Peng, F., & Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;700  701). Boston, MA, United States. July 1923: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''Konig, A.''' (2009). [http://dl.acm.org/citation.cfm?id=1572002 Click-Through Prediction for News Queries]. ''In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;347  354). Boston, MA, United States. July 1923: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166  175). Suwon, Republic of Korea. January 1415: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.''' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: ''In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;11  20). New York, United States. February 36: ACM Press. || 2010 || WSDM || T-RModels ||
|-
| '''Kanhabua, N., & Nrvag, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. ''In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries''. Glasgow, Scotland. September 610: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||
|-
| '''Zhang, R., Konda, Y., Dong, A., Kolari, P., Chang, Y., & Zheng, Z.''' (2010). [http://dl.acm.org/citation.cfm?id=1870768 Learning Recurrent Event Queries for Web Search]. ''In [http://www.lsi.upc.edu/events/emnlp2010/ EMNLP2010]: Proceedings of the Conference on Empiral Methods in Natural Language Processing'' (pp.&nbsp;1129  1139). Massachusetts, United States. October 911: Association for Computational Linguistics. || 2010 || EMNLP || TQ-Understanding ||
|-
| '''Kulkarni, A., Teevan, J., Svore, K. M., & Dumais, S. T.''' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]]. ''In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;167  176). Hong Kong, China. February 912: ACM Press. || 2011 || WSDM || T-Dynamics ||
|-
| '''Campos, R.''' (2011). [http://dl.acm.org/citation.cfm?id=2010182 Using k-top Retrieved Web Snippets to Date Temporal Implicit Queries based on Web Content Analysis]. ''In [http://www.sigir2011.org/%20 SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (p.&nbsp;1325). Beijing, China. July 2428.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||
|-
| '''Campos, R., Jorge, A., & Dias, G.''' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. ''In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2005 Conference on Research and Development in Information Retrieval'', (pp.&nbsp;13  16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||
|-
| '''Shokouhi, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;1171  1172). Beijing, China. July 2428.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||
|-
| '''Campos, R., Dias, G., Jorge, A., & Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2169103&CFID=102654836&CFTOKEN=48651941 Enriching Temporal Query Understanding through Date Identification: How to Tag Implicit Temporal Queries?] ''In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference'' (pp.&nbsp;41  48). Lyon, France. April 17.: ACM - DL. || 2012 || WWW - TWAW || TQ-Understanding ||
|-
| '''Shokouhi, M., & Radinsky, K.''' (2012). [http://dl.acm.org/citation.cfm?id=2348364 Time-Sensitive Query Auto-Completion]. ''In [http://www.sigir.org/sigir2012/ SIGIR 2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;601  610). Portland, United States. August 1216.: ACM Press. || 2012 || SIGIR || TQ-Understanding ||
|-
| '''Campos, R., Dias, G., Jorge, A., & Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2398567&dl=ACM&coll=DL&CFID=204979644&CFTOKEN=99312511 GTE: A Distributional Second-Order Co-Occurrence Approach to Improve the Identification of Top Relevant Dates] ''In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management'' (pp.&nbsp;2035  2039). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || TQ-Understanding ||
|-
| '''Campos, R., Jorge, A., Dias, G., & Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] ''In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,'' Vol. 1, (pp.&nbsp;1  8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||
|}

== Time-aware retrieval/ranking models (T-RModels) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Li, X., & Croft, B. W.''' (2003). [http://dl.acm.org/citation.cfm?doid=956863.956951 Time-Based Language Models]. ''In CIKM 2003: Proceedings of the 12th International ACM Conference on Information and Knowledge Management'' (pp.&nbsp;469  475). New Orleans, Louisiana, United States. November 28: ACM Press. || 2003 || CIKM || T-RModels ||
|-
| '''Sato, N., Uehara, M., & Sakai, Y.''' (2003). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=1232026&contentType=Conference+Publications Temporal Information Retrieval in Cooperative Search Engine]. ''In [http://www.dexa.org/previous/dexa2003/cfp/dexa.html DEXA2003]: Proceedings of the 14th International Workshop on Database and Expert Systems Applications'' (pp.&nbsp;215  220). Prague, Czech Republic. September 15: IEEE. || 2003 || DEXA || T-RModels ||
|-
| '''Berberich, K., Vazirgiannis, M., & Weikum, G.''' (2005). [http://projecteuclid.org/DPubS?verb=Display&version=1.0&service=UI&handle=euclid.im/1150474885&page=record Time-Aware Authority Ranking]. ''In [http://www.tandf.co.uk/journals/journal.asp?issn=1542-7951&linktype=44 IM: Internet Mathematics]'', 2(3), 301 - 332. || 2005 || IM || T-RModels ||
|-
| '''Cho, J., Roy, S., & Adams, R.''' (2005). [http://dl.acm.org/citation.cfm?id=1066220 Page Quality: In Search of an Unbiased Web Ranking]. In [http://cimic.rutgers.edu/~sigmod05/ SIGMOD2005]: Proceedings of the International Conference on Management of Data (pp.&nbsp;551  562). Baltimore, United States. June 1316: ACM Press. || 2005 || SIGMOD || T-RModels ||
|-
| '''Perkio, J., Buntine, W., & Tirri, H.''' (2005). [http://dl.acm.org/citation.cfm?id=1076171 A Temporally Adaptative Content-Based Relevance Ranking Algorithm]. ''In [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;647  648). Salvador, Brazil. August 1516: ACM Press. || 2005 || SIGIR || T-RModels ||
|-
| '''Jones, R., & Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||
|-
| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&nbsp;1117  1121). Fortaleza, Ceara, Brazil. March 1620: ACM Press. || 2008 || SAC || T-QAnswering ||
|-
| '''Dakka, W., Gravano, L., & Ipeirotis, P. G.''' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]. ''In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1437  1438). Napa Valley, California, United States. October 2630: ACM Press. || 2008 || CIKM || TQ-Understanding ||
|-
| '''Jin, P., Lian, J., Zhao, X., & Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&nbsp;220  224). Shanghai, China. December 2122: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||
|-
| '''Arikan, I., Bedathur, S., & Berberich, K.''' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. ''In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining''. Barcelona, Spain. February 912: ACM Press. || 2009 || WSDM ||| T-RModels ||
|-
| '''Zhang, R., Chang, Y., Zheng, Z., Metzler, D., & Nie, J.-y.''' (2009). [http://dl.acm.org/citation.cfm?id=1620899 Search Result Re-ranking by Feedback Control Adjustment for Time-sensitive Query]. ''In [http://www.naaclhlt2009.org/ NAACL2009]: Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies'', (pp.&nbsp;165  168). Boulder, Colorado, United States. May 31 - June 5. || 2009 || NAACL || T-RModels ||
|-
| '''Metzler, D., Jones, R., Peng, F., & Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;700  701). Boston, MA, United States. July 1923: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 26: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166  175). Suwon, Republic of Korea. January 1415: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Elsas, J. L., & Dumais, S. T.''' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. ''In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;1  10). New York, United States. February 306: ACM Press. || 2010 || WSDM || T-Dynamics ||
|-
| '''Aji, A., Wang, Y., Agichtein, E., Gabrilovich, E.''' (2010). [http://dl.acm.org/citation.cfm?id=1871519 Using the Past to Score the Present: Extending Term Weighting Models Through Revision History Analysis] ''In [http://www.cikm2010.org/ CIKM 2010]: Proceedings of the 19th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;629  638). Toronto, ON, Canada. October 26 - October 30: ACM Press. || 2010 || CIKM || T-RModels ||
|-
| '''Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.''' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: ''In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;11  20). New York, United States. February 36: ACM Press. || 2010 || WSDM || T-RModels ||
|-
| '''Berberich, K., Bedathur, S., Alonso, O., & Weikum, G.''' (2010). [http://www.springerlink.com/content/b193008160713350/ A Language Modeling Approach for Temporal Information Needs]. In C. Gurrin, Y. He, G. Kazai, U. Kruschwitz, S. Little, T. Roelleke, et al. (Eds.), ''In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://kmi.open.ac.uk/events/ecir2010/ ECIR 2010]: 32nd European Conference on Information Retrieval'' (Vol. 5993/2010, pp.&nbsp;13  25). Milton Keynes, UK. March 2831: Springer Berlin / Heidelberg. || 2010 || ECIR || T-RModels ||
|-
| '''Dong, A., Zhang, R., Kolari, P., Jing, B., Diaz, F., Chang, Y., Zheng, Z., & Zha, H.''' (2010). [http://dl.acm.org/citation.cfm?id=1772725&dl=ACM&coll=DL&CFID=204979644&CFTOKEN=99312511 Time is of the Essence: Improving Recency Ranking Using Twitter Data]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&nbsp;331  340). Raleigh, United States. April 2630: ACM Press. || 2010 || WWW || T-RModels ||
|-
| '''Inagaki, Y., Sadagopan, N., Dupret, G., Dong, A., Liao, C., Chang, Y., & Zheng, Z.''' (2010). [http://labs.yahoo.com/files/aaai10_recencyfeature_2.pdf Session Based Click Features for Recency Ranking]. ''In [http://www.aaai.org/Conferences/AAAI/aaai10.php AAAI2010]: Proceedings of the 24th AAAI Conference on Artificial Intelligence'' (pp.&nbsp;331  340). Atlanta, United States. June 1115: AAAI Press. || 2010 || AAAI || T-RModels ||
|-
| '''Dai, N., & Davison, B.''' (2010). [http://dl.acm.org/citation.cfm?id=1835471 Freshness Matters: In Flowers, Food, and Web Authority]. ''In [http://www.sigir2010.org/doku.php SIGIR 2010]: Proceedings of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;114  121). Geneve, Switzerland. July 1923: ACM Press. || 2010 || SIGIR || T-RModels ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41  44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Kanhabua, N., & Nrvag, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. ''In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries''. Glasgow, Scotland. September 610: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||
|-
| '''Efron, M., & Golovchinsky, G.''' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009984 Estimation Methods for Ranking Recent Information]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;495  504). Beijing, China. July 2428.: ACM Press. || 2011 || SIGIR || T-RModels ||
|-
| '''Dai, N., Shokouhi, M., & Davison, B. D.''' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009933 Learning to Rank for Freshness and Relevance]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;95  104). Beijing, China. July 2428.: ACM Press. || 2011 || SIGIR || T-RModels ||
|-
| '''Kanhabua, N., Blanco, R., & Matthews, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010018&dl=ACM&coll=DL&CFID=102654836&CFTOKEN=48651941 Ranking Related News Predictions]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;755  764). Beijing, China. July 2428: ACM Press. || 2011 || SIGIR || F-IRetrieval ||
|-
| '''Chang, P-T., Huang, Y-C., Yang, C-L., Lin, S-D., & Cheng, P-J.''' (2012). [http://dl.acm.org/citation.cfm?id=2348489 Learning-Based Time-Sensitive Re-Ranking for Web Search]. ''In Proceedings of the [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval'', (pp.&nbsp;1101  1102). Portland, United States. August 12 - 16. || 2012 || SIGIR || T-RModels ||
|-
| '''Efron, M.''' (2012). [http://research.microsoft.com/en-us/people/milads/efrontemporalwsv02.pdf Query-Specific Recency Ranking: Survival Analysis for Improved Microblog Retrieval]. ''In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval''. Portland, United States. August 16. || 2012 || SIGIR - TAIA || T-RModels ||
|-
| '''Kanhabua, N., & Nrvag, K.''' (2012). [http://dl.acm.org/citation.cfm?id=2398667 Learning to Rank Search Results for Time-Sensitive Queries] ''In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management'' (pp.&nbsp;2463  2466). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || T-RModels ||
|-
| '''Kim G., and Xing E. P.''' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;163  172). Rome, Italy. February 48: ACM Press. || 2013 || WSDM || T-IRetrieval ||
|-
| '''Costa, M., & Silva, M. J., & Couto, F. M.''' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. ''In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference'' (pp.&nbsp;757--766). Gold Coast, Australia. July 611: ACM Press. || 2014 || SIGIR || T-RModels ||
|}

== Temporal clustering (T-clustering) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Shaparenko, B., Caruana, R., Gehrke, J., & Joachims, T.''' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. ''In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]'' (pp.&nbsp;165  174). Houston, United States. November 2730: IEEE Press. || 2005 || ICDM - TDM || TDT ||
|-
| '''Alonso, O., & Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;597  598). Seattle, Washington, United States. August 611: ACM Press. || 2006 || SIGIR || T-Clustering ||
|-
| '''Mori, M., Miura, T., & Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&nbsp;338  342). Hong Kong, China. December 1822: IEEE Computer Society Press. || 2006 || WIC || TDT ||
|-
| '''Alonso, O., Baeza-Yates, R., & Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.''' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. ''In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries'' (pp.&nbsp;115  124). Austin, United States. June 1519.: ACM Press. || 2009 || JCDL || F-IRetrieval ||
|-
| '''Campos, R., Dias, G., & Jorge, A.''' (2009). [http://www.ccc.ipt.pt/~ricardo/ficheiros/KDIR2009.pdf Disambiguating Web Search Results By Topic and Temporal Clustering: A Proposal]. In [http://www.kdir.ic3k.org/ KDIR2009]: Proceedings of the International Conference on Knowledge Discovery and Information Retrieval, (pp.&nbsp;292  296). Funchal - Madeira, Portugal. October 68. || 2009 || KDIR || T-Clustering ||
|-
| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 26: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166  175). Suwon, Republic of Korea. January 1415: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Jatowt, A., & Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1259  1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Campos, R., Jorge, A., Dias, G., & Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] ''In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,'' Vol. 1, (pp.&nbsp;1  8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||
|}

== Temporal text classification (T-classification) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Jong, F., Rode, H., & Hiemstra, D.''' (2006). [http://doc.utwente.nl/66448/ Temporal Language Models for the Disclosure of Historical Text]. ''In [http://www.dans.knaw.nl/en AHC2005]: Proceedings of the XVIth International Conference of the Association for History and Computing'' (pp.&nbsp;161  168). Amsterdam, Netherlands. September 1417 || 2005 || AHC || T-Classification ||
|-
| '''Toyoda, M., & Kitsuregawa, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1135777.1135815 What's Really New on the Web? Identifying New Pages from a Series of Unstable Web Snapshots]. ''In [http://www2006.org WWW2006]: Proceedings of the 15th International World Wide Web Conference'' (pp.&nbsp;233  241). Edinburgh, Scotland. May 2326: ACM Press. || 2006 || WWW || T-Classification ||
|-
| '''Nunes, S., Ribeiro, C., & David, G.''' (2007). [http://dl.acm.org/citation.cfm?id=1316924 Using Neighbors to Date Web Documents]. ''In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 9th ACM International Workshop on Web Information and Data Management associated to [[www2.fc.ul.pt/cikm2007|CIKM2007]]: 16th International Conference on Knowledge and Information Management'' (pp.&nbsp;129  136). Lisboa, Portugal. November 9: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||
|-
| '''Jatowt, A., Kawai, Y., & Tanaka, K.''' (2007). [http://dl.acm.org/citation.cfm?id=1316925 Detecting Age of Page Content]. ''In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 8th International Workshop on Web Information and Data Management associated to [http://www2.fc.ul.pt/cikm2007 CIKM2007]: 16th International Conference on Knowledge and Information Management'' (pp.&nbsp;137  144). Lisbon. Portugal. November 9.: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||
|-
| '''Kanhabua, N., & Nrvag, K.''' (2008). [http://dl.acm.org/citation.cfm?id=1429902 Improving Temporal Language Models for Determining Time of Non-timestamped Documents]. ''In Christensen-Dalsgaard, B., Castelli, D., Jurik, B. A., Lippincott, J. (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2008.org/ ECDL 2008]: 12th European Conference on Research and Advances Technology for Digital Libraries'' (Vol. 5173/2008, pp.&nbsp;358  370). Aarhus, Denmark. September 1419: Springer Berlin / Heidelberg. || 2008 || ECDL || T-Classification ||
|-
| '''Jatowt, A., & Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1259  1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Strotgen, J., Alonso, O., & Gertz, M.''' (2012). [http://dl.acm.org/citation.cfm?id=2169095.2169102&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Identification of Top Relevant Temporal Expressions in Documents]. ''In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference'' (pp.&nbsp;33  40). Lyon, France. April 17: ACM - DL. || 2012 || WWW - TWAW || T-Classification ||
|-
| '''Filannino, M., and Nenadic, G.''' (2014). [http://www.aclweb.org/anthology/W/W14/W14-4502.pdf Mining temporal footprints from Wikipedia]. ''In Proceedings of the First AHA!-Workshop on Information Discovery in Text'' (Dublin, Ireland, August 2014), Association for Computational Linguistics and Dublin City University, pp. 713. || 2014 || COLING || T-Classification || [http://www.cs.man.ac.uk/~filannim/projects/temporal_footprints/ online demo]
|}

== Temporal visualization (T-interfaces) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Swan, R., & Allan, J.''' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. ''In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;49  56). Athens, Greece. July 2428: ACM Press. || 2000 || SIGIR || TDT ||
|-
| '''Swan, R., & Jensen, D.''' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. ''In M. Grobelnik, D. Mladenic, & N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining'' (pp.&nbsp;73  80). Boston, Massachusetts, United States. August 2023: ACM Press. || 2000 || KDD - TM || TDT ||
|-
| [http://books.google.com/ngrams Google Ngram Viewer] ||  ||  || T-Interfaces ||
|-
| '''Cousins, S., & Kahn, M.''' (1991). [http://www.sciencedirect.com/science/article/pii/093336579190005V The Visual Display of Temporal Information]. (E. Keravnou, Ed.) ''In AIM: Artificial Intelligence in Medicine'', 3(6), 341 - 357. || 1991 || AIM || T-Interfaces ||
|-
| '''Karam, G. M.''' (1994). [http://dl.acm.org/citation.cfm?id=187157 Visualization Using Timelines]. In T. J. Ostrand (Ed.), ''ISSTA1994: Proceedings of the International Symposium on Software Testing and Analysis associated to SIGSOFT: ACM Special Interest Group on Software Engineering'' (pp.&nbsp;125  137). Seattle, Washington, United States. August 1719: ACM Press. || 1994 || ISSTA || T-Interfaces ||
|-
| '''Plaisant, C., Miiash, B., Rose, A., Widoff, S., & Shneiderman, B.''' (1996). [http://dl.acm.org/citation.cfm?id=238493 LifeLines: Visualizing Personal Histories]. ''In [http://www.sigchi.org/chi96/proceedings/index.htm CHI1996]: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems'' (pp.&nbsp;221  227). Vancouver, British Columbia, Canada. April 1318: ACM Press. || 1996 || CHI || T-Interfaces ||
|-
| '''Toyoda, M., & Kitsuregawa, M.''' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. ''In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia'' (pp.&nbsp;151  160). Salzburg, Austria. September 69: ACM Press. || 2005 || HT || W-Archives ||
|-
| '''Efendioglu, D., Faschetti, C., & Parr, T.''' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]. In ''D. Wolber, N. Calder, & ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering'' (pp.&nbsp;119  120). Palo Alto, California, United States. July 1114: ACM Press. || 2006 || ICWE || W-Archives ||
|-
| '''Catizone, R., Dalli, A., & Wilks, Y.''' (2006). [http://www.lrec-conf.org/proceedings/lrec2006/pdf/702_pdf.pdf Evaluating Automatically Generated Timelines from the Web]. ''In [http://www.lrec-conf.org/lrec2006/ LREC2006]: Proceedings of the 5th International Conference on Language Resources and Evaluation''. Genoa, Italy. May 2426: ELDA. || 2006 || LREC || T-Interfaces ||
|-
| '''Mori, M., Miura, T., & Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&nbsp;338  342). Hong Kong, China. December 1822: IEEE Computer Society Press. || 2006 || WIC || TDT ||
|-
| '''Alonso, O., Baeza-Yates, R., & Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||
|-
| '''Jatowt, A., Kawai, Y., & Tanaka, K.''' (2008). [http://dl.acm.org/citation.cfm?id=1367497.1367736 Visualizing Historical Content of Web pages]]. ''In [http://www2008.org/ WWW2008]: Proceedings of the 17th International World Wide Web Conference'' (pp.&nbsp;1221  1222). Beijing, China. April 2125: ACM Press. || 2008 || WWW || W-Archives ||
|-
| '''Nunes, S., Ribeiro, C., & David, G.''' (2008). [http://dl.acm.org/citation.cfm?id=1822292 WikiChanges - Exposing Wikipedia Revision Activity]. ''In [http://www.wikisym.org/ws2008/ WikiSym2008]: Proceedings of the 4th International Symposium on Wikis''. Porto, Portugal. September 810: ACM Press. || 2008 || WikiSym || T-Interfaces ||
|-
| '''Nunes, S., Ribeiro, C., & David, G.''' (2009). [http://epia2009.web.ua.pt/onlineEdition/601.pdf Improving Web User Experience with Document Activity Sparklines]. ''In L. S. Lopes, N. Lau, P. Mariano, & L. Rocha (Ed.), [http://epia2009.web.ua.pt EPIA2009]: Proceedings of the 14th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'', (pp.&nbsp;601  604). Aveiro, Portugal. October 1215. || 2009 || EPIA || T-Interfaces ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166  175). Suwon, Republic of Korea. January 1415: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41  44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Khurana, U., Nguyen V., Cheng H., Ahn, J., Chen X., & Shneiderman, B.''' (2011). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6113166 Visual Analysis of Temporal Trends in Social Networks Using Edge Color Coding and Metric Timelines]. ''In [http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6112285]: Proceedings of the IEEE Social Computing'', (pp.&nbsp;549  554). Boston, United States. || 2011 || SocialCom || T-Interfaces ||
|}

== Temporal search engines (T-SEngine) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Alonso, O., & Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;597  598). Seattle, Washington, United States. August 611: ACM Press. || 2006 || SIGIR || T-Clustering ||
|-
| '''Alonso, O., Baeza-Yates, R., & Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||
|-
| '''Jin, P., Lian, J., Zhao, X., & Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&nbsp;220  224). Shanghai, China. December 2122: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||
|-
| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 26: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166  175). Suwon, Republic of Korea. January 1415: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41  44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|}

== Temporal question answering (T-QAnswering) ==
{| class="wikitable sortable"
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&nbsp;1117  1121). Fortaleza, Ceara, Brazil. March 1620: ACM Press. || 2008 || SAC || T-QAnswering ||
|}

== Temporal snippets (T-snippets) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Alonso, O., Baeza-Yates, R., & Gertz, M.''' (2009). [http://www.wssp.info/2009/WSSP2009AlonsoBaezaYatesGertz.pdf Effectiveness of Temporal Snippets]. ''In [http://www.wssp.info/2009.html WSSP2009]: Proceedings of the Workshop on Web Search Result Summarization and Presentation associated to [[www2009.org/|WWW2009]]: 18th International World Wide Web Conference''. Madrid, Spain. April 2024: ACM Press. || 2009 || WWW - WSSP || T-Snippets ||
|-
| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2011). [http://www.springerlink.com/content/u78qu8x10h613471/ Enhancing Document Snippets Using Temporal Information]. ''In R. Grossi, F. Sebastiani, & F. Silvestri (Eds.), Lecture Notes in Computer Science, [http://spire2011.isti.cnr.it/ SPIRE2011]: 18th International Symposium on String Processing and Information Retrieval'' (Vol. 7024, pp.&nbsp;26  31). Pisa, Italy. October 1721.: Springer Berlin / Heidelberg. || 2011 || SPIRE || T-Snippets ||
|-
| '''Svore, K. M., Teevan, J., Dumais, S. T., & Kulkarni, A.''' (2012). [http://dl.acm.org/citation.cfm?id=2348461 Creating Temporally Dynamic Web Search Snippets]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&nbsp;1045  1046). Portland, United States. August 12-16. ACM Press  || 2012 || SIGIR || T-Snippets ||
|}

== Future information retrieval (F-IRetrieval) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Baeza-Yates, R.''' (2005). [http://www.dcs.vein.hu/CIR/cikkek/searching_the_future.pdf Searching the Future]. ''In S. Dominich, I. Ounis, & J.-Y. Nie (Ed.), MFIR2005: Proceedings of the Mathematical/Formal Methods in Information Retrieval Workshop associated to [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Salvador, Brazil. August 1519: ACM Press. || 2005 || SIGIR - MFIR || F-IRetrieval ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.''' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. ''In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries'' (pp.&nbsp;115  124). Austin, United States. June 1519.: ACM Press. || 2009 || JCDL || F-IRetrieval ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166  175). Suwon, Republic of Korea. January 1415: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&nbsp;1123  1124). Raleigh, United States. April 2630: ACM Press. || 2010 || WWW || F-IRetrieval ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41  44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Dias, G., Campos, R., & Jorge, A.''' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] ''In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||
|-
| '''Kanhabua, N., Blanco, R., & Matthews, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010018&dl=ACM&coll=DL&CFID=82290723&CFTOKEN=53881602 Ranking Related News Predictions]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;755  764). Beijing, China. July 2428: ACM Press. || 2011 || SIGIR || F-IRetrieval ||
|-
| '''Kanazawa, K., Jatowt, A., & Tanaka, K.''' (2011). [http://dl.acm.org/citation.cfm?id=2052362 Improving Retrieval of Future-Related Information in Text Collections]. ''In [http://liris.cnrs.fr/~wi-iat11/WI 2011/ WIC2011]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&nbsp;278  283). Lyon, France. August 2227: IEEE Computer Society Press. || 2011 || WIC || F-IRetrieval ||
|-
| '''Campos, R., Dias, G., & Jorge, A. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. ''In L. Antunes, & H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'' (Vol. 7026/2011, pp.&nbsp;581  596). Lisboa, Portugal. October 1013: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||
|-
| '''Jatowt, A., & Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1259  1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Weerkamp, W., & Rijke, M.''' (2012). [http://research.microsoft.com/en-us/people/milads/taia2012-activities.pdf Activity Prediction: A Twitter-based Exploration]. ''In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval''. Portland, United States. August 16. || 2012 || SIGIR - TAIA || F-IRetrieval ||
|-
| '''Radinski, K., & Horvitz, E.''' (2013). [http://dl.acm.org/citation.cfm?id=2433431 Mining the Web to Predict Future Events]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;255  264). Rome, Italy. February 48: ACM Press. || 2013 || WSDM || F-IRetrieval ||
|}

== Temporal image retrieval (T-IRetrieval) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Dias, G., Moreno, J. G., Jatowt, A., & Campos, R.''' (2012). [http://link.springer.com/content/pdf/10.1007%2F978-3-642-34109-0_21 Temporal Web Image Retrieval]. In Calderon-Benavides, L., Gonzalez-Caro, C., Chavez, E., Ziviani, N. (Eds.), ''In Lecture Notes in Computer Science - [http://catic.unab.edu.co/spire/ SPIRE2012]: 19th International Symposium on String Processing and Information Retrieval'' (Vol. 7608/2012, pp.&nbsp;199  204). Cartagena de Indias, Colombia. October 2125: Springer Berlin / Heidelberg. || 2012 || SPIRE || T-IRetrieval ||
|-
| '''Palermo, F., Hays, J., & Efros, A.''' (2012). [http://link.springer.com/content/pdf/10.1007/978-3-642-33783-3_36 Dating Historical Color Images]. In Fitzgibbon, A., Lazebnik, S., Sato, Y., Schmid, C. (Eds.), ''In Lecture Notes in Computer Science - [http://eccv2012.unifi.it/ ECCV2012]: 12th European Conference on Computer Vision'' (Vol. 7577/2012, pp.&nbsp;499  512). Firenze, Italy. October 0713: Springer Berlin / Heidelberg. || 2012 || ECCV || T-IRetrieval ||
|-
| '''Kim, G., & Xing, E. P.''' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;163  172). Rome, Italy. February 48: ACM Press. || 2013 || WSDM || T-IRetrieval ||
|-
| '''Martin, P., Doucet, A., & Jurie, F.''' (2014). [http://dl.acm.org/citation.cfm?id=2578790 Dating Color Images with Ordinal Classification]. ''In [http://www.icmr2014.org/ ICMR2014]: Proceedings of International Conference on Multimedia Retrieval'' (pp. 447). Glasgow, United Kingdom. April 01-04: ACM Press. || 2014 || ICMR || T-IRetrieval ||
|}

== Collective memory (C-memory) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Surowiecki, J.''' (2004). [http://www.amazon.com/The-Wisdom-Crowds-Collective-Economies/dp/0385503865 The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations]. USA: DoubleDay. || 2004 ||  || C-Memory ||
|-
| '''Hall, D., Jurafsky, D., & Manning, C. D.''' (2008). [http://dl.acm.org/citation.cfm?id=1613715.1613763 Studying the History of Ideas using Topic Models]. ''In [http://conferences.inf.ed.ac.uk/emnlp08 EMNLP 2008]: Proceedings of the Conference on Empirical Methods in Natural Language Processing'' (pp.&nbsp;363  371). Waikiki, Honolulu, Hawaii. October 2527: Association for Computational Linguistics. || 2008 || EMNLP || C-Memory ||
|-
| '''Shahaf, D., & Guestrin, C.''' (2010). [http://dl.acm.org/citation.cfm?id=1835884 Connecting the dots between News Articles]]. In [http://www.sigkdd.org/kdd2010/ KDD2010]: Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp.&nbsp;623  632). Washington, United States. July 2528: ACM Press. || 2010 || KDD || C-Memory ||
|-
| '''Takahashi, Y., Ohshima, H., Yamamoto, M., Iwasaki, H., Oyama, S., & Tanaka, K.''' (2011). [http://dl.acm.org/citation.cfm?id=1995980 Evaluating Significance of Historical Entities based on Tempo-spatial Impacts Analysis using Wikipedia Link Structure]]. ''In [http://www.ht2011.org/ HT2011]: Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia'' (pp.&nbsp;83  92). Eindhoven, Netherlands. June 69: ACM Press. || 2011 || HT || C-Memory ||
|-
| '''Michel, J.-B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., Team, T. G., et al.''' (2011). [http://www.sciencemag.org/content/331/6014/176 Quantitative Analysis of Culture Using Millions of Digitized Books]. In [http://www.sciencemag.org/ Science], 331(6014), 176 - 182. || 2011 || Science || C-Memory ||
|-
| '''Yeung, C.-m. A., & Jatowt, A.''' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1231  1240). Glasgow, Scotland, UK. October 2428: ACM Press. || 2011 || CIKM || C-Memory ||
|}

== Web archives (W-archives) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| [[List of Web archiving initiatives|List of Web Archive Initiatives]] || 2011 ||  || W-Archives ||
|-
| '''Kahle, B.''' (1997, 03). [http://www.sciamdigital.com/index.cfm?fa=Products.ViewIssuePreview&ISSUEID_CHAR=00B8E369-1805-4A27-A331-9D727FEAC21&ARTICLEID_CHAR=00B10B9E-5F13-40B2-AA51-0A4D5C41549 Preserving the Internet]. ''In [https://www.scientificamerican.com/sciammag/ Scientific American Magazine]'', 276(3), pp.&nbsp;72  73. || 1997 || SAM || W-Archives ||
|-
| '''Toyoda, M., & Kitsuregawa, M.''' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. ''In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia'' (pp.&nbsp;151  160). Salzburg, Austria. September 69: ACM Press. || 2005 || HT || W-Archives ||
|-
| '''Efendioglu, D., Faschetti, C., & Parr, T.''' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]]. In ''D. Wolber, N. Calder, & ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering'' (pp.&nbsp;119  120). Palo Alto, California, United States. July 1114: ACM Press. || 2006 || ICWE || W-Archives ||
|-
| '''Jatowt, A., Kawai, Y., Nakamura, S., Kidawara, Y., & Tanaka, K.''' (2006). [http://dl.acm.org/citation.cfm?id=1149969 Journey to the Past: Proposal of a Framework for Past Web Browser]. ''In HT2006: Proceedings of the 17th Conference on Hypertext and Hypermedia'' (pp.&nbsp;135  144). Odense, Denmark. August 2225: ACM Press. || 2006 || HT || W-Archives ||
|-
| '''Adar, E., Dontcheva, M., Fogarty, J., & Weld, D. S.''' (2008). [http://dl.acm.org/citation.cfm?id=1449756 Zoetrope: Interacting with the Ephemeral Web]]. ''In S. B. Cousins, & M. Beaudouin-Lafon (Ed.), [http://www.acm.org/uist/uist2008/ UIST 2008]: Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology'' (pp.&nbsp;239  248). Monterey, CA, United States. October 1922: ACM Press. || 2008 || UIST || W-Archives ||
|-
| '''Song, S., & JaJa, J.''' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||
|-
| '''Gomes, D., Miranda, J., & Costa, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2042590 A Survey on Web Archiving Initiatives]]. ''In [http://www.tpdl2011.org/ TPDL2011]: Proceedings of the 15th international conference on Theory and practice of digital libraries: research and advanced technology for digital libraries'' (pp.&nbsp;408  420). Berlin, Germany. September 2529: Springer-Verlag || 2011 || TPDL || W-Archives ||
|-
| '''Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.''' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&nbsp;235  243). Portland, United States. August 12-16. ACM Press || 2012 || SIGIR || W-Archives ||
||
|-
| '''Costa, M., & Silva, M.J.''' (2012). [http://link.springer.com/chapter/10.1007%2F978-3-642-35063-4_32 Evaluating Web Archive Search Systems]. ''In [http://www.wise2012.cs.ucy.ac.cy/ WISE2012]: Proceedings of the 13th International Conference on Web Information System Engineering'', (pp.&nbsp;440 - 454). Paphos, Cyprus. November 28-30. Springer-Verlag || 2012 || WISE || W-Archives ||
|}

== Topic detection and tracking (TDT) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Allan, J., Carbonell, J., Doddington, G., & Yamron, J.''' (1998). [http://www.cs.pitt.edu/~chang/265/proj10/sisref/1.pdf Topic Detection and Tracking Pilot Study Final Report]. In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, (pp.&nbsp;194  218). Lansdowne, Virginia, United States. February. || 1998 || Technical Report || TDT ||
|-
| '''Swan, R., & Allan, J.''' (1999). [http://dl.acm.org/citation.cfm?id=319956 Extracting Significant Time-Varying Features from Text]]. ''In [http://cikmconference.org/1999/ CIKM 1999]]: Proceedings of the 8th International ACM Conference on Information and Knowledge Management'' (pp.&nbsp;38  45). Kansas City, Missouri, United States. November 26: ACM Press. || 1999 || CIKM || TDT ||
|-
| '''Swan, R., & Jensen, D.''' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. ''In M. Grobelnik, D. Mladenic, & N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining'' (pp.&nbsp;73  80). Boston, Massachusetts, United States. August 2023: ACM Press. || 2000 || KDD - TM || TDT ||
|-
| '''Swan, R., & Allan, J.''' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. ''In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;49  56). Athens, Greece. July 2428: ACM Press. || 2000 || SIGIR || TDT ||
|-
| '''Makkonen, J., & Ahonen-Myka, H.''' (2003). [http://www.springerlink.com/content/a5ev5br7wwh5lvyl/ Utilizing Temporal Information in Topic Detection and Tracking]. ''In T. Koch, & I. T. Solvberg (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2003.org/ ECDL 2003]: 7th European Conference on Research and Advances Technology for Digital Libraries'' (Vol. 2769/2004, pp.&nbsp;393  404). Trondheim, Norway. August 1722: Springer Berlin / Heidelberg. || 2003 || ECDL || TDT ||
|-
| '''Shaparenko, B., Caruana, R., Gehrke, J., & Joachims, T.''' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. ''In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]'' (pp.&nbsp;165  174). Houston, United States. November 2730: IEEE Press. || 2005 || ICDM - TDM || TDT ||
|-
| '''Mori, M., Miura, T., & Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&nbsp;338  342). Hong Kong, China. December 1822: IEEE Computer Society Press. || 2006 || WIC || TDT ||
|-
| '''Kim, P., Myaeng, S.H.''' (2004). [http://dl.acm.org/citation.cfm?id=1039624 Usefulness of Temporal Information Automatically Extracted from News Articles for Topic Tracking]. ''In [http://talip.acm.org/index.htm TALIP]:Journal of ACM Transactions on Asian Language Information Processing'' (pp.&nbsp;227  242). New York, United States. || 2004 || TALIP || TDT ||
|}

==References==
{{reflist}}

[[Category:Information retrieval]]
>>EOP<<
139<|###|>Category:Personalized search
{{catmore}}

[[Category:Information retrieval]]
[[Category:Internet search engines| ]]
>>EOP<<
145<|###|>Schoolr
{{orphan|date=April 2010}}
'''Schoolr''' is a [[front and back ends|front-end]] academic directory that features frequently used third-party search engines and resources from [[Google]], [[Wikipedia]], [[Reference.com]], [[Acronym Finder]], [[Wolfram Alpha]], [[Yahoo! Babel Fish]], and the [[University of North Carolina]].<ref>[http://www.lifehack.org/articles/technology/schoolr-google-wikipedia-dictionarycom-and-more-on-one-site.html "Schoolr: Google, Wikipedia, Dictionary.com, and more on one site]", ''Stepcase Lifehack'', March 19, 2007</ref>

Schoolr went live on December 3, 2006<ref>[http://lifehacker.com/290027/schoolr-search-start-page "Schoolr search start page]", ''Lifehacker'', August 16, 2007</ref> and was developed by Sasan Aghdasi at the [[University of Victoria]].

== References ==
<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->
{{Reflist}}

== External links ==
* [http://www.schoolr.com/ Schoolr]

{{Internet search}}

{{DEFAULTSORT:Schoolr}}
[[Category:Information retrieval]]
[[Category:Internet search engines]]
[[Category:Internet terminology]]
>>EOP<<
151<|###|>Latent semantic indexing
'''Latent semantic indexing''' ('''LSI''') is an indexing and retrieval method that uses a mathematical technique called [[singular value decomposition]] (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text.  LSI is based on the principle that words that are used in the same contexts tend to have similar meanings.  A key feature of LSI is its ability to extract the conceptual content of a body of text by establishing associations between those terms that occur in similar contexts.<ref>Deerwester, S., et al, Improving Information Retrieval with Latent Semantic Indexing, Proceedings of the 51st Annual Meeting of the American Society for Information Science 25, 1988, pp. 3640.</ref>

LSI is also an application of [[correspondence analysis]], a multivariate statistical technique developed by [[Jean-Paul Benzecri]]<ref>{{ cite book
 | author = Benzecri, J.-P.
 | publisher=Dunod |location= Paris, France
 | year = 1973
 | title = L'Analyse des Donnees. Volume II. L'Analyse des Correspondences
 }}</ref> in the early 1970s, to a [[contingency table]] built from word counts in documents.

Called Latent Semantic Indexing because of its ability to correlate semantically related terms that are latent in a collection of text, it was first applied to text at Bellcore in the late 1980s.   The method, also called [[latent semantic analysis]] (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches.  Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results dont share a specific word or words with the search criteria.

__TOC__

== Benefits of LSI ==

LSI overcomes two of the most problematic constraints of Boolean keyword queries:  multiple words that have similar meanings ([[synonymy]]) and words that have more than one meaning ([[polysemy]]).  Synonymy is often the cause of [[vocabulary mismatch|mismatches in the vocabulary]] used by the authors of documents and the users of information retrieval systems.<ref>{{cite doi|10.1145/32206.32212}}</ref><ref>{{cite doi|10.1145/1871437.1871474}}</ref>   As a result, Boolean or keyword queries often return irrelevant results and miss information that is relevant.

LSI is also used to perform automated document categorization.  In fact, several experiments have demonstrated that there are a number of correlations between the way LSI and humans process and categorize text.<ref>Landauer, T., et al., Learning Human-like Knowledge by Singular Value Decomposition: A Progress Report, M. I. Jordan, M. J. Kearns & S. A. Solla (Eds.), Advances in Neural Information Processing Systems 10, Cambridge: MIT Press, 1998, pp. 4551.</ref>    Document categorization is the assignment of documents to one or more predefined categories based on their similarity to the conceptual content of the categories.<ref>{{cite doi|10.1145/288627.288651}}</ref>   LSI uses ''example'' documents to establish the conceptual basis for each category.  During categorization processing, the concepts contained in the documents being categorized are compared to the concepts contained in the example items, and a category (or categories) is assigned to the documents based on the similarities between the concepts they contain and the concepts that are contained in the example documents.

Dynamic clustering based on the conceptual content of documents can also be accomplished using LSI.  Clustering is a way to group documents based on their conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster.  This is very useful when dealing with an unknown collection of unstructured text.

Because it uses a strictly mathematical approach, LSI is inherently independent of language.  This enables LSI to elicit the semantic content of information written in any language without requiring the use of auxiliary structures, such as dictionaries and thesauri.  LSI can also perform cross-linguistic concept searching and example-based categorization.  For example, queries can be made in one language, such as English, and conceptually similar results will be returned even if they are composed of an entirely different language or of multiple languages.

LSI is not restricted to working only with words.  It can also process arbitrary character strings.  Any object that can be expressed as text can be represented in an LSI vector space.<ref>Zukas, Anthony, Price, Robert J., Document Categorization Using Latent Semantic Indexing, White Paper, [[Content Analyst Company]], LLC</ref>   For example, tests with MEDLINE abstracts have shown that LSI is able to effectively classify genes based on conceptual modeling of the biological information contained in the titles and abstracts of the MEDLINE citations.<ref>{{cite doi|10.1093/bioinformatics/bth464}}</ref>

LSI automatically adapts to new and changing terminology, and has been shown to be very tolerant of noise (i.e., misspelled words, typographical errors, unreadable characters, etc.).<ref>{{cite doi|10.1007/11427995_68}}</ref>   This is especially important for applications using text derived from Optical Character Recognition (OCR) and speech-to-text conversion.  LSI also deals effectively with sparse, ambiguous, and contradictory data.

Text does not need to be in sentence form for LSI to be effective.  It can work with lists, free-form notes, email, Web-based content, etc.  As long as a collection of text contains multiple terms, LSI can be used to identify patterns in the relationships between the important terms and concepts contained in the text.

LSI has proven to be a useful solution to a number of conceptual matching problems.<ref>Ding, C., A Similarity-based Probability Model for Latent Semantic Indexing, Proceedings of the 22nd International ACM SIGIR Conference on Research and Development in Information Retrieval, 1999, pp. 5965.</ref><ref>Bartell, B., Cottrell, G., and Belew, R., Latent Semantic Indexing is an Optimal Special Case of Multidimensional Scaling, Proceedings, ACM SIGIR Conference on Research and Development in Information Retrieval, 1992, pp. 161167.</ref>  The technique has been shown to capture key relationship information, including causal, goal-oriented, and taxonomic information.<ref>Graesser, A., and Karnavat, A., Latent Semantic Analysis Captures Causal, Goal-oriented, and Taxonomic Structures, Proceedings of CogSci 2000, pp. 184189.</ref>

== LSI timeline ==

'''Mid-1960s'''  Factor analysis technique first described and tested (H. Borko and M. Bernick)

'''1988'''  Seminal paper on LSI technique published (Deerwester et al.)

'''1989'''  Original patent granted (Deerwester et al.)

'''1992'''  First use of LSI to assign articles to reviewers<ref>Dumais, S., and Nielsen, J., Automating the Assignment of Submitted Manuscripts to Reviewers, Proceedings of the Fifteenth Annual International Conference on Research and Development in Information Retrieval, 1992, pp. 233244.</ref>  (Dumais and Nielsen)

'''1994'''  Patent granted for the cross-lingual application of LSI (Landauer et al.)

'''1995'''  First use of LSI for grading essays (Foltz, et al., Landauer et al.)

'''1999'''  First implementation of LSI technology for intelligence community for analyzing unstructured text (SAIC).

'''2002'''  LSI-based product offering to intelligence-based government agencies (SAIC)

'''2005'''  First vertical-specific application  publishing  EDB (EBSCO, [[Content Analyst Company]])

== Mathematics of LSI ==

LSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text.  In general, the process involves constructing a weighted term-document matrix, performing a '''Singular Value Decomposition''' on the matrix, and using the matrix to identify the concepts contained in the text.

=== Term-document matrix ===

LSI begins by constructing a term-document matrix, <math>A</math>, to identify the occurrences of the <math>m</math> unique terms within a collection of <math>n</math> documents.  In a term-document matrix, each term is represented by a row, and each document is represented by a column, with each matrix cell, <math>a_{ij}</math>, initially representing the number of times the associated term appears in the indicated document, <math>\mathrm{tf_{ij}}</math>.  This matrix is usually very large and very sparse.

Once a term-document matrix is constructed, local and global weighting functions can be applied to it to condition the data.  The weighting functions transform each cell, <math>a_{ij}</math> of <math>A</math>, to be the product of a local term weight, <math>l_{ij}</math>, which describes the relative frequency of a term in a document, and a global weight, <math>g_i</math>, which describes the relative frequency of the term within the entire collection of documents.

Some common local weighting functions <ref>
Berry, M. W., and Browne, M., Understanding Search Engines: Mathematical Modeling and Text Retrieval, Society for Industrial and Applied Mathematics, Philadelphia, (2005).</ref> are defined in the following table.

{| style="width:60%" cellpadding="25" cellspacing="5" align="center"
|-
|  style="width:22%" | '''Binary''' ||
| <math>l_{ij} = 1</math> if the term exists in the document, or else <math>0</math>
|-
|  style="width:22%" | '''TermFrequency''' ||
| <math>l_{ij} = \mathrm{tf}_{ij}</math>, the number of occurrences of term <math>i</math> in document <math>j</math>
|-
|  style="width:22%" | '''Log''' ||
| <math>l_{ij} = \log(\mathrm{tf}_{ij} + 1)</math>
|-
|  style="width:22%" | '''Augnorm''' ||
| <math>l_{ij} = \frac{\Big(\frac{\mathrm{tf}_{ij}}{\max_i(\mathrm{tf}_{ij})}\Big) + 1}{2}</math>
|}

Some common global weighting functions are defined in the following table.

{| style="width:60%" cellpadding="25" cellspacing="5" align="center"
|-
| style="width:22%" | '''Binary''' ||
| <math>g_i = 1</math>
|-
| style="width:22%" | '''Normal''' ||
| <math>g_i = \frac{1}{\sqrt{\sum_j \mathrm{tf}_{ij}^2}}</math>
|-
| style="width:22%" | '''GfIdf''' ||
| <math>g_i = \mathrm{gf}_i / \mathrm{df}_i</math>, where <math>\mathrm{gf}_i</math> is the total number of times term <math>i</math> occurs in the whole collection, and <math>\mathrm{df}_i</math> is the number of documents in which term <math>i</math> occurs.
|-
| style="width:22%" | '''Idf''' ||
| <math>g_i = \log_2 \frac{n}{1+ \mathrm{df}_i}</math>
|-
| style="width:22%" | '''Entropy''' ||
| <math>g_i = 1 + \sum_j \frac{p_{ij} \log p_{ij}}{\log n}</math>, where <math>p_{ij} = \frac{\mathrm{tf}_{ij}}{\mathrm{gf}_i}</math>
|}

Empirical studies with LSI report that the Log Entropy weighting functions work well, in practice, with many data sets.<ref>Landauer, T., et al., Handbook of Latent Semantic Analysis, Lawrence Erlbaum Associates, 2007.</ref>  In other words, each entry <math>a_{ij}</math> of <math>A</math> is computed as:

:<math>g_i = 1 + \sum_j \frac{p_{ij} \log p_{ij}}{\log n}</math>

:<math>a_{ij} = g_i \ \log (\mathrm{tf}_{ij} + 1)</math>

=== Rank-reduced singular value decomposition ===

A rank-reduced, [[singular value decomposition]] is performed on the matrix to determine patterns in the relationships between the terms and concepts contained in the text.  The SVD forms the foundation for LSI.<ref>Berry, Michael W., Dumais, Susan T., O'Brien, Gavin W., Using Linear Algebra for Intelligent Information Retrieval, December 1994, SIAM Review 37:4 (1995), pp. 573595.</ref>   It computes the term and document vector spaces by approximating the single term-frequency matrix, <math>A</math>, into three other matrices an '''''m''''' by '''''r'''''  term-concept vector matrix <math>T</math>, an '''''r''''' by '''''r''''' singular values matrix <math>S</math>, and a '''''n''''' by '''''r''''' concept-document vector matrix, <math>D</math>, which satisfy the following relations:

<math>A \approx TSD^T</math>

<math>T^T T = I_r \quad D^T D = I_r </math>

<math>S_{1,1} \geq S_{2,2} \geq \ldots \geq  S_{r,r} > 0 \quad S_{i,j} = 0 \; \text{where} \; i \neq j</math>

In the formula, '''A''' is the supplied '''''m''''' by '''''n''''' weighted matrix of term frequencies in a collection of text where '''''m''''' is the number of unique terms, and '''''n''''' is the number of documents.  '''T''' is a computed '''''m''''' by '''''r''''' matrix of term vectors where '''''r''''' is the rank of '''A'''a measure of its unique dimensions ''' min(''m,n'')'''.  '''S''' is a computed '''''r''''' by '''''r''''' diagonal matrix of decreasing singular values, and '''D''' is a computed '''''n''''' by '''''r''''' matrix of document vectors.

The LSI modification to a standard SVD is to reduce the rank or truncate the singular value matrix '''S''' to size '''''k'''''  '''''r''''', typically on the order of a '''''k''''' in the range of 100 to 300 dimensions, effectively reducing the term and document vector matrix sizes to '''''m''''' by '''''k''''' and '''''n''''' by '''''k''''' respectively.  The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of '''A'''.  This reduced set of matrices is often denoted with a modified formula such as:

:::::::'''A  A''<sub>k''</sub> = T''<sub>k''</sub> S''<sub>k''</sub> D''<sub>k''</sub><sup>T</sup>'''

Efficient LSI algorithms only compute the first '''''k''''' singular values and term and document vectors as opposed to computing a full SVD and then truncating it.

Note that this rank reduction is essentially the same as doing [[Principal Component Analysis]] (PCA) on the matrix '''A''', except that PCA subtracts off the means.  PCA loses the sparseness of the '''A''' matrix, which can make it infeasible for large lexicons.

== Querying and augmenting LSI vector spaces ==

The computed '''T''<sub>k''</sub>''' and '''D''<sub>k''</sub>''' matrices define the term and document vector spaces, which with the computed singular values, '''S''<sub>k''</sub>''', embody the conceptual information derived from the document collection.  The similarity of terms or documents within these spaces is a factor of how close they are to each other in these spaces, typically computed as a function of the angle between the corresponding vectors.

The same steps are used to locate the vectors representing the text of queries and new documents within the document space of an existing LSI index.  By a simple transformation of the '''A = T S D<sup>T</sup>''' equation into the equivalent '''D = A<sup>T</sup> T S<sup>1</sup>''' equation, a new vector, '''''d''''', for a query or for a new document can be created by computing a new column in '''A''' and then multiplying the new column by '''T S<sup>1</sup>'''.  The new column in '''A''' is computed using the originally derived global term weights and applying the same local weighting function to the terms in the query or in the new document.

A drawback to computing vectors in this way, when adding new searchable documents, is that terms that were not known during the SVD phase for the original index are ignored.  These terms will have no impact on the global weights and learned correlations derived from the original collection of text.  However, the computed vectors for the new text are still very relevant for similarity comparisons with all other document vectors.

The process of augmenting the document vector spaces for an LSI index with new documents in this manner is called ''folding in''.  Although the folding-in process does not account for the new semantic content of the new text, adding a substantial number of documents in this way will still provide good results for queries as long as the terms and concepts they contain are well represented within the LSI index to which they are being added.  When the terms and concepts of a new set of documents need to be included in an LSI index, either the term-document matrix, and the SVD, must be recomputed or an incremental update method (such as the one described in <ref name="brand2006">{{cite journal | url=http://www.merl.com/reports/docs/TR2006-059.pdf |format=PDF| title=Fast Low-Rank Modifications of the Thin Singular Value Decomposition | author=Matthew Brand | journal=Linear Algebra and Its Applications | volume=415 | pages=2030 | year=2006 | doi=10.1016/j.laa.2005.07.021 }}</ref>) be used.

== Additional uses of LSI ==

It is generally acknowledged that the ability to work with text on a semantic basis is essential to modern information retrieval systems.  As a result, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.

LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.<ref>Dumais, S., Latent Semantic Analysis, ARIST Review of Information Science and Technology, vol. 38, 2004, Chapter 4.</ref>   Below are some other ways in which LSI is being used:

* Information discovery<ref>Best Practices Commentary on the Use of Search and Information Retrieval Methods in E-Discovery, the Sedona Conference, 2007, pp. 189223.</ref>  (eDiscovery, Government/Intelligence community, Publishing)
* Automated document classification (eDiscovery, Government/Intelligence community, Publishing)<ref>Foltz, P. W. and Dumais, S. T. Personalized Information Delivery:  An analysis of information filtering methods, Communications of the ACM, 1992, 34(12), 51-60.</ref>
* Text summarization<ref>Gong, Y., and Liu, X., Creating Generic Text Summaries, Proceedings, Sixth International Conference on Document Analysis and Recognition, 2001, pp. 903907.</ref>  (eDiscovery, Publishing)
* Relationship discovery<ref>Bradford, R., Efficient Discovery of New Information in Large Text Databases, Proceedings, IEEE International Conference on Intelligence and Security Informatics, Atlanta, Georgia, LNCS Vol. 3495, Springer, 2005, pp. 374380.</ref>  (Government, Intelligence community, Social Networking)
* Automatic generation of link charts of individuals and organizations<ref>Bradford, R., Application of Latent Semantic Indexing in Generating Graphs of Terrorist Networks, in: Proceedings, IEEE International Conference on Intelligence and Security Informatics, ISI 2006, San Diego, CA, USA, May 2324, 2006, Springer, LNCS vol. 3975, pp. 674675.</ref>  (Government, Intelligence community)
* Matching technical papers and grants with reviewers<ref>Yarowsky, D., and Florian, R., Taking the Load off the Conference Chairs: Towards a Digital Paper-routing Assistant, Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in NLP and Very-Large Corpora, 1999, pp. 220230.</ref>  (Government)
* Online customer support<ref>Caron, J., Applying LSA to Online Customer Support: A Trial Study, Unpublished Master's Thesis, May 2000.</ref>  (Customer Management)
* Determining document authorship<ref>Soboroff, I., et al, Visualizing Document Authorship Using N-grams and Latent Semantic Indexing,   Workshop on New Paradigms in Information Visualization and Manipulation, 1997, pp. 4348.</ref>  (Education)
* Automatic keyword annotation of images<ref>Monay, F., and Gatica-Perez, D., On Image Auto-annotation with Latent Space Models, Proceedings of the 11th ACM international conference on Multimedia, Berkeley, CA, 2003, pp. 275278.</ref>
* Understanding software source code<ref>Maletic, J., and Marcus, A., Using Latent Semantic Analysis to Identify Similarities in Source Code to Support Program Understanding, Proceedings of 12th IEEE International Conference on Tools with Artificial Intelligence, Vancouver, British Columbia, November 1315, 2000, pp. 4653.</ref>  (Software Engineering)
* Filtering [[Spam (electronic)|spam]]<ref>Gee, K., Using Latent Semantic Indexing to Filter Spam, in: Proceedings, 2003 ACM Symposium on Applied Computing, Melbourne, Florida, pp. 460464.</ref>  (System Administration)
* Information visualization<ref>Landauer, T., Laham, D., and Derr, M., From Paragraph to Graph: Latent Semantic Analysis for Information Visualization, Proceedings of the National Academy of Science, 101, 2004, pp. 52145219.</ref>
* [[Automated essay scoring|Essay scoring]]<ref>Foltz, Peter W., Laham, Darrell, and Landauer, Thomas K., Automated Essay Scoring: Applications to Educational Technology, Proceedings of EdMedia,  1999.</ref>  (Education)
* [[Literature-based discovery]]<ref>Gordon, M., and Dumais, S., Using Latent Semantic Indexing for Literature Based Discovery, Journal of the American Society for Information Science, 49(8), 1998, pp. 674685.</ref>

LSI is increasingly being used for electronic document discovery (eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is essential.  Concept-based searching using LSI has been applied to the eDiscovery process by leading providers as early as 2003.<ref>There Has to be a Better Way to Search, 2008, White Paper, Fios, Inc.</ref>

== Challenges to LSI ==

Early challenges to LSI focused on scalability and performance.  LSI requires relatively high computational performance and memory in comparison to other information retrieval techniques.<ref>Karypis, G., Han, E., Fast Supervised Dimensionality Reduction Algorithm with Applications to Document Categorization and Retrieval, Proceedings of CIKM-00, 9th ACM Conference on Information and Knowledge Management.</ref>  However, with the implementation of modern high-speed processors and the availability of inexpensive memory, these considerations have been largely overcome.  Real-world applications involving more than 30 million documents that were fully processed through the matrix and SVD computations are not uncommon in some LSI applications. A fully scalable (unlimited number of documents, online training) implementation of LSI is contained in the open source [[gensim]] software package.<ref name="rehurek2011">{{cite journal | url=http://dx.doi.org/10.1007/978-3-642-20161-5_29 |format=PDF| title=Subspace Tracking for Latent Semantic Analysis | author=Radim Rehurek | journal=Advances in Information Retrieval - 33rd European Conference on IR Research, ECIR 2011 | volume=6611 | pages=289300 | year=2011 | doi=10.1007/978-3-642-20161-5_29 }}</ref>

Another challenge to LSI has been the alleged difficulty in determining the optimal number of dimensions to use for performing the SVD.  As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts.  The actual number of dimensions that can be used is limited by the number of documents in the collection.  Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents).<ref>Bradford, R., An Empirical Study of Required Dimensionality for Large-scale Latent Semantic Indexing Applications, Proceedings of the 17th ACM Conference on Information and Knowledge Management, Napa Valley, California, USA, 2008, pp. 153162.</ref>   However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection.<ref>Landauer, Thomas K., and Dumais, Susan T., Latent Semantic Analysis, Scholarpedia, 3(11):4356, 2008.</ref>

Checking the amount of variance in the data after computing the SVD can be used to determine the optimal number of dimensions to retain.  The variance contained in the data can be viewed by plotting the singular values (S) in a [[scree plot]].  Some LSI practitioners select the dimensionality associated with the knee of the curve as the cut-off point for the number of dimensions to retain.  Others argue that some quantity of the variance must be retained, and the amount of variance in the data should dictate the proper dimensionality to retain.  Seventy percent is often mentioned as the amount of variance in the data that should be used to select the optimal dimensionality for recomputing the SVD.<ref>Cangelosi, R., Goriely A., Component Retention In Principal Component Analysis With Application to Cdna Microarray Data, BMC Biology Direct 2(2) (2007).</ref><ref>Jolliffe, L. T., Principal Component Analysis, Springer-Verlag, New York, (1986).</ref><ref>Hu, X., Z. Cai, et al., LSA: First Dimension and Dimensional Weighting, 25th Annual Meeting of the Cognitive Science Society, Boston, MA.</ref>

==See also==
* [[Latent semantic analysis]]
* [[Latent Semantic Structure Indexing]]
* [[Principal component analysis]]
* [[Correspondence analysis]]
* [[Probabilistic latent semantic analysis]]

{{Natural Language Processing}}

== References ==
{{Reflist}}

== Further reading ==
*{{cite book|authors=Berry, M. W., Browne M.|title=Understanding Search Engines: Mathematical Modeling and Text Retrieval|location=Philadelphia|publisher=Society for Industrial and Applied Mathematics|year=2005|isbn=978-0898715811|url=http://www.mblazquez.es/blog-ccdoc-recuperacion/documentos/book_understanding-search-engines.pdf}}
*{{cite book|editors=Berry, M. W.|title=Survey of Text Mining: Clustering, Classification, and Retrieval|location=New York|publisher=Springer|year=2004|url=https://perso.uclouvain.be/vincent.blondel/publications/08-textmining.pdf|isbn=978-0387955636}}
*{{cite book|authors=Landauer, T., et al.|title=Handbook of Latent Semantic Analysis|publisher=Lawrence Erlbaum Associates|year=2007|isbn= 978-0805854183|url=http://books.google.de/books/about/Handbook_of_latent_semantic_analysis.html?id=jgVWCuFXePEC&redir_esc=y}}
*{{cite book|authors=Manning, C. D., Schutze H.|title=Foundations of Statistical Natural Language Processing|location=Cambridge, MA|publisher=The MIT Press|year=1999|url=http://nlp.stanford.edu/fsnlp/promo/contents.ps|isbn=9780262133609 }} [http://nlp.stanford.edu/fsnlp/ Companion webpage]

==External links==
* [http://www.cs.utk.edu/~lsi/ Michael Berrys site]
* [http://radimrehurek.com/gensim Gensim] contains a scalable Python+[[NumPy]] implementation of LSI, even for datasets larger than the available RAM.
* [http://scgroup.hpclab.ceid.upatras.gr/scgroup/Projects/TMG/ Text to Matrix Generator (TMG)]  MATLAB toolbox that can be used for various tasks in text mining (TM) specifically  i) indexing, ii) retrieval, iii) dimensionality reduction, iv) clustering, v) classification. Most of TMG is written in MATLAB and parts in Perl. It contains implementations of LSI, clustered LSI, NMF and other methods.
* [http://www.youtube.com/watch?v=QGd06MTRMHs Stanford University Andrew Ng Video on LSI]

{{DEFAULTSORT:Latent semantic indexing}}
[[Category:Information retrieval]]
[[Category:Semantic Web]]
>>EOP<<
157<|###|>Contextual searching
'''Contextual search''' is a form of optimizing web-based search results based on context provided by the user and the computer being used to enter the query.<ref>Susan E. Feldman. ''The Answer Machine'', Synthesis Lectures on Information Concepts, Retrieval, and Services. [http://www.morganclaypool.com/doi/abs/10.2200/S00442ED1V01Y201208ICR023 http://www.morganclaypool.com/doi/abs/10.2200/S00442ED1V01Y201208ICR023]</ref> Contextual search services differ from current search engines based on traditional information retrieval that return lists of documents based on their [[Relevance (information retrieval)|relevance]] to the query. Rather, contextual search attempts to increase the [[Precision and recall|precision]] of results based on how valuable they are to individual users.<ref>Pitokow, James; Hinrich Schutze; Todd Cass; Rob Cooley; Don Turnbull; Andy Edmonds; Eytan Adar; Thomas Breuel (2002). "Personalized search". [http://www.cond.org/p50-pitkow.pdf http://www.cond.org/p50-pitkow.pdf] Communications of the ACM (CACM) 45 (9): 5055.</ref>

== Basic Contextual Search ==
The basic form of contextual search is the process of scanning the full-text of a query in order to understand what the user needs. Web search engines scan HTML pages for content and return an index rating based on how relevant the content is to the entered query. HTML pages that have a higher occurrence of query keywords within their content are rated higher. Users have limited control over the context of their query based on the words they use to search with.<ref>Steve Lawrence. ''Context in Web Search'', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 25, 2000.</ref>  For example, users looking for the menu portion of a website can add menu to the end of their query to provide the search engine with context of what they need. The next step in contextualizing search is for the search service itself to request information that narrows down the results, such as Google asking for a time range to search within.

== Explicitly Supplied Context ==
Certain search services, including many Meta search engines, request individual contextual information from users to increase the precision of returned documents. Inquirus 2 is a Meta search engine that acts as a mediator between the user query and other search engines. When searching on Inquirus 2, users enter a query and specify constraints such as the information need category, maximum number of hits, and display formats.<ref>Steve Lawrence. ''Context in Web Search'', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 27, 2000.</ref> For example a user looking for research papers can specify documents with references or abstracts to be rated higher. If another user is searching for general information on the topic rather than research papers, they can specify the GenScore attribute to have a heavier weight.<ref>Steve Lawrence, C. Lee Giles. ''Inquirus, the NECI meta search engine''[http://www7.scu.edu.au/1906/com1906.htm]</ref>

Explicitly supplied context effectively increases the precision of results, however, these search services tend to suffer from poor user-experience. Learning the interface of programs like Inquirus can prove challenging for general users without knowledge of search metrics. Aspects of supplied context do appear on major search engines with better user-interaction such as Google and Bing. Google allows users to filter by type: Images, Maps, Shopping, News, Videos, Books, Flights, and Apps.<ref>[https://support.google.com/websearch/answer/142143?hl=en https://support.google.com/websearch/answer/142143?hl=en], Filter your search results</ref> Google has an extensive [https://support.google.com/websearch/answer/2466433?rd=1 list of search operators] that allow users to explicitly limit results to fit their needs such as restricting certain file types or removing certain words.<ref>[https://support.google.com/websearch/answer/2466433?rd=1 https://support.google.com/websearch/answer/2466433?rd=1], Search Operators</ref> Bing also uses a similar set of search operators to assist users in explicitly narrowing down the context of their queries. Bing allows users to search within a time range, by file type, by location, language, and more.<ref>[http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/ http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/], Bing Tricks</ref>

== Automatically Inferred Context ==
There are other systems being developed that are working on automatically inferring the context of user queries based on the content of other documents they view or edit. [[Watson (computer)|IBM's Watson Project]] aims to create a cognitive technology that dynamically learns as it processes user queries. When presented with a query Watson creates a hypothesis that is evaluated against its present bank of knowledge based on previous questions. As related terms and relevant documents are matched against the query, Watson's hypothesis is modified to reflect the new information provided through unstructured data based on information it has obtained in previous situations.<ref>[http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html], How Watson Works - IBM</ref> Watson's ability to build off previous knowledge allows queries to be automatically filtered for similar contexts in order to supply precise results.

Major search services such as Google, Bing, and Yahoo also have a system of automatically inferring the context of particular user queries. Google tracks user's previous queries and selected results to further personalize results for those individuals. For example if a user consistently searches for articles related to animals, wild animals, or animal care a search for "jaguar" would rank an article on jaguar cats higher than links to Jaguar Cars.<ref>Eric J Glover, Steve Lawrence, Michael D. Gordon, William P. Birmingham, C. Lee Giles. ''Web Search - Your Way'', NEC Research Institution [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.41.7499&rep=rep1&type=pdf]</ref> Similar to Watson, search services strive to learn from users based on previous experiences to automatically provide context on current queries. Bing also provides automatic context for particular queries based on content of the query itself. A [http://www.bing.com/search?q=pizza&go=Submit&qs=n&form=GEOMA1&pq=pizza&sc=8-1&sp=-1&sk=&cvid=883269b61529466e810bc096e371ec19 search of "pizza"] returns an interactive list of restaurants and their ratings based on the approximate location of the user's computer. The Bing server automatically infers that when a user searches for a food item they are interested in documents within the context of purchasing that food item or finding restaurants that sell that particular item.

=== Contextual Mobile Search ===
The drive to develop better contextualized search coincides with the increasing popularity of using mobile phones to complete searches. BIA/Kelsey research marketing firm projects that by 2015 mobile local search will "exceed local search by more than 27 billion queries".<ref>[http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp], Mobile Search to Surpass Desktop</ref> Mobile phones provide the opportunity to provide search services with a broader supply of contextual information, particularly for location services but also personalized searches based on the wealth of information stored locally on the phone including contacts information, geometric analysis such as speed and elevation, and installed apps.<ref>[http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/ http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/], Contextually Aware Mobile Devices</ref> Mobile start up company [http://everything.Me Everything.Me] is one company that is moving towards turning the smartphone into an all-in-one device the provides relevant information for specific users. Everything.Me pushes app updates and suggestion to a user's home-screen based on user movement, location, current time, past search queries, and entertainment preferences.<ref>[http://socialtimes.com/mobile-contextual-search-future_b149394 http://socialtimes.com/mobile-contextual-search-future_b149394], Contextual Search through Mobile and Everything.Me</ref> For example when a user opens their phone in the morning Everything.Me will present users with apps relevant to how that users interacts with their phone in the morningpresenting weather apps, bus apps, and news apps.<ref>[http://everything.me/about/ Everything.Me]</ref> Later when that user goes to work Everything.Me will update the work related applications to be prioritized over other apps. Everything.Me anticipates a user's needs based on their current actions and past interactions on the web. This process of automatically obtaining context from mobile phones can help to increase the precision of user queries. For instance if a user searches for a place to eat while at work Everything.Me will take work into context and return restaurants that would be more appropriate for a lunch break at the office.<ref>[http://everything.me/ http://everything.me/], Video Information</ref>

== References ==
{{reflist}}

{{Internet search}}

{{DEFAULTSORT:Contextual Searching}}
[[Category:Internet search engines]]
[[Category:Semantic Web]]
[[Category:Information retrieval]]
[[Category:Internet terminology]]
>>EOP<<
163<|###|>Explicit semantic analysis
In [[natural language processing]] and [[information retrieval]], '''explicit semantic analysis''' ('''ESA''') is a [[Vector space model|vectorial]] representation of text (individual words or entire documents) that uses a document corpus as a [[knowledge base]]. Specifically, in ESA, a word is represented as a column vector in the [[tf*idf|tfidf]] matrix of the text corpus and a document (string of words) is represented as the [[centroid]] of the vectors representing its words. Typically, the text corpus is [[Wikipedia]], though other corpora including the [[Open Directory Project]] have been used.<ref name="infosys">{{cite journal |authors=Ofer Egozi, Shaul Markovitch and Evgeniy Gabrilovich |year=2011 |title=Concept-Based Information Retrieval using Explicit Semantic Analysis |url=http://www.cs.technion.ac.il/~gabr/publications/papers/Egozi2011CBI.pdf|format=pdf|accessdate=January 3, 2015|journal=ACM Transactions on Information Systems |volume=29 |issue=2}}</ref>

ESA was designed by [[Evgeniy Gabrilovich]] and Shaul Markovitch as a means of improving [[document classification|text categorization]]<ref>{{cite conference |first1=Evgeniy |last1=Gabrilovich |first2=Shaul |last2=Markovitch |title=Overcoming the brittleness bottleneck using Wikipedia: enhancing text categorization with encyclopedic knowledge |conference=Proc. 21st National Conference on Artificial Intelligence (AAAI) |pages=13011306 |year=2006 |url=http://www.aaai.org/Papers/AAAI/2006/AAAI06-204.pdf}}</ref>
and has been used by this pair of researchers to compute what they refer to as "[[Semantics|semantic]] relatedness" by means of [[cosine similarity]] between the aforementioned vectors, collectively interpreted as a space of "concepts explicitly defined and described by humans", where Wikipedia articles (or ODP entries, or otherwise titles of documents in the knowledge base corpus) are equated with concepts.
The name "explicit semantic analysis" contrasts with [[latent semantic analysis]] (LSA), because the use of a knowledge base makes it possible to assign human-readable labels to the concepts that make up the vector space.<ref>{{cite conference |first1=Evgeniy |last1=Gabrilovich |first2=Shaul |last2=Markovitch |title=Computing semantic relatedness using Wikipedia-based Explicit Semantic Analysis |conference=Proc. 20th Int'l Joint Conf. on Artificial Intelligence (IJCAI) |pages=16061611 |year=2007 |url=http://www.cs.technion.ac.il/~gabr/papers/ijcai-2007-sim.pdf}}</ref><ref name="infosys"/>

ESA, as originally posited by Gabrilovich and Markovitch, operates under the assumption that the knowledge base contains topically [[Orthogonality|orthogonal]] concepts. However, it was later shown by Anderka and Stein that ESA also improves the performance of [[information retrieval]] systems when it is based not on Wikipedia, but on the [[Reuters]] corpus of newswire articles, which does not satisfy the orthogonality property; in their experiments, Anderka and Stein used newswire stories as "concepts".<ref>Maik Anderka and Benno Stein. [http://www.uni-weimar.de/medien/webis/publications/papers/stein_2009c.pdf The ESA retrieval model revisited]. Proceedings of the 32nd International ACM Conference on Research and Development in Information Retrieval (SIGIR), pp. 670-671, 2009.</ref>
To explain this observation, links have been shown between ESA and the [[generalized vector space model]].<ref>Thomas Gottron, Maik Anderka and Benno Stein. [http://www.uni-weimar.de/medien/webis/publications/papers/stein_2011o.pdf Insights into explicit semantic analysis]. Proceedings of the 20th ACM International Conference on Information and Knowledge Management (CIKM), pp. 1961-1964, 2011.</ref>
Gabrilovich and Markovitch replied to Anderka and Stein by pointing out that their experimental result was achieved using "a single application of ESA (text similarity)" and "just a single, extremely small and homogenous test collection of 50 news documents".<ref name="infosys" />

'''Cross-language explicit semantic analysis''' ('''CL-ESA''') is a multilingual generalization of ESA.<ref>Martin Potthast, Benno Stein, and Maik Anderka. [http://www.uni-weimar.de/medien/webis/publications/papers/stein_2008b.pdf A Wikipedia-based multilingual retrieval model]. Proceedings of the 30th European Conference on IR Research (ECIR), pp. 522-530, 2008.</ref>
CL-ESA exploits a document-aligned multilingual reference collection (e.g., again, Wikipedia) to represent a document as a language-independent concept vector. The relatedness of two documents in different languages is assessed by the cosine similarity between the corresponding vector representations.

== See also ==
* [[Topic model]]

== External links ==
* [http://www.cs.technion.ac.il/~gabr/resources/code/esa/esa.html Explicit semantic analysis] on Evgeniy Gabrilovich's homepage; has links to implementations

== References ==
{{reflist|2}}

[[Category:Natural language processing]]
[[Category:Vector space model]]
>>EOP<<
169<|###|>Ptx (Unix)
{{Unreferenced stub|auto=yes|date=December 2009}}
{{Lowercase|title=ptx}}
'''ptx''' is a [[Unix]] utility, named for the ''permuted index'' which can perform the function of the Keyword in Context ([[Key Word in Context|KWIC]]) search mode. There is a corresponding [[IBM mainframe]] utility which performs the same function. permuted indexes are often used in such places as bibliographic or medical databases, thesauruses, or web sites to aid in locating entries of interest.

==See also==
* [[Concordancer]]

[[Category:Searching]]
[[Category:Unix text processing utilities]]


{{Unix-stub}}
>>EOP<<
175<|###|>Category:Internet search
{{Cat main|Internet search}}

[[Category:Web services]]
[[Category:Searching]]
[[Category:World Wide Web|Search]] <!-- searching is a web function. Note that "Internet search" redirects to "Web page search" (or something like that)--->
>>EOP<<
181<|###|>Reverse telephone directory
A '''reverse telephone directory''' (also known as a '''gray pages''' directory, criss-cross directory or '''reverse phone lookup''') is a collection of telephone numbers and associated customer details. However, unlike a standard [[telephone directory]], where the user uses customer's details (such as name and address) in order to retrieve the telephone number of that person or business, a reverse telephone directory allows users to search by a telephone service number in order to retrieve the customer details for that service.

Reverse telephone directories are used by law enforcement and other emergency services in order to determine the origin of any request for assistance, however these systems include both publicly accessible (listed) and private (unlisted) services. As such, these directories are restricted to internal use only.

Publicly accessible reverse telephone directories may be provided as part of the standard directory services from the telecommunications carrier in some countries. In other countries these directories are often created by [[phreaking|phone phreaker]]s by collecting the information available via the publicly accessible directories and then providing a search function which allows users to search by the telephone service details.

==History==
Printed reverse phone directories have been produced by the telephone companies (in the United States) for decades, and were distributed to the phone companies, law enforcement, and [[public library|public libraries]].<ref>{{cite news | url=http://news.google.com/newspapers?nid=1454&dat=19720102&id=87osAAAAIBAJ&sjid=vgkEAAAAIBAJ&pg=3122,379459 | title=Clinton Directory Issued | date=Jan 2, 1972 | accessdate=9 February 2014 | location=Page 16}}</ref> In the early 1990s, businesses started offering reverse telephone lookups for fees, and by the early 2000s advertising-based reverse directories were available online, prompting occasional alarms about privacy concerns.

==Australia==
In 2001, a legal case ''[[Telstra|Telstra Corporation Ltd]] v Desktop Marketing Systems Pty Ltd'' was heard in the Australian Federal Court.<ref>{{cite web|url=http://www.austlii.edu.au/au/cases/cth/federal_ct/2001/612.html|title=Telstra Corporation Limited v Desktop Marketing Systems Pty Ltd (2001) FCA 612 (25 May 2001)|author=[[Federal Court of Australia]]|publisher=Australasian Legal Information Institute|accessdate=2008-01-03}}</ref><ref name=austliiPP>{{cite web|url=http://www.austlii.edu.au/au/journals/PLPR/2001/25.html|title=Private parts - PLPR 25; (2001) 8 PLPR 24|publisher=Australasian Legal Information Institute|accessdate=2008-01-03}}</ref> gave Telstra, the predominant carrier within Australia and the maintainer of the publicly accessible [[White Pages]] (residential) and [[Yellow Pages]] (commercial) directories, [[copyright]] over the content of these directories.

In February 2010 a Federal Court of Australia case ''[[Telstra|Telstra Corporation Ltd]] v Phone Directories Company Pty Ltd'' determined that Telstra does not hold copyright in the White Pages or the Yellow Pages.<ref>{{cite news|url=http://www.smh.com.au/business/copyright-to-enter-a-new-dimension-20101215-18y9o.html|title=Copyright to enter a new dimension|newspaper=[[The Sydney Morning Herald]]| first=Malcolm|last=Maiden|date=16 December 2010|accessdate=20 December 2012}}</ref>

As it currently{{when|date=October 2014}} stands there is no legal way to ensure a particular number is not listed in the directories currently available.

==United States==

In United States, landline phone subscribers can pay a small fee to exclude their number from the directory. This service is usually called "Your Listing Not Published" and the cost ranges between $0.80 and $1.50 for residential customers.

As [[cellular phones]] become more popular, there has been debate about releasing cell phone numbers into public [[4-1-1|411]] and reverse number directories. (S. 1963, the "Wireless 411 Privacy Act" 9/2004). However, opposition led by leading consumer-protection organization [[Consumers Union]] presented several privacy concerns in their congressional [http://www.consumersunion.org/pub/wireless%20411%20senate%20testimony%20final.pdf testimony]. Right now,{{when|date=October 2014}} cell phone numbers are not available in any public 411 or reverse-number directories. However, several information companies provide reverse cell phone lookups that are obtained from utility resources, and are available online. Because there is no central database of cell phone numbers, reverse phone directories that claim to be free cannot return information on those numbers.<ref>{{cite web | url=http://www.ncbi.nlm.nih.gov/pubmed/15652722 | title=Evaluating the utility and accuracy of a reverse telephone directory to identify the location of survey respondents. | publisher=Ncbi.nlm.nih.gov | work=2005 Feb | accessdate=9 February 2014 | author=Schootman M, Jeffe D, Kinman E, Higgs G, Jackson-Thompson J.}}</ref>

In recent years{{when|date=October 2014}} community web based services offer a reverse telephone directory of known telemarketers, debt collectors, fund raisers, and other solicitors which contact consumers by telephone.  Users of these services can perform a search of the telephone number which showed up on their caller ID and read through user comments to find the identity of the calling company or individual.

==United Kingdom==
In the United Kingdom proper, reverse directory information is not publicly available.<ref>{{cite web | url=http://ico.org.uk/for_organisations/privacy_and_electronic_communications/the_guide/directories_of_subscribers | title=Directories of subscribers | publisher=Information Commissioner's Office | accessdate=9 February 2014}}</ref> However, in the [[Channel Islands]] it is provided in the printed telephone directories. 

Although the information is, of necessity, available to emergency services, for other agencies it is treated as 'communication data' in the [[RIPA]] regime and subject to the same controls as requests for lists of and content of calls.

==References==
{{reflist}}
==External links==
<!-- Do not delete these comments. -->
<!-- Do not put commercial links into this list. Doing so can get you blocked with no further warning. --> 
*[http://web.archive.org/web/20010721175437/http://blackpages.2600.org.au/ Wayback Machine (21 July 2001) archive of http://blackpages.2600.org.au]
*[http://www.austlii.edu.au/au/cases/cth/federal_ct/2001/612.html Federal Court of Australia Case 612 (25 May 2001): Telstra Corporation Limited v Desktop Marketing Systems Pty Ltd]


[[Category:Telephone numbers]]
[[Category:Directories]]
[[Category:Searching]]
>>EOP<<
187<|###|>Web indexing
{{no footnotes|date=December 2014}}
'''Web indexing''' (or '''Internet indexing''') refers to various methods for indexing the contents of a [[website]] or of the [[Internet]] as a whole. Individual websites or [[intranet]]s may use a [[back-of-the-book index]], while [[search engines]] usually use keywords and [[Metadata (computing)|metadata]] to provide a more useful vocabulary for Internet or onsite searching. With the increase in the number of [[periodical]]s that have articles online, web indexing is also becoming important for periodical websites.

Back-of-the-book-style web indexes may be called "web site A-Z indexes". The implication with "A-Z" is that there is an alphabetical browse view or interface. This interface differs from that of a browse through layers of hierarchical categories (also known as a [[Taxonomy (general)|taxonomy]]) which are not necessarily alphabetical, but are also found on some web sites. Although an A-Z index could be used to index multiple sites, rather than the multiple pages of a single site, this is unusual.

Metadata web indexing involves assigning keywords or phrases to web pages or web sites within a [[metadata tag]] (or "meta-tag") field, so that the web page or web site can be retrieved with a search engine that is customized to search the keywords field. This may or may not involve using keywords restricted to a controlled vocabulary list. This method is commonly used by [[search engine indexing]].

==See also==
* [[Information architecture]]
* [[Search engine indexing]]
* [[Search engine optimization]]
* [[Site map]]
* [[Web navigation]]
* [[Web search engine]]

==References==
{{reflist}}

==External links==
<!--========================({{No More Links}})============================
    | PLEASE BE CAUTIOUS IN ADDING MORE LINKS TO THIS ARTICLE. WIKIPEDIA  |
    | IS NOT A COLLECTION OF LINKS NOR SHOULD IT BE USED FOR ADVERTISING. |
    |                                                                     |
    |           Excessive or inappropriate links WILL BE DELETED.         |
    | See [[Wikipedia:External links]] & [[Wikipedia:Spam]] for details.  |
    |                                                                     |
    | If there are already plentiful links, please propose additions or   |
    | replacements on this article's discussion page, or submit your link |
    | to the relevant category at the Open Directory Project (dmoz.org)   |
    | and link back to that category using the {{dmoz}} template.         |
    =======================({{No More Links}})=============================-->
*TheAlphaWeb [http://www.eprodoffice.com/shhh/abcdefghijklmnopqrstuvwxyz.htm ''An example of an Internet A-Z'']
*Glenda Browne and Jonathan Jermey, [http://www.webindexing.biz/ ''Website indexing: enhancing access to information within websites, 2nd Edition''], ISBN 1-875145-56-7
*James Lamb, [http://www.jalamb.com/publications.html ''Website Indexes: visitors to content in two clicks, or website indexing with XRefHT32 freeware''], ISBN 978-1-4116-7937-5
*[http://www.infotoday.com/books/books/BeyondBookIndex.shtml ''Beyond Book Indexing: How to Get Started in Web Indexing, Embedded Indexing, and Other Computer-Based Media''], edited by Marilyn Rowland and Diane Brenner, American Society of Indexers, Info Today, Inc, NJ, 2000, ISBN 1-57387-081-1
* {{Cite web
  |url=http://www.boxesandarrows.com/view/improving_usability_with_a_website_index
  |title=Improving Usability with a Website Index
  |archiveurl=http://www.webcitation.org/5vJwZDVkj
  |archivedate=2010-12-28
  |accessdate=2010-12-28
  |first=Fred
  |last=Leise
  |date=2002-07-15
}}
* [http://www.web-indexing.org/article-brown.htm Why Create an Index?]
* [http://www.theeasybee.com/directory/web-content-extraction Open Social Web 3.0 Directory]  Compare and review web indexing programs
{{Internet search}}
[[Category:Searching]]
[[Category:Indexing]]


{{internet-stub}}
>>EOP<<
193<|###|>Category:Search engine software
[[Category:Searching]]
[[Category:Data search engines]]
[[Category:Utility software by type]]
[[Category:Marketing software]]
[[Category:Web software]]
>>EOP<<
199<|###|>Hamming distance
{| align="right"
|-
| [[Image:Hamming distance 3 bit binary.svg|thumb|140px|3-bit binary [[cube]] for finding Hamming distance]]
| [[Image:Hamming distance 3 bit binary example.svg|thumb|140px|Two example distances: 100011 has distance 3 (red path); 010111 has distance 2 (blue path)]]
|-
|colspan=2 | [[Image:Hamming distance 4 bit binary.svg|thumb|280px|4-bit binary [[tesseract]] for finding Hamming distance]]
|-
|colspan=2 | [[Image:Hamming distance 4 bit binary example.svg|thumb|280px|Two example distances: 01001001 has distance 3 (red path); 01101110 has distance 1 (blue path)]]
|}

In [[information theory]], the '''Hamming distance''' between two [[String (computer science)|string]]s of equal length is the number of positions at which the corresponding symbols are different. In another way, it measures the minimum number of ''substitutions'' required to change one string into the other, or the minimum number of ''errors'' that could have transformed one string into the other.

==Examples==
The Hamming distance between:
* "'''</span>ka<span style="color:#0082ff">rol</span>in</span>'''" and "'''</span>ka<span style="color:red;">thr</span>in</span>'''" is 3.
* "'''</span>k<span style="color:#0082ff">a</span>r<span style="color:#0082ff">ol</span>in</span>'''" and "'''</span>k<span style="color:red;">e</span>r<span style="color:red;">st</span>in</span>'''" is 3.
* '''10<span style="color:#0082ff">1</span>1<span style="color:#0082ff">1</span>01''' and '''10<span style="color:red;">0</span>1<span style="color:red;">0</span>01''' is 2.
* '''2<span style="color:#0082ff">17</span>3<span style="color:#0082ff">8</span>96''' and '''2<span style="color:red;">23</span>3<span style="color:red;">7</span>96''' is 3.

==Special properties==
For a fixed length ''n'', the Hamming distance is a [[Metric (mathematics)|metric]] on the vector space of the words of length n, as it fulfills the conditions of non-negativity, identity of indiscernibles and symmetry, and it can be shown by [[complete induction]] that it satisfies the [[triangle inequality]] as well. The Hamming distance between two words ''a'' and ''b'' can also be seen as the [[Hamming weight]] of ''a''&minus;''b'' for an appropriate choice of the &minus; operator.

For '''binary strings''' ''a'' and ''b'' the Hamming distance is equal to the number of ones ([[Hamming weight|population count]]) in ''a'' [[Exclusive or|XOR]] ''b''. The metric space of length-''n'' binary strings, with the Hamming distance, is known as the ''Hamming cube''; it is equivalent as a metric space to the set of distances between vertices in a [[hypercube graph]]. One can also view a binary string of length ''n'' as a vector in <math>R^n</math> by treating each symbol in the string as a real coordinate; with this embedding, the strings form the vertices of an ''n''-dimensional [[hypercube]], and the Hamming distance of the strings is equivalent to the [[Manhattan distance]] between the vertices.

==History and applications==

The Hamming distance is named after [[Richard Hamming]], who introduced it in his fundamental paper on [[Hamming code]]s ''Error detecting and error correcting codes'' in 1950.<ref>{{harvtxt|Hamming|1950}}.</ref> It is used in [[telecommunication]] to count the number of flipped bits in a fixed-length binary word as an estimate of error, and therefore is sometimes called the '''signal distance'''. Hamming weight analysis of bits is used in several disciplines including [[information theory]], [[coding theory]], and [[cryptography]]. However, for comparing strings of different lengths, or strings where not just substitutions but also insertions or deletions have to be expected, a more sophisticated metric like the [[Levenshtein distance]] is more appropriate.
For ''q''-ary strings over an [[alphabet]] of size ''q''&nbsp;&nbsp;2 the Hamming distance is applied in case of orthogonal [[modulation]], while the [[Lee distance]] is used for phase modulation. If ''q''&nbsp;=&nbsp;2 or ''q''&nbsp;=&nbsp;3 both distances coincide.

The Hamming distance is also used in [[systematics]] as a measure of genetic distance.<ref name="pmid18351799">{{harvtxt|Pilcher|Wong|Pillai|2008}}.</ref>

On a grid such as a chessboard, the Hamming distance is the minimum number of moves it would take a [[Rook_(chess)|rook]] to move from one cell to the other.

== Algorithm example ==
The [[Python (programming language)|Python]] function <code>hamming_distance()</code> computes the Hamming distance between
two strings (or other [[Iterator|iterable]] objects) of equal length, by creating a sequence of Boolean values indicating mismatches and matches between corresponding positions in the two inputs, and then summing the sequence with False and True values being interpreted as zero and one.
{{-}}

<syntaxhighlight lang="python">
def hamming_distance(s1, s2):
    """Return the Hamming distance between equal-length sequences"""
    if len(s1) != len(s2):
        raise ValueError("Undefined for sequences of unequal length")
    return sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2))
</syntaxhighlight>

The following [[C (programming language)|C]] function will compute the Hamming distance of two integers (considered as binary values, that is, as sequences of bits). The running time of this procedure is proportional to the Hamming distance rather than to the number of bits in the inputs. It computes the [[bitwise operation|bitwise]] [[exclusive or]] of the two inputs, and then finds the [[Hamming weight]] of the result (the number of nonzero bits) using an algorithm of {{harvtxt|Wegner|1960}} that repeatedly finds and clears the lowest-order nonzero bit.

<syntaxhighlight lang="c">
int hamming_distance(unsigned x, unsigned y)
{
    int       dist;
    unsigned  val;

    dist = 0;
    val = x ^ y;    // XOR

    // Count the number of bits set
    while (val != 0)
    {
        // A bit is set, so increment the count and clear the bit
        dist++;
        val &= val - 1;
    }

    // Return the number of differing bits
    return dist;
}
</syntaxhighlight>

==See also==
{{Portal|Mathematics}}
* [[Closest string]]
* [[DamerauLevenshtein distance]]
* [[Euclidean distance]]
* [[Mahalanobis distance]]
* [[Jaccard index]]
* [[String metric]]
* [[Srensen similarity index]]
* [[Word ladder]]

==Notes==
{{Reflist}}

==References==
*{{FS1037C}}
*{{citation
 | last = Hamming | first = Richard W. | author-link = Richard W. Hamming
 | mr = 0035935
 | issue = 2
 | journal = [[Bell System Technical Journal]]
 | pages = 147160
 | title = Error detecting and error correcting codes
 | url = http://wayback.archive.org/web/20060525060427/http://www.caip.rutgers.edu/~bushnell/dsdwebsite/hamming.pdf
 | volume = 29
 | year = 1950
 | doi=10.1002/j.1538-7305.1950.tb00463.x}}.
*{{citation
 | last1 = Pilcher | first1 = C. D.
 | last2 = Wong | first2 = J. K.
 | last3 = Pillai | first3 = S. K.
 | date = March 2008
 | doi = 10.1371/journal.pmed.0050069
 | issue = 3
 | journal = PLoS Med.
 | page = e69
 | pmid = 18351799
 | title = Inferring HIV transmission dynamics from phylogenetic sequence relationships
 | volume = 5
 | pmc = 2267810}}.
*{{citation
 | last = Wegner | first = Peter | author-link = Peter Wegner
 | doi = 10.1145/367236.367286
 | issue = 5
 | journal = [[Communications of the ACM]]
 | page = 322
 | title = A technique for counting ones in a binary computer
 | volume = 3
 | year = 1960}}.

[[Category:String similarity measures]]
[[Category:Coding theory]]
[[Category:Articles with example Python code]]
[[Category:Articles with example C++ code]]
[[Category:Metric geometry]]
[[Category:Cubes]]
>>EOP<<
205<|###|>Euclidean distance
In [[mathematics]], the '''Euclidean distance''' or '''Euclidean metric''' is the "ordinary" [[distance]] between two points in [[Euclidean space]]. With this distance, Euclidean space becomes a [[metric space]]. The associated [[Norm (mathematics)|norm]] is called the '''[[Norm (mathematics)#Euclidean norm|Euclidean norm]].''' Older literature refers to the metric as '''Pythagorean metric'''.

==Definition==
The '''Euclidean distance''' between points '''p''' and '''q''' is the length of the [[line segment]] connecting them (<math>\overline{\mathbf{p}\mathbf{q}}</math>).

In [[Cartesian coordinates]], if '''p'''&nbsp;=&nbsp;(''p''<sub>1</sub>,&nbsp;''p''<sub>2</sub>,...,&nbsp;''p''<sub>''n''</sub>) and '''q'''&nbsp;=&nbsp;(''q''<sub>1</sub>,&nbsp;''q''<sub>2</sub>,...,&nbsp;''q''<sub>''n''</sub>) are two points in [[Euclidean space|Euclidean ''n''-space]], then the distance (d) from '''p''' to '''q''', or from '''q''' to '''p''' is given by the [[Pythagorean theorem|Pythagorean formula]]:

{{NumBlk|:|<math>\begin{align}\mathrm{d}(\mathbf{p},\mathbf{q}) = \mathrm{d}(\mathbf{q},\mathbf{p}) & = \sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \cdots + (q_n-p_n)^2} \\[8pt]
& = \sqrt{\sum_{i=1}^n (q_i-p_i)^2}.\end{align}</math>|{{EquationRef|1}}}}

The position of a point in a Euclidean ''n''-space is a [[Euclidean vector]]. So, '''p''' and '''q''' are Euclidean vectors, starting from the origin of the space, and their tips indicate two points. The '''[[Euclidean norm]]''', or '''Euclidean length''', or '''magnitude''' of a vector measures the length of the vector:
:<math>\|\mathbf{p}\| = \sqrt{p_1^2+p_2^2+\cdots +p_n^2} = \sqrt{\mathbf{p}\cdot\mathbf{p}}</math>
where the last equation involves the [[dot product]].

A vector can be described as a directed line segment from the [[Origin (mathematics)|origin]] of the Euclidean space (vector tail), to a point in that space (vector tip). If we consider that its length is actually the distance from its tail to its tip, it becomes clear that the Euclidean norm of a vector is just a special case of Euclidean distance: the Euclidean distance between its tail and its tip.

The distance between points '''p''' and '''q''' may have a direction (e.g. from '''p''' to '''q'''), so it may be represented by another vector, given by

:<math>\mathbf{q} - \mathbf{p} = (q_1-p_1, q_2-p_2, \cdots, q_n-p_n)</math>

In a three-dimensional space (''n''=3), this is an arrow from '''p''' to '''q''', which can be also regarded as the position of '''q''' relative to '''p'''. It may be also called a [[displacement (vector)|displacement]] vector if '''p''' and '''q''' represent two positions of the same point at two successive instants of time.

The Euclidean distance between '''p''' and '''q''' is just the Euclidean length of this distance (or displacement) vector:
{{NumBlk|:|<math>\|\mathbf{q} - \mathbf{p}\| = \sqrt{(\mathbf{q}-\mathbf{p})\cdot(\mathbf{q}-\mathbf{p})}.</math>|{{EquationRef|2}}}}

which is equivalent to equation 1, and also to:

:<math>\|\mathbf{q} - \mathbf{p}\| = \sqrt{\|\mathbf{p}\|^2 + \|\mathbf{q}\|^2 - 2\mathbf{p}\cdot\mathbf{q}}.</math>

===One dimension===
In one dimension, the distance between two points on the [[real line]] is the [[absolute value]] of their numerical difference.  Thus if ''x'' and ''y'' are two points on the real line, then the distance between them is given by:
:<math>\sqrt{(x-y)^2} = |x-y|.</math>

In one dimension, there is a single homogeneous, translation-invariant [[Metric (mathematics)|metric]] (in other words, a distance that is induced by a [[Norm (mathematics)|norm]]), up to a scale factor of length, which is the Euclidean distance. In higher dimensions there are other possible norms.

===Two dimensions===
In the [[Euclidean plane]], if '''p'''&nbsp;=&nbsp;(''p''<sub>1</sub>,&nbsp;''p''<sub>2</sub>) and '''q'''&nbsp;=&nbsp;(''q''<sub>1</sub>,&nbsp;''q''<sub>2</sub>) then the distance is given by

:<math>\mathrm{d}(\mathbf{p},\mathbf{q})=\sqrt{(p_1-q_1)^2 + (p_2-q_2)^2}.</math>

This is equivalent to the [[Pythagorean theorem]].

Alternatively, it follows from ({{EquationRef|2}}) that if the [[polar coordinates]] of the point '''p''' are (''r''<sub>1</sub>,&nbsp;<sub>1</sub>) and those of '''q''' are (''r''<sub>2</sub>,&nbsp;<sub>2</sub>), then the distance between the points is

:<math>\sqrt{r_1^2 + r_2^2 - 2 r_1 r_2 \cos(\theta_1 - \theta_2)}.</math>

===Three dimensions===
In three-dimensional Euclidean space, the distance  is

:<math>d(p, q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2+(p_3 - q_3)^2}.</math>

===''n'' dimensions <!-- This is a lower-case italicized "n" for a reason. -->===
In general, for an ''n''-dimensional space, the distance is

:<math>d(p, q) = \sqrt{(p_1- q_1)^2 + (p_2 - q_2)^2+\cdots+(p_i - q_i)^2+\cdots+(p_n - q_n)^2}.</math>

===Squared Euclidean distance===
The standard Euclidean distance can be squared in order to place progressively greater weight on objects that are farther apart. In this case, the equation becomes

:<math>d^2(p, q) = (p_1 - q_1)^2 + (p_2 - q_2)^2+\cdots+(p_i - q_i)^2+\cdots+(p_n - q_n)^2.</math>

Squared Euclidean Distance is not a metric as it does not satisfy the [[triangle inequality]], however it is frequently used in optimization problems in which distances only have to be compared.

It is also referred to as [[rational trigonometry#Quadrance|quadrance]] within the field of [[rational trigonometry]].

==See also==
*[[Chebyshev distance]] measures distance assuming only the most significant dimension is relevant.
*[[Euclidean distance matrix]]
*[[Hamming distance]] identifies the difference bit by bit of two strings
*[[Mahalanobis distance]] normalizes based on a covariance matrix to make the distance metric scale-invariant.
*[[Manhattan distance]] measures distance following only axis-aligned directions.
*[[Metric (mathematics)|Metric]]
*[[Minkowski distance]] is a generalization that unifies Euclidean distance, Manhattan distance, and Chebyshev distance.
*[[Pythagorean addition]]

==References==
* {{cite book |first=Elena |last=Deza |first2=Michel Marie |last2=Deza |year=2009 |title=Encyclopedia of Distances |page=94 |publisher=Springer }}
* {{cite web |url=http://www.statsoft.com/textbook/cluster-analysis/ |title=Cluster analysis |date=March 2, 2011 }}

{{DEFAULTSORT:Euclidean Distance}}
[[Category:Metric geometry]]
[[Category:Length]]
[[Category:String similarity measures]]
>>EOP<<
211<|###|>Levenshtein distance
{{refimprove|date=February 2010}}

In [[information theory]] and [[computer science]], the '''Levenshtein distance''' is a [[string metric]] for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (i.e. insertions, deletions or substitutions) required to change one word into the other. It is named after [[Vladimir Levenshtein]], who considered this distance in 1965.<ref>{{cite journal |author= .  |script-title=ru:    ,     |language=Russian |trans_title=Binary codes capable of correcting deletions, insertions, and reversals |journal=   CCP |volume=163 |issue=4 |pages=8458 |year=1965}} Appeared in English as: {{cite journal |author=Levenshtein, Vladimir I. |title=Binary codes capable of correcting deletions, insertions, and reversals |journal=Soviet Physics Doklady |volume=10 |number=8 |pages=707710 |date=February 1966  |url=<!--http://profs.sci.univr.it/~liptak/ALBioinfo/files/levenshtein66.pdf right to publish copy of journal unclear: see http://www.sherpa.ac.uk/romeo/search.php?issn=1028-3358&type=issn&la=en/&fIDnum=%7C&mode=simple ; in any event, liptak does not appear to be the author or the translator -->}}</ref>

Levenshtein distance may also be referred to as '''edit distance''', although that may also denote a larger [[Edit distance|family of distance metrics]].<ref name="navarro">{{Cite doi/10.1145.2F375360.375365}}</ref>{{rp|32}} It is closely related to [[Sequence alignment#Pairwise alignment|pairwise string alignments]].

== Definition ==
Mathematically, the Levenshtein distance between two strings <math>a, b</math> is given by <math>\operatorname{lev}_{a,b}(|a|,|b|)</math> where

:<math>\qquad\operatorname{lev}_{a,b}(i,j) = \begin{cases}
  \max(i,j) & \text{ if} \min(i,j)=0, \\
  \min \begin{cases}
          \operatorname{lev}_{a,b}(i-1,j) + 1 \\
          \operatorname{lev}_{a,b}(i,j-1) + 1 \\
          \operatorname{lev}_{a,b}(i-1,j-1) + 1_{(a_i \neq b_j)}
       \end{cases} & \text{ otherwise.}
\end{cases}</math>
where  <math>1_{(a_i \neq b_j)}</math> is the [[indicator function]] equal to 0 when  <math>a_i = b_j</math> and equal to 1 otherwise.

Note that the first element in the minimum corresponds to deletion (from <math>a</math> to <math>b</math>), the second to insertion and the third to match or mismatch, depending on whether the respective symbols are the same.

=== Example ===
For example, the Levenshtein distance between "kitten" and "sitting" is 3, since the following three edits change one into the other, and there is no way to do it with fewer than three edits:

# '''k'''itten  '''s'''itten (substitution of "s" for "k")
# sitt'''e'''n  sitt'''i'''n (substitution of "i" for "e")
# sittin  sittin'''g''' (insertion of "g" at the end).

===Upper and lower bounds===
The Levenshtein distance has several simple upper and lower bounds. These include:
* It is always at least the difference of the sizes of the two strings.
* It is at most the length of the longer string.
* It is zero if and only if the strings are equal.
* If the strings are the same size, the [[Hamming distance]] is an upper bound on the Levenshtein distance.
* The Levenshtein distance between two strings is no greater than the sum of their Levenshtein distances from a third string ([[triangle inequality]]).

==Applications==
In [[approximate string matching]], the objective is to find matches for short strings in many longer texts, in situations where a small number of differences is to be expected. The short strings could come from a dictionary, for instance. Here, one of the strings is typically short, while the other is arbitrarily long. This has a wide range of applications, for instance, [[spell checker]]s, correction systems for [[optical character recognition]], and software to assist natural language translation based on [[translation memory]].

The Levenshtein distance can also be computed between two longer strings, but the cost to compute it, which is roughly proportional to the product of the two string lengths, makes this impractical.  Thus, when used to aid in [[fuzzy string searching]] in applications such as [[record linkage]], the compared strings are usually short to help improve speed of comparisons.

==Relationship with other edit distance metrics==
{{main|Edit distance}}
There are other popular measures of [[edit distance]], which are calculated using a different set of allowable edit operations. For instance,
* the [[DamerauLevenshtein distance]] allows insertion, deletion, substitution, and the [[Transposition (mathematics)|transposition]] of two adjacent characters;
* the [[longest common subsequence problem|longest common subsequence]] metric allows only insertion and deletion, not substitution;
* the [[Hamming distance]] allows only substitution, hence, it only applies to strings of the same length.

[[Edit distance]] is usually defined as a parameterizable metric calculated with a specific set of allowed edit operations, and each operation is assigned a cost (possibly infinite).  This is further generalized by DNA [[sequence alignment]] algorithms such as the [[SmithWaterman algorithm]], which make an operation's cost depend on where it is applied.

==Computing Levenshtein distance==

===Recursive===
This is a straightforward, but inefficient, recursive [[pseudocode]] implementation of a <code>LevenshteinDistance</code> function that takes two strings, ''s'' and ''t'', together with their lengths, and returns the Levenshtein distance between them:

<!--
  Please do not add an additional implementation in your language of choice.
  Many of those have been added to and deleted from this article in the past.
  See the talk page archive for relevant discussion
-->
<source lang="C">
// len_s and len_t are the number of characters in string s and t respectively
int LevenshteinDistance(string s, int len_s, string t, int len_t)
{
  /* base case: empty strings */
  if (len_s == 0) return len_t;
  if (len_t == 0) return len_s;

  /* test if last characters of the strings match */
  if (s[len_s-1] == t[len_t-1])
      cost = 0;
  else
      cost = 1;

  /* return minimum of delete char from s, delete char from t, and delete char from both */
  return minimum(LevenshteinDistance(s, len_s - 1, t, len_t    ) + 1,
                 LevenshteinDistance(s, len_s    , t, len_t - 1) + 1,
                 LevenshteinDistance(s, len_s - 1, t, len_t - 1) + cost);
}
</source>

Unfortunately, this straightforward recursive implementation is very inefficient because it recomputes the Levenshtein distance of the same substrings many times.

A more efficient method would never repeat the same distance calculation. For example, the Levenshtein distance of all possible prefixes might be stored in an array <code>d[][]</code> where <code>d[i][j]</code> is the distance between the first <code>i</code> characters of string <code>s</code> and the first <code>j</code> characters of string <code>t</code>. The table is easy to construct one row at a time starting with row 0. When the entire table has been built, the desired distance is <code>d[len_s][len_t]</code>. While this technique is significantly faster, it will consume <code>len_s * len_t</code> more memory than the straightforward recursive implementation.

===Iterative with full matrix===
{{main|WagnerFischer algorithm}}
::{{small|Note: This section uses 1-based strings instead of 0-based strings}}
Computing the Levenshtein distance is based on the observation that if we reserve a [[Matrix (mathematics)|matrix]] to hold the Levenshtein distances between all [[prefix (computer science)|prefix]]es of the first string and all prefixes of the second, then we can compute the values in the matrix in a [[dynamic programming]] fashion, and thus find the distance between the two full strings as the last value computed.

This algorithm, an example of bottom-up [[dynamic programming]], is discussed, with variants, in the 1974 article ''The [[String-to-string correction problem]]'' by Robert A. Wagner and Michael J. Fischer.<ref>{{citation |first=Robert A. |last=Wagner |first2=Michael J. |last2=Fischer |author2-link=Michael J. Fischer |title=The String-to-String Correction Problem |journal=Journal of the ACM |volume=21 |issue=1 |year=1974 |pages=168173 |doi= 10.1145/321796.321811}}</ref>

This is a straightforward pseudocode implementation for a function ''LevenshteinDistance'' that takes two strings, ''s'' of length ''m'', and ''t'' of length ''n'', and returns the Levenshtein distance between them:

<!--
  Please do not add an additional implementation in your language of choice.
  Many of those have been added to and deleted from this article in the past.
  See the talk page archive for relevant discussion
-->
<!-- choose random language for highlights -->
<source lang="C">
int LevenshteinDistance(char s[1..m], char t[1..n])
{
  // for all i and j, d[i,j] will hold the Levenshtein distance between
  // the first i characters of s and the first j characters of t;
  // note that d has (m+1)*(n+1) values
  declare int d[0..m, 0..n]
 
  clear all elements in d // set each element to zero
 
  // source prefixes can be transformed into empty string by
  // dropping all characters
  for i from 1 to m
    {
      d[i, 0] := i
    }
 
  // target prefixes can be reached from empty source prefix
  // by inserting every character
  for j from 1 to n
    {
      d[0, j] := j
    }
 
  for j from 1 to n
    {
      for i from 1 to m
        {
          if s[i] = t[j] then
            d[i, j] := d[i-1, j-1]       // no operation required
          else
            d[i, j] := minimum
                    (
                      d[i-1, j] + 1,  // a deletion
                      d[i, j-1] + 1,  // an insertion
                      d[i-1, j-1] + 1 // a substitution
                    )
        }
    }
 
  return d[m, n]
}
</source>

Note that this implementation does not fit the [[#Definition|definition]] precisely: it always prefers matches, even if insertions or deletions provided a better score. This is equivalent; it can be shown that for every optimal alignment (which induces the Levenshtein distance) there is another optimal alignment that prefers matches in the sense of this implementation.<ref>[http://cs.stackexchange.com/a/2997 Micro-optimisation for edit distance computation: is it valid?]</ref>

Two examples of the resulting matrix (hovering over a number reveals the operation performed to get that number):
<center>
{{col-begin|width=auto}}
{{col-break}}
{|class="wikitable"
|-
| 
| 
!k
!i
!t
!t
!e
!n
|-
| ||0 ||1 ||2 ||3 ||4 ||5 ||6
|-
!s
|1 ||{{H:title|substitution of 'k' for 's'|1}} ||2 ||3 ||4 ||5 ||6
|-
!i
|2 ||2 ||{{H:title|'i' equals 'i'|1}} ||2 ||3 ||4 ||5
|-
!t
|3 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3 ||4
|-
!t
|4 ||4 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3
|-
!i
|5 ||5 ||4 ||3 ||2 ||{{H:title|substitution of 'e' for 'i'|2}} ||3
|-
!n
|6 ||6 ||5 ||4 ||3 ||3 ||{{H:title|'n' equals 'n'|2}}
|-
!g
|7 ||7 ||6 ||5 ||4 ||4 ||{{H:title|insert 'g'|3}}
|}
{{col-break|gap=1em}}
{|class="wikitable"
|
|
!S
!a
!t
!u
!r
!d
!a
!y
|-
| 
|0 ||1 ||2 ||3 ||4 ||5 ||6 ||7 ||8
|-
!S
|1 ||{{H:title|'S' equals 'S'|0}} ||{{H:title|delete 'a'|1}} ||{{H:title|delete 't'|2}} ||3 ||4 ||5 ||6 ||7
|-
!u
|2 ||1 ||1 ||2 ||{{H:title|'u' equals 'u'|2}} ||3 ||4 ||5 ||6
|-
!n
|3 ||2 ||2 ||2 ||3 ||{{H:title|substitution of 'r' for 'n'|3}} ||4 ||5 ||6
|-
!d
|4 ||3 ||3 ||3 ||3 ||4 ||{{H:title|'d' equals 'd'|3}} ||4 ||5
|-
!a
|5 ||4 ||3 ||4 ||4 ||4 ||4 ||{{H:title|'a' equals 'a'|3}} ||4
|-
!y
|6 ||5 ||4 ||4 ||5 ||5 ||5 ||4 ||{{H:title|'y' equals 'y'|3}}
|}
{{col-end}}
</center>

The [[invariant (mathematics)|invariant]] maintained throughout the algorithm is that we can transform the initial segment <code>s[1..i]</code> into <code>t[1..j]</code> using a minimum of <code>d[i,j]</code> operations. At the end, the bottom-right element of the array contains the answer.

===Iterative with two matrix rows===
It turns out that only two rows of the table are needed for the construction if one does not want to reconstruct the edited input strings (the previous row and the current row being calculated).

The Levenshtein distance may be calculated iteratively using the following algorithm:<ref>{{Citation |title=Fast, memory efficient Levenshtein algorithm |first=Sten |last=Hjelmqvist |date=26 Mar 2012 |url=http://www.codeproject.com/Articles/13525/Fast-memory-efficient-Levenshtein-algorithm}}</ref>
<syntaxhighlight lang="CSharp">
int LevenshteinDistance(string s, string t)
{
    // degenerate cases
    if (s == t) return 0;
    if (s.Length == 0) return t.Length;
    if (t.Length == 0) return s.Length;

    // create two work vectors of integer distances
    int[] v0 = new int[t.Length + 1];
    int[] v1 = new int[t.Length + 1];

    // initialize v0 (the previous row of distances)
    // this row is A[0][i]: edit distance for an empty s
    // the distance is just the number of characters to delete from t
    for (int i = 0; i < v0.Length; i++)
        v0[i] = i;

    for (int i = 0; i < s.Length; i++)
    {
        // calculate v1 (current row distances) from the previous row v0

        // first element of v1 is A[i+1][0]
        //   edit distance is delete (i+1) chars from s to match empty t
        v1[0] = i + 1;

        // use formula to fill in the rest of the row
        for (int j = 0; j < t.Length; j++)
        {
            var cost = (s[i] == t[j]) ? 0 : 1;
            v1[j + 1] = Minimum(v1[j] + 1, v0[j + 1] + 1, v0[j] + cost);
        }

        // copy v1 (current row) to v0 (previous row) for next iteration
        for (int j = 0; j < v0.Length; j++)
            v0[j] = v1[j];
    }

    return v1[t.Length];
}
</syntaxhighlight>

==See also==
{{colbegin||25em}}
*[[agrep]]
*[[Approximate string matching]]
*[[Bitap algorithm]]
*[[DamerauLevenshtein distance]]
*[[diff]]
*[[MinHash]]
*[[Dynamic time warping]]
*[[Euclidean distance]]
*[[Fuzzy string searching]]
*[[Hamming weight]]
*[[Hirschberg's algorithm]]
*[[Homology (biology)#Sequence homology|Homology of sequences in genetics]]
*[[HuntMcIlroy algorithm]]
*[[Jaccard index]]
*[[JaroWinkler distance]]
*[[Levenshtein automaton]]
*[[Locality-sensitive hashing]]
*[[Longest common subsequence problem]]
*[[Lucene]] (an open source search engine that implements edit distance)
*[[Manhattan distance]]
*[[Metric space]]
*[[Most frequent k characters]]
*[[NeedlemanWunsch algorithm]]
*[[Optimal matching]] algorithm
*[[Sequence alignment]]
*[[Similarity space]] on [[Numerical taxonomy]]
*[[SmithWaterman algorithm]]
*[[Srensen similarity index]]
*[[String distance metric]]
*[[Wagner-Fischer algorithm]]
{{colend}}

==References==
{{reflist|30em}}

==External links==
{{Wikibooks| R_Programming|Text_Processing#Edit_distance|Levenshtein distance in R}}
{{Wikibooks| Algorithm implementation|Strings/Levenshtein distance|Levenshtein distance}}
*[http://www.postgresql.org/docs/current/static/fuzzystrmatch.html Levenshtein in PostgreSQL]
*{{citation |contribution=Levenshtein distance |title=Dictionary of Algorithms and Data Structures [online] |editor-first=Paul E. |editor-last=Black |publisher=U.S. National Institute of Standards and Technology |date=14 August 2008 |accessdate=3 April 2013 |url=http://www.nist.gov/dads/HTML/Levenshtein.html }}

{{DEFAULTSORT:Levenshtein Distance}}
[[Category:String similarity measures]]
[[Category:Dynamic programming]]
[[Category:Articles with example pseudocode]]
[[Category:Quantitative linguistics]]
>>EOP<<
217<|###|>Category:Legal citators
{{Cat main|Citator}}

[[Category:Citation indices]]
[[Category:Legal research]]
[[Category:Legal citation]]
>>EOP<<
223<|###|>SPIN bibliographic database
{{Infobox Bibliographic Database
|title =SPIN  (Searchable Physics Information Notices)  
|image = 
|caption = 
|producer =[[American Institute of Physics]] (AIP) 
|country =USA, Russia, Ukraine
|history = 
|languages =English, [[Russian language|Russian]], [[Ukrainian language|Ukrainian]] 
|providers =[[Dialog (online database)|Dialog]], [[American Institute of Physics|AIP website]], [[SPIE|SPIE Digital Library]] 
|cost = 
|disciplines =Physics, Astronomy, Mathematics, Geophysics, Geosciences, Nuclear Science, Science & Technology 
|depth =Word, Phrase, Abstract, Author and Author affiliations, Descriptor, Errata (coden, or date, or volume) Identifier, Title, Astronomical objects, CODEN, Conference (location, or title, or year), Journal name, and more...   
|formats =Journal Articles, Book Reviews, Conferences, Meetings, Patents, Symposia
|temporal =1975 to the present  
|geospatial =International 
|number =over 1.5 million 
|updates =Weekly 
|p_title =No print counterparts 
|p_dates = 
|ISSN =
|web =https://scitation.aip.org/jhtml/scitation/coverage.jsp 
|titles =  
}}

'''SPIN''' (Searchable Physics Information Notices) '''bibliographic database''' is an indexing and abstracting service produced by the [[American Institute of Physics]] (AIP). The content focus of SPIN is described as the most significant areas of [[physics]] [[research]]. This type of [[scientific literature|literature coverage]] spans the major [[scientific journal|physical science journals]] and magazines. Major [[conference proceedings]] that are reported by the American Institute of Physics, member societies, as well as affiliated organizations are also included as part of this database. References, or citations, provide access to more than 1.5 million articles as of 2010. ''SPIN''  has no print counterpart.<ref name=DialogSpin/><ref name=AIP-SPIN/>

==Journals==
Delivery of timely indexing and abstracting is for, what are deemed to be, the significant or important [[physics]] and [[astronomy]] journals from the [[United States]], [[Russia]], and the [[Ukraine]]. Citations for journal articles are derived from original publications of the ''AIP'', which includes published translated works. At the same time, citations are included from member societies, and selectively chosen American journals. Citations become typically available online on the same date as the corresponding journal article.<ref name=DialogSpin/><ref name=AIP-SPIN> {{Cite web
  | title =What is the SPIN database? 
  | work =Information about SPIN 
  | publisher =[[American Institute of Physics]] 
  | date =July 2010 
  | url =http://scitation.aip.org/servlet/HelpSystem?KEY=SCI&TYPE=HELP/FAQ#ques3 
  | format = 
  | accessdate =2010-07-12}}</ref>

==Sources==
Overall, the source citations are derived from material published by the AIP and member societies,  which are English-speaking, Russian, and Ukrainian journals and conference proceedings. Certain American physics-related articles are also sources of citations. About 60 journals have cover to cover indexing, and about 100 journals, overall, are indexed.<ref name=DialogSpin/><ref name=pub-coverage>{{Cite web
  | title =SPIN Publication Coverage 
  | work =Complete list of publications covered and coverage years. 
  | publisher =American Institute of Physics 
  | date =July 2010 
  | url =http://scitation.aip.org/jhtml/scitation/spincodens.jsp 
  | format = 
  | accessdate =2010-07-12}}</ref>  

==Scope==
Subject coverage encompasses the following: <ref name=DialogSpin>  {{Cite web
  | title =Indexes and Databases 
  | work =SPIN: Searchable Physics Information Notices
  | publisher =Raymond H. Fogler Library, The University of Maine
  | date =October 2010 
  | url =http://www.library.umaine.edu/indexesdb/dbdetails.asp?field=Name&search=SPIN:+Searchable+Physics+Information+Notices 
  | format = 
  | accessdate =2010-07-12}}</ref>

*[[Applied physics]], [[Electromagnetic spectrum|Electromagnetic]] technology, [[Microelectronics]] 
*[[Atomic physics]] and [[Molecular physics]] 
*[[Biological physics]] and [[Medical physics]] 
*[[Classical physics]] and [[Quantum physics]] 
*[[Condensed matter physics]] 
*[[Elementary particle physics]] 
*[[Physics|General physics]], [[Optics]], [[Acoustics]], and [[Fluid dynamics]] 
*[[Geophysics]], [[Astronomy]], [[Astrophysics]] 
*[[Materials science]] 
*[[Nuclear physics]] 
*[[Plasma physics]] 
*[[Physical chemistry]]

==See also==
*[[List of academic databases and search engines]]

==References==
{{Reflist}}

==External links==
*[http://www.aip.org/press_release/spin.html AIP'S SPIN Database Reaches One Million Records].  American Institute of Physics. March 1, 2002.
*[http://scholarlykitchen.sspnet.org/2009/06/17/physics-papers-and-the-arxiv/ Can everything published in physics can be found in the [[arXiv]]?]. The Scholarly Kitchen. [[Society for Scholarly Publishing]]. June, 2010.
*[http://www.pub4stm.org/ AIP partnerships] (society publishing). July 2010.


[[Category:Bibliographic databases]]
[[Category:Bibliographic indexes]]
[[Category:Citation indices]]
[[Category:Scientific databases]]
>>EOP<<
229<|###|>Web of Knowledge
{{Mergeto|Web of Science|date=June 2014|discuss=Talk:Web of Science#Merge}}
[[File:Web of Science Logo.png|thumb|The current Web of Science logo]]
[[Image:ISI Web of Knowledge updated.png|thumb|400px|An example search result from Web of Knowledge version 3.0]]

'''Web of Knowledge''' (formerly known as [[Institute for Scientific Information|ISI]] Web of Knowledge) is an academic [[citation index]]ing and search service, which is combined with web linking and is provided by [[Thomson Reuters]]. Web of Knowledge covers the sciences, [[social science]]s, arts and [[humanities]]. It provides [[bibliography|bibliographic]] content and tools to access, analyze, and manage research information. Multiple databases can be searched simultaneously.<ref name=describe/><ref name=tutor>[http://science.thomsonreuters.com/tutorials/wok4/wok4tut3.html Tutorial]. ISI Web of Knowledge. Thomson Reuters. 2010. Accessed on 2010-06-24</ref>

==Overview==
Web of Knowledge is described as a unifying research tool which enables the user to acquire, analyze, and disseminate database information in a timely manner. This is accomplished because of the creation of a common vocabulary, called [[Ontology (information science)|ontology]], for varied search terms and varied data. Moreover, search terms generate related information across categories.

Acceptable content for Web of Knowledge is determined by an evaluation and selection process based on the following criteria: impact, influence, timeliness, [[peer review]], and geographic representation.<ref name=describe/>

===Search and analysis===
<!-- Deleted image removed: [[File:ISI Web of knowledge logo.jpg|thumb||Former Web of Knowledge logo]] -->

Web of Knowledge employs various search and analysis capabilities. First, citation indexing is employed, which is enhanced by the capability to search for results across disciplines. The influence, impact, history, and [[methodology]] of an idea can be followed from its first instance, notice, or referral to the present day. This technology points to a deficiency with the [[Index term|keyword]]-only method of searching. 

Second, subtle trends and patterns relevant to the literature or research of interest, become apparent. Broad trends indicate significant topics of the day, as well as the history relevant to both the work at hand, and particular areas of study. 

Third, trends can be [[mathematical modeling|graphically]] represented.<ref name=describe>[http://thomsonreuters.com/content/science/pdf/Web_of_Knowledge_factsheet.pdf Overview and Description]. ISI Web of Knowledge. Thomson Reuters. 2010. Accessed on 2010-06-24</ref><ref>{{cite web|url=http://wokinfo.com/realfacts/qualityandquantity/|title=Web of Knowledge > Real Facts > Quality and Quantity|accessdate = 2010-05-05}}</ref>

=== Content ===
The combined databases includes the following:
*23,000 [[Academic journal|academic]] and [[scientific journal]]s (including [[Web of Science]] journal listings)
*23,000,000 [[patent]]s
*110,000 conference [[proceedings]]
*9,000 websites
*Coverage from the year 1900 to present day (with Web of Science)
*Over 40 million source items
*Integrated and simultaneous searching across multiple databases<ref name=describe/>

=== Included databases ===
The Web of Knowledge suite encompasses the following databases:<ref name=dbase-List>{{Cite web| last =''ISI Web of Knowledge''| title =Suite of databases| publisher =Thomson Reuters| year =2010| url = http://thomsonreuters.com/products_services/science/science_products/a-z/isi_web_of_knowledge?parentKey=555184 | format =List of databases that are part of the Web of Knowledge suite.| accessdate =2010-06-24}}</ref><ref name=AtoZ>{{Cite web| last = ISI Web of Knowledge platform| title =Available databases A to Z| publisher =Thomson Reuters| year =2010| url =http://wokinfo.com/products_tools/products/ | format =Choose databases on method of discovery and analysis| accessdate =2010-06-24}}</ref><ref>[http://wokinfo.com/media/pdf/SSR1103443WoK5-2_web3.pdf Thomson Reuters Web of Knowledge. Thomson Reuters, 2013.]</ref>
{{columns-list|colwidth=30em|
*[[Biological Abstracts]]
*[[Biosis Previews]] 
*[[CAB Abstracts]]
*[[CAB Direct|CAB Global Health]]
*[[Chinese Science Citation Database]]
*[[Conference Proceedings Citation Index]] 
*[[Current Contents|Current Contents Connect]]
*[[Data Citation Index]]
*[[Derwent Innovations Index]]
*[[Essential Science Indicators]]
*[[Food Science and Technology Abstracts]]
*[[Inspec]] 
*[[ISI Highly Cited]]
*[[Journal Citation Reports]]
*[[MEDLINE]] 
*[[Web of Science]]
**[[Arts & Humanities Citation Index]]
**[[Book Citation Index]] 
**[[Current Chemical Reactions]]
**[[Index Chemicus]]
**[[Science Citation Index Expanded]]
**[[Social Sciences Citation Index]]
*[[The Zoological Record]]
}}

==See also==
*[[List of academic journal search engines]]

==References==
{{Reflist|30em}}

==External links==
* {{Official website|http://wokinfo.com/}}

{{Thomson Reuters}}

[[Category:Bibliographic databases]]
[[Category:Online databases]]
[[Category:Thomson Reuters]]
[[Category:Citation indices]]
[[Category:Scholarly search services]]
>>EOP<<
235<|###|>SensMe
{{Refimprove|date=April 2010}}
{{Infobox Software|
|name                   = SensMe
|logo                   = [[File:SensMelogo.png|center|64px|SensMe Logo]]
|screenshot             = [[File:SensMe.jpg|200px|center]]
|caption                = Screenshot of SensMe on Media Go
|developer              = [[Sony Corporation]]
|genre                  = Mood detection software for music data.
|platform               = [[Sony Walkman]] MP3/4 players<br />[[Sony Ericsson]]<br />[[Media Go]]<br />[[PlayStation Portable]]
|license                = [[Proprietary software|Proprietary]]
|website                = [http://www.sonyericsson.com/cws/support/phones/detailed/whatissenseme/w910i?cc=gb&lc=en http://www.sonyericsson.com]
}}

'''SensMe''' is a [[Proprietary software|proprietary]] music mood and tempo detection system created by [[Sony|Sony Corporation]], and employed in numerous Sony branded products, most notably the [[Walkman]] MP3/MP4 players (E <ref>[http://presscentre.sony.eu/content/detail.aspx?ReleaseID=6052&NewsAreaId=2 Your cool, colourful music partner Feature-packed WALKMAN E450 Video MP3 player from Sony with premium sound for young music fans (15 July 2010 )]</ref> and S series<ref>[http://presscentre.sony.eu/content/detail.aspx?ReleaseID=6203&NewsAreaId=2 Sony introduces super-slim WALKMAN S750 (15 September 2010)]</ref>), [[Media Go]], [[PlayStation Portable]], and [[Sony Ericsson]] phone series, .

==Technical specifications==

''SensMe'' works by mapping music to a dual axis map based on the mood and tempo of music tracks.<ref>What is SensMe? http://www.sonyericsson.com/cws/support/phones/detailed/whatissenseme/w980</ref> Mood and tempo is determined by using the appropriate Sony compatible software which analyzes music tracks individually and computes the relevant track information. Analyzed tracks can then be plotted onto an intuitive dual axis map through which the music library on the device can be navigated, and playlists can be generated based on relative speed and mood. The horizontal axis is based on mood and the vertical axis is based on [[tempo]].

==PlayStation Portable==

SensMe was made available on the PlayStation Portable as of system software version 6.10.<ref name="update610" /> It can be downloaded via the [[XrossMediaBar|XMB]] or by using a computer.<ref name="pspdownload">SensMe PSP Download http://www.playstation.com/psp-app/sensme/en/</ref> The application features twelve channels by which music is categorized. These include Favorites, Newly Added, Dance, Extreme, Lounge, Emotional, Mellow, Upbeat, Relax, Energetic, Morning/Day/Night/Midnight, and Shuffle All.

===PlayStation Portable Version History===
{| class="wikitable"
!width="180"|Version<br> Release date (UTC)
!class="unsortable"|Description
|-
|align=center|'''1.50'''<br>March 31, 2010
|
* Music tracks transferred using a PlayStation 3 system or music management application other than Media Go are now also categorized into channels.
* Users can now add music tracks to a block list so they do not play.
* Users can now activate or deactivate the [Dynamic Normalizer] feature.
|-
|align=center|'''1.01'''<br>October 22, 2009
|
* Descriptions of some menu items in some languages have been revised.
|-
|align=center|'''1.00'''<br>October 1, 2009
|
* Initial release.
|}

==SensMe compatible products==
* [[Walkman]]
* [[Media Go]]
* [[PlayStation Portable]]<ref name="update610">PSP Firmware Update (v6.10) http://blog.us.playstation.com/2009/09/psp-firmware-update-v6-10/</ref>

[[File:Sony Ericsson W760i running SensMe.JPG|thumb|right|Screenshot of SensMe on a Sony Ericsson]]

===Sony Ericsson handsets===
*''[[Sony Ericsson Aino|Aino]]''
*''[http://www.sony.co.uk/product/nws-s-series/nwz-s639f Sony NWZ-S639F Media Player]''
*''[[Sony Ericsson Elm|elm]]''
*''[[Sony Ericsson W380|W380]]''
*''[[Sony Ericsson W518a|W508]]''
*''[[Sony Ericsson W518a|W518a]]''
*''[[Sony Ericsson W595|W595]]''
*''[[Sony Ericsson W705|W705]]''
*''[[Sony Ericsson W705|W715]]''
*''[[Sony Ericsson W760|W760]]''
*''[[Sony Ericsson W890i|W890i]]''
*''[[Sony Ericsson W902|W902]]''
*''[[Sony Ericsson W910|W910i]]''
*''[[Sony Ericsson W980|W980]]''
*''[[Sony Ericsson W995|W995]]''
*''[[Sony Ericsson Xperia X10|Xperia X10]]''
*''[[Sony Ericsson Xperia Neo|Xperia Neo]]''
*''[[Sony Ericsson Xperia Play|Xperia Play]]''
*''[[Sony Ericsson Xperia ray|Xperia Ray]]''
===Sony handsets===
*''[[Sony Xperia E|Xperia E]]''
*''[[Sony Xperia M|Xperia M]]''
*''[[Sony Xperia Sola|Xperia Sola]]''
* [[Sony_Xperia_L|Xperia L]]
*''[[Sony Xperia S|Xperia S]]''
*''[[Sony Xperia P|Xperia P]]''
*''[[Sony Xperia U|Xperia U]]''
*''[[Sony Xperia T|Xperia T]]''
*''[[Sony Xperia TX|Xperia TX]]''
*''[[Sony Xperia TL|Xperia TL]]''
*''[[Sony Xperia tipo|Xperia tipo]]''
*''[[Sony Xperia T|Xperia Go]]''
*''[[Sony Xperia V|Xperia V]]''
*''[[Sony Xperia Z|Xperia Z]]''
*''[[Sony Xperia Z1|Xperia Z1]]''
*''[[Sony Xperia Z1 Compact|Xperia Z1 Compact]]''
*''[[Sony Xperia Z Ultra|Xperia Z Ultra]]''
*''[[Sony Xperia Z1f|Xperia Z1f/Z1s]]''
*''[[Sony Xperia ZL|Xperia ZL]]''
*''[[Sony Xperia ZL|Xperia SP]]''
*''[[Sony Xperia Z2|Xperia Z2]]''
*''[[Sony Xperia Z3|Xperia Z3]]''
*''[[Sony Xperia Z3 Compact|Xperia Z3 Compact]]''
*''[http://www.sonyericsson.com/cws/products/mobilephones/overview/zylo?cc=ph&lc=en#view=features_specifications Zylo (W20i)]''

==References==
{{reflist}}

[[Category:Sony software]]
[[Category:Music search engines]]
>>EOP<<
