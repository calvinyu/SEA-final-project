4<|###|>Divergence-from-randomness model
In the field of [[information retrieval]], '''divergence from randomness''' is one type of [[probabilistic]] model.

Term weights are computed by measuring the divergence between a term distribution produced by a random process and the actual term distribution.

==External links==
*[http://terrier.org/docs/v3.5/dfr_description.html Terrier's DFR Web page]
*[http://ir.dcs.gla.ac.uk/wiki/DivergenceFromRandomness Glasgow IR group Wiki DFR page]

[[Category:Ranking functions]]
[[Category:Information retrieval]]
[[Category:Probabilistic models]]


{{comp-sci-stub}}
>>EOP<<
10<|###|>Harshness
{{Original research|date=September 2007}}
{{Other uses|Harsh (disambiguation)}}
'''Harshness''' (also called '''raucousness'''), in [[music information retrieval]], is a Non-Contextual Low-Level Audio Descriptors (NLDs) that represents one dimension of the multi-dimensional [[psychoacoustic]] feature called as musical [[timbre]].

Classical timbre NLDs are [[surface roughness|roughness]], [[spectral centroid]], and [[spectral flux]]. While harmonicity and inharmonicity can also be considered NLDs, harshness differs from them, as well as from roughness, once it reckons for a distinguished perceptual audio feature expressed by the summary spectral periodicity. This feature is especially clear in single-[[Pitch (music)|pitch]], single-[[note]], musical audio, where the timbre of two different musical instruments can greatly differ in levels of harshness (e.g., the difference in harshness between a flute and a saxophone is evident). As it is supposed to be, harshness is independent of all others NLDs.

[[Category:Musicology]]
[[Category:Music technology]]
[[Category:Information retrieval]]
>>EOP<<
16<|###|>Category:Data management
{{catdiffuse}}

'''[[Data management]]''' comprises all the disciplines related to managing data as a valuable resource.


{{Commons cat|Data management}}

[[Category:Computer data|Management]]
[[Category:Data|Management]]
[[Category:Project management]]
[[Category:Information retrieval]]
[[Category:Information technology management]]
>>EOP<<
22<|###|>Audio mining
{{unreferenced|date=January 2012}}
'''Audio mining''' is a technique by which the content of an audio signal can be automatically analysed and searched. It is most commonly used in the field of [[speech recognition|automatic speech recognition]], where the analysis tries to identify any speech within the audio. The audio will typically be processed by a speech recognition system in order to identify word or [[phoneme]] units that are likely to occur in the spoken content. This information may either be used immediately in pre-defined searches for keywords or phrases (a real-time "word spotting" system), or the output of the speech recogniser may be stored in an index file. One or more audio mining index files can then be loaded at a later date in order to run searches for keywords or phrases.

The results of a search will normally be in terms of hits, which are regions within files that are good matches for the chosen keywords. The user may then be able to listen to the audio corresponding to these hits in order to verify if a correct match was found.

Audio mining systems used in the field of speech recognition are often divided into two groups: those that use [[Large Vocabulary Continuous Speech Recogniser]]s (LVCSR) and those that use phonetic recognition. 

Musical audio mining (also known as [[Music information retrieval]]) relates to the identification of perceptually important characteristics of a piece of music such as melodic, harmonic or rhythmic structure. Searches can then be carried out to find pieces of music that are similar in terms of their melodic, harmonic and/or rhythmic characteristics.

==See also==
* [[Speech Analytics]]


[[Category:Speech recognition]]
[[Category:Information retrieval]]
[[Category:Computational linguistics]]
>>EOP<<
28<|###|>Ordered weighted averaging aggregation operator
In applied mathematics  specifically in [[fuzzy logic]]  the '''ordered weighted averaging (OWA) operators''' provide a [[parameter]]ized class of mean type aggregation operators. They were introduced by [[Ronald R. Yager]]. Many notable mean operators such as the max, [[arithmetic average]], median and min, are members of this class. They have been widely used in [[computational intelligence]] because of their ability to model linguistically expressed aggregation instructions.

== Definition ==

Formally an '''OWA''' operator of dimension <math> \ n </math> is a mapping <math> F: R_n \rightarrow R </math> that has an associated collection of weights <math> \  W = [w_1, \ldots, w_n] </math> lying in the unit interval and summing to one and with 		

:<math> F(a_1, \ldots , a_n) =  \sum_{j=1}^n  w_j b_j</math>

where <math> b_j </math> is the ''j''<sup>th</sup> largest of the <math> a_i </math>.

By choosing different ''W'' one can implement different aggregation operators. The OWA operator is a non-linear operator as a result of the process of determining the ''b''<sub>''j''</sub>.

== Properties ==

The OWA operator is a mean operator. It is [[Bounded operator|bounded]], [[monotonic]], [[symmetric operator|symmetric]], and [[idempotent]], as defined below.

{|class="wikitable"
|[[Bounded operator|Bounded]]
|<math>   \min(a_1, \ldots, a_n) \le F(a_1, \ldots, a_n) \le \max(a_1, \ldots, a_n) </math>
|-
|[[Monotonic]]
|<math>   F(a_1, \ldots, a_n) \ge F(g_1, \ldots, g_n) </math> if <math> a_i \ge g_i </math> for <math>\ i = 1,2,\ldots,n </math>
|-
|[[symmetric operator|Symmetric]]
|<math>   F(a_1, \ldots, a_n)  = F(a_\boldsymbol{\pi(1)}, \ldots, a_\boldsymbol{\pi(n)})</math> if <math>\boldsymbol{\pi} </math> is a permutation map
|-
|[[Idempotent]]
|<math>  \ F(a_1, \ldots, a_n)  =  a </math> if all <math> \ a_i = a </math>
|}

== Notable OWA operators ==
:<math> \ F(a_1, \ldots, a_n) = \max(a_1, \ldots, a_n) </math> if <math> \ w_1 = 1 </math> and <math> \ w_j = 0 </math> for <math> j \ne 1 </math>

:<math> \ F(a_1, \ldots, a_n) = \min(a_1, \ldots, a_n) </math> if <math> \ w_n = 1 </math> and <math> \ w_j = 0 </math> for <math> j \ne n </math>

== Characterizing features ==

Two features have been used to characterize the OWA operators. The first is the attitudinal character(orness).

This is defined as
:<math>A-C(W)= \frac{1}{n-1} \sum_{j=1}^n (n - j) w_j. </math>

It is known that <math> A-C(W) \in [0, 1] </math>.

In addition ''A''&nbsp;&minus;&nbsp;''C''(max) = 1, A&nbsp;&minus;&nbsp;C(ave) = A&nbsp;&minus;&nbsp;C(med) = 0.5 and A&nbsp;&minus;&nbsp;C(min) = 0. Thus the A&nbsp;&minus;&nbsp;C goes from 1 to 0 as we go from Max to Min aggregation. The attitudinal character characterizes the similarity of aggregation to OR operation(OR is defined as the Max).

The second feature is the dispersion. This defined as

:<math>H(W) = -\sum_{j=1}^n w_j \ln (w_j).</math>

An alternative definition is <math>E(W) = \sum_{j=1}^n w_j^2 .</math> The dispersion characterizes how uniformly the arguments are being used

== A literature survey: OWA (1988-2014)==
The historical reconstruction of scientific development of the OWA field, the identification of the dominant direction of knowledge accumulation that emerged since the publication of the first OWA paper, and to discover the most active lines of research has recently been published, (see: http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full). The results suggest, as expected, that Yager's paper[1] (IEEE Trans. Systems Man Cybernet, 18(1), 183190, 1988) is the most influential paper and the starting point of all other research using OWA. Starting from his contribution, other lines of research developed and we describe them. Full list of papers published in OWA is also available at http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full) 

== Type-1 OWA aggregation operators ==

The above Yager's OWA operators are used to aggregate the crisp values. Can we aggregate fuzzy sets in the OWA mechanism ? The
'''[[Type-1 OWA operators]]''' have been proposed for this purpose. So the '''[[type-1 OWA operators]]''' provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.

The '''[[Type-1 OWA operators|type-1 OWA operator]]''' is defined according to the alpha-cuts of fuzzy sets as follows:

Given the ''n'' linguistic weights <math>\left\{ {W^i} \right\}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\;\;1]</math>, then for each <math>\alpha \in [0,\;1]</math>, an <math>\alpha </math>-level type-1 OWA operator with <math>\alpha </math>-level sets <math>\left\{ {W_\alpha ^i } \right\}_{i = 1}^n </math> to aggregate the <math>\alpha </math>-cuts of fuzzy sets <math>\left\{ {A^i} \right\}_{i =1}^n </math> is given as

: <math>
\Phi_\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right) =\left\{ {\frac{\sum\limits_{i = 1}^n {w_i a_{\sigma (i)} } }{\sum\limits_{i = 1}^n {w_i } }\left| {w_i \in W_\alpha ^i ,\;a_i } \right. \in A_\alpha ^i ,\;i = 1, \ldots ,n} \right\}</math>

where <math>W_\alpha ^i= \{w| \mu_{W_i }(w) \geq \alpha \}, A_\alpha ^i=\{ x| \mu _{A_i }(x)\geq \alpha \}</math>, and <math>\sigma :\{\;1, \ldots ,n\;\} \to \{\;1, \ldots ,n\;\}</math> is a permutation function such that <math>a_{\sigma (i)} \ge a_{\sigma (i + 1)} ,\;\forall \;i = 1, \ldots ,n - 1</math>, i.e., <math>a_{\sigma (i)} </math> is the <math>i</math>th largest
element in the set <math>\left\{ {a_1 , \ldots ,a_n } \right\}</math>.

The computation of the '''[[Type-1 OWA operators|type-1 OWA]]''' output is implemented by computing the left end-points and right end-points of the intervals <math>\Phi _\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right)</math>:
<math>\Phi _\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right)_{-} </math> and <math>
\Phi _\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right)_ {+},</math>
where <math>A_\alpha ^i=[A_{\alpha-}^i, A_{\alpha+}^i], W_\alpha ^i=[W_{\alpha-}^i, W_{\alpha+}^i]</math>. Then membership function of resulting aggregation fuzzy set is:

:<math>\mu _{G} (x) = \mathop \vee \limits_{\alpha :x \in \Phi _\alpha \left( {A_\alpha ^1 , \cdots
,A_\alpha ^n } \right)_\alpha } \alpha </math>

For the left end-points, we need to solve the following programming problem:

:<math> \Phi _\alpha \left( {A_\alpha ^1 , \cdots ,A_\alpha ^n } \right)_{-} = \mathop {\min }\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i = 1}^n {w_i } } </math>

while for the right end-points, we need to solve the following programming problem:

:<math>\Phi _\alpha \left( {A_\alpha ^1 , \cdots , A_\alpha ^n } \right)_{+} = \mathop {\max }\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i  A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i =
1}^n {w_i } } </math>

[http://dx.doi.org/10.1109/TKDE.2010.191 This paper] has presented a fast method to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently.

== References ==

* Yager, R. R., "On ordered weighted averaging aggregation operators in multi-criteria decision making," IEEE Transactions on Systems, Man and Cybernetics 18, 183190, 1988.

* Yager, R. R. and Kacprzyk, J., [http://www.amazon.com/dp/079239934X The Ordered Weighted Averaging Operators: Theory and Applications], Kluwer: Norwell, MA, 1997.

* Liu, X., "The solution equivalence of minimax disparity and minimum variance problems for OWA operators," International Journal of Approximate Reasoning 45, 6881, 2007.

* Emrouznejad (2009) SAS/OWA: ordered weighted averaging in SAS optimization, Soft Computing [http://www.springerlink.com/content/7277l73334r108x5/]

* Emrouznejad, A. and M. Marra (2014), Ordered Weighted Averaging Operators 19882014: A citation-based literature survey, International Journal of Intelligent Systems, 29:994-1014 [http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full  & http://onlinelibrary.wiley.com/store/10.1002/int.21673/asset/supinfo/int21673-sup-0001-SupMat.docx?v=1&s=c0d8bdd220a31c876eb5885521cfa16d191f334d]. 

* Torra, V. and Narukawa, Y., Modeling Decisions: Information Fusion and Aggregation Operators, Springer: Berlin, 2007.

* Majlender, P., "OWA operators with maximal Renyi entropy," Fuzzy Sets and Systems 155, 340360, 2005.

* Szekely, G. J. and Buczolich, Z., " When is a weighted average of ordered sample elements a maximum likelihood estimator of the location parameter?" Advances in Applied Mathematics 10, 1989, 439456.

* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, "Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers," Fuzzy Sets and Systems, Vol.159, No.24, pp.&nbsp;32813296, 2008 [http://dx.doi.org/10.1016/j.fss.2008.06.018]

* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, "Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments," IEEE Transactions on Knowledge and Data Engineering, vol. 23, no.10, 2011, pp.&nbsp;14551468.[http://dx.doi.org/10.1109/TKDE.2010.191]

* S.-M. Zhou, R. I. John, F. Chiclana and J. M. Garibaldi, "On aggregating uncertain information by type-2 OWA operators for soft decision making," International Journal of Intelligent Systems, vol. 25, no.6, pp.&nbsp;540558, 2010.[http://dx.doi.org/10.1002/int.20420]

[[Category:Artificial intelligence]]
[[Category:Logic in computer science]]
[[Category:Fuzzy logic]]
[[Category:Information retrieval]]
>>EOP<<
34<|###|>Information Retrieval Facility
{{Advert|date=May 2012}}

[[Image:IRF logo 350x350.png|thumb|200px|right|IRF logo]]

The '''Information Retrieval Facility''' ('''IRF'''), founded 2006 and located in [[Vienna]], [[Austria]], was a research platform for networking and collaboration for professionals in the field of [[information retrieval]]. It ceased operations in 2012.

The IRF had members in the following categories:

* Researchers in [[information retrieval]] (IR) or related scientific areas
* Industrial/corporate information management professionals
* Patent authorities and governmental institutions
* Students of one of the above

==The Scientific Board==
'''Maristella Agosti''', Professor, [http://www.dei.unipd.it/wdyn/?IDsezione=1 Department of Information Engineering, University of Padova]

'''Gerhard Budin''', Director of the [http://transvienna.univie.ac.at/forschung/professuren/dr-gerhard-budin/ Center of Translation Studies at the University of Vienna],
Director of the [http://www.oeaw.ac.at/icltt/ Department of Corpuslinguistics and Text Technology, Austrian Academy of Sciences]

'''Jamie Callan''', Professor, [http://www.cs.cmu.edu/~callan/Bio.html Language Technologies Institute, CMU, Carnegie Mellon University]

'''Yves Chiaramella''', Professor Emeritus, [http://www-clips.imag.fr/mrim/User/yves.chiaramella/ Department of Computer Science and Applied Mathematics, Joseph Fourier University]

'''Kilnam Chon''', Professor, Computer Science Department, [http://cosmos.kaist.ac.kr/salab/professor/index02.html Korea Advanced Institute of Science and Technology (KAIST)]

'''W. Bruce Croft''', Distinguished Professor, [http://ciir.cs.umass.edu/personnel/croft.html Department of Computer Science and Director Center for Intelligent IR University of Massachusetts Amherst]

'''Hamish Cunningham''', Research Professor, [http://www.dcs.shef.ac.uk/~hamish/ Computer Science Department University Sheffield]

'''Norbert Fuhr''', Chairman of the Scientific Board, Professor, [http://www.is.informatik.uni-duisburg.de/staff/fuhr.html Institute of Informatics and Interactive Systems University Duisburg-Essen]

'''David Hawking''', Science Leader, Project Leader, [http://es.csiro.au/people/Dave/ CSIRO ICT Centre]

'''Noriko Kando''', Professor, [http://www.nii.ac.jp/index.shtml.en Software Engineering Research, Software Research Division, National Institute of Informatics (NII)]

'''Arcot Desai Narasimhalu''', Associate Dean, [http://www.sis.smu.edu.sg/faculty/infosys/arcotdesai.asp School of Information Systems Singapore Management University]

'''John Tait''', Chief Scientific Officer of the IRF, [http://www.johntait.net/ Until July 2007 Professor of Intelligent Information Systems and Associate Dean of the School of Computing and Technology]

'''Benjamin T'sou''', Director, [http://www.cityu.edu.hk/ Language Information Sciences Research Centre, City University of Hong Kong]

'''[[C. J. van Rijsbergen|C.J. van Rijsbergen]]''',
[http://www.dcs.gla.ac.uk/~keith/ Dept. Computer Science at the University of Glasgow]

==Scientific Goals==

* Modelling innovative and specialised information retrieval systems for global patent document collections.
* Investigating and developing an adequate technical infrastructure that allows interactive experimentation with formal, mathematical retrieval concepts for very large-scale document collections.<
* Studying the usability of multi modal user-interfaces to very large-scale information retrieval systems.
* Integrating real users with actual information needs into the research process of modelling information retrieval systems to allow accurate performance evaluation.
* Ability to create different views of patent data depending on the focus of the information need.
* Defining standardised methods for benchmarking the information retrieval process in patent document collections.
* Ability to handle text and non-text parts of a patent in a coherent manner.
* Designing, experimenting and evaluating search engines able to retrieve structured and semi-structured documents in very large-scale patent collections.
* Integrating the temporal dimension of patent documents in retrieval strategies.
* Improving effectiveness and precision of patent retrieval, based on ontologies and natural-language understanding techniques.
* Refining IR methods that allow unstructured querying by exploiting available structure within the patent documents.
* Formal (mathematical) identification and specification of relevant business information needs in the field of intellectual property information.
* Investigating efficient scaling mechanisms for information retrieval taking into account the characteristics of patent data.
* Investigating and experimenting with computing architectures for very high-capacity information management.
* Establishing an open [[eScience]] platform that enables a standardised and easy way of creating and performing IR experiments on a common research infrastructure.
* Discovering and investigating novel use cases and business applications deriving from intellectual property information.
* Enabling the formal information retrieval, natural language and semantic processing research to grow into the field of applied sciences in the global, industrial context.
* Development and integration of different information access methods.
* Research on effective methods for interactive information retrieval.

==Semantic Supercomputing==
Current technologies to extract concepts from unstructured documents are extremely computational intensive. To allow interactive experimentation with rich and huge text corpora, the IRF has built a high performance computing environment, into which the latest technological advances have been implemented:

* multi-node clusters (currently 80 cores, up to 1024)
* highest speed interconnect technology
* single system image with large compound memory (currently 320 GB, up to 4 TB)
* fully integrated configurable computing (currently 4 FPGA cores, up to 256)

The combination of these HPC features to accelerate text mining represents the IRF implementation of semantic supercomputing.

==The World Patent Corpus==
The IRF aims to bring state-of-the-art information retrieval technology to the community of patent information professionals. We expect information retrieval (IR) technology to become the focus of information technology very soon. All industry sectors can profit from applying modern and future text mining processes to the special requirements of patent research. Although all ideas and concepts are universally applicable to all sorts of intellectual property information, patents require the most sophistication, and confront us with challenging technical and organisational problems. 
The entire body of patent-related documents possibly constitutes the largest corpus of compound documents, making it a rewarding target for text mining scientists and end-users alike. Whats more, patents have become a crucial issue, in particular for large global corporations and universities. The industrial users of patent data are among the most demanding and important information professionals. As a consequence, they could benefit the most from technology that relieves the burden of researching the large body of patent information.

== Research Collections ==
The IRF provides a number of test data collections that have either been developed by the IRF, by one of its members or by third parties. These data collections can be used freely for scientific experimentations.

The MAtrixware REsearch Collection ([[MAREC]]) is the first standardised patent data corpus for research purposes. It consists of 19 million patent documents in different languages, normalised to a highly specific XML format. The collection has been developed by Matrixware for the IRF.

The ClueWeb09 collection is a 25 terabyte dataset of about 1 billion web pages crawled in January and February, 2009. It has been created by the Language Technologies Institute at [[Carnegie Mellon University]] to support research on information retrieval and related human language technologies.

==External links==
* [http://www.ir-facility.org/ Official site: ir-facility.org]
* [http://youtube.com/watch?v=XpXtRu0XfeA YouTube: The future of information retrieval Part1] 
* [http://youtube.com/watch?v=dRaTeTaHBsI YouTube: The future of information retrieval Part2]

==References==
* [http://www.iwr.co.uk/information-world-review/analysis/2231880/patent-medicine-info-retrievers?page=2 Patent medicine for information retrievers, Information World Review]
* [http://ecir2008.dcs.gla.ac.uk/industry.html The IRF and its Role in Professional Information Research, ECIR 2008]

[[Category:Organizations established in 2006]]
[[Category:Computer science organizations]]
[[Category:Information retrieval]]
[[Category:Education in Vienna]]
>>EOP<<
40<|###|>Text Retrieval Conference
{{Other uses of|TREC|TREC (disambiguation)}}
The '''Text REtrieval Conference (TREC)''' is an on-going series of [[workshop]]s focusing on a list of different [[information retrieval]] (IR) research areas, or ''tracks.'' It is co-sponsored by the [[National Institute of Standards and Technology]] (NIST) and the [[Intelligence Advanced Research Projects Activity]] (part of the office of the [[Director of National Intelligence]]), and began in 1992 as part of the [[DARPA TIPSTER Program|TIPSTER Text program]]. Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale ''evaluation'' of [[text retrieval]] methodologies and to increase the speed of lab-to-product [[technology transfer|transfer of technology]].

Each track has a challenge wherein NIST provides participating groups with data sets and test problems. Depending on track, test problems might be questions, topics, or target extractable [[Features (pattern recognition)|features]]. Uniform scoring is performed so the systems can be fairly evaluated. After evaluation of the results, a workshop provides a place for participants to collect together thoughts and ideas and present current and future research work.

== Tracks ==

===Current Tracks===
''New tracks are added as new research needs are identified, this list is current for TREC 2014.''
* Contextual Suggestion Track - '''Goal:''' to investigate search techniques for complex information needs that are highly dependent on context and user interests.
* Clinical Decision Support Track - '''Goal:''' to investigate techniques for linking medical cases to information relevant for patient care
* Federated Web Search Track - '''Goal:''' to investigate techniques for the selection and combination of search results from a large number of real on-line web search services.
* Knowledge Base Acceleration Track - '''Goal:''' to develop techniques to dramatically improve the efficiency of (human) knowledge base curators by having the system suggest modifications/extensions to the KB based on its monitoring of the data streams.
* [[Microblog]] Track - '''Goal:''' to examine the nature of real-time information needs and their satisfaction in the context of microblogging environments such as Twitter. 
* Session Track - '''Goal:''' to develop methods for measuring multiple-query sessions where information needs drift or get more or less specific over the session.
* Temporal Summarization Track - '''Goal:''' to develop systems that allow users to efficiently monitor the information associated with an event over time.
* Web Track - '''Goal:''' to explore information seeking behaviors common in general web search.

===''Past tracks''===
* Chemical Track - '''Goal:''' to develop and evaluate technology for large scale search in [[chemistry]]-related documents, including academic papers and patents, to better meet the needs of professional searchers, and specifically [[patent search]]ers and chemists.
* [[Crowdsourcing]] Track - '''Goal:''' to provide a collaborative venue for exploring [[crowdsourcing]] methods both for evaluating search and for performing search tasks. 
* [[TREC Genomics|Genomics Track]] - '''Goal:''' to study the retrieval of [[Genomics|genomic]] data, not just gene sequences but also supporting documentation such as research papers, lab reports, etc. Last ran on TREC 2007.
* [[Enterprise search|Enterprise Track]] - '''Goal:''' to study search over the data of an organization to complete some task. Last ran on TREC 2008.
* Entity Track - '''Goal:''' to perform entity-related search on Web data. These search tasks (such as finding entities and properties of entities) address common information needs that are not that well modeled as ad hoc document search.
* [[Cross-language information retrieval|Cross-Language]] Track - '''Goal:''' to investigate the ability of retrieval systems to find documents topically regardless of source language.
* [[Federated search|FedWeb]] Track - '''Goal:''' to select best resources to forward a query to, and merge the results so that most relevant are on the top.
* Filtering Track - '''Goal:''' to binarily decide retrieval of new incoming documents given a stable [[information need]].
* HARD Track - '''Goal:''' to achieve High Accuracy Retrieval from Documents by leveraging additional information about the searcher and/or the search context.
* Interactive Track - '''Goal:''' to study user [[Human-computer interaction|interaction]] with text retrieval systems.
* Legal Track - '''Goal:''' to develop search technology that meets the needs of lawyers to engage in effective [[discovery (law)|discovery]] in digital document collections.
* Medical Records Track - '''Goal:''' to explore methods for searching unstructured information found in patient medical records. 
* Novelty Track - '''Goal:''' to investigate systems' abilities to locate new (i.e., non-redundant) information.
* [[Question answering|Question Answering]] Track - '''Goal:''' to achieve more [[information retrieval]] than just [[document retrieval]] by answering factoid, list and definition-style questions.
* Robust Retrieval Track - '''Goal:''' to focus on individual topic effectiveness.
* [[Relevance feedback|Relevance Feedback]] Track - '''Goal:''' to further deep evaluation of relevance feedback processes.
* [[Spam (electronic)|Spam]] Track - '''Goal:''' to provide a standard evaluation of current and proposed [[spam filter]]ing approaches.
* [[Terabyte]] Track - '''Goal:''' to investigate whether/how the [[information retrieval|IR]] community can scale traditional IR test-collection-based evaluation to significantly large collections.
* [[Video search engine|Video]] Track - '''Goal:''' to research in automatic segmentation, [[index (search engine)|index]]ing, and content-based retrieval of [[digital video]].
:In 2003, this track became its own independent evaluation named [[TRECVID]].

===Related Events===
In 1997, a Japanese counterpart of TREC was launched (first workshop in 1999), called [http://research.nii.ac.jp/ntcir/ NTCIR] ([[National Institute of Informatics|NII]] Test Collection for IR Systems), and in 2000, a European counterpart was launched, called [http://www.clef-campaign.org/ CLEF] (Cross Language Evaluation Forum).

== Conference Contributions ==
<!-- contributions of conference to research/IR community -->
NIST claims that within the first six years of the workshops, the effectiveness of retrieval systems approximately doubled.<ref>[http://trec.nist.gov/overview.html From TREC homepage: "... effectiveness approximately doubled in the first six years of TREC"]</ref> The conference was also the first to hold large-scale evaluations of non-English documents, speech, video and retrieval across languages. Additionally, the challenges have inspired a large body of [http://trec.nist.gov/pubs.html publications]. Technology first developed in TREC is now included in many of the world's commercial [[search engine]]s.  An independent report by RTII found that "about one-third of the improvement in web search engines from 1999 to 2009 is attributable to TREC. Those enhancements likely saved up to 3 billion hours of time using web search engines. ... Additionally, the report showed that for every $1 that NIST and its partners invested in TREC, at least $3.35 to $5.07 in benefits were accrued to U.S. information retrieval researchers in both the private sector and academia."
<ref>{{cite web|url=http://rti.org/page.cfm?objectid=75E125DC-5056-B100-31A5A6BDE897DE6D |title=NIST Investment Significantly Improved Search Engines |publisher=Rti.org |date= |accessdate=2012-01-19}}</ref>
<ref>http://www.nist.gov/director/planning/upload/report10-1.pdf</ref>

While one study suggests that the state of the art for ad hoc search  has not advanced substantially in the past decade,<ref>Timothy G. Armstrong, Alistair Moffat, William Webber, Justin Zobel.  Improvements that don't add up: ad hoc retrieval results since 1998.  CIKM 2009.  ACM.</ref> it is referring just to search for topically relevant documents in small news and web collections of a few gigabytes.  There have been advances in other types of ad hoc search in the past decade.  For example, test collections were created for known-item web search which found improvements from the use of anchor text, title weighting and url length, which were not useful techniques on the older ad hoc test collections.  In 2009, a new billion-page web collection was introduced, and spam filtering was found to be a useful technique for ad hoc web search, unlike in past test collections.

The test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.  In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.

TREC systems often provide a baseline for further research.  Examples include:
* [[Hal Varian]], Chief Economist at [[Google]], says ''Better data makes for better science. The history of information retrieval illustrates this principle well," and describes TREC's contribution.<ref>[http://googleblog.blogspot.com/2008/03/why-data-matters.html Why Data Matters]</ref>
* TREC's Legal track has influenced the e-Discovery community both in research and in evaluation of commercial vendors.<ref>[http://blogs.the451group.com/information_management/2009/01/29/standards-in-e-discovery-%E2%80%93-walking-the-walk/ The 451 Group: Standards in e-Discovery -- walking the walk]</ref>
* The [[IBM]] researcher team building [[IBM Watson]] (aka [[DeepQA]]), which beat the world's best [[Jeopardy!]] players,<ref>[http://www-03.ibm.com/press/us/en/presskit/27297.wss IBM and Jeopardy! Relive History with Encore Presentation of Jeopardy!: The IBM Challenge]</ref> used data and systems from TREC's QA Track as baseline performance measurements.<ref>[http://www.aaai.org/AITopics/articles&columns/Ferrucci-Watson2010.pdf David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welt. '''Building Watson:  An Overview of the DeepQA Project''']</ref>

== Participation ==
The conference is made up of a varied, international group of researchers and developers.<ref>{{cite web|url=https://wiki.ir-facility.org/index.php/Participants |title=Participants - IRF Wiki |publisher=Wiki.ir-facility.org |date=2009-12-01 |accessdate=2012-01-19}}</ref><ref>http://trec.nist.gov/pubs/trec17/papers/LEGAL.OVERVIEW08.pdf</ref><ref>{{cite web|url=http://trec.nist.gov/pubs/trec17/appendices/million.query.results.html |title=Text REtrieval Conference (TREC) TREC 2008 Million Query Track Results |publisher=Trec.nist.gov |date= |accessdate=2012-01-19}}</ref> In 2003, there were 93 groups from both academia and industry from 22 countries participating.

==References==
{{reflist}}

== External links ==
*[http://trec.nist.gov/ TREC website at NIST]
*[http://www.nist.gov/itl/div894/894.02/related_projects/tipster/ TIPSTER]
*[http://www.amazon.com/TREC-Experiment-Evaluation-Information-Electronic/dp/0262220733/ The TREC book (at Amazon)]

[[Category:Information retrieval]]
[[Category:Computational linguistics]]
[[Category:Natural language processing]]
[[Category:Computer science competitions]]
>>EOP<<
46<|###|>Category:Sound production technology
{{Commons category|Sound production technology}}
[[Category:Audio electronics|Production]]
[[Category:Information retrieval]]
[[Category:Sound production|Technology]]
>>EOP<<
52<|###|>Collaborative search engine
{{Recommender systems}}
'''Collaborative search engines''' (CSE) are [[Web search engine]]s and [[enterprise search]]es within company intranets that let users combine their efforts in [[information retrieval]] (IR) activities, share information resources collaboratively using [[knowledge tags]], and allow experts to guide less experienced people through their searches. Collaboration partners do so by providing query terms, collective tagging, adding comments or opinions, rating search results, and links clicked of former (successful) IR activities to users having the same or a related [[information need]].

== Models of collaboration ==

Collaborative search engines can be classified along several dimensions: intent (explicit and implicit) and synchronization
<ref name=Golo2007>{{citation
 | title = Collaborative Exploratory Search
 | year = 2007
 | author = Golovchinsky Gene, Pickens Jeremy
 | journal = Proceedings of HCIR 2007 workshop
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://projects.csail.mit.edu/hcir/web/hcir07.pdf
}}</ref> and depth of mediation 
,<ref name=Pickens2008>{{citation
 | title = Collaborative Exploratory Search
 | year = 2008
 | author = Pickens Jeremy, Golovchinsky Gene, Shah Chirag, Qvarfordt Pernilla, Back Maribeth
 | booktitle = SIGIR '08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval
 | pages = 315322
 | volume = 
 | issue = 
 | doi = 10.1145/1390334.1390389
 | isbn = 
 9781605581644| url = http://portal.acm.org/citation.cfm?id=1390389
| chapter = Algorithmic mediation for collaborative exploratory search
 }}</ref> task vs. trait,<ref name=Morris2008>{{citation
 | contribution = Understanding Groups Properties as a Means of Improving Collaborative Search Systems
 | year = 2008
 | author = Morris Meredith, Teevan Jaime
 | title = 1st International Workshop on Collaborative Information Retrieval, held in conjunction with [[Joint Confrence on Digital Libraries|JCDL]] 2008
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | contribution-url = http://workshops.fxpal.com/jcdl2008/submissions/tmpDF.pdf
}}</ref> and division of labor and sharing of knowledge.<ref name=Foley2008>{{citation
 | title = Division of Labour and Sharing of Knowledge for Synchronous Collaborative Information Retrieval
 | year = 2008
 | author = Foley Colum
 | booktitle = PhD Thesis, Dublin City University
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://www.computing.dcu.ie/~cfoley/cfoley-PhD_thesis.pdf
}}</ref>

=== Explicit vs. implicit collaboration ===

Implicit collaboration characterizes [[Collaborative filtering]] and [[recommendation systems]] in which the system infers similar information needs. I-Spy,<ref name=Smith2003>{{citation
 | title = Collaborative Web Search
 | year = 2003
 | author = Barry Smyth, Evelyn Balfe, Peter Briggs, Maurice Coyle, Jill Freyne
 | journal = IJCAI
 | pages = 14171419
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}</ref> [[Jumper 2.0]], [[Seeks]], the Community Search Assistant,<ref name=Glance2001>{{citation
 | title = Community search assistant
 | year = 2001
 | author = Natalie S. Glance
 | journal = Workshop on AI for Web Search AAAI'02
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}</ref> the CSE of Burghardt et al.,<ref name=BurghardtWI2008>{{citation
 | title = Discovering the Scope of Privacy Needs in Collaborative Search
 | year = 2008
 | author = Thorben Burghardt, Erik Buchmann, Klemens Bohm
 | journal = Web Intelligence (WI)
 | pages = 
 910| volume = 
 | issue = 
 | doi = 10.1109/WIIAT.2008.165
 | isbn = 
 978-0-7695-3496-1}}</ref> and the works of Longo et al.
<ref name=Longo2009a>{{citation
 | title = Toward Social Search - From Explicit to Implicit Collaboration
               to Predict Users' Interests
 | year = 2009
 | author = Longo Luca, Barrett Stephen, Dondio Pierpaolo
 | journal = WEBIST 2009 - Proceedings of the Fifth International Conference
               on Web Information Systems and Technologies, Lisbon, Portugal,
               March 2326, 2009
 | pages = 693696
 | volume = 1
 | issue = 
 | doi = 
 | isbn = 978-989-8111-81-4
 | url = 
}}</ref> 
<ref name=Longo2010>{{citation
 | title = Enhancing Social Search: A Computational Collective Intelligence Model of Behavioural Traits, Trust and Time
 | year = 2010
 | author = Longo Luca, Barrett Stephen, Dondio Pierpaolo
 | journal = Transaction Computational Collective Intelligence II
 | pages = 4669
 | volume = 2
 | issue = 
 | doi = 10.1007/978-3-642-17155-0_3
 | isbn = 
 978-3-642-17154-3| url = http://www.springerlink.com/content/e12233858017h042/
| series = Lecture Notes in Computer Science
 }}</ref> 
<ref name=Longo2009b>{{citation
 | title = Information Foraging Theory as a Form of Collective Intelligence                for Social Search
 | year = 2009
 | author = Longo Luca, Barrett Stephen, Dondio Pierpaolo
 | journal = Computational Collective Intelligence. Semantic Web, Social
               Networks and Multiagent Systems, First International Conference,
               ICCCI 2009, Wroclaw, Poland, October 57, 2009. Proceedings
 | pages = 6374
 | volume = 1
 | issue = 
 | doi = 
 | isbn = 978-3-642-04440-3
 | url = http://dl.acm.org/citation.cfm?id=1692026
}}</ref> 
all represent examples of implicit collaboration. Systems that fall under this category identify similar users, queries and links clicked automatically, and recommend related queries and links to the searchers.

Explicit collaboration means that users share an agreed-upon information need and work together toward that goal. For example, in a chat-like application, query terms and links clicked are automatically exchanged. The most prominent example of this class is SearchTogether<ref name=Morris2007>{{citation
 | title = SearchTogether: An Interface for Collaborative Web Search
 | year = 2007
 | author = Meredith Ringel Morris, Eric Horvitz
 | journal = UIST
| url = http://portal.acm.org/citation.cfm?id=1294211.1294215
}}</ref> published in 2007. SearchTogether offers an interface that combines search results from standard search engines and a chat to exchange queries and links. Reddy et al.<ref name=Redy2008>{{citation
 | title = The Role of Communication in Collaborative Information Searching
 | year = 2008
 | author = Madhu C. Reddy, Bernhard J. Jansen, Rashmi Krishnappa
 | journal = ASTIS
}}</ref> (2008) follow a similar approach and compares two implementations of their CSE called MUSE and MUST. Reddy et al. focuses on the role of communication required for efficient CSEs. Representatives for the class of implicit collaboration are I-Spy,<ref name="Smith2003"/> the Community Search Assistant,<ref name="Glance2001"/> and the CSE of Burghardt et al.<ref name="BurghardtWI2008" /> Cerciamo <ref name=Pickens2008 /> supports explicit collaboration by allowing one person to concentrate on finding promising groups of documents, while having the other person make in-depth judgments of relevance on documents found by the first person.

However, in Papagelis et al.<ref name=Papagelis2007>{{citation| title = Searchius: A Collaborative Search Engine| year = 2007| author = Athanasios Papagelis, Christos Zaroliagis| journal = ENC '07: Proceedings of the Eighth Mexican International Conference on Current Trends in Computer Science| pages = 8898| doi = 10.1109/ENC.2007.34| url = http://portal.acm.org/citation.cfm?id=1302894| isbn = 0-7695-2899-6}}</ref> terms are used differently: they combine explicitly shared links and implicitly collected browsing histories of users to a hybrid CSE.

=== Community of practice  ===

Recent work in collaborative filtering and information retrieval has shown that sharing of search experiences among users having similar interests, typically called a [[community of practice]] or [[community of interest]], reduces the effort put in by a given user in retrieving the exact information of interest.<ref name=Rohini&Ambati>{{citation
 | title = A Collaborative Filtering based Re-ranking Strategy for Search in Digital Libraries
 | year = 2002
 | author = Rohini U, Vamshi Ambati
 | journal = ICADL2005: the 8th International Conference on Asian Digital Libraries
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://www.aaai.org/Papers/Workshops/2006/WS-06-10/WS06-10-004.pdf }}</ref>

Collaborative search deployed within a community of practice deploys novel techniques for exploiting context during search by indexing and ranking search results based on the learned preferences of a community of users.<ref name=Coyle2008>{{citation
 | title = Social Aspects of a Collaborative, Community-Based Search Network
 | editor4-first = Eelco
 | editor3-first = Pearl
 | editor2-first = Judy
 | editor1-first = Wolfgang
 | year = 2008
 | editor1-last = Nejdl
 | author = Maurice Coyle and Barry Smyth
 | journal = Adaptive Hypermedia and Adaptive Web-Based Systems
 | pages =  103112  
 | volume = 5149/2008
 | issue = 
 
 | series = Volume| doi = 10.1007/978-3-540-70987-9
 | isbn = 978-3-540-70984-8
 | url = http://portal.acm.org/citation.cfm?id=1485050
 | editor2-last = Kay
 | editor4-last = Herder
 | editor3-last = Pu}}</ref> The users benefit by sharing information, experiences and awareness to personalize result-lists to reflect the preferences of the community as a whole. The community representing a group of users who share common interests, similar professions.  The best known example is the open-source project Jumper 2.0.<ref name=Jumper2010>{{citation
 | title = Jumper Networks Releases Jumper 2.0.1.5 Platform with New Community Search Features
 | year = 2010
 | author = Jumper Networks Inc
 | journal = Press release
 | pages = 
 | volume =
 | issue = 
 | doi =
 | isbn =
 | url = http://www.trilexnet.com/labs/jumper}}</ref>

=== Depth of mediation ===

This refers to the degree that the CSE mediates search.<ref name=Pickens2008 /> SearchTogether<ref name=Morris2007 /> is an example of UI-level mediation: users exchange query results and judgments of relevance, but the system does not distinguish among users when they run queries. Cerchiamo<ref name=Pickens2008 /> and recommendation systems such as I-Spy<ref name=Smith2003 /> keep track of each person's search activity independently, and use that information to affect their search results. These are examples of deeper algorithmic mediation.

=== Task vs. trait ===

This model classifies people's membership in groups based on the task at hand vs. long-term interests; these may be correlated with explicit and implicit collaboration.<ref name=Morris2008 />

== Privacy-aware collaborative search engines ==

Search terms and links clicked that are shared among users reveal their interests, habits, social
relations and intentions.<ref name=EUArticle29>{{citation
 | title = Article 29 EU Data Protection Working Party
 | year = 2008
 | author = Data Protection Working Party
 | journal = EU
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}</ref> In other words, CSEs put the privacy of the users at risk. Studies have shown that CSEs increase efficiency. 
<ref name="Morris2007"/><ref name=Smith2005>{{citation
 | title = A Live-User Evaluation of Collaborative Web Search
 | year = 2005
 | author = Barry Smyth, Evelyn Balfe, Oisin Boydell, Keith Bradley, Peter Briggs, Maurice Coyle, Jill Freyne
 | journal = IJCAI
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}</ref>
<ref name=Smith2006>{{citation
 | title = Anonymous personalization in collaborative web search
 | year = 2005
 | author = Smyth,, Barry and Balfe,, Evelyn
 | journal = Inf. Retr.
 | pages = 165190
 | volume = 9
 | issue = 2| doi = 10.1007/s10791-006-7148-z| isbn = 
 | url = 
}}</ref>
<ref name=Jung2004>{{citation
 | title = Applying Collaborative Filtering for Efficient Document Search
 | year = 2004
 | author = Seikyung Jung, Juntae Kim, Herlocker, J.L.
 | journal = Inf. Retr.
 | pages = 640643
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}</ref> Unfortunately, by the lack of privacy enhancing technologies, a privacy aware user who wants to benefit from a CSE has to disclose his entire search log. (Note, even when explicitly sharing queries and links clicked, the whole (former) log is disclosed to any user that joins a search session).  Thus, sophisticated mechanisms that allow on a more fine grained level which information is disclosed to whom are desirable.

As CSEs are a new technology just entering the market, identifying user privacy preferences and integrating [[Privacy enhancing technologies]] (PETs) into collaborative search are in conflict. On one hand, PETs have to meet user preferences, on the other hand one cannot identify these preferences without using a CSE, i.e., implementing PETs into CSEs. Today, the only work addressing this problem comes from Burghardt et al.<ref name=BurghardtCC2008>{{citation
 | title = Collaborative Search And User Privacy: How Can They Be Reconciled?
 | year = 2008
 | author = Thorben Burghardt, Erik Buchmann, Klemens Bohm, Chris Clifton
 | journal = CollaborateCom
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://dbis.ipd.uni-karlsruhe.de/1184.php
}}</ref> They implemented a CSE with experts from the information system domain and derived the scope of possible privacy preferences in a user study with these experts. Results show that users define preferences referring to (i) their current context (e.g., being at work), (ii) the query content (e.g., users exclude topics from sharing), (iii) time constraints (e.g., do not publish the query X hours after the query has been issued, do not store longer than X days, do not share between working time), and that users intensively use the option to (iv) distinguish between different social groups when sharing information. Further, users require (v) anonymization and (vi) define reciprocal constraints, i.e., they refer to the behavior of other users, e.g., if a user would have shared the same query in turn.

== References ==
{{reflist|2}}
{{Internet search}}

[[Category:Information retrieval]]
>>EOP<<
58<|###|>Web query classification
{{Cleanup|date=March 2011}}
''' 
A Web query topic classification/categorization is a problem in [[information science]]. The task is to assign a [[Web search query]] to one or more predefined [[Categorization|categories]], based on its topics. The importance of query classification is underscored by many services provided by Web search. A direct application is to provide better search result pages for users with interests of different categories. For example, the users issuing a Web query ''apple'' might expect to see Web pages related to the fruit apple, or they may prefer to see products or news related to the computer company. Online advertisement services can rely on the query classification results to promote different products more accurately. Search result pages can be grouped according to the categories predicted by a query classification algorithm. However, the computation of query classification is non-trivial. Different from the [[document classification]] tasks, queries submitted by Web search users are usually short and ambiguous; also the meanings of the queries are evolving over time. Therefore, query topic classification is much more difficult than traditional document classification tasks.

== KDDCUP 2005 ==

KDDCUP 2005 competition<ref>[http://www.sigkdd.org/kdd2005/kddcup.html KDDCUP 2005 dataset]</ref> highlighted the interests in query classification. The objective of this competition is to classify 800,000 real user queries into 67 target categories. Each query can belong to more than one target category. As an example of a QC task, given the query ''apple'', it should be classified into ranked categories: ''Computers \ Hardware''; ''Living \ Food & Cooking''.

{| class="wikitable"
|-
! Query
! Categories
|-
| apple
| Computers \ Hardware<br />Living \ Food & Cooking
|-
| FIFA 2006
| Sports \ Soccer<br />Sports \ Schedules & Tickets<br />Entertainment \ Games & Toys
|-
| cheesecake recipes
| Living \ Food & Cooking<br />Information \ Arts & Humanities
|-
| friendships poem
| Information \ Arts & Humanities<br />Living \ Dating & Relationships
|}

[[Image:Web query length.gif]]
[[Image:Web query meaning.gif]]

== Difficulties ==

Web query topic classification is to automatically assign a query to some predefined categories. Different from the traditional document classification tasks, there are several major difficulties which hinder the progress of Web query understanding:

=== How to derive an appropriate feature representation for Web queries? ===

Many queries are short and query terms are noisy. As an example, in the KDDCUP 2005 dataset, queries containing 3 words are most frequent (22%). Furthermore, 79% queries have no more than 4 words. A user query often has multiple meanings. For example, "''apple''" can mean a kind of fruit or a computer company. "''Java''" can mean a programming language or an island in Indonesia. In the KDDCUP 2005 dataset, most of the queries contain more than one meaning. Therefore, only using the keywords of the query to set up a [[vector space model]] for classification is not appropriate.

* Query-enrichment based methods<ref>Shen et al.  [http://www.sigkdd.org/sites/default/files/issues/7-2-2005-12/KDDCUP2005Report_Shen.pdf "Q2C@UST: Our Winning Solution to Query Classification"]. ''ACM SIGKDD Exploration, December 2005, Volume 7, Issue 2''.</ref><ref>Shen et al. [http://portal.acm.org/ft_gateway.cfm?id=1165776 "Query Enrichment for Web-query Classification"]. ''ACM TOIS, Vol. 24, No. 3, July 2006''.</ref> start by enriching user queries to a collection of text documents through [[search engines]]. Thus, each query is represented by a pseudo-document which consists of the snippets of top ranked result pages retrieved by search engine. Subsequently, the text documents are classified into the target categories using synonym based classifier or statistical classifiers, such as [[Naive Bayes]] (NB) and [[Support Vector Machines]] (SVMs).

How about disadvantages and advantages??
give the answers:

=== How to adapt the changes of the queries and categories over time? ===

The meanings of queries may also evolve over time. Therefore, the old labeled training queries may be out-of-data and useless soon. How to make the classifier adaptive over time becomes a big issue. For example, the word "''Barcelona''" has a new meaning of the new micro-processor of AMD, while it refers to a city or football club before 2007. The distribution of the meanings of this term is therefore a function of time on the Web.

* Intermediate taxonomy based method<ref>Shen et al.  [http://portal.acm.org/ft_gateway.cfm?id=1148196 "Building bridges for web query classification"]. ''ACM SIGIR, 2006''.</ref> first builds a bridging classifier on an intermediate taxonomy, such as [[Open Directory Project]] (ODP), in an offline mode. This classifier is then used in an online mode to map user queries to the target categories via the intermediate taxonomy. The advantage of this approach is that the bridging classifier needs to be trained only once and is adaptive for each new set of target categories and incoming queries.

=== How to use the unlabeled query logs to help with query classification? ===

Since the manually labeled training data for query classification is expensive, how to use a very large web search engine query log as a source of unlabeled data to aid in automatic query classification becomes a hot issue. These logs record the Web users' behavior when they search for information via a search engine. Over the years, query logs have become a rich resource which contains Web users' knowledge about the World Wide Web.

* Query clustering method<ref>Wen et al. [http://portal.acm.org/ft_gateway.cfm?id=503108 "Query Clustering Using User Logs"], ''ACM TOIS, Volume 20, Issue 1, January 2002''.</ref> tries to associate related queries by clustering session data, which contain multiple queries and click-through information from a single user interaction. They take into account terms from result documents that a set of queries has in common. The use of query keywords together with session data is shown to be the most effective method of performing query clustering.

* Selectional preference based method<ref>Beitzel et al. [http://portal.acm.org/ft_gateway.cfm?id=1229183 "Automatic Classification of Web Queries Using Very Large Unlabeled Query Logs"], ''ACM TOIS, Volume 25, Issue 2, April 2007''.</ref> tries to exploit some [[association rules]] between the query terms to help with the query classification. Given the training data, they exploit several classification approaches including exact-match using labeled data, N-Gram match using labeled data and classifiers based on perception. They emphasize on an approach adapted from computational linguistics named selectional preferences. If x and y form a pair (x; y) and y belongs to category c, then all other pairs (x; z) headed by x belong to c. They use unlabeled query log data to mine these rules and validate the effectiveness of their approaches on some labeled queries.

== Applications ==

* '''[[metasearch|Metasearch engines]]''' send a user's query to multiple search engines and blend the top results from each into one overall list. The search engine can organize the large number of Web pages in the search results, according to the potential categories of the issued query, for the convenience of Web users' navigation.
* '''[[Vertical search]]''', compared to general search, focuses on specific domains and addresses the particular information needs of niche audiences and professions. Once the search engine can predict the category of information a Web user is looking for, it can select a certain vertical search engine automatically, without forcing the user to access the vertical search engine explicitly.
* '''[[Online advertising]]'''<ref>[http://www.kdd2007.com/workshops.html#adkdd Data Mining and Audience Intelligence for Advertising (ADKDD'07)], KDD workshop 2007</ref><ref>[http://research.yahoo.com/workshops/troa-2008/ Targeting and Ranking for Online Advertising (TROA'08)], WWW workshop 2008</ref> aims at providing interesting advertisements to Web users during their search activities. The search engine can provide relevant advertising to Web users according to their interests, so that the Web users can save time and effort in research while the advertisers can reduce their advertising costs.
All these services rely on the understanding Web users' search intents through their Web queries.

== See also ==

* [[Document classification]]
* [[Web search query]]
* [[Information retrieval]]
* [[Query expansion]]
* [[Naive Bayes classifier]]
* [[Support vector machines]]
* [[Meta search]]
* [[Vertical search]]
* [[Online advertising]]

== References ==

{{reflist}}

== Further reading ==
* Shen.  [http://lbxml.ust.hk/th/th_search.pl?smode=VIEWBYCALLNUM&skeywords=CSED%202007%20Shen "Learning-based Web Query Understanding"]. ''Phd Thesis'', ''HKUST'', June 2007.
{{Internet search}}

{{DEFAULTSORT:Web Query Classification}}
[[Category:Information retrieval]]
[[Category:Internet search]]
>>EOP<<
64<|###|>Gain (information retrieval)
{{other uses2|Gain}}
{{Unreferenced|date=August 2009}}
The '''gain''', also called '''improvement over random''' {{cn|date=March 2013}} can be specified for a [[classifier (mathematics)|classifier]] and is an important measure {{dubious|date=March 2013}} to describe the performance of it.

== Definition ==
In the following a random classifier is defined such that it randomly predicts the same amount of either class.

The gain is defined as described in the following:

=== Gain in Precision ===

The random [[positive predictive value|precision]] of a classifier is defined as

<math>
r = \frac{TP+FN}{TP+TN+FP+FN} = \frac{\textit{Positives}}{N}
</math>

where TP, TN, FP and FN are the numbers of true positives, true negatives, false positives and false negatives respectively, positives is the number of positive instances in the target dataset and N is the size of the dataset.

The random precision defines the lowest baseline of a classifier.

And '''Gain''' is defined as 

<math>
G = \frac{\textit{precision}}{r}
</math>

which gives a factor by which a classifier is better when compared to its random counterpart. A Gain of 1 would indicate a classifier that is not better than random. The larger the gain, the better.

=== Gain in Overall Accuracy ===

The [[accuracy]] of a classifier in general is defined as

<math>
Acc = \frac{TP+TN}{TP+TN+FP+FN} = \frac{\textit{Corrects}}{N}
</math>

Here, the random accuracy of a classifier can be defined as

<math>
r = \left ( \frac{\textit{Positives}}{N} \right ) ^2+ \left ( \frac{\textit{Negatives}}{N} \right ) ^2=f(\textit{Positives})^2 + f(\textit{Negatives})^2
</math>

f(Positives) and f(Negatives) is the fraction of positive and negative classes in the dataset.

And again '''gain''' is

<math>
G = \frac{\textit{Acc}}{r}
</math>

This time the gain is measured not only with respect to the prediction of a so-called positive class, but with respect to the overall classifier ability to distinguish the two equally important classes.

== Application ==
In [[Bioinformatics]] as an example, the gain is measured for methods that predict residue contacts in proteins.

== See also ==
* [[Accuracy and precision]]
* [[Binary classification]]
* [[Brier score]]
* [[Confusion matrix]]
* [[Detection theory]]
* [[F-score]]
* [[Information retrieval]]
* [[Matthews correlation coefficient]]
* [[Receiver operating characteristic]] or ROC curve
* [[Selectivity (electronic)|Selectivity]]
* [[Sensitivity and specificity]]
* [[Sensitivity index]]
* [[Statistical significance]]
* [[Youden's J statistic]]

{{DEFAULTSORT:Gain (Information Retrieval)}}
[[Category:Logic]]
[[Category:Information retrieval]]
>>EOP<<
70<|###|>DtSearch
{{Lowercase}}

{{Infobox company |
  name   = dtSearch Corp. |
  slogan = "The Smart Choice for Text Retrieval since 1991" |
  type   =  Private company |
  foundation     = 1991 |
  location       = [[Bethesda, Maryland|Bethesda]], [[Maryland]] |
  key_people     = David Thede, President |
  industry       = [[Software]] |

  homepage       = [http://www.dtsearch.com/ www.dtsearch.com]
}}

'''dtSearch Corp.''' is a [[software company]] which specializes in [[text retrieval]] software. It was founded in 1991, and is headquartered in [[Bethesda, Maryland|Bethesda]], [[Maryland]]. Its current range of software includes products for enterprise [[desktop search]], Intranet/Internet [[spidering]] and search, and [[search engines]] for developers ([[Software development kit|SDK]]) to integrate into other software applications.

==History==
dtSearch Corp was founded by David Thede<ref>[http://www.lets-talk-computers.com/guests/dtsearch/6.2/index.htm; Lets talk computers - Interview May 31, 2003]</ref><ref>[https://www.google.com/patents/US6782380 Method and system for indexing and searching contents of extensible mark-up language(XML) documents US 6782380 B1]</ref><ref>[https://www.google.com/patents/US7464098 Method for rapidly searching elements or attributes or for rapidly filtering fragments in binary representations of structured, for example, XML-based documents US 7464098 B2]</ref> the company started research and development in text retrieval in 1988  and incorporated in Virginia in 1991 as D T Software. Marketing of dtSearch 1.0 a DOS Text Retrieval software product began in the first quarter of 1991. Initially it was distributed as [[Association of Shareware Professionals]]-approved [[shareware]]. The product was featured in an article entitled "Text Retrieval Software" in an early edition of ''[[PC Magazine]]''<ref>"Text Retrieval Software". (July 1992). [[PC Magazine]] (UK ed)</ref> as a shareware alternative to the commercial products reviewed; these included [[ISYS Search Software|ISYS]], [[ZyLAB Technologies|ZyIndex]], Strix, [[AskSAM]], [[ideaList]], Assassin PC, [[Folio Corporation|Folio Views]] and Lotus SmartText.

In the first few years after its initial release, dtSearch was an end-user application only. Then, in 1994, [[Symantec]] approached dtSearch about including its search technology into one of the first applications for 32-bit Windows; the dtSearch end-user application was developed into a [[Dynamic-link library]] (DLL) which Symantec embedded in Norton Navigator, which was released alongside Microsofts initial release of its 32-bit Windows operating system, [[Windows 95]].<ref>[http://www.processor.com/editorial/article.asp?article=articles%2Fp3012%2F11p12%2F11p12.asp dtSearch Performs Incredible Feats. Processor Mag. March 21, 2008]</ref>

In 2007 the company was listed in the [[EContent]] 100 list, a list of companies that matter most in the digital content industry.<ref>[http://www.econtentmag.com/Articles/ArticleReader.aspx?ArticleID=40160&PageNum=22007 EContent 100 list]</ref>

==Products==
The current (v 7.7) product range is [[Unicode]]-based and has an index that can handle over 1 [[terabyte|TB]] of data per index.

*dtSearch Desktop with Spider -  Windows client Desktop search software (32 and 64 bit indexers)
*dtSearch Network with Spider -  as dtSearch Desktop but licensed for Network use (32 and 64 bit indexers)
*dtSearch Web with Spider -  browser based search-only client for Intranet/Internet usage based on Microsoft IIS (32 and 64 bit indexers)
*dtSearch Engine with Spider - SDK with C++, .NET, COM, Java, Delphi APIs (32-bit and 64-bit versions)
*dtSearch Engine for Linux - SDK with C++ and Java APIs
*dtSearch Publish <ref>[http://www.law.com/jsp/lawtechnologynews/PubArticleLTN.jsp?id=1202463957873&slreturn=1&hbxlogin=1 dtSearch Publish for EDD Production Law Technology News July 29, 2010]</ref> - a search front-end for CD\DVD publishing (32 and 64 bit indexers)

==See also==
* [[Enterprise search]]
* [[List of enterprise search vendors]]

==References==
{{Reflist}}

==External links==
*[http://www.dtsearch.com/ Company Website]
*[http://www.searchtools.com/tools/dtsearch.html Product description on SearchTools.com ]
*[http://www.windowsitpro.com/article/desktop-management/dtsearch-7-desktop-with-spider.aspx The index is mightier than the sword - Windows IT Pro. August 27, 2008]
*[http://www.infoworld.com/t/platforms/desktop-search-gets-down-business-610 Desktop search gets down to business - InfoWorld. September 01, 2005]
*[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC150357/ Integrating Query of Relational and Textual Data in Clinical Databases - J Am Med Inform Assoc. 2003 JanFeb]
*[http://radiographics.rsna.org/content/29/5/1233.full.pdf Informatics in Radiology. Render: An Online Searchable Radiology Study Repository - RadioGraphics 2009; 29:12331246] 
*[http://jms.ndmctsgh.edu.tw/fdarticlee%5C2606199.pdf Use Of Intelligent Computer Search for the Patterns of Abnormal Lymphatic Uptake by F-18 FDG PET in Primary Lung Cancers - J Med Sci 2006;26(6):199-204]

{{DEFAULTSORT:Dtsearch Corp.}}
[[Category:Desktop search engines]]
[[Category:Information retrieval]]
[[Category:Software companies based in Maryland]]
>>EOP<<
76<|###|>Rocchio algorithm
The '''Rocchio algorithm''' is based on a method of [[relevance feedback]] found in [[information retrieval]] systems which stemmed from the [[SMART Information Retrieval System]] around the year 1970. Like many other retrieval systems, the Rocchio feedback approach was developed using the [[Vector Space Model]].  The [[algorithm]] is based on the assumption that most users have a general conception of which documents should be denoted as  [[Relevance (information retrieval)|relevant]] or non-relevant.<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schutze: ''An Introduction to Information Retrieval'', page 181. Cambridge University Press, 2009.</ref>  Therefore, the user's search query is revised to include an arbitrary percentage of  relevant and non-relevant documents as a means of increasing the [[search engine]]'s [[Information_retrieval#Recall|recall]], and possibly the precision as well.  The number of  relevant and non-relevant documents allowed to enter a [[Information retrieval|query]] is dictated by the weights of the a, b, c variables listed below in the [[Rocchio_Classification#Algorithm|Algorithm section]].<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schutze: ''An Introduction to Information Retrieval'', page 292. Cambridge University Press, 2009.</ref>

==Algorithm==
The [[Formula (mathematical logic)|formula]] and variable definitions for Rocchio relevance feedback is as follows:<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schutze: ''An Introduction to Information Retrieval'', page 182. Cambridge University Press, 2009.</ref>

<math> \overrightarrow{Q_m} = \bigl(a \cdot \overrightarrow{Q_o}\bigr) + \biggl(b \cdot {\tfrac{1}{|D_r|}} \cdot \sum_{\overrightarrow{D_j} \in D_r} \overrightarrow{D_j}\biggr)
- \biggl(c \cdot {\tfrac{1}{|D_{nr}|}} \cdot \sum_{\overrightarrow{D_k} \in D_{nr}} \overrightarrow{D_k}\biggr) </math>

{| class="wikitable"
|-
! Variable
! Value
|-
| <math> \overrightarrow{Q_m} </math>
| Modified Query Vector
|-
| <math> \overrightarrow{Q_o} </math>
| Original Query Vector
|-
| <math> \overrightarrow{D_j} </math>
| Related Document Vector
|-
| <math> \overrightarrow{D_k} </math>
| Non-Related Document Vector
|-
| <math> a </math>
| Original Query Weight
|-
| <math> b </math>
| Related Documents Weight
|-
| <math> c </math>
| Non-Related Documents Weight
|-
| <math> D_r </math>
| Set of Related Documents
|-
| <math> D_{nr} </math>
| Set of Non-Related Documents
|}
[[Image:Rocchioclassgraph.jpg|thumb|right|250px|Rocchio Classification]]

As demonstrated in the Rocchio formula, the associated weights ('''a''', '''b''', '''c''') are responsible for shaping the modified [[vector space|vector]] in a direction closer, or farther away, from the original query, related documents, and non-related documents.  In particular, the values for '''b''' and '''c''' should be incremented or decremented proportionally to the set of documents classified by the user.  If the user decides that the modified query should not contain terms from either the original query, related documents, or non-related documents, then the corresponding weight ('''a''', '''b''', '''c''') value for the category should be set to 0.

In the later part of the algorithm, the variables '''Dr''', and '''Dnr''' are presented to be sets of [[Tuple|vectors]] containing the coordinates of related documents and non-related documents.  Though '''Dr''' and '''Dnr''' are not  vectors themselves, <math> \overrightarrow{Dj} </math> and <math> \overrightarrow{Dk} </math> are the vectors used to iterate through the two sets and form vector [[summation]]s. These summations will be multiplied against the [[Multiplicative inverse]] of their respective document set ('''Dr''', '''Dnr''') to complete the addition or subtraction of related or non-related documents.

In order to visualize the changes taking place on the modified vector, please refer to the image below.<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schutze: ''An Introduction to Information Retrieval'', page 293. Cambridge University Press, 2009.</ref> As the weights are increased or decreased for a particular category of documents, the coordinates for the modified vector begin to move either closer, or farther away, from the [[centroid]] of the document collection. Thus if the weight is increased for related documents, then the modified vectors [[coordinate]]s will reflect being closer to the centroid of related documents.

==Time complexity==
The [[time complexity]] for training and testing the algorithm are listed below and followed by the definition of each [[variable (mathematics)|variable]]. Note that when in testing phase, the time complexity can be reduced to that of calculating the [[euclidean distance]] between a class [[centroid]] and the respective document.  As shown by: <math>\Theta(\vert\mathbb{C}\vert M_{a})</math>.

Training = <math>\Theta(\vert\mathbb{D}\vert L_{ave}+\vert\mathbb{C}\vert\vert V\vert)</math> <br>
Testing = <math>\Theta( L_{a}+\vert\mathbb{C}\vert M_{a})= \Theta(\vert\mathbb{C}\vert M_{a})</math> <ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schutze: ''An Introduction to Information Retrieval'', page 296. Cambridge University Press, 2009.</ref>

{| class="wikitable"
|-
! Variable
! Value
|-
| <math> \mathbb{D} </math>
| Labeled Document Set
|-
| <math> L_{ave} </math>
| Average Tokens Per Document
|-
| <math> \mathbb{C} </math>
| Class Set
|-
| <math> V </math>
| Vocabulary/Term Set
|-
| <math> L_{a} </math>
| Number of Tokens in Document
|-
| <math> M_{a} </math>
| Number of Types in Document
|}

==Usage==
Though there are benefits to ranking documents as not-relevant, a [[relevant]] document ranking will result in more precise documents being made available to the user. Therefore, traditional values for the algorithm's weights ('''a''', '''b''', '''c''') in Rocchio Classification are typically around '''a = 1''', '''b = 0.8''', and ''' c = 0.1'''. Modern [[information retrieval]] systems have moved towards eliminating the non-related documents by setting '''c = 0''' and thus only accounting for related documents. Although not all [[Information retrieval|retrieval systems]] have eliminated the need for non-related documents, most have limited the effects on modified query by only accounting for strongest non-related documents in the '''Dnr''' set.

==Limitations==
The Rocchio algorithm often fails to classify multimodal classes and relationships. For instance, the country of [[Burma]] was renamed to [[Myanmar]] in 1989. Therefore the two queries of "Burma" and "Myanmar" will appear much farther apart in the [[vector space model]], though they both contain similar origins.<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schutze: ''An Introduction to Information Retrieval'', page 296. Cambridge University Press, 2009.</ref>

== See also ==
*  [[Nearest centroid classifier]], aka Rocchio classifier

==References==
{{reflist}}
* [http://nlp.stanford.edu/IR-book/pdf/09expand.pdf Relevance Feedback and Query Expansion]
* [http://nlp.stanford.edu/IR-book/pdf/14vcat.pdf Vector Space Classification]
* [http://cs.nyu.edu/courses/fall07/G22.2580-001/lec7.html Data Classification]

[[Category:Information retrieval]]
>>EOP<<
82<|###|>Uncertain inference
'''Uncertain inference''' was first described by [[C. J. van Rijsbergen]]<ref>{{cite | author=C. J. van Rijsbergen | title=A non-classical logic for information retrieval | publisher=The Computer Journal | pages=481485 | year=1986}}</ref> as a way to formally define a query and document relationship in [[Information retrieval]]. This formalization is a [[logical consequence|logical implication]] with an attached measure of uncertainty.

==Definitions==
Rijsbergen proposes that the measure of [[uncertainty]] of a document ''d'' to a query ''q'' be the probability of its logical implication, i.e.:

<math>P(d \to q)</math>

A user's query can be interpreted as a set of assertions about the desired document. It is the system's task to [[inference|infer]], given a particular document, if the query assertions are true. If they are, the document is retrieved.
In many cases the contents of documents are not sufficient to assert the queries. A [[knowledge base]] of facts and rules is needed, but some of them may be uncertain because there may be a probability associated to using them for inference. Therefore, we can also refer to this as ''plausible inference''. The [[plausibility]] of an inference <math>d \to q</math> is a function of the plausibility of each query assertion. Rather than retrieving a document that exactly matches the query we should rank the documents based on their plausibility in regards to that query.
Since ''d'' and ''q'' are both generated by users, they are error prone; thus <math>d \to q</math> is uncertain. This will affect the plausibility of a given query.

By doing this it accomplishes two things:
* Separate the processes of revising probabilities from the logic
* Separate the treatment of relevance from the treatment of requests

[[Multimedia]] documents, like images or videos, have different inference properties for each datatype. They are also different from text document properties. The framework of plausible inference allows us to measure and combine the probabilities coming from these different properties.

Uncertain inference generalizes the notions of [[autoepistemic logic]], where truth values are either known or unknown, and when known, they are true or false.

==Example==
If we have a query of the form:

<math>q = A \wedge B \wedge C</math>

where A, B and C are query assertions, then for a document D we want the probability:

<math>P (D \to (A \wedge B \wedge C))</math>

If we transform this into the [[conditional probability]] <math>P ((A \wedge B \wedge C) | D)</math> and if the query assertions are independent we can calculate the overall probability of the implication as the product of the individual assertions probabilities.

==Further work==
Croft and Krovetz<ref>{{cite | title=Interactive retrieval office documents | url=http://doi.acm.org/10.1145/45410.45435 | author=W. B. Croft | coauthors=R. Krovetz | year=1988 }}</ref> applied uncertain inference to an information retrieval system for office documents they called ''OFFICER''. In office documents the independence assumption is valid since the query will focus on their individual attributes. Besides analysing the content of documents one can also query about the author, size, topic or collection for example. They devised methods to compare document and query attributes, infer their plausibility and combine it into an overall rating for each document. Besides that uncertainty of document and query contents also had to be addressed.

[[Probabilistic logic network]]s is a system for performing uncertain inference; crisp true/false truth values are replaced not only by a probability, but also by a confidence level, indicating the certitude of the probability.

[[Markov logic network]]s allow uncertain inference to be performed; uncertainties are computed using the [[maximum entropy principle]], in analogy to the way that [[Markov chain]]s describe the uncertainty of [[finite state machine]]s.

== See also ==
* [[Fuzzy logic]]
* [[Probabilistic logic]]
* [[Plausible reasoning]]
* [[Imprecise probability]]

==References==
{{reflist}}

[[Category:Fuzzy logic]]
[[Category:Information retrieval]]
[[Category:Inference]]
>>EOP<<
88<|###|>Music information retrieval
{{multiple issues|
{{technical|date=October 2012}}
{{Expert-subject|date=July 2010}}
{{Expert-subject|Science|date=July 2010}}
}}


'''Music information retrieval''' ('''MIR''') is the interdisciplinary science of retrieving [[information]] from [[music]]. MIR is a small but growing field of research with many real-world applications. Those involved in MIR may have a background in [[musicology]], [[psychology]], academic music study, [[signal processing]], [[machine learning]] or some combination of these.

== Applications ==
MIR is being used by businesses and academics to categorize, manipulate and even create music.

=== Recommender systems ===
Several [[recommender systems]] for music already exist, but surprisingly few are based upon MIR techniques, instead making use of similarity between users or laborious data compilation. [[Pandora]], for example, uses experts to tag the music with particular qualities such as "female singer" or "strong bassline". Many other systems find users whose listening history is similar and suggests unheard music to the users from their respective collections. MIR techniques for similarity in music are now beginning to form part of such systems.

=== Track separation and instrument recognition ===
Track separation is about extracting the original tracks as recorded, which could have more than one instrument played per track. Instrument recognition is about identifying the instruments involved and/or separating the music into one track per instrument. Various programs have been developed that can separate music into its component tracks without access to the master copy. In this way e.g. karaoke tracks can be created from normal music tracks, though the process is not yet perfect owing to vocals occupying some of the same frequency space as the other instruments.

===Automatic music transcription===
Automatic music transcription is the process of converting an audio recording into symbolic notation, such as a score or a [[MIDI_file#File_formats|MIDI file]].<ref>A. Klapuri and M. Davy, editors. Signal Processing Methods for Music Transcription. Springer-Verlag, New York, 2006.</ref> This process involves several subtasks, which include multi-pitch detection, [[Onset_detection#Onset_detection|onset detection]], duration estimation, instrument identification, and the extraction of rhythmic information. This task becomes more difficult with greater numbers of instruments and a greater [[Polyphony and monophony in instruments|polyphony level]].

===Automatic categorization===
Musical genre categorization is a common task for MIR and is the usual task for the yearly Music Information Retrieval Evaluation eXchange(MIREX).<ref>http://www.music-ir.org/mirex/wiki/MIREX_HOME - Music Information Retrieval Evaluation eXchange.</ref> Machine learning techniques such as [[Support Vector Machines]] tend to perform well, despite the somewhat subjective nature of the classification. Other potential classifications include identifying the artist, the place of origin or the mood of the piece. Where the output is expected to be a number rather than a class, [[regression analysis]] is required.

===Music generation===
The automatic generation of music is a goal held by many MIR researchers. Attempts have been made with limited success in terms of human appreciation of the results.

==Methods used==

===Data source===
[[Sheet music|Scores]] give a clear and logical description of music from which to work, but access to sheet music, whether digital or otherwise, is often impractical. [[MIDI]] music has also been used for similar reasons, but some data is lost in the conversion to MIDI from any other format, unless the music was written with the MIDI standards in mind, which is rare. Digital audio formats such as [[WAV]], [[mp3]], and [[ogg]] are used when the audio itself is part of the analysis. Lossy formats such as mp3 and ogg work well with the human ear but may be missing crucial data for study. Additionally some encodings create artifacts which could be misleading to any automatic analyser. Despite this the ubiquity of the mp3 has meant much research in the field involves these as the source material. Increasingly, metadata mined from the web is incorporated in MIR for a more rounded understanding of the music within its cultural context, and this recently includes analysis of [[social tagging|social tags]] for music.

===Feature representation===
Analysis can often require some summarising,<ref>Eidenberger, Horst (2011). Fundamental Media Understanding, atpress. ISBN 978-3-8423-7917-6.</ref> and for music (as with many other forms of data) this is achieved by feature extraction, especially when the audio content itself is analysed and machine learning is to be applied. The purpose is to reduce the sheer quantity of data down to a manageable set of values so that learning can be performed within a reasonable time-frame. One common feature extracted is the [[Mel-frequency cepstral coefficient|Mel-Frequency Cepstral Coefficient]] (MFCC) which is a measure of the [[timbre]] of a piece of music. Other features may be employed to represent the [[Tonality#Computational_methods_to_determine_the_key|key]], chords, harmonies, melody, main pitch, beats per minute or rhythm in the piece.

===Statistics and machine learning===
*Computational methods for classification, clustering, and modelling  musical feature extraction for mono- and [[polyphonic]] music, similarity and [[pattern matching]], retrieval
* Formal methods and databases  applications of automated [[music identification]] and recognition, such as [[score following]], automatic accompaniment, routing and filtering for music and music queries, query languages, standards and other metadata or protocols for music information handling and [[information retrieval|retrieval]], [[multi-agent system]]s, distributed search)
*Software for music information retrieval  [[Semantic Web]] and musical digital objects, intelligent agents, collaborative software, web-based search and [[semantic retrieval]], [[query by humming]], [[acoustic fingerprinting]]
* Music analysis and knowledge representation  automatic summarization, citing, excerpting, downgrading, transformation, formal models of music, digital scores and representations, music indexing and [[metadata]].

==Other issues==
*Human-computer interaction and interfaces  multi-modal interfaces, [[user interface]]s and [[usability]], mobile applications, user behavior
* Music perception, cognition, affect, and emotions  music [[similarity metrics]], syntactical parameters, semantic parameters, musical forms, structures, styles ands, music annotation methodologies
* Music archives, libraries, and digital collections  music [[digital library|digital libraries]], public access to musical archives, benchmarks and research databases
* [[Intellectual property]] rights and music  national and international [[copyright]] issues, [[digital rights management]], identification and traceability
* Sociology and Economy of music  music industry and use of MIR in the production, distribution, consumption chain, user profiling, validation, user needs and expectations, evaluation of music IR systems, building test collections, experimental design and metrics

== See also ==
* [[Audio mining]]
* [[Artificial intelligence]]
* [[Digital rights management]]
* [[Digital signal processing]]
* [[Ethnomusicology]]
* [[Multimedia Information Retrieval]]
* [[Music notation]]
* [[Musicology]]
* [[Parsons code]]
* [[Sound and music computing]]
* [[Music OCR]]

== References ==
{{Reflist}}
* Michael Fingerhut (2004). [http://mediatheque.ircam.fr/articles/textes/Fingerhut04b "Music Information Retrieval, or how to search for (and maybe find) music and do away with incipits"], ''IAML-IASA Congress'', Oslo (Norway), August 813, 2004.

==External links==
* [http://www.ismir.net/ International Society for Music Information Retrieval]
* [http://music-ir.org/ Music Information Retrieval research]
* [http://www.music-ir.org/jdownie_papers/downie_mir_arist37.pdf J. Stephen Downie: Music information retrieval]
* [http://dx.doi.org/10.1561/1500000042 M. Schedl, E. Gomez and J. Urbano: Music Information Retrieval: Recent Developments and Applications]
* [http://www.nowpublishers.com/product.aspx?product=INR&doi=1500000002 Nicola Orio: Music Retrieval: A Tutorial and Review]
* [https://ccrma.stanford.edu/wiki/MIR_workshop_2011 Intelligent Audio Systems: Foundations and Applications of Music Information Retrieval, introductory course at Stanford University's Center for Computer Research in Music and Acoustics]
* [http://biblio.ugent.be/record/470088 Micheline Lesaffre: Music Information Retrieval: Conceptual Framework, Annotation and User behavior.]
* [http://the.echonest.com/ The Echo Nest: a company specialising in MIR research and applications.]
* [http://www.imagine-research.com/ Imagine Research : develops platform and software for MIR applications ]
* [http://www.AudioContentAnalysis.org/ AudioContentAnalysis.org: MIR resources and matlab code ]

==Example MIR applications==
* [http://www.musipedia.org/ Musipedia  A melody search engine that offers several modes of searching, including whistling, tapping, piano keyboard, and Parsons code.]
* [http://www.listengame.org/ The Listen Game  UCSD Computer Audition Lab MIR music ranking game]
* [http://www.peachnote.com/ Peachnote  A melody search engine and n-gram viewer that searches through digitized music scores]

[[Category:Information retrieval]]
[[Category:Music software]]
>>EOP<<
94<|###|>Collaborative filtering
{{external links|date=November 2013}}
{{Use dmy dates|date=June 2013}}
{{Recommender systems}}
[[File:Collaborative filtering.gif|600px|thumb|

This image shows an example of predicting of the user's rating using collaborative filtering. At first, people rate different items (like videos, images, games). After that, the system is making predictions about user's rating for an item, which the user hasn't rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user. For instance, in our case the system has made a prediction, that the active user won't like the video.

]]

'''Collaborative filtering''' ('''CF''') is a technique used by some [[recommender system]]s.<ref name="handbook">Francesco Ricci and Lior Rokach and Bracha Shapira, [http://www.inf.unibz.it/~ricci/papers/intro-rec-sys-handbook.pdf Introduction to Recommender Systems Handbook], Recommender Systems Handbook, Springer, 2011, pp. 1-35</ref> Collaborative filtering has two senses, a narrow one and a more general one.<ref name=recommender>{{cite web|title=Beyond Recommender Systems: Helping People Help Each Other|url=http://www.grouplens.org/papers/pdf/rec-sys-overview.pdf|publisher=Addison-Wesley|accessdate=16 January 2012|page=6|year=2001|last1=Terveen|first1=Loren|last2=Hill|first2=Will}}</ref>  In general, collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc.<ref name="recommender" />  Applications of collaborative filtering typically involve very large data sets.   Collaborative filtering methods have been applied to many different kinds of data including: sensing and monitoring data, such as in mineral exploration, environmental sensing over large areas or multiple sensors; financial data, such as financial service institutions that integrate many financial sources; or in electronic commerce and web applications  where the focus is on user data, etc.  The remainder of this discussion focuses on collaborative filtering for user data, although some of the methods and approaches may apply to the other major applications as well.

In the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or [[taste (sociology)|taste]] information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person ''A'' has the same opinion as a person ''B'' on an issue, A is more likely to have B's opinion on a different issue ''x'' than to have the opinion on x of a person chosen randomly. For example, a collaborative filtering recommendation system for [[television]] tastes could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes).<ref>[http://www.redbeemedia.com/insights/integrated-approach-tv-vod-recommendations An integrated approach to TV & VOD Recommendations]</ref> Note that these predictions are specific to the user, but use information gleaned from many users. This differs from the simpler approach of giving an [[average]] (non-specific) score for each item of interest, for example based on its number of [[vote]]s.

==Introduction==
The growth of the Internet has made it much more difficult to effectively extract useful information from all the available online information. The overwhelming amount of data necessitates  mechanisms for efficient information filtering. One of the techniques used for dealing with this problem is called collaborative filtering.

The motivation for collaborative filtering comes from the idea that people often get the best recommendations from someone with similar tastes to themselves. Collaborative filtering explores techniques for matching people with similar interests and making recommendations on this basis.

Collaborative filtering algorithms often require (1) users active participation, (2) an easy way  to represent users interests to the system, and (3) algorithms that are able to match people with similar interests.

Typically, the workflow of a collaborative filtering system is:
# A user expresses his or her preferences by rating items (e.g. books, movies or CDs) of the system. These ratings can be viewed as an approximate representation of the user's interest in the corresponding domain.
# The system matches this users ratings against other users  and finds the people with most similar tastes.
# With similar users, the system recommends items that the similar users have rated highly but not yet being rated by this user (presumably the absence of rating is often considered as the unfamiliarity of an item)
A key problem of collaborative filtering is how to combine and weight the preferences of user neighbors. Sometimes, users can immediately rate the recommended items. As a result, the system gains an increasingly accurate representation of user preferences over time.

==Methodology==

[[File:Collaborative Filtering in Recommender Systems.jpg|thumb|Collaborative Filtering in Recommender Systems]]

Collaborative filtering systems have many forms, but many common systems can be reduced to two steps:
# Look for users who share the same rating patterns with the active user (the user whom the prediction is for).
# Use the ratings from those like-minded users found in step 1 to calculate a prediction for the active user
This falls under the category of user-based collaborative filtering. A specific application of this is the user-based [[K-nearest neighbor algorithm|Nearest Neighbor algorithm]].

Alternatively, [[item-item collaborative filtering|item-based collaborative filtering]] (users who bought x also bought y), proceeds in an item-centric manner:
# Build an item-item matrix determining relationships between pairs of items
# Infer the tastes of the current user by examining the matrix and matching that user's data
See, for example, the [[Slope One]] item-based collaborative filtering family.

Another form of collaborative filtering can be based on implicit observations of normal user behavior (as opposed to the artificial behavior imposed by a rating task). These systems observe what a user has done together with what all users have done (what music they have listened to, what items they have bought) and use that data to predict the user's behavior in the future, or to predict how a user might like to behave given the chance.  These predictions then have to be filtered through [[business logic]] to determine how they might affect the actions of a business system.  For example, it is not useful to offer to sell somebody a particular album of music if they already have demonstrated that they own that music.

Relying on a scoring or rating system which is averaged across all users ignores specific demands of a user, and is particularly poor in tasks where there is large variation in interest (as in the recommendation of music). However, there are other methods to combat information explosion, such as [[WWW|web]] search and [[data clustering]].

==Types==

===Memory-based===
This mechanism uses user rating data to compute similarity between users or items. This is used for making recommendations. This was the earlier mechanism and is used in many commercial systems. It is easy to implement and is effective. Typical examples of this mechanism are neighbourhood based CF and item-based/user-based top-N recommendations.[3] For example, in user based approaches, the value of ratings user 'u' gives to item 'i' is calculated as an aggregation of some similar users rating to the item:
:<math>r_{u,i} = \operatorname{aggr}_{u^\prime \in U} r_{u^\prime, i}</math>

where 'U' denotes the set of top 'N' users that are most similar to user 'u' who rated item 'i'. Some examples of the aggregation function includes:
:<math>r_{u,i} = \frac{1}{N}\sum\limits_{u^\prime \in U}r_{u^\prime, i}</math>
:<math>r_{u,i} = k\sum\limits_{u^\prime \in U}\operatorname{simil}(u,u^\prime)r_{u^\prime, i}</math>
:<math>r_{u,i} = \bar{r_u} +  k\sum\limits_{u^\prime \in U}\operatorname{simil}(u,u^\prime)(r_{u^\prime, i}-\bar{r_{u^\prime}} )</math>

where k is a normalizing factor defined as <math>k =1/\sum_{u^\prime \in U}|\operatorname{simil}(u,u^\prime)| </math>. and <math>\bar{r_u}</math> is the average rating of user u for all the items rated by that user.

The neighborhood-based algorithm calculates the similarity between two users or items, produces a prediction for the user taking the weighted average of all the ratings. Similarity computation between items or users is an important part of this approach. Multiple mechanisms such as [[Pearson product-moment correlation coefficient|Pearson correlation]] and [[Cosine similarity|vector cosine]] based similarity are used for this.

The Pearson correlation similarity of two users x, y is defined as 
:<math> \operatorname{simil}(x,y) = \frac{\sum\limits_{i \in I_{xy}}(r_{x,i}-\bar{r_x})(r_{y,i}-\bar{r_y})}{\sqrt{\sum\limits_{i \in I_{xy}}(r_{x,i}-\bar{r_x})^2\sum\limits_{i \in I_{xy}}(r_{y,i}-\bar{r_y})^2}} </math>

where I<sub>xy</sub> is the set of items rated by both user x and user y.

The cosine-based approach defines the cosine-similarity between two users x and y as:<ref name="Breese1999">John S. Breese, David Heckerman, and Carl Kadie, [http://uai.sis.pitt.edu/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=231&proceeding_id=14 Empirical Analysis of Predictive Algorithms for Collaborative Filtering], 1998</ref>
:<math>\operatorname{simil}(x,y) = \cos(\vec x,\vec y) = \frac{\vec x \cdot \vec y}{||\vec x|| \times ||\vec y||} = \frac{\sum\limits_{i \in I_{xy}}r_{x,i}r_{y,i}}{\sqrt{\sum\limits_{i \in I_{x}}r_{x,i}^2}\sqrt{\sum\limits_{i \in I_{y}}r_{y,i}^2}}</math>

The user based top-N recommendation algorithm identifies the k most similar users to an active user using similarity based vector model. After the k most similar users are found, their corresponding user-item matrices are aggregated to identify the set of items to be recommended. A popular method to find the similar users is the [[Locality-sensitive hashing]], which implements the [[Nearest neighbor search|nearest neighbor mechanism]] in linear time.

The advantages with this approach include:  the explainability of the results, which is an important aspect of recommendation systems; it is easy to create and use; new data can be added easily and incrementally; it need not consider the content of the items being recommended; and the mechanism scales well with co-rated items.

There are several disadvantages with this approach.  Its performance decreases when data gets sparse, which is frequent with web related items. This prevents the scalability of this approach and has problems with large datasets. Although it can efficiently handle new users because it relies on a data structure, adding new items becomes more complicated since that representation usually relies on a specific vector space. That would require to include the new item and re-insert all the elements in the structure.

===Model-based===
Models are developed using [[data mining]], [[machine learning]] algorithms to find patterns based on training data. These are used to make predictions for real data. There are many model-based CF algorithms. These include [[Bayesian networks]], [[Cluster Analysis|clustering models]], [[Latent Semantic Indexing|latent semantic models]] such as [[singular value decomposition]], [[probabilistic latent semantic analysis]], Multiple Multiplicative Factor, [[Latent Dirichlet allocation]] and [[markov decision process]] based models.<ref name="Suetal2009">Xiaoyuan Su, Taghi M. Khoshgoftaar, [http://www.hindawi.com/journals/aai/2009/421425/ A survey of collaborative filtering techniques], Advances in Artificial Intelligence archive, 2009.</ref>

This approach has a more holistic goal to uncover latent factors that explain observed ratings.<ref>[http://research.yahoo.com/pub/2435 Factor in the Neighbors: Scalable and Accurate Collaborative Filtering]</ref> Most of the models are based on creating a classification or clustering technique to identify the user based on the test set. The number of the parameters can be reduced based on types of [[Principal Component Analysis|principal component analysis]].

There are several advantages with this paradigm. It handles the sparsity better than memory based ones. This helps with scalability with large data sets. It improves the prediction performance. It gives an intuitive rationale for the recommendations.

The disadvantages with this approach are in the expensive model building. One needs to have a tradeoff between prediction performance and scalability. One can lose useful information due to reduction models. A number of models have difficulty explaining the predictions.

===Hybrid===
A number of applications combines the memory-based and the model-based CF algorithms. These overcome the limitations of native CF approaches. It improves the prediction performance. Importantly, it overcomes the CF problems such as sparsity and loss of information. However, they have increased complexity and are expensive to implement.<ref>Kernel Mapping Recommender System Algorithms, www.sciencedirect.com/science/article/pii/S0020025512002587
</ref> Usually most of the commercial recommender systems are hybrid, for example, Google news recommender system.<ref>[http://dl.acm.org/citation.cfm?id=1242610 Google News Personalization: Scalable Online Collaborative Filtering]</ref>

==Application on social web==
Unlike the traditional model of mainstream media, in which there are few editors who set guidelines, collaboratively filtered social media can have a very large number of editors, and content improves as the number of participants increases. Services like [[Reddit]], [[YouTube]], and [[Last.fm]] are typical example of collaborative filtering based media.<ref>[http://www.readwriteweb.com/archives/collaborative_filtering_social_web.php Collaborative Filtering: Lifeblood of The Social Web]</ref>

One scenario of collaborative filtering application is to recommend interesting or popular information as judged by the community. As a typical example, stories appear in the front page of [[Digg]] as they are "voted up" (rated positively) by the community. As the community becomes larger and more diverse, the promoted stories can better reflect the average interest of the community members.

Another aspect of collaborative filtering systems is the ability to generate more personalized recommendations by analyzing information from the past activity of a specific user, or the history of other users deemed to be of similar taste to a given user. These resources are used as user profiling and helps the site recommend content on a user-by-user basis. The more a given user makes use of the system, the better the recommendations become, as the system gains data to improve its model of that user.

===Problems===
A collaborative filtering system does not necessarily succeed in automatically matching content to one's preferences. Unless the platform achieves unusually good diversity and independence of opinions, one point of view will always dominate another in a particular community. As in the personalized recommendation scenario, the introduction of new users or new items can cause the [[cold start]] problem, as there will be insufficient data on these new entries for the collaborative filtering to work accurately. In order to make appropriate recommendations for a new user, the system must first learn the user's preferences by analysing past voting or rating activities. The collaborative filtering system requires a substantial number of users to rate a new item before that item can be recommended.

==Challenges of collaborative filtering==

===Data sparsity===
In practice, many commercial recommender systems are based on large datasets. As a result, the user-item matrix used for collaborative filtering could be extremely large and sparse, which brings about the challenges in the performances of the recommendation.

One typical problem caused by the data sparsity is the [[cold start]] problem. As collaborative filtering methods recommend items based on users past preferences,  new users will need to rate sufficient number of items to enable the system to capture their preferences accurately and thus provides reliable recommendations.

Similarly,  new items also have the same problem. When new items are added to system, they need to be rated by substantial number of users before they could be recommended to users who have similar tastes with the ones rated them. The new item problem does not limit the [[Recommender system#Content-based filtering|content-based recommendation]], because the recommendation of an item is based on its discrete set of descriptive qualities rather than its ratings.

===Scalability===
As the numbers of users and items grow, traditional CF algorithms will suffer serious scalability problems{{Citation needed|date=April 2013}}. For example, with tens of millions of customers <math>O(M)</math> and millions of items <math>O(N)</math>, a CF algorithm with the complexity of <math>n</math> is already too large. As well, many systems need to react immediately to online requirements and make recommendations for all users regardless of their purchases and ratings history, which demands a higher scalability of a CF system. Large web companies such as Twitter use clusters of machines to scale recommendations for their millions of users, with most computations happening in very large memory machines.<ref name="twitterwtf">Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong Wang, and Reza Bosagh Zadeh [http://dl.acm.org/citation.cfm?id=2488433 WTF: The who-to-follow system at Twitter], Proceedings of the 22nd international conference on World Wide Web</ref>

===Synonyms===
[[Synonyms]] refers to the tendency of a number of the same or very similar items to have different names or entries. Most recommender systems are unable to discover this latent association and thus treat these products differently.

For example, the seemingly different items children movie and children film are actually referring to the same item. Indeed, the degree of variability in descriptive term usage is greater than commonly suspected.{{citation needed|date=September 2013}} The prevalence of synonyms decreases the recommendation performance of CF systems. Topic Modeling (like the Latent Dirichlet Allocation technique) could solve this by grouping different words belonging to the same topic.{{citation needed|date=September 2013}}

===Grey sheep===
Grey sheep refers to the users whose opinions do not consistently agree or disagree with any group of people and thus do not benefit from collaborative filtering. [[Black sheep]] are the opposite group whose idiosyncratic tastes make recommendations nearly impossible. Although this is a failure of the recommender system, non-electronic recommenders also have great problems in these cases, so black sheep is an acceptable failure.

===Shilling attacks===
In a recommendation system where everyone can give the ratings, people may give lots of positive ratings  for their own items and negative ratings for their competitors. It is often necessary for the collaborative filtering systems to introduce precautions to discourage such kind of manipulations.

===Diversity and the Long Tail===
Collaborative filters are expected to increase diversity because they help us discover new products. Some algorithms, however, may unintentionally do the opposite. Because collaborative filters recommend products based on past sales or ratings, they cannot usually recommend products with limited historical data. This can create a rich-get-richer effect for popular products, akin to [[positive feedback]]. This bias toward popularity can prevent what are otherwise better consumer-product matches. A [[Wharton School of the University of Pennsylvania|Wharton]] study details this phenomenon along with several ideas that may promote diversity and the "[[long tail]]."<ref>{{cite journal| last1= Fleder | first1= Daniel | first2= Kartik |last2= Hosanagar | title=Blockbuster Culture's Next Rise or Fall: The Impact of Recommender Systems on Sales Diversity|journal=Management Science |date=May 2009|url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=955984}}</ref>

==Innovations==
{{Prose|date=May 2012}}
* New algorithms have been developed for CF as a result of the [[Netflix prize]].
* Cross-System Collaborative Filtering where user profiles across multiple [[recommender systems]] are combined in a privacy preserving manner.
* Robust Collaborative Filtering, where recommendation is stable towards efforts of manipulation. This research area is still active and not completely solved.<ref>{{cite web|url=http://dl.acm.org/citation.cfm?id=1297240 |title=Robust collaborative filtering |doi=10.1145/1297231.1297240 |publisher=Portal.acm.org |date=19 October 2007 |accessdate=2012-05-15}}</ref>

==See also==
* [[Attention Profiling Mark-up Language|Attention Profiling Mark-up Language (APML)]]
* [[Cold start]]
* [[Collaborative model]]
* [[Collaborative search engine]]
* [[Collective intelligence]]
* [[Customer engagement]]
* [[Delegative Democracy]], the same principle applied to voting rather than filtering
* [[Enterprise bookmarking]]
* [[Firefly (website)]], a defunct website which was based on collaborative filtering
* [[Long tail]]
* [[Preference elicitation]]
* [[Recommendation system]]
* [[Relevance (information retrieval)]]
* [[Reputation system]]
* [[Robust collaborative filtering]]
* [[Similarity search]]
* [[Slope One]]
* [[Social translucence]]

==References==
{{Reflist|30em}}

==External links==
*[http://www.grouplens.org/papers/pdf/rec-sys-overview.pdf ''Beyond Recommender Systems: Helping People Help Each Other''], page 12, 2001
*[http://www.prem-melville.com/publications/recommender-systems-eml2010.pdf Recommender Systems.] Prem Melville and Vikas Sindhwani. In Encyclopedia of Machine Learning, Claude Sammut and Geoffrey Webb (Eds), Springer, 2010.
*[http://arxiv.org/abs/1203.4487 Recommender Systems in industrial contexts - PHD thesis (2012) including a comprehensive overview of many collaborative recommender systems]
*[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1423975  Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions]. Adomavicius, G. and Tuzhilin, A. IEEE Transactions on Knowledge and Data Engineering 06.2005
*[http://ectrl.itc.it/home/laboratory/meeting/download/p5-l_herlocker.pdf Evaluating collaborative filtering recommender systems]{{dead link|date=May 2012}} ([http://www.doi.org/ DOI]: [http://dx.doi.org/10.1145/963770.963772 10.1145/963770.963772])
*[http://www.grouplens.org/publications.html GroupLens research papers].
*[http://www.cs.utexas.edu/users/ml/papers/cbcf-aaai-02.pdf Content-Boosted Collaborative Filtering for Improved Recommendations.] Prem Melville, Raymond J. Mooney, and Ramadass Nagarajan. Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), pp.&nbsp;187192, Edmonton, Canada, July 2002.
*[http://agents.media.mit.edu/projects.html A collection of past and present "information filtering" projects (including collaborative filtering) at MIT Media Lab]
*[http://www.ieor.berkeley.edu/~goldberg/pubs/eigentaste.pdf Eigentaste: A Constant Time Collaborative Filtering Algorithm. Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Information Retrieval, 4(2), 133-151. July 2001.]
*[http://downloads.hindawi.com/journals/aai/2009/421425.pdf A Survey of Collaborative Filtering Techniques] Su, Xiaoyuan and Khoshgortaar, Taghi. M
*[http://dl.acm.org/citation.cfm?id=1242610 Google News Personalization: Scalable Online Collaborative Filtering] Abhinandan Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. International World Wide Web Conference, Proceedings of the 16th international conference on World Wide Web
*[http://research.yahoo.com/pub/2435 Factor in the Neighbors: Scalable and Accurate Collaborative Filtering] Yehuda Koren, Transactions on Knowledge Discovery from Data (TKDD) (2009)
*[http://webpages.uncc.edu/~asaric/ISMIS09.pdf Rating Prediction Using Collaborative Filtering]
*[http://www.cis.upenn.edu/~ungar/CF/ Recommender Systems]
*[http://www2.sims.berkeley.edu/resources/collab/ Berkeley Collaborative Filtering]

{{DEFAULTSORT:Collaborative Filtering}}
[[Category:Collaboration]]
[[Category:Collaborative software| Collaborative filtering]]
[[Category:Collective intelligence]]
[[Category:Information retrieval]]
[[Category:Recommender systems]]
[[Category:Social information processing]]
[[Category:Behavioral and social facets of systemic risk]]
>>EOP<<
100<|###|>Legal information retrieval
'''Legal information retrieval''' is the science of [[information retrieval]] applied to legal text, including [[legislation]], [[case law]], and scholarly works.<ref>Maxwell, K.T., and Schafer, B. 2009, p. 1</ref> Accurate legal information retrieval is important to provide access to the law to laymen and legal professionals. Its importance has increased because of the vast and quickly increasing amount of legal documents available through electronic means.<ref name=Jackson>Jackson et al., p. 60</ref> Legal information retrieval is a part of the growing field of [[legal informatics]].  

== Overview ==

In a legal setting, it is frequently important to retrieve all information related to a specific query. However, commonly used [[boolean search]] methods (exact matches of specified terms) on full text legal documents have been shown to have an average [[recall rate]] as low as 20 percent,<ref name="Blair, D.C. 1985, p.293">Blair, D.C., and Maron, M.E., 1985, p.293</ref> meaning that only 1 in 5 relevant documents are actually retrieved. In that case, researchers believed that they had retrieved over 75% of relevant documents.<ref name="Blair, D.C. 1985, p.293"/> This may result in failing to retrieve important or [[precedential]] cases. In some jurisdictions this may be especially problematic, as legal professionals are [[legal ethics|ethically]] obligated to be reasonably informed as to relevant legal documents.<ref>American Bar Association, Model Rules of Professional Conduct Rule 1.1, http://www.abanet.org/cpr/mrpc/rule_1_1.html</ref> 

Legal Information Retrieval attempts to increase the effectiveness of legal searches by increasing the number of relevant documents (providing a high [[recall rate]]) and reducing the number of irrelevant documents (a high [[precision rate]]). This is a difficult task, as the legal field is prone to [[jargon]],<ref>Peters, W. et al. 2007, p. 118</ref> [[polysemes]]<ref>Peters, W. et al. 2007, p. 130</ref> (words that have different meanings when used in a legal context), and constant change. 

Techniques used to achieve these goals generally fall into three categories: [[boolean search|boolean]] retrieval, manual classification of legal text, and [[natural language processing]] of legal text.

== Problems ==

Application of standard [[information retrieval]] techniques to legal text can be more difficult than application in other subjects. One key problem is that the law rarely has an inherent [[Taxonomy (general)|taxonomy]].<ref name=LOIS1>Peters, W. et al. 2007, p. 120</ref> Instead, the law is generally filled with open-ended terms, which may change over time.<ref name=LOIS1 /> This can be especially true in [[common law]] countries, where each decided case can subtly change the meaning of a certain word or phrase.<ref>Saravanan, M. et al.  2009, p. 101</ref>

Legal information systems must also be programmed to deal with law-specific words and phrases. Though this is less problematic in the context of words which exist solely in law, legal texts also frequently use polysemes, words may have different meanings when used in a legal or common-speech manner, potentially both within the same document. The legal meanings may be dependent on the area of law in which it is applied. For example, in the context of European Union legislation, the term "worker" has four different meanings:<ref name="Peters, W. et al. 2007, p. 131">Peters, W. et al. 2007, p. 131</ref> 

#Any worker as defined in Article 3(a) of [[Directive 89/391/EEC]] who habitually uses display screen equipment as a significant part of his normal work.
#Any person employed by an employer, including trainees and apprentices but excluding domestic servants;
#Any person carrying out an occupation on board a vessel, including trainees and apprentices, but excluding port pilots and shore personnel carrying out work on board a vessel at the quayside;
#Any person who, in the Member State concerned, is protected as an employee under national employment law and in accordance with national practice;

In addition, it also has the common meaning: 
<ol start="5">
<li>A person who works at a specific occupation.<ref name="Peters, W. et al. 2007, p. 131"/> </li>
</ol>

Though the terms may be similar, correct information retrieval must differentiate between the intended use and irrelevant uses in order to return the correct results. 

Even if a system overcomes the language problems inherent in law, it must still determine the relevancy of each result. In the context of judicial decisions, this requires determining the precedential value of the case.<ref name=MaxwellA >Maxwell, K.T., and Schafer, B. 2008, p. 8</ref> Case decisions from senior or [[superior court]]s may be more relevant than those from [[lower court]]s, even where the lower court's decision contains more discussion of the relevant facts.<ref name=MaxwellA  /> The opposite may be true, however, if the senior court has only a minor discussion of the topic (for example, if it is a secondary consideration in the case).<ref name=MaxwellA  /> A information retrieval system must also be aware of the authority of the jurisdiction. A case from a binding authority is most likely of more value than one from a non-binding authority.

Additionally, the intentions of the user may determine which cases they find valuable. For instance, where a legal professional is attempting to argue a specific interpretation of law, he might find a minor court's decision which supports his position more valuable than a senior courts position which does not.<ref name=MaxwellA  /> He may also value similar positions from different areas of law, different jurisdictions, or dissenting opinions.<ref name=MaxwellA />

Overcoming these problems can be made more difficult because of the large number of cases available. The number of legal cases available via electronic means is constantly increasing (in 2003, US appellate courts handed down approximately 500 new cases per day<ref name=Jackson />), meaning that an accurate legal information retrieval system must incorporate methods of both sorting past data and managing new data.<ref name=Jackson /><ref>Maxwell, K.T., and Schafer, B. 2007, p.1</ref>

== Techniques ==

===Boolean searches===

[[Boolean search]]es, where a user may specify terms such as use of specific words or judgments by a specific court, are the most common type of search available via legal information retrieval systems. They are widely implemented by services such as [[Westlaw]], [[LexisNexis]], and [[Findlaw]].  However, they overcome few of the problems discussed above. 

The recall and precision rates of these searches vary depending on the implementation and searches analyzed. One study found a basic boolean search's [[recall rate]] to be roughly 20%, and its precision rate to be roughly 79%.<ref name="Blair, D.C. 1985, p.293"/> Another study implemented a generic search (that is, not designed for legal uses) and found a recall rate of 56% and a precision rate of 72% among legal professionals. Both numbers increased when searches were run by non-legal professionals, to a 68% recall rate and 77% precision rate. This is likely explained because of the use of complex legal terms by the legal professionals.<ref>Saravanan M., et al. 2009, p. 116</ref>

===Manual classification===

In order to overcome the limits of basic boolean searches, information systems have attempted to classify case laws and statutes into more computer friendly structures. Usually, this results in the creation of an [[ontology]] to classify the texts, based on the way a legal professional might think about them.<ref name="Maxwell, K.T. 2008, p. 2">Maxwell, K.T., and Schafer, B. 2008, p. 2</ref> These attempt to link texts on the basis of their type, their value, and/or their topic areas. Most major legal search providers now implement some sort of classification search, such as [[Westlaw]]'s Natural Language<ref name=WL>Westlaw Research, http://www.westlaw.com</ref> or [[LexisNexis]]' Headnote<ref name=LN>Lexis Research, http://www.lexisnexis.com</ref> searches. Additionally, both of these services allow browsing of their classifications, via Westlaw's West Key Numbers<ref name=WL /> or Lexis' Headnotes.<ref name=LN /> Though these two search algorithms are proprietary and secret, it is known that they employ manual classification of text (though this may be computer-assisted).<ref name="Maxwell, K.T. 2008, p. 2"/>

These systems can help overcome the majority of problems inherent in legal information retrieval systems, in that manual classification has the greatest chances of identifying landmark cases and understanding the issues that arise in the text.<ref name="Maxwell, K.T. 2008, p. 3">Maxwell, K.T., and Schafer, B. 2008, p. 3</ref> In one study, ontological searching resulted in a precision rate of 82% and a recall rate of 97% among legal professionals.<ref>Saravanan, M. et al.  2009, p. 116</ref> The legal texts included, however, were carefully controlled to just a few areas of law in a specific jurisdiction.<ref>Saravanan, M. et al. 2009, p. 103</ref>

The major drawback to this approach is the requirement of using highly skilled legal professionals and large amounts of time to classify texts.<ref name="Maxwell, K.T. 2008, p. 3"/><ref>Schweighofer, E. and Liebwald, D. 2008, p. 108</ref> As the amount of text available continues to increase, some have stated their belief that manual classification is unsustainable.<ref>Maxwell, K.T., and Schafer, B. 2008, p. 4</ref>

===Natural language processing===

In order to reduce the reliance on legal professionals and the amount of time needed, efforts have been made to create a system to automatically classify legal text and queries.<ref name=Jackson /><ref name=AshleyA>Ashley, K.D. and Bruninghaus, S. 2009, p. 125</ref><ref name=Gelbart>Gelbart, D. and Smith, J.C. 1993, p. 142</ref> Adequate translation of both would allow accurate information retrieval without the high cost of human classification. These automatic systems generally employ [[Natural Language Processing]] (NLP) techniques that are adapted to the legal domain, and also require the creation of a legal [[ontology]]. Though multiple systems have been postulated,<ref name=Jackson /><ref name=AshleyA /><ref name=Gelbart /> few have reported results. One system, SMILE, which attempted to automatically extract classifications from case texts, resulted in an [[f-measure]] (which is a calculation of both recall rate and precision) of under 0.3 (compared to perfect f-measure of 1.0).<ref name=AshleyB >Ashley, K.D. and Bruninghaus, S. 2009, p. 159</ref> This is probably much lower than an acceptable rate for general usage.<ref name=AshleyB /><ref>Maxwell, K.T., and Schafer, B. 2009, p. 3</ref>

Despite the limited results, many theorists predict that the evolution of such systems will eventually replace manual classification systems.<ref>Maxwell, K.T., and Schafer, B. 2009, p. 9</ref><ref>Ashley, K.D. and Bruninghaus, S. 2009, p. 126</ref>

== Notes ==
{{Reflist|2}}

==References==
{{Refbegin}}
*{{cite journal
|author     = Maxwell, K.T., and Schafer, B.
|year       = 2008
|title      = Concept and Context in Legal Information Retrieval
|url        = http://portal.acm.org/citation.cfm?id=1564016
|journal    = Frontiers in Artificial Intelligence and Applications
|volume     = 189
|pages      = 6372
|publisher  = IOS Press
|accessdate = 2009-11-07
}}
*{{cite journal
|author     = Jackson, P. et al.
|year       = 1998
|title      = Information extraction from case law and retrieval of prior cases by partial parsing and query generation
|url        = http://portal.acm.org/citation.cfm?id=288627.288642
|journal    = Conference on Information and Knowledge Management
|pages      = 6067
|publisher  = ACM
|accessdate = 2009-11-07
}}
*{{cite journal
|author     = Blair, D.C., and Maron, M.E.
|year       = 1985
|title      = An evaluation of retrieval effectiveness for a full-text document-retrieval
|url        = http://portal.acm.org/citation.cfm?id=3166.3197&coll=GUIDE&dl=GUIDE&CFID=61732097&CFTOKEN=95519997
|journal    = Communications of the ACM
|volume     = 28
|issue      = 3 
|pages      = 289299
|publisher  = ACM
|accessdate = 2009-11-07
|doi=10.1145/3166.3197
}}
*{{cite journal
|author     = Peters, W. et al.
|year       = 2007
|title      = The structuring of legal knowledge in LOIS
|url        = http://www.springerlink.com/content/d04l7h2507700g45/
|journal    = Artificial Intelligence and Law
|volume     = 15
|issue      = 2
|pages      = 117135
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-007-9034-4
}}
*{{cite journal
|author     = Saravanan, M. et al.
|year       = 2007
|title      = Improving legal information retrieval using an ontological framework 
|url        = http://www.springerlink.com/content/h66412k08h855626/
|journal    = Artificial Intelligence and Law
|volume     = 17
|issue      = 2
|pages      = 101124
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-009-9075-y
}}
*{{cite journal
|author     = Schweighofer, E. and Liebwald, D.
|year       = 2007
|title      = Advanced lexical ontologies and hybrid knowledge based systems: First steps to a dynamic legal electronic commentary
|url        = http://www.springerlink.com/content/v62v7131x10413v0/
|journal    = Artificial Intelligence and Law
|volume     = 15
|issue      = 2
|pages      = 103115
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-007-9029-1
}}
*{{cite journal
|author     = Gelbart, D. and Smith, J.C.
|year       = 1993
|title      = FLEXICON: an evaluation of a statistical ranking model adapted to intelligent legal text management
|url        = http://portal.acm.org/citation.cfm?id=158994
|journal    = International Conference on Artificial Intelligence and Law
|pages      = 142151
|publisher  = ACM
|accessdate = 2009-11-07
}}
*{{cite journal
|author     = Ashley, K.D. and Bruninghaus, S.
|year       = 2009
|title      = Automatically classifying case texts and predicting outcomes
|url        = http://www.springerlink.com/content/lhg8837331hgu024/
|journal    = Artificial Intelligence and Law
|volume     = 17
|issue      = 2
|pages      = 125165
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-009-9077-9
}}
{{Refend}}

{{DEFAULTSORT:Legal Information Retrieval}}
[[Category:Information retrieval]]
[[Category:Natural language processing]]
[[Category:Legal research]]
>>EOP<<
106<|###|>Policy framework
{{refimprove|date=March 2009}}
A '''policy framework''' is a logical structure that is established to organize policy documentation into groupings and categories that make it easier for employees to find and understand the contents of various [[policy]] documents. Policy frameworks can also be used to help in the planning and development of the policies for an organization.

==Principles==
[[State Services Commission]] of [[New Zealand]] outlines eleven principles of policy framework as below.<ref>http://www.ssc.govt.nz/Documents/policy_framework_for_Government_.htm</ref>

===Availability===
Government departments should make information available easily, widely and equitably to the people of New Zealand (except where reasons preclude such availability as specified in legislation).....

===Coverage===
Government departments should make the following information increasingly available on an electronic basis:
* all published material or material already in the public domain
* all policies that could be released publicly
* all information created or collected on a statutory basis (subject to commercial sensitivity and privacy considerations)
* all documents that the public may be required to complete
* corporate documentation in which the public would be interested

===Pricing=== 
a) Free dissemination of Government-held information is appropriate where:
* dissemination to a target audience is desirable for a public policy purpose, or
* a charge to recover the cost of dissemination is not feasible or cost-effective

b) Pricing to recover the cost of dissemination is appropriate where:
* there is no particular public policy reason to disseminate the information, and 
* a charge to recover the cost of dissemination is both feasible and cost effective

c) Pricing to recover the cost of transformation is appropriate where:
* pricing to recover the cost of dissemination is appropriate, and
* there is an avoidable cost involved in transforming the information from the form in which it is held into a form preferred by the recipient, where it is feasible and cost-effective to recover in addition to the cost of dissemination

d) Pricing to recover the full costs of information production and dissemination is appropriate where:
* the information is created for the commercial purpose of sale at a profit, and 
* to do so would not breach the other pricing principles

===Ownership===
Government-held information, created or collected by any person employed or engaged by the Crown is a strategic resource 'owned' by the Government as a steward on behalf of the public.

===Stewardship===
Government departments are stewards of Government-held information, and it is their responsibility to implement good information management.

===Collection===
Government departments should only collect information for specified public policy, operational business or legislative purposes.

===Copyright===
Information created by departments is subject to Crown copyright but where wide dissemination is desirable, the Crown should permit use of its copyrights subject to acknowledgement of source.
 
===Preservation===
Government-held information should be preserved only where a public business need, legislative or policy requirement, or a historical or archival reason, exists.

===Quality===
The key qualities underpinning Government-held information include accuracy, relevancy, timeliness, consistency and collection without bias so that the information supports the purposes for which it is collected.

===Integrity===
The integrity of Government-held information will be achieved when:
* all guarantees and conditions surrounding the information are met
* the principles are clear and communicated
* any situation relating to Government-held information is handled openly and consistently
* those affected by changes to Government-held information are consulted on those changes
* those charged as independent guardians of the public interest  (e.g. the Ombudsman) have confidence in the ability of departments to manage the information well
* there are minimum exceptions to the principles.

===Privacy===
The principles of the Privacy Act 1993 apply.

==References==
{{reflist}}

{{DEFAULTSORT:Policy Framework}}
[[Category:Information retrieval]]
[[Category:Government of New Zealand]]
>>EOP<<
112<|###|>Literature-based discovery
'''Literature-based discovery''' refers to the use of papers and other [[Academic publishing|academic publications]] (the "literature") to find new relationships between existing knowledge (the "discovery"). The technique was pioneered by [[Don R. Swanson]] in the 1980s and has since seen widespread use. 

Literature-based discovery does not generate new knowledge through laboratory experiments, as is customary for [[empirical]] sciences. Instead it seeks to connect existing knowledge from empirical results by bringing to light relationships that are implicated and "neglected".<ref>{{cite journal | last1 = Swanson | first1 = Don | year = 1988 | title = Migraine and Magnesium: Eleven Neglected Connections | url = | journal = Perspectives in Biology and Medicine | volume = 31 | issue = 4| pages = 526557 }}</ref> It is marked by [[empiricism]] and [[rationalism]] in concert or [[consilience]].

==Swanson linking==
[[File:Swanson linking.jpg|thumb|Swanson linking example diagram]]
''Swanson linking'' is a term proposed in 2003<ref>Stegmann J, Grohmann G. Hypothesis generation guided by co-word clustering. Scientometrics. 2003;56:111135. As quoted by Bekhuis</ref> that refers to connecting two pieces of knowledge previously thought to be unrelated.<ref>{{cite journal|last=Bekhuis|first=Tanja|title=Conceptual biology, hypothesis discovery, and text mining: Swanson's legacy|publisher=BioMed Central Ltd.|year=2006|pmc=1459187|pmid=16584552|doi=10.1186/1742-5581-3-2|volume=3|journal=Biomed Digit Libr|pages=2}}</ref> For example, it may be known that illness A is caused by chemical B, and that drug C is known to reduce the amount of chemical B in the body. However, because the respective articles were published separately from one another (called "disjoint data"), the relationship between illness A and drug C may be unknown. ''Swanson linking'' aims to find these relationships and report them.

==See also==
*[[Arrowsmith System]]
*[[Implicature]]
*[[Latent semantic indexing]]
*[[Metaphor]]

==References==
* Chen, Ran; Hongfei Lin & Zhihao Yang (2011). "Passage retrieval based hidden knowledge discovery from biomedical literature." ''Expert Systems with Applications: An International Journal'' (August, 2011), vol. 38, no. 8, pp.&nbsp;99589964.
*:  '''Abstract''': [...] automatic extraction of the implicit biological relationship from biomedical literature contributes to building the biomedical hypothesis that can be explored further experimentally. This paper presents a passage retrieval based method which can explore the hidden connection from MEDLINE records. [...] Experimental results show this method can significantly improve the hidden knowledge discovery performance. @ [http://portal.acm.org/citation.cfm?id=1967763.1968003&coll=DL&dl=GUIDE&CFID=23143258&CFTOKEN=52033794 ACM DL]

; Further readings
* [[Patrick Wilson (librarian)|Wilson, Patrick]] (1977). ''Public Knowledge, Private Ignorance: Toward a Library and Information Policy''. Greenwood Publishing Group. p.&nbsp;156. ISBN 0-8371-9485-7.

; Footnotes
{{reflist}}

[[Category:Information retrieval]]
[[Category:Medical research]]


{{science-stub}}
>>EOP<<
118<|###|>Masterseek
{{refimprove|date=July 2014}}
{{Infobox company
| name     = Masterseek
| logo     = [[Image:masterseek logo.png|260px]]
| type     = [[Private company|Private]]
| traded_as        = 
| foundation       = [[Denmark]] (1999)
| founder          = [[Rasmus Refer]]
| location_city    = [[New York City]]
| location_country = {{nowrap|United States}}
| area_served      = Worldwide
| key_people       = Rasmus Refer <small>(Co-Founder, [[Chief executive officer|CEO]])</small><br />Jrgen Trygved <small>(Co-Founder)</small><br />Qasim Raza <small>([[Chief technology officer|CTO]])</small><br />Robert Perz <small>(COO, 2005-08)</small>
| industry         = Internet<br>Computer software
| products         = [[Business-to-business|B2B]] [[Search Engine]]
| revenue          = 
| operating_income =
| net_income       = 
| assets           = 
| num_employees    =
| subsid           = [[Accoona]]
| homepage         = {{URL|http://www.masterseek.com/}}
| intl = yes
}}

'''Masterseek Corp.''' is a [[Business-to-business|B2B]] (Business to Business) [[search engine]] founded in [[Denmark]] in 1999.<ref name="hartzer"/> It currently hosts over 83 million worldwide company profiles from 75 countries,<ref name="usti"/> and business subscribers are given complete control over their corporate profiles.<ref name="hartzer"/> According to the amount of listed profiles, they are the largest B2B search engine worldwide.<ref name="seochat"/>

==Founding==
<!-- Deleted image removed: [[File: Rasmus refer.png|thumb|150px|left|[[Rasmus Refer]]]] -->
Masterseek was founded in Denmark by [[Rasmus Refer]] in 1999.<ref name="hartzer"/> Their Denmark headquarters is located at Bredgade 29, DK-1260 Kbh. K, and they also have a current headquarters in [[New York City]], at 82 [[Wall Street]].<ref name="csc"/>

According to its executives, Masterseek utilizes a business model based on an annual business subscription fee of USD $149, in return for which subscribers receive full editing control over their corporate profile, content and advertising, and control over widgets and embedded video, among other factors.<ref name="betaversion"/>

==Finances==
As of June 2008, accountancy firm Horwart International had approximated the raw market value of the Masterseek company at $150 million.<ref name="investors"/> The company remains privately owned, but also in June 2008, it sold 10% of its authorized stocks to a range of foreign investors.<ref name="investors"/> The company announced on January 31, 2009 that they company was again offering a limited number of shares for sale in order to raise $46 million in order to gain a listing on the [[Swedish people|Swedish]] marketplace AktieTorget. Founder Refer also announced there were plans for an [[Initial Public Offering|IPO]].<ref name="ipo"/> By October 2009, they had signed with the Swedish-based company Thenberg & Kinde Fondkommission AB for financing.<ref name="seochat"/>

In the Danish company register the name Masterseek is coming up in three bankruptcies and one compulsory dissolution.<ref name="businessdk-cheats" />

==Statistics==
In June 2008, the company stated it had 50 million company profiles, from over 75 countries, and handled 90,000 B2B searches daily.<ref name="hartzer"/><ref name="ipo"/><ref name="strengthen"/> The company stated they had 82 million profiles on March 21, 2011, with an average of 300,000 new profiles added monthly.<ref name="betaversion"/>

==Acquiring Accoona==
[[File:accoona logo.png|right|220px]]
On October 30, 2008, it was announced that Masterseek had acquired the B2B search engine [[Accoona]].<ref name="hartzer"/><ref name="paidcontent"/> 
"When [Business Insider] first heard about the money-losing Jersey City-based startup filing for IPO last year, [their] impulse was to run away screaming."<ref name="accoonnaddead"/> The search engine had been fairly successful in the United States and [[China]],<ref name="search"/> where it had an exclusive partnership with ''[[China Daily News]]''.<ref name="accoonnaddead"/> On August 3, 2006, ''[[Time Magazine|TIME]]'' had dubbed Accoona one of its "50 best websites," illustrating how the search engine used [[artificial intelligence]] to "understand" the meaning of keyword queries.<ref name="coolest"/> Accoona had run into difficulties and gone defunct by early October 2008, withdrawing its [[Initial Public Offering|IPO]].<ref name="accoonnaddead"/>

After Masterseek bought the remaining search engine codes, domain name, and assets,<ref name="hartzer"/> Accoona was integrated with Masterseek, and re-launched in the USA and China. It was launched in Europe in January 2009.<ref name="search"/> Accoona information was also integrated into the Masterseek search engine.<ref name="hartzer"/>

==Technology==
The Masterseek search engine relies on web crawlers that automatically collect and sort company details from the internet.<ref name="strengthen"/> Searches can look up company profiles, contact information, and descriptions of products and services. Searches can be global, national, regional, or involved local markets. Hits are listed by relevance according to search terms. There are different search options, including a specific product search, company searches, and people searches. Results can be displayed in most languages.<ref name="seochat"/> The search engine also offers MasterRank, a point system for ranking corporate websites.<ref name="Bussinessweek"/>

===Beta version===
Masterseek released a new [[Beta Version]] of its search engine on March 25, 2011. Before then, it was accessible to business owners, managers, and other professionals, but the Beta Version made searching the site free and open to the public.<ref name="betaversion"/>

== Warnings against Masterseek Corp. ==
In December 2009 the Swedish [[Financial Supervisory Authority (Sweden)|Financial Supervisory Authority]] issued a warning against Masterseek Corp. to warn investors <ref name="fise" /><ref name="businessdk" />

In February 2010 the  issued a warning against Masterseek and related companies Bark Group and Blogger Wave.<ref name="shareholdersdk" />

==Sponsorships==
On July 5, 2007, Masterseek announced they were cosponsors to [[Team CSC]], Denmark's cycling team, beginning with the team's involvement in the [[Tour de France]]. The Masterseek name began to be displayed on the team's apparel that week, with the Tour's start in [[London]].<ref name="csc"/>

==Management==
*[[Rasmus Refer]] - Founder, Director, President of Technology
*Qasim Raza - Chief Technology Officer<ref name="Bussinessweek"/>

==See also==
*[[Accoona]]
*[[Business-to-business|B2B]]
*[[Search engines]]

== References ==
{{reflist|2| refs =

<ref name="shareholdersdk">{{cite news
|url=http://www.shareholders.dk/art/templates/pressemeddelelse.aspx?articleid=512&zoneid=29
|title=Danish Shareholders Association warns against Bark Group, Blogger Wave and Masterseek
|publisher=[http://shareholders.dk/ Dansk Aktionrforening]
|accessdate=2014-07-27}}</ref>

<ref name="businessdk">{{cite news
|url=http://www.business.dk/digital/it-firma-snyder-investorer
|title=IT-firma snyder investorer (IT firm cheating investors)
|publisher=[http://business.dk/ Business.dk]
|accessdate=2014-07-03}}</ref>

<ref name="businessdk-cheats">{{cite news
|url=http://bizzen.blogs.business.dk/2010/02/09/plattenslagere-skamrider-danske-varem%C3%A6rker-som-carlsberg-danisco-og-coop/
|title=Plattenslagere skamrider danske varemrker som Carlsberg, Danisco og Coop (Cheats shame rides Danish brands)
|publisher=[http://business.dk/ Business.dk]
|accessdate=2014-07-25}}</ref>

<ref name="fise">{{cite news
|url=http://www.fi.se/Folder-EN/Startpage/Register/Investor-alerts/Warning-list/Warning-against-Masterseek-Corp/
|title=Warning against Masterseek Corp.
|publisher=Finansinspektionen
|accessdate=2014-07-03}}</ref>

<ref name="Bussinessweek">{{cite news
|url=http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=29327873
|title=Masterseek Corp.
|last=
|first=
|date=
|publisher=''[[Businessweek]]''
|accessdate=2011-05-08}}</ref>

<ref name="coolest">{{cite news
|url=http://www.time.com/time/business/article/0,8599,1222614,00.html
|title=50 Coolest Websites: 2006
|last=Buechner
|first=Maryanne
|date=August 3, 2006
|publisher=''[[Time Magazine|TIME]]''
|accessdate=2011-05-08}}</ref>

<ref name="strengthen">{{cite news
|url=http://www.reuters.com/article/2008/06/26/idUS149456+26-Jun-2008+MW20080626
|title=Global Business Search Engine to Strengthen Its Advertising Network for B2B Search
|last=
|first=
|date=June 26, 2008
|publisher=''[[Reuters]]''
|accessdate=2011-05-08}}</ref>

<ref name="investors">{{cite news
|url=http://www.reuters.com/article/2008/06/27/idUS85249+27-Jun-2008+MW20080627
|title=Search Engine is Looking for Strategic Investors
|last=
|first=Masterseek
|date=June 27, 2008
|publisher=''[[Reuters]]''
|accessdate=2011-05-08}}</ref>

<ref name="accoonnaddead">{{cite news
|url=http://www.businessinsider.com/2008/10/dead-search-engine-accoona-officially-dead
|title=Dead Search Engine Accoona Officially Dead
|last=Krangel
|first=Eric
|date=October 3, 2008
|publisher=''[[Business Insider]]''
|accessdate=2011-05-08}}</ref>

<ref name="hartzer">{{cite news
|url=https://www.billhartzer.com/pages/b2b-search-engine-accoona-acquired-by-masterseek/
|title=B2B Search Engine Accoona Acquired by Masterseek
|last=Hartzer
|first=Bill
|date=November 5, 2008
|publisher=BillHartzer.com: Search Engine Marketing
|accessdate=2011-05-08}}</ref>

<ref name="search">{{cite news
|url=http://blog.searchenginewatch.com/081105-115108
|title=Accoona Acquired by Masterseek
|last=Johnson
|first=Nathania 
|date=November 5, 2008
|publisher=''[[Search Engine Watch]]''
|accessdate=2011-05-08}}</ref>

<ref name="paidcontent">{{cite news
|url=http://paidcontent.org/article/419-almost-dead-search-engine-accoona-bought-by-denmarks-masterseek/
|title=Almost-Dead Search Engine Accoona Bought by Denmark's Masterseek
|last=
|first=
|date=November 2008
|publisher=''Paid Content''
|accessdate=2011-05-08}}</ref>

<ref name="ipo">{{cite news
|url=http://www.reuters.com/article/2009/01/31/idUS85625+31-Jan-2009+MW20090131
|title=Masterseek.com Is Planning for an IPO
|last=
|first=Masterseek
|date=January 31, 2009
|publisher=''[[Reuters]]''
|accessdate=2011-05-08}}</ref>

<ref name="seochat">{{cite news
|url=http://www.seochat.com/c/a/Search-Engine-News/Masterseek-a-Global-Business-Search-Engine/
|title=Masterseek: a Global Business Search Engine
|last=Morgan
|first=KC
|date=October 27, 2009
|publisher=''SEOchat''
|accessdate=2011-05-08}}</ref>

<ref name="betaversion">{{cite news
|url=http://www.wiredprnews.com/2011/03/21/masterseek-expeced-to-release-beta-version-of-search-engine-continue-dominance-in-business-to-business-search_2011032117895.html
|title=Masterseek Expected to Release Beta Version of Search Engine
|last=
|first=
|date=March 21, 2011
|publisher=WirePRNews
|accessdate=2011-05-08}}</ref>

<ref name="usti">{{cite news
|url=http://usbusinesstimes.com/internet/3424-business-search-powerhouse-masterseek-partners-with-cutting-edge-job-database-simply-hired.html
|title=Masterseek.com Partners with Simply Hired
|last=
|first=admin
|date=April 16, 2011
|publisher=''US Business Times''
|accessdate=2011-05-08}}</ref>

<ref name="csc">{{cite news
|url=http://www.global-business-profiles.com/masterseek-cosponsors-team-csc/
|title=Masterseek Cosponsors Team Csc
|last=
|first=
|date=May 4, 2011
|publisher=Global Business Profiles
|accessdate=2011-05-08}}</ref>

}}

==External links==
*{{Official website|http://www.masterseek.com/}}
*[http://twitter.com/#!/masterseek_tw1 Masterseek] on [[Twitter]]

[[Category:Internet search engines]]
[[Category:Web service providers]]
[[Category:Internet properties established in 1999]]
[[Category:Business services companies established in 1999]]
[[Category:Information retrieval]]
[[Category:Online companies]]
[[Category:Software companies established in 1999]]

[[da:Dansk Aktionrforening|Danish Shareholders Association]]
>>EOP<<
124<|###|>Automatic Content Extraction
{{Multiple issues|
{{citation style|date=December 2011}}
{{technical|date=October 2012}}
{{abbreviations|date=October 2012}}
}}
'''Automatic Content Extraction (ACE)''' is a program for developing advanced [[Information extraction]] [[technologies]]. Given a text in [[natural language]], the ACE challenge is to detect:
# '''entities''' mentioned in the text, such as: persons, organizations, locations, facilities, weapons, vehicles, and geo-political entities.
# '''relations''' between entities, such as: person A is the manager of company B. Relation types include: role, part, located, near, and social.
# '''events''' mentioned in the text, such as: interaction, movement, transfer, creation and destruction.

This program began with a [[pilot study]] in 1999.

While the ACE program is directed toward extraction of information from [[Sound|audio]] and [[image]] sources in addition to pure text, the research effort is restricted to information extraction from text. The actual [[transduction (machine learning)|transduction]] of audio and image data into text is not part of the ACE research effort, although the processing of ASR and OCR output from such transducers is.

The program relates to [[English language|English]], [[Arabic language|Arabic]] and [[Chinese language|Chinese]] texts.

The effort involves:
* defining the research tasks in detail,
* collecting and annotating data needed for training, development, and evaluation,
* supporting the research with evaluation tools and [[research workshop]]s.

In general objective, the ACE program is motivated by and addresses the same issues as the MUC program that preceded it. The ACE program, however, defines the research objectives in terms of the target objects (i.e., the entities, the relations, and the events) rather than in terms of the words in the text. For example, the so-called named entity task, as defined in MUC, is to identify those words (on the page) that are names of entities. In ACE, on the other hand, the corresponding task is to identify the entity so named. This is a different task, one that is more abstract and that involves inference more explicitly in producing an
answer. In a real sense, the task is to detect things that arent there.

The ACE corpus is one of the standard benchmarks for testing new information extraction [[algorithm]]s.

==References==
* [http://www.citeulike.org/user/erelsegal-halevi/article/10003935 George Doddington@NIS T, Alexis Mitchell@LD C, Mark Przybocki@NIS T, Lance Ramshaw@BB N, Stephanie Strassel@LD C, Ralph Weischedel@BB N. The automatic content extraction (ACE) programtasks, data, and evaluation. 2004]

==External links==
* [http://www.itl.nist.gov/iaui/894.02/related_projects/muc/ MUC] - ACE's predecessor.
* [http://projects.ldc.upenn.edu/ace/ ACE] (LDC)
* [http://www.itl.nist.gov/iad/894.01/tests/ace/ ACE] (NIST)

[[Category:Information retrieval]]
>>EOP<<
130<|###|>Question answering
{{other uses|question|answer}}
{{multiple issues|
{{cleanup|date=January 2012|reason=extensive use of jargon to define jargon, and inconsistent use of bold and italics font styles}}
{{cleanup-rewrite|date=January 2012}}
{{more footnotes|date=February 2014}}
}}

'''Question Answering''' ('''QA''') is a computer science discipline within the fields of [[information retrieval]] and [[natural language processing]] (NLP), which is concerned with building systems that automatically answer questions posed by humans in a [[natural language]].

A QA implementation, usually a computer program, may construct its answers by querying a structured [[database]] of knowledge or information, usually a [[knowledge base]]. More commonly, QA systems can pull answers from an unstructured collection of natural language documents<ref>"[https://www.academia.edu/2475776/Versatile_question_answering_systems_seeing_in_synthesis Versatile question answering systems: seeing in synthesis]", Mittal et al., IJIIDS, 5(2), 119-142, 2011  
</ref>

Some examples of natural language document collections used for QA systems include:
* a local collection of reference texts

* internal organization documents and web pages
* compiled [[newswire]] reports
* a set of [[Wikipedia]] pages
* a subset of [[World Wide Web]] pages

QA research attempts to deal with a wide range of question types including: fact, list, definition, ''How'', ''Why'', hypothetical, semantically constrained, and cross-lingual questions.

* ''Closed-domain'' question answering deals with questions under a specific domain (for example, medicine or automotive maintenance), and can be seen as an easier task because NLP systems can exploit domain-specific knowledge frequently formalized in [[Ontology (computer science)|ontologies]]. Alternatively, ''closed-domain'' might refer to a situation where only a limited type of questions are accepted, such as questions asking for [[descriptive knowledge|descriptive]] rather than [[procedural knowledge|procedural]] information. QA systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimers disease <ref>Roser Morante , Martin Krallinger , Alfonso Valencia and  Walter Daelemans. Machine Reading of Biomedical Texts about Alzheimers Disease. CLEF 2012 Evaluation Labs and Workshop. September 17 2012</ref>
* ''[[Open domain#References|Open-domain]]'' question answering deals with questions about nearly anything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer.

==History==

Two early QA systems were BASEBALL and LUNAR.{{when|date=November 2012}}{{who|date=November 2012}}{{citation needed|date=November 2012}} BASEBALL answered questions about the US baseball league over a period of one year. LUNAR, in turn, answered questions about the geological analysis of rocks returned by the Apollo moon missions. Both QA systems were very effective in their chosen domains. In fact, LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain posed by people untrained on the system. Further restricted-domain QA systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain. The language abilities of BASEBALL and LUNAR used techniques similar to [[ELIZA]] and [[DOCTOR]], the first [[chatterbot]] programs.

[[SHRDLU]] was a highly successful question-answering program developed by [[Terry Winograd]] in the late 60s and early 70s. It simulated the operation of a robot in a toy world (the "blocks world"), and it offered the possibility to ask the robot questions about the state of the world. Again, the strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program.

In the 1970s, [[knowledge base]]s were developed that targeted narrower domains of knowledge. The QA systems developed to interface with these [[expert system]]s produced more repeatable and valid responses to questions within an area of knowledge. These [[expert systems]] closely resembled modern QA systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized [[knowledge base]]s, whereas many modern QA systems rely on statistical processing of a large, unstructured, natural language text corpus.

The 1970s and 1980s saw the development of comprehensive theories in [[computational linguistics]], which led to the development of ambitious projects in text comprehension and question answering. One example of such a system was the Unix Consultant (UC), developed by [[Robert Wilensky]] at [[U.C. Berkeley]] in the late 1980s. The system answered questions pertaining to the [[Unix]] operating system. It had a comprehensive hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning.

Recently, specialized natural language QA systems have been developed, such as [http://bitem.hesge.ch/content/eagli-eagle-eye EAGLi] for health and life scientists.

==Architecture==
Most modern QA systems use [[natural language]] text documents as their underlying knowledge source.  [[Natural language processing]] techniques are used to both process the question and index or process the text [[Text corpus|corpus]] from which answers are extracted. An increasing number of QA systems use the [[World Wide Web]] as their corpus of text and knowledge. However, many of these tools do not produce a human-like answer, but rather employ "shallow" methods (keyword-based techniques, templates...) to produce a list of documents or a list of document excerpts containing the probable answer highlighted.

In an alternative QA implementation, human users assemble knowledge in a structured database, called a [[knowledge base]], similar to those employed in the [[expert systems]] of the 1970s. It is also possible to employ a combination of structured databases and natural language text documents in a hybrid QA system. Such a hybrid system may employ data mining algorithms to populate a structured knowledge base that is also populated and edited by human contributors. An example hybrid QA system is the [[Wolfram Alpha]] QA system which employs natural language processing to transform human questions into a form that is processed by a curated knowledge base.

Current QA systems<ref>Hirschman, L. & Gaizauskas, R. (2001) [http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=96167 Natural Language Question Answering. The View from Here]. Natural Language Engineering (2001), 7:4:275-300 Cambridge University Press.</ref> typically include a '''question classifier''' module that determines the type of question and the type of answer. After the question is analysed, the system typically uses several modules that apply increasingly complex NLP techniques on a gradually reduced amount of text. Thus, a '''document retrieval module''' uses [[search engine]]s to identify the documents or paragraphs in the document set that are likely to contain the answer. Subsequently a '''filter''' preselects small text fragments that contain strings of the same type as the expected answer. For example, if the question is "Who invented
Penicillin" the filter returns text that contain names of people. Finally, an '''answer extraction''' module looks for further clues in the text to determine if the answer candidate can indeed answer the question.

A '''multiagent''' question-answering architecture has been proposed, where each domain is represented by an agent which tries to answer questions taking into account its specific knowledge. The metaagent controls the cooperation between question answering agents and chooses the most relevant answer(s).<ref>{{vcite journal |author=Galitsky B, Pampapathi R|title=Can many agents answer questions better than one|journal=First Monday |volume = 10| Number=1 |date=2005 | url = http://firstmonday.org/ojs/index.php/fm/article/view/1204/1124
}}</ref>

==Question answering methods==
QA is very dependent on a good search [[text corpus|corpus]] - for without documents containing the answer, there is little any QA system can do. It thus makes sense that larger collection sizes generally lend well to better QA performance, unless the question domain is orthogonal to the collection. The notion of [[data redundancy]] in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents,<ref>Lin, J. (2002). The Web as a Resource for Question Answering: Perspectives and Challenges. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC 2002).</ref> leading to two benefits:
# By having the right information appear in many forms, the burden on the QA system to perform complex NLP techniques to understand the text is lessened.
# Correct answers can be filtered from [[false positive]]s by relying on the correct answer to appear more times in the documents than instances of incorrect ones.

Question answering heavily relies on [[reasoning]]. There are a number of question answering systems designed in [[Prolog]],<ref>{{cite book |last=Galitsky |first=Boris |title=Natural Language Question Answering System: Technique of Semantic Headers |url=http://books.google.com/books?id=LkNmAAAACAAJ |series=International Series on Advanced Intelligence |volume=Volume 2 |year=2003 |publisher=Advanced Knowledge International |location=Australia |isbn=978-0-86803-979-4}}</ref> a [[logic programming]] language associated with [[artificial intelligence]].

===Open domain question answering===
In [[information retrieval]], an open domain question answering system aims at returning an answer in response to the users question. The returned answer is in the form of short texts rather than a list of relevant documents. The system uses a combination of techniques from [[computational linguistics]], [[information retrieval]] and [[knowledge representation]] for finding answers.

The system takes a [[natural language]] question as an input rather than a set of keywords, for example, When is the national day of China? The sentence is then transformed into a query through its [[logical form]]. Having the input in the form of a natural language question makes the system more user-friendly, but harder to implement, as there are various question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a question type to the question is a crucial task, the entire answer extraction process relies on finding the correct question type and hence the correct answer type.

Keyword [[Data extraction|extraction]] is the first step for identifying the input question type. In some cases, there are clear words that indicate the question type directly. i.e. Who, Where or How many, these words tell the system that the answers should be of type Person, Location, Number respectively. In the example above, the word When indicates that the answer should be of type Date. POS tagging and syntactic parsing techniques can also be used to determine the answer type. In this case, the subject is Chinese National Day, the predicate is is and the adverbial modifier is when, therefore the answer type is Date. Unfortunately, some interrogative words like Which, What or How do not give clear answer types. Each of these words can represent more than one type. In situations like this, other words in the question need to be considered. First thing to do is to find the words that can indicate the meaning of the question. A lexical dictionary such as [[WordNet]] can then be used for understanding the context.

Once the question type has been identified, an [[Information retrieval]] system is used to find a set of documents containing the correct key words. A tagger and NP/Verb Group chunker can be used to verify whether the correct entities and relations are mentioned in the found documents. For questions such as Who or Where, a Named Entity Recogniser is used to find relevant Person and Location names from the retrieved documents. Only the relevant paragraphs are selected for ranking.

A [[vector space model]] can be used as a strategy for classifying the candidate answers. Check if the answer is of the correct type as determined in the question type analysis stage. Inference technique can also be used to validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate, the more and the closer the better. The answer is then translated into a compact and meaningful representation by parsing. In the previous example, the expected output answer is 1st Oct.

==Issues==
In 2002 a group of researchers wrote a roadmap of research in question answering.<ref>Burger, J., Cardie, C., Chaudhri, V., Gaizauskas, R., Harabagiu, S., Israel, D., Jacquemin, C., Lin, C-Y., Maiorano, S., Miller, G., Moldovan, D., Ogden, B., Prager, J., Riloff, E., Singhal, A., Shrihari, R., Strzalkowski, T., Voorhees, E., Weishedel, R. [http://www-nlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper_v2.doc Issues, Tasks and Program Structures to Roadmap Research in Question Answering (QA)].</ref> The following
issues were identified.<!-- much of the text in this section is copied and pasted from the "roadmap" document; somebody may try and simplify the text -->

;Question classes : Different types of questions (e.g., "What is the capital of [[Liechtenstein]]?" vs. "Why does a [[rainbow]] form?" vs. "Did [[Marilyn Monroe]] and [[Cary Grant]] ever appear in a movie together?") require the use of different strategies to find the answer. Question classes are arranged hierarchically in taxonomies.{{example needed|date=February 2011}}

;Question processing : The same information request can be expressed in various ways, some interrogative ("Who is the King of Lesotho?") and some assertive ("Tell me the name of the King of Lesotho."). A semantic model of question understanding and processing would recognize equivalent questions, regardless of how they are presented. This model would enable the translation of a complex question into a series of simpler questions, would identify ambiguities and treat them in context or by interactive clarification.

;Context and QA : Questions are usually asked within a context and answers are provided within that specific context. The context can be used to clarify a question, resolve ambiguities or keep track of an investigation performed through a series of questions. (For example, the question, "Why did Joe Biden visit Iraq in January 2010?" might be asking why Vice President Biden visited and not President Obama, why he went to Iraq and not Afghanistan or some other country, why he went in January 2010 and not before or after, or what Biden was hoping to accomplish with his visit. If the question is one of a series of related questions, the previous questions and their answers might shed light on the questioner's intent.)

;Data sources for QA : Before a question can be answered, it must be known what knowledge sources are available and relevant. If the answer to a question is not present in the data sources, no matter how well the question processing, information retrieval and answer extraction is performed, a correct result will not be obtained.

;Answer extraction : Answer extraction depends on the complexity of the question, on the answer type provided by question processing, on the actual data where the answer is searched, on the search method and on the question focus and context.{{example needed|date=February 2011}}

;Answer formulation : The result of a QA system should be presented in a way as natural as possible. In some cases, simple extraction is sufficient. For example, when the question classification indicates that the answer type is a name (of a person, organization, shop or disease, etc.), a quantity (monetary value, length, size, distance, etc.) or a date (e.g. the answer to the question, "On what day did Christmas fall in 1989?") the extraction of a single datum is sufficient. For other cases, the presentation of the answer may require the use of fusion techniques that combine the partial answers from multiple documents.

;Real time question answering : There is need for developing Q&A systems that are capable of extracting answers from large data sets in several seconds, regardless of the complexity of the question, the size and multitude of the data sources or the ambiguity of the question.

;Multilingual (or cross-lingual) question answering : The ability to answer a question posed in one language using an answer corpus in another language (or even several). This allows users to consult information that they cannot use directly. (See also [[Machine translation]].)

;Interactive QA : It is often the case that the information need is not well captured by a QA system, as the question processing part may fail to classify properly the question or the information needed for extracting and generating the answer is not easily retrieved. In such cases, the questioner might want not only to reformulate the question, but to have a dialogue with the system. In addition, system may also use previously answered questions.<ref>Perera, R. and Nand, P. 2014. [http://link.springer.com/chapter/10.1007%2F978-3-319-11716-4_11 Interaction History Based Answer Formulation for Question Answering.]</ref> (For example, the system might ask for a clarification of what sense a word is being used, or what type of information is being asked for.)

;Advanced reasoning for QA : More sophisticated questioners expect answers that are outside the scope of written texts or structured databases. To upgrade a QA system with such capabilities, it would be necessary to integrate reasoning components operating on a variety of knowledge bases, encoding world knowledge and common-sense reasoning mechanisms, as well as knowledge specific to a variety of domains. [[Evi (software)|Evi]] is an example of such as system.

;Information clustering for QA: Information clustering for question answering systems is a new trend that originated to increase the accuracy of question answering systems through search space reduction. In recent years this was widely researched through development of question answering systems which support information clustering in their basic flow of process.<ref>Perera, R. 2012. [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6305919&isnumber=6305918 IPedagogy: Question Answering System Based on Web Information Clustering.]</ref>

;User profiling for QA : The user profile captures data about the questioner, comprising context data, domain of interest, reasoning schemes frequently used by the questioner, common ground established within different dialogues between the system and the user, and so forth. The profile may be represented as a predefined template, where each template slot represents a different profile feature. Profile templates may be nested one within another.{{example needed|date=February 2011}}

==Progress==
QA systems have been extended in recent years to encompass additional domains of knowledge<ref>Maybury, M. T. editor. 2004. [http://www.mitpressjournals.org/doi/pdf/10.1162/089120105774321055 New Directions in Question Answering.] AAAI/MIT Press.</ref>  For example, systems have been developed to automatically answer temporal and geospatial questions, questions of definition and terminology, biographical questions, multilingual questions, and questions about the content of audio, images, and video. Current QA research topics include:

* interactivityclarification of questions or answers
* answer reuse or caching
* knowledge representation and reasoning
* social media analysis with QA systems
* [[sentiment analysis]]<ref>[http://totalgood.com/bitcrawl/ BitCrawl] by Hobson Lane</ref>
* utilization of thematic roles<ref>Perera, R. and Perera, U. 2012. [http://www.aclweb.org/anthology/W12-6004 Towards a thematic role based target identification model for question answering.]</ref>
* semantic resolution: to bridge the gap between syntactically different questions and answer-bearing texts<ref>{{cite conference | author=Bahadorreza Ofoghi, John Yearwood, and Liping Ma | year=2008 | conference=The 30th European Conference on Information Retrieval (ECIR'08)| pages= 430437 | publisher=Springer Berlin Heidelberg | title= [http://link.springer.com/chapter/10.1007/978-3-540-78646-7_40 The impact of semantic class identification and semantic role labeling on natural language answer extraction]}}</ref>
* utilization of linguistic resources,<ref>{{cite journal |author=Bahadorreza Ofoghi, John Yearwood, and Liping Ma|title=[http://onlinelibrary.wiley.com/doi/10.1002/asi.20989/abstract;jsessionid=099F3D167FD0511A48FB1C19C1060676.f02t02?deniedAccessCustomisedMessage=&userIsAuthenticated=false The impact of frame semantic annotation levels, framealignment techniques, and fusion methods on factoid answer processing] | journal=Journal of the American Society for Information Science and Technology |volume=60 |issue=2 |pages=247263 |year =2009}}</ref> such as [[WordNet]], [[FrameNet]], and the similar

IBM's question answering system, Watson, defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin.
<ref>http://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html?_r=0</ref>

==References==
* Dragomir R. Radev, John Prager, and Valerie Samn. [http://clair.si.umich.edu/~radev/papers/anlp00.pdf Ranking suspected answers to natural language questions using predictive annotation]. In Proceedings of the 6th Conference on Applied Natural Language Processing, Seattle, WA, May 2000.
* John Prager, Eric Brown, Anni Coden, and Dragomir Radev. [http://clair.si.umich.edu/~radev/papers/sigir00.pdf Question-answering by predictive annotation]. In Proceedings, 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, July 2000.
*{{cite book | last = Hutchins | first = W. John | authorlink = John Hutchins |author2=Harold L. Somers  | year = 1992 | title = An Introduction to Machine Translation | url = http://www.hutchinsweb.me.uk/IntroMT-TOC.htm | publisher = Academic Press | location = London | isbn = 0-12-362830-X}}
* L. Fortnow, Steve Homer (2002/2003).   [http://people.cs.uchicago.edu/~fortnow/papers/history.pdf A Short History of Computational Complexity].  In D. van Dalen, J. Dawson, and A. Kanamori, editors, ''The History of Mathematical Logic''. North-Holland, Amsterdam.

<references/>

==External links==
* [http://aclia.lti.cs.cmu.edu/ntcir8 Question Answering Evaluation at NTCIR]
* [http://trec.nist.gov/data/qamain.html Question Answering Evaluation at TREC]
* [http://celct.fbk.eu/QA4MRE/ Question Answering Evaluation at CLEF]

{{Computable knowledge}}
{{Natural Language Processing}}

[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval]]
>>EOP<<
136<|###|>Pleade
{{Infobox software
| name                   = Pleade-infoxbox
| title                  = Pleade
| logo                   = [[File:Pleade-logo.png]]
| logo caption           = Logo de Pleade
| screenshot             = <!-- [[File: ]] -->
| caption                = 
| collapsible            = 
| author                 = AJLSM
| developer              = AJLSM
| released               = <!-- {{Start date|YYYY|MM|DD|df=yes/no}} -->
| discontinued           = 
| latest release version = 3.4
| latest release date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->
| latest preview version = <!-- 3.5 -->
| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->
| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->
| programming language   = [[Java]], [[XSLT]], [[Apache Cocoon|Cocoon]]
| operating system       = [[Unix-like]], [[Microsoft Windows]]
| platform               = 
| size                   = 
| language               = French, English, German, Chinese
| language count         = <!-- DO NOT include this parameter unless you know what it does -->
| language footnote      = 
| status                 = Active
| genre                  = Digital Library
| license                = GNU General Public License
| alexa                  = 
| website                = {{URL|http://www.pleade.com/}}
}}

'''Pleade''' is an open source [[search engine]] and browser for [[Finding aid|archival finding aids]] encoded in [[Encoded Archival Description|EAD]] (an XML standard for encoding archival finding aids). Based on the [[SDX]] platform, it is a very flexible web application.

== History ==
The software was jointly started by the companies AJLSM and Anaphore and was originally intended for publication and dissemination only of archival research tools like EAD finding aids, but it has become a library portal and a medium for digital libraries.<ref>[http://www.digicult.info/downloads/dc_info_issue6_december_20031.pdf DigiCult.Info issue #6, page 16]</ref>

==Technologies==
Pleade is published in GPL 3. It is based on the [[Apache Cocoon|Apache Cocoon framework]] and it works with the search engine SDX.

It is able to publish and distribute the following format : [[Encoded Archival Description|EAD]], [[Comma-separated values|CSV]] (internally converted to XML), [[XMLMarc]], [[Text Encoding Initiative|TEI]], [[Dublin Core]]. Support for [[METS]] and [[ALTO (XML)|ALTO]] is under active development.<ref>[http://pleade.com/ Pleade 2012 : les imprimes numerises et les formats XML METS / ALTO]</ref>

== Features ==
* Customizable publication ;
* Customizable index creation ;
* Customizable search form ;
* Simple and advanced search among publish documents ;
* Federate search among different bases (e.g. EAD, METS) ;
* basket (for database and for images), a search history, printing, etc. ;
* document viewer supporting : [[JPEG]], [[TIFF]] and for high resolution TIFF and [[JPEG2000]] it use [http://iipimage.sourceforge.net/ IIPImage image server] ;
* [[OAI-PMH]] repositories and expose them, by default, the format EAD, Dublin Core and [[Dublin Core#Qualified Dublin Core|Qualified DC]] ;
* The viewer has a Pleade indexing module (paleographic) that can be used to permit correction of the OCR. This tool is a TEI export of data input. A workflow management allows annotators and validation records seized ;
* Printing resulting and finding aids as PDF documents (with embedded images) ;
* Compatible with standard archival format : [[Text Encoding Initiative|TEI]], [[BiblioML]] ;
* Ability to import metadata from an [[Integrated library system|ILS]].

=== Pleade-Entreprise ===
* Pleade-Entreprise extended features to others XML format, such as [[METS]] and [[ALTO (XML)|ALTO]].

== Examples ==
These are examples of websites based on Pleade:
{{columns-list|2|
* Archival portals
** [http://archives-inventaires.loire-atlantique.fr/ Departmental records of Loire-Atlantique (AD 44) (AD 44)]
** [http://gael.gironde.fr/ GAEL : GAEL: Gironde archives online]
** [http://odysseo.org/ Odysseo: Resources for the history of immigration]
** [http://taubira.anaphore.org/ Parliamentary work of Christiane Taubira]
** [http://archivesetmanuscrits.bnf.fr/ Archives and manuscrits of the BNF French National Library]
** [http://jubilotheque.upmc.fr/ Jubilotheque, UPMC's scientific digital library]
** [http://lbf-ehess.ens-lyon.fr/pages/fonds.html Michel Foucault's Library "les Mots et les Choses" ENS]

* Portals documentary
** [http://www.michael-culture.org/fr/home Michael]
** [http://www.numerique.culture.fr/mpf/pub-fr/index.html Digital Heritage]

* Digital Libraries
** Digital Library of Lille
** Lille III
** [http://archivesetmanuscrits.bnf.fr/ BNF: Archives and manuscripts (French National Library)]
}}

== Related resources ==
* {{Official website|http://pleade.com}}
* [http://demo.pleade.com Official demo]
* [http://www.pleadeenpratique.org/ Pleade in practice]
* [http://www.ajlsm.com/produits/sdx SDX]
* [http://www.ajlsm.com AJLSM company]

== References ==
<references/>

[[Category:Digital library software]]
[[Category:Free software]]
[[Category:Information retrieval]]
[[Category:Archival science]]
>>EOP<<
142<|###|>Thesaurus (information retrieval)
{{about|thesauri used to support indexing, tagging or searching for information|thesauri used in general/literary applications|Thesaurus|the Clare Fischer album|Thesaurus (album)}}

In the context of [[information retrieval]], a '''thesaurus''' (plural: "thesauri") is a form of controlled vocabulary that seeks to dictate semantic manifestations of [[metadata]] in the indexing of content objects. A thesaurus serves to minimise semantic ambiguity by ensuring uniformity and consistency in the storage and retrieval of the manifestations of content objects. ANSI/NISO Z39.19-2005 defines a content object as "any item that is to be described for inclusion in an information retrieval system, website, or other source of information".<ref>ANSI & NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.11</ref> The thesaurus aids the assignment of preferred terms to convey semantic metadata associated with the content object.<ref>ANSI & NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.12</ref>

A thesaurus serves to guide both an indexer and a searcher in selecting the same preferred term or combination of preferred terms to represent a given subject. [[ISO 25964]], the international standard for information retrieval thesauri, defines a thesaurus as a controlled and structured vocabulary in which concepts are represented by terms, organized so that relationships between concepts are made explicit, and preferred terms are accompanied by lead-in entries for synonyms or quasi-synonyms.

A thesaurus is composed by at least three elements: 1-a list of words (or terms), 2-the relationship amongst the words (or terms), indicated by their hierarchical relative position (e.g. parent/broader term; child/narrower term, synonym, etc.), 3-a set of rules on how to use the thesaurus.

== History ==
Wherever there have been large collections of information, whether on paper or in computers, scholars have faced a challenge in pinpointing the items they seek. The use of classification schemes to arrange the documents in order was only a partial solution. Another approach was to index the contents of the documents using words or terms, rather than classification codes. In the 1940s and 1950s some pioneers, such as [[Calvin Mooers]], Charles L. Bernier, [http://pubs.acs.org/cen/priestley/recipients/1951crane.html Evan J. Crane] and [[Hans Peter Luhn]], collected up their index terms in various kinds of list that they called a thesaurus (by analogy with the well known thesaurus developed by [[Peter Roget]]).<ref>Roberts, N. The pre-history of the information retrieval thesaurus. ''Journal of Documentation'', 40(4), 1984, p.271-285.</ref> The first such list put seriously to use in information retrieval was the thesaurus developed in 1959 at the E I Dupont de Nemours Company.<ref>Aitchison, J. and Dextre Clarke, S. The thesaurus: a historical viewpoint, with a look to the future. ''Cataloging & Classification Quarterly'', 37 (3/4), 2004, p.5-21.</ref><ref>Krooks, D.A. and Lancaster, F.W. The evolution of guidelines for thesaurus construction. ''Libri'', 43(4), 1993, p.326-342.</ref>

The first two of these lists to be published were the ''Thesaurus of ASTIA Descriptors'' (1960) and the ''Chemical Engineering Thesaurus'' of the American Institute of Chemical Engineers (1961), a descendant of the Dupont thesaurus. More followed, culminating in the influential ''Thesaurus of Engineering and Scientific Terms'' (TEST) published jointly by the Engineers Joint Council and the US Department of Defense in 1967. TEST did more than just serve as an example; its Appendix 1 presented ''Thesaurus rules and conventions'' that have guided thesaurus construction ever since.
Hundreds of thesauri have been produced since then, perhaps thousands. The most notable innovations since TEST have been:
(a)	Extension from monolingual to multilingual capability; and 
(b)	Addition of a conceptually organized display to the basic alphabetical presentation.

Here we mention only some of the national and international standards that have built steadily on the basic rules set out in TEST:

* [[UNESCO]] ''Guidelines for the establishment and development of monolingual thesauri''. 1970 (followed by later editions in 1971 and 1981)
* DIN 1463 ''Guidelines for the establishment and development of monolingual thesauri''. 1972 (followed by later editions)
* ISO 2788 ''Guidelines for the establishment and development of monolingual thesauri''. 1974 (revised 1986)
* ANSI ''American National Standard for Thesaurus Structure, Construction, and Use''. 1974 (revised 1980 and superseded by ANSI/NISO Z39.19-1993)
* ISO 5964 ''Guidelines for the establishment and development of multilingual thesauri''. 1985
* ANSI/NISO Z39.19 ''Guidelines for the construction, format, and management of monolingual thesauri''. 1993 (revised 2005 and renamed ''Guidelines for the construction, format, and management of monolingual controlled vocabularies''.)
* ISO 25964 ''Thesauri and interoperability with other vocabularies''. Part 1 (''Thesauri for information retrieval'' published 2011; Part 2 (''Interoperability with other vocabularies'') published 2013.

The most clearly visible trend across this history of thesaurus development has been from the context of small-scale isolation to a networked world.<ref>Dextre Clarke, Stella G. and Zeng, Marcia Lei. [http://www.niso.org/publications/isq/2012/v24no1/clarke/ From ISO 2788 to ISO 25964: the evolution of thesaurus standards towards interoperability and data modeling] ''Information standards quarterly'', 24(1), 2012, p.20-26.</ref> Access to information was notably enhanced when thesauri crossed the divide between monolingual and multilingual applications. More recently, as can be seen from the titles of the latest ISO and NISO standards, there is a recognition that thesauri need to work in harness with other forms of vocabulary or knowledge organization system, such as subject heading schemes, classification schemes, taxonomies and ontologies. The official website for ISO 25964 gives more information, including a reading list.<ref>''[http://www.niso.org/schemas/iso25964/ ISO 25964  the international standard for thesauri and interoperability with other vocabularies.]'' National Information Standards Organization, 2013.</ref>

== Purpose ==
In information retrieval, a thesaurus can be used as a form of controlled vocabulary to aid in the indexing of appropriate metadata for information bearing entities. A thesaurus helps with expressing the manifestations of a concept in a prescribed way, to aid in improving [[precision and recall]]. This means that the semantic conceptual expressions of information bearing entities are easier to locate due to uniformity of language. Additionally, a thesaurus is used for maintaining a hierarchical listing of terms; usually single words or bound phrases that aid the indexer in narrowing the terms and limiting semantic ambiguity.

The [[Art and Architecture Thesaurus|Art & Architecture Thesaurus]], for example, is used by countless museums around the world, to catalogue their collections. [[AGROVOC]], the thesaurus of the UNs [[Food and Agriculture Organization]], is used to index and/or search its AGRIS database of worldwide literature on agricultural research.

== Structure ==
Information retrieval thesauri are formally organized so that existing relationships between concepts are made clear. For example, citrus fruits might be linked to the broader concept of fruits, and the narrower ones of oranges, lemons, etc. When the terms are displayed online, the links between them make it very easy to surf around the thesaurus, selecting useful terms for a search. When a single term could have more than one meaning, like tables (furniture) or tables (data), these are listed separately so that the user can choose which concept to search for and avoid retrieving irrelevant results. For any one concept, all the known synonyms are listed, such as mad cow disease, bovine spongiform encephalopathy, BSE, etc. The idea is to guide all the indexers and all the searchers to use the same term for the same concept, so that search results will be as complete as possible. If the thesaurus is multilingual, equivalent terms in other languages are shown too. Following international standards, concepts are generally arranged hierarchically within facets or grouped by themes or topics. Unlike a general thesaurus used for literary purposes, information retrieval thesauri typically focus on one discipline, subject or field of study.

== See also ==
* [[Controlled vocabulary]]
* [[ISO 25964]]
* [[Thesaurus]]

== References ==
{{Reflist}}

== External links ==
* [http://www.niso.org/schemas/iso25964/ Official site for ISO 25964] 
* [http://www.taxonomywarehouse.com/ Taxonomy Warehouse]

[[Category:Information retrieval]]
[[Category:Thesauri]]
>>EOP<<
148<|###|>Gerard Salton Award
The '''Gerard Salton Award''' is presented by the [[Association for Computing Machinery]] (ACM) [[Special Interest Group on Information Retrieval]] (SIGIR) every three years to an individual who has made "significant, sustained and continuing contributions to research in [[information retrieval]]". SIGIR also co-sponsors (with [[SIGWEB]]) the [[Vannevar Bush Award]], for the best paper at the [[Joint Conference on Digital Libraries]].

==Chronological honorees and lectures==
* 1983 - [[Gerard Salton]], [[Cornell University]] : "About the future of automatic information retrieval."
* 1988 - [[Karen Sparck Jones]], [[University of Cambridge]] : "A look back and a look forward."
* 1991 - [[Cyril Cleverdon]], [[Cranfield Institute of Technology]] : "The significance of the Cranfield tests on index languages."
* 1994 - William S. Cooper, [[University of California, Berkeley]] : "The formalism of probability theory in IR: a foundation or an encumbrance?"
* 1997 - [[Tefko Saracevic]], [[Rutgers University]] : "Users lost (summary): reflections on the past, future, and limits of information science." 
* 2000 - [[Stephen Robertson (computer scientist)|Stephen E. Robertson]], [[City University, London|City University London]] : "On theoretical argument in information retrieval."<BR>'''For ...''' ''"Thirty years of significant, sustained and continuing contributions to research in information retrieval. Of special importance are the theoretical and empirical contributions to the development, refinement, and evaluation of probabilistic models of information retrieval."''
* 2003 - [[W. Bruce Croft]], [[University of Massachusetts Amherst]] : "Information retrieval and computer science: an evolving relationship."<BR>'''For ...''' ''"More than twenty years of significant, sustained and continuing contributions to research in information retrieval. His contributions to the theoretical development and practical use of [[Bayesian inference]] networks and [[language modelling]] for retrieval, and to their evaluation through extensive experiment and application, are particularly important. The Center for Intelligent Information Retrieval which he founded illustrates the strong synergies between fundamental research and its application to a wide range of practical information management problems."''
* 2006 - [[C. J. van Rijsbergen]], [[University of Glasgow]] : 	"Quantum haystacks."
* 2009 - [[Susan Dumais]], [[Microsoft Research]] : "An Interdisciplinary Perspective on Information Retrieval."
* 2012 - [[Norbert Fuhr]], [[University of Duisburg-Essen]]: "Information Retrieval as Engineering Science."

==External links==
* [http://www.acm.org/sigir/ ACM SIGIR homepage]
* [http://www.sigir.org/awards/awards.html ACM SIGIR awards]

[[Category:Association for Computing Machinery]]
[[Category:Computer science awards]]
[[Category:Information retrieval]]
>>EOP<<
154<|###|>AUTINDEX
{{multiple issues|
{{COI|date=September 2014}}
{{notability|Products|date=September 2014}}
}}

'''AUTINDEX''' is a commercial [[text mining]] software package based on sophisticated linguistics.<ref>Ripplinger, Barbel 2001: Das Indexierungssystem AUTINDEX, in GLDV Tagung, Giessen</ref><ref>Paul Schmidt, Mahmoud Gindiyeh & Gintare Grigonyte, 2009: Language Technology for Information Systems. In: Proceedings of KDIR - The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira, 68 October 2009, Portugal</ref><ref>Paul Schmidt & Mahmoud Gindiyeh, 2009: Language Technology for Multilingual Information and Document Management. In: Proceedings of ASLIB, London, 1920 November</ref>

'''AUTINDEX''' resulting from research in [[information extraction]] <ref>Paul Schmidt, Thomas Bahr & Dr.-Ing. Jens Biesterfeld &Thomas Risse & Kerstin Denecke & Claudiu Firan, 2008: LINSearch. Aufbereitung von Fachwissen fur die gezielte Informationsversorgung. In: Proceedings of Knowtech, Frankfurt</ref><ref>Ursula Deriu, Jorn Lehmann & Paul Schmidt, 2009: Erstellung einer Technik-Ontologie auf der Basis ausgefeilter Sprachtechnologie. In: Proceedings Knowtech, Frankfurt</ref> is a product of the Institute of Applied Information Sciences (IAI) which is a non-profit institute that has been researching and developing [[language technology]] since its foundation in 1985. IAI is an institute affiliated to [[Saarland University]] in Saarbrucken, Germany.

'''AUTINDEX''' is the result of a number of research projects funded by the EU (Project BINDEX <ref>[//www.lrec-conf.org/proceedings/lrec2002/pdf/255.pdf]. Dieter Maas, Nuebel Rita, Catherine Pease, Paul Schmidt: Bilingual Indexing for Information Retrieval with AUTINDEX. LREC 2002.</ref>), by Deutsche Forschungsgemeinschaft and the German Ministry for Economy. Amongst the latter there are the projects LinSearch <ref>[//www.l3s.de/AR07/layout/L3S-AR2007_screen.pdf]. Project LinSearch. P. 32.</ref> and WISSMER,<ref>[//www.wissmer.info/index.php/de/]. Project Wissmer.</ref> see also the reference to IAI-Webite.<ref>[//www.iai-sb.de/forschung/content/view/67/89/]. Wissmer-Project on IAI-Site.</ref>

The basic functionality of AUTINDEX is the extraction of key words from a document to represent the semantics of the document.<ref>Paul Schmidt, Mahmoud Gindiyeh, Gintare Grigonyte: ''Language Technology for Information Systems.'' In: ''Proceedings of KDIR  The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira.'' 6.8. Oktober 2009, Portugal. 2009, S. 259 - 262.</ref> Ideally the system is integrated with a [[thesaurus]] that defines the standardised terms to be used for key word assignment.<br> 
AUTINDEX is used in library applications (e.g. integrated in [[dandelon.com]]) as well as in high quality (expert) information systems <ref>[//www.wti-frankfurt.de]. WTI Information system.</ref> and in document management and content management environments. <br> 
 
Together with AUTINDEX a number of additional software comes along such as an integration with [[Apache Solr]] / [[Lucene]] to provide a complete [[information retrieval]] environment, a classification and [[categorisation]] system on the basis of a [[machine learning]] <ref>Mahmoud Gindiyeh: Anwendung wahrscheinlichkeitstheoretischer Methoden in der linguistischen Informationsverarbeitung, Logos Verlag, Berlin, 2013.</ref> software that assigns domains to the document, and a system for searching with semantically similar terms that are collected in so called [[tag clouds]].<ref>[//www.wissmer.info]. Electro mobility information system.</ref>

==See also==

* [[Information retrieval]]
* [[Linguistics]]
* [[Knowledge Management]]
* [[Natural Language Processing]]
* [[Semantics]]

== References ==
{{reflist}}

== Publications ==
* Ripplinger, Barbel 2001: Das Indexierungssystem AUTINDEX, in GLDV Tagung, Giessen.
* Paul Schmidt, Mahmoud Gindiyeh & Gintare Grigonyte, 2009: Language Technology for Information Systems. In: Proceedings of KDIR - The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira, 68 October 2009, Portugal.
* Paul Schmidt & Mahmoud Gindiyeh, 2009: Language Technology for Multilingual Information and Document Management. In: Proceedings of ASLIB, London, 1920 November.
* Paul Schmidt, Thomas Bahr & Dr.-Ing. Jens Biesterfeld &Thomas Risse & Kerstin Denecke & Claudiu Firan, 2008: LINSearch. Aufbereitung von Fachwissen fur die gezielte Informationsversorgung. In: Proceedings of Knowtech, Frankfurt.
* Paul Schmidt, Mahmoud Gindiyeh, Gintare Grigonyte: ''Language Technology for Information Systems.'' In: ''Proceedings of KDIR  The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira.'' 6.8. Oktober 2009, Portugal. 2009, S. 259 - 262.
* Paul Schmidt, Mahmoud Gindiyeh: ''Language Technology for Multilingual Information and Document Management.'' In: ''Proceedings of ASLIB.'' London, 19.20. November 2009.
* Rosener, Christoph, Ulrich Herb: ''Automatische Schlagwortvergabe aus der SWD fur Repositorien.'' Zusammen mit Ulrich Herb in ''Proceedings.'' Berufsverband Information Bibliothek, Bibliothekartage. 97. Deutscher Bibliothekartag, Mannheim, 2008.
* Svenja Siedle: ''Suchst du noch oder weit du schon? Inhaltserschlieung leicht gemacht mit automatischer Indexierung.'' In: ''tekom-Jahrestagung und tcworld conference 2013''
* Michael Gerards, Adreas Gerards, Peter Weiland: ''Der Einsatz der automatischen Indexierungssoftware AUTINDEX im Zentrum fur Psychologische Information und Dokumentation (ZPID).'' 2006 ([http://zpid.de/download/PSYNDEXmaterial/autindex.pdf Online] bei zpid.de, PDF-Datei)
* Mahmoud Gindiyeh: Anwendung wahrscheinlichkeitstheoretischer Methoden in der linguistischen Informationsverarbeitung. Logos Verlag, Berlin, 2013.

== External links ==
* http://www.iai-sb.de/ Institute for Applied Information Sciences

[[Category:Natural language processing]]
[[Category:Information retrieval]]
>>EOP<<
160<|###|>Comprehensive Model of Information Seeking
The '''Comprehensive Model of Information Seeking''', or CMIS, is a theoretical construct designed to predict how people will seek information.  It was first developed by J. David Johnson and has been utilized by a variety of disciplines including [[Library and Information Science]] and [[Health Communication]].

The CMIS has been empirically tested in health and organizational contexts<ref>Johnson, J. D., & Meischke, H. (1993). Cancer-related channel selection:  An extensionfor a sample of women who have had a mammogram. Women & Health, 20, 31-44.; Johnson, J. D., Donohue, W. A., Atkin, C. K., & Johnson, S. H. (1995). A comprehensive model of information seeking: Tests focusing on a technical organization. 
Science Communication, 16, 274-303.</ref> The CMIS has inherent strengths for studying how people react to health problems such as cancer.<ref name="auto">Johnson, J. D., Andrews, J. E. & Allard, S. (2001). A Model for Understanding and Affecting Genetics Information Seeking. Library and Information Science Research 23(4): 335-349.</ref> The CMIS specifies ''antecedents'' that explain why people become information seekers, ''information carrier characteristics'' that shape how people go about looking for information, and ''information seeking actions'' that reflect the nature of the search itself.

==Design==

[[File:Diagram of the Comprehensive Model of Information Seeking.jpg|thumb|right|The Comprehensive Model of Information Seeking]]
The CMIS has been quantitatively tested and performs well when it comes to health information seeking behaviors (HISB).<ref name="auto"/> There are three main schemas in the CMIS. These are:  Antecedents, information field, and information seeking actions.  The antecedents are those factors that determine how an information consumer will receive the information.  Those factors are:  Demographics, personal experience, salience, and beliefs.  These factors are fluid and can change during the health information seeking process.  The second schema is the information fields that consist of characteristics and utilities.  This schema is concerned with the channels and carriers of information.  A persons understanding is developed through the information field.  The third schema involves the transformational processes and measured by the consumers understanding of the messages received through the information field.  The final schema involves information seeking actions.  This is what the consumer does as a result of the first two schemas through information seeking.  There are three major dimensions:  the scope, depth, and method of information seeking.<ref name="auto"/>

==Antecdents==
The CMIS antecedentsdemographics, personal experience, salience, and beliefsare factors that determine an individual's natural predisposition to search for information from particular information carriers. Certain types of health information seeking can be triggered by an individual's degree of personal experience with disease.<ref>Johnson, J. D. (1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.</ref> In the CMIS framework, two personal relevance factors, salience and beliefs, are seen as the primary determinants in translating a perceived gap into an active search for information. Salience refers to the personal significance of health information to the individual, such as perceptions of risk to one's health, which are likely to result in information seeking action. However, people also may be motivated to gather information to determine the implications of health events for themselves and/or others related to their future activities, a factor directly related to the rapidly growing field of genetics. An individual's beliefs about the nature of a particular disease, its impacts, and level of control, all directly relate to self-efficacy, one of our key variables, and one that plays an important role in information seeking and people's more general pattern of actions related to health.<ref>Johnson, J. D.(1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.</ref>

==Information Carrier Characteristics==

The information carrier characteristics are drawn from a model of Media Exposure and Appraisal (MEA) that has been tested on a variety of information carriers, including both sources and channels, and in a variety of cultural settings. Following the MEA, the CMIS focuses on editorial tone, communication potential, and utility. In the CMIS, characteristics are composed of editorial tone, which reflects an audience member's perception of credibility, while communication potential relates to issues of style and comprehensiveness. Utility relates the characteristics of a medium directly to the needs of an individual, and shares much with the uses and gratifications perspectives. For example, is the information contained in the medium relevant, topical, and important for the individual's purposes? In general, utility is very important for health information seeking.<ref name="auto"/>

==Information Seeking Actions==

There are several types of information seeking actions that can result from the impetus provided by the factors identified by the CMIS. For example, search behavior can be characterized by its extent, or the number of activities carried out, which has two components: scope, the number of alternatives investigated; and, depth, the number of dimensions of an alternative investigated. There is also the method of the search, or channel, as another major dimension of the search.  For instance, an individual might choose the method of consulting a telephone information service, decide to have a narrow scope by only asking questions about smoking cessation clinics, but investigate every recommendation in detail, thus increasing the depth of the search.<ref name="auto"/>

==Stages in the CMIS==

A key concept from the CMIS is the notion of stages, or cancer involvement.  According to the CMIS, an individual may be at one of four stages regarding a cancer threat, and thereby have differing information needs and behaviors.

The first stage, ''Casual'', is characterized by a general lack of concern or interest. At this stage, individuals are not purposive in their search for cancer-related information; rather, their search is accidental and aimless, even apathetic.
 
The second stage is ''Purposive-Placid''. This is characterized by the question, What can I do to prevent cancer? Individuals here might have some passing interest in cancer or genetic information, but are generally still not affected or directly concerned.

The third stage is ''Purposive-Clustered''. Here, an individual will be in closer proximity to cancer. This is the point at which a person is motivated to look for practical information that will address the specific problem. For example, a first-degree relative of a recently diagnosed breast cancer patient may seek genetic screening or [[BRCA mutation|BRCA]] 1/2 testing. The person could clearly benefit from such information- seeking behavior since medical authorities acknowledge that early detection of cancer leads to earlier treatments and better treatment outcomes.

The fourth stage, ''Directed'', includes individuals who have been diagnosed as having cancer. Such individuals need knowledge for making informed decisions about treatment and management of the disease.<ref name="auto"/>

== References ==
{{Reflist}}



[[Category:Communication]]
[[Category:Information retrieval]]
[[Category:Health sciences]]
>>EOP<<
166<|###|>Generalized vector space model
{{Confusing|date=January 2010}}
The '''Generalized vector space model''' is a generalization of the [[vector space model]] used in [[information retrieval]].  Many classifiers, especially those which are related to document or text classification, use the TFIDF basis of VSM.  However, this is where the similarity between the models ends - the generalized model uses the results of the TFIDF dictionary to generate similarity metrics based on distance or angle difference, rather than centroid based classification.  '''Wong et al.'''<ref name="wong">{{cite | title=Generalized vector spaces model in information retrieval | url=http://doi.acm.org/10.1145/253495.253506 | first=S. K. M. | last=Wong | coauthors=Wojciech Ziarko, Patrick C. N. Wong | publisher=[[Association for Computing Machinery|SIGIR ACM]] | date=1985-06-05}}</ref> presented an analysis of the problems that the pairwise orthogonality assumption of the [[vector space model]] (VSM) creates. From here they extended the VSM to the generalized vector space model (GVSM).

==Definitions==

GVSM introduces a term to term correlations, which deprecate the pairwise orthogonality assumption. More specifically, the factor considered a new space, where each term vector ''t<sub>i</sub>'' was expressed as a linear combination of ''2<sup>n</sup>'' vectors ''m<sub>r</sub>'' where ''r = 1...2<sup>n</sup>''.

For a document ''d<sub>k</sub>'' and a query ''q'' the similarity function now becomes:

:<math>sim(d_k,q) = \frac{\sum _{j=1}^n \sum _{i=1}^n w_{i,k}*w_{j,q}*t_i \cdot t_j }{\sqrt{\sum _{i=1}^n w_{i,k}^2}*\sqrt{\sum _{i=1}^n w_{i,q}^2}}</math>

where ''t<sub>i</sub>'' and ''t<sub>j</sub>'' are now vectors of a ''2<sup>n</sup>'' dimensional space.

Term correlation <math>t_i \cdot t_j</math> can be implemented in several ways. For an example, Wong et al. uses the term occurrence frequency matrix obtained from automatic indexing as input to their algorithm. The term occurrence  and the output is the term correlation between any pair of index terms.

==Semantic information on GVSM==

There are at least two basic directions for embedding term to term relatedness, other than exact keyword matching, into a retrieval model:
# compute semantic correlations between terms
# compute frequency co-occurrence statistics from large corpora

Recently Tsatsaronis<ref>{{cite | title=A Generalized Vector Space Model for Text Retrieval Based on Semantic Relatedness | url=http://www.aclweb.org/anthology/E/E09/E09-3009.pdf | last= Tsatsaronis | first=George | coauthors=Vicky Panagiotopoulou | publisher=[[Association for Computing Machinery|EACL ACM]] |date=2009-04-02}}</ref> focused on the first approach.

They measure semantic relatedness (''SR'') using a thesaurus (''O'') like [[WordNet]]. It considers the path length, captured by compactness (''SCM''), and the path depth, captured by semantic path elaboration (''SPE'').
They estimate the <math>t_i \cdot t_j</math> inner product by:

<math>t_i \cdot t_j = SR((t_i, t_j), (s_i, s_j), O)</math>

where ''s<sub>i</sub>'' and ''s<sub>j</sub>'' are senses of terms ''t<sub>i</sub>'' and ''t<sub>j</sub>'' respectively, maximizing <math>SCM \cdot SPE</math>.

== References ==
{{reflist}}

[[Category:Vector space model]]
>>EOP<<
172<|###|>Controlled vocabulary
{{refimprove|date=June 2012}}

'''Controlled vocabularies''' provide a way to organize knowledge for subsequent retrieval.  They are used in [[subject indexing]] schemes, [[subject heading]]s, [[thesauri]], [[Taxonomy (general)|taxonomies]] and other forms of [[knowledge organization system]]s. Controlled vocabulary schemes mandate the use of predefined, authorised terms that have been preselected by the designer of the vocabulary, in contrast to natural language vocabularies, where there is no restriction on the vocabulary.

== In library and information science ==

In [[library and information science]] controlled vocabulary is a carefully selected list of [[word (linguistics)|word]]s and [[phrase]]s, which are used to [[Tag (metadata)|tag]] units of information (document or work) so that they may be more easily retrieved by a search.{{ref|warner}}{{ref|fast}} Controlled vocabularies solve the problems of [[homographs]], [[synonyms]] and [[polyseme]]s by a [[bijection]] between concepts and authorized terms. In short, controlled vocabularies reduce ambiguity inherent in normal human languages where the same concept can be given different names and ensure consistency.

For example, in the [[Library of Congress Subject Headings]] (a subject heading system that uses a controlled vocabulary), authorized terms -- subject headings in this case -- have to be chosen to handle choices between variant spellings of the same concept (American versus British), choice among scientific and popular terms (Cockroaches versus ''Periplaneta americana''), and choices between synonyms (automobile versus cars), among other difficult issues.

Choices of authorized terms are based on the principles of ''user warrant'' (what terms users are likely to use), ''literary warrant'' (what terms are generally used in the literature and documents), and ''structural warrant'' (terms chosen by considering the structure, scope of the controlled vocabulary).

Controlled vocabularies also typically handle the problem of [[homographs]], with qualifiers. For example, the term "pool" has to be qualified to refer to either swimming pool, or the game pool to ensure that each authorized term or heading refers to only one concept.

There are two main kinds of controlled vocabulary tools used in libraries: subject headings and thesauri. While the differences between the two are diminishing, there are still some minor differences.

Historically subject headings were designed to describe books in library catalogs by catalogers while thesauri were used by indexers to apply index terms to documents and articles. Subject headings tend to be broader in scope describing whole books, while thesauri tend to be more specialized covering very specific disciplines. Also because of the card catalog system, subject headings tend to have terms that are in indirect order (though with the rise of automated systems this is being removed), while thesaurus terms are always in direct order. Subject headings also tend to use more pre-coordination of terms such that the designer of the controlled vocabulary will combine various concepts together to form one authorized subject heading. (e.g., children and terrorism) while thesauri tend to use singular direct terms. Lastly thesauri list not only equivalent terms but also narrower, broader terms and related terms among various authorized and non-authorized terms, while historically most subject headings did not.

For example, the [[Library of Congress Subject Heading]] itself did not have much syndetic structure until 1943, and it was not until 1985 when it began to adopt the thesauri type term "Broader term" and "Narrow term".

The [[terminology|terms]] are chosen and organized by trained professionals (including librarians and information scientists) who possess expertise in the subject area. Controlled vocabulary terms can accurately describe what a given document is actually about, even if the terms themselves do not occur within the document's text. Well known subject heading systems include the [[Library of Congress Subject Headings|Library of Congress system]], [[Medical Subject Headings|MeSH]], and [[Sears Subject Headings|Sears]]. Well known thesauri include the [[Art and Architecture Thesaurus]] and the [[Education Resources Information Center|ERIC]] Thesaurus.

Choosing authorized terms to be used is a tricky business, besides the areas already considered above, the designer has to consider the specificity of the term chosen, whether to use direct entry, inter consistency and stability of the language. Lastly the amount of pre-co-ordinate (in which case the degree of enumeration versus synthesis becomes an issue) and post co-ordinate in the system is another important issue.

Controlled vocabulary elements (terms/phrases) employed as [[Tag (metadata)|tags]], to aid in the content identification process of documents, or other information system entities (e.g. DBMS, Web Services) qualifies as [[metadata]].

== Indexing languages ==

There are three main types of indexing languages.

* Controlled indexing language - Only approved terms can be used by the indexer to describe the document
* [[Natural language]] indexing language - Any term from the document in question can be used to describe the document.
* Free indexing language  - Any term (not only from the document) can be used to describe the document.

When indexing a document, the indexer also has to choose the level of indexing exhaustivity, the level of detail in which the document is described. For example using low indexing exhaustivity, minor aspects of the work will not be described with index terms. In general the higher the indexing exhaustivity, the more terms indexed for each documen

In recent years [[free text search]] as a means of access to documents has become popular. This involves using natural language indexing with an indexing exhaustively set to maximum (every word in the text is ''indexed''). Many studies have been done to compare the efficiency and effectiveness of free text searches against documents that have been indexed by experts using a few well chosen controlled vocabulary descriptors.

Controlled vocabularies are often claimed to improve the accuracy of free text searching, such as to reduce [[Relevance (Information Retrieval)|irrelevant]] items in the retrieval list. These irrelevant items ([[false positives]]) are often caused by the inherent ambiguity of [[natural language]]. Take the English word ''football'' for example. ''Football'' is the name given to a number of different [[team sport]]s. Worldwide the most popular of these team sports is [[Football (soccer)|Association football]], which also happens to be called ''[[soccer]]'' in several countries. The [[English language]] [[football (word)|word football]] is also applied to [[Rugby football]] ([[Rugby union]] and [[rugby league]]), [[American football]], [[Australian rules football]], [[Gaelic football]], and [[Canadian football]]. A search for ''football'' therefore will retrieve documents that are about several completely different sports. Controlled vocabulary solves this problem by [[Tag (metadata)|tagging]] the documents in such a way that the ambiguities are eliminated.

Compared to free text searching, the use of a controlled vocabulary can dramatically increase the performance of an information retrieval system, if performance is measured by precision (the percentage of documents in the retrieval list that are actually [[relevance|relevant]] to the search topic).

In some cases controlled vocabulary can enhance recall as well, because unlike natural language schemes, once the correct authorized term is searched, you don't need to worry about searching for other terms that might be synonyms of that term.

However, a controlled vocabulary search may also lead to unsatisfactory [[Recall (information retrieval)|recall]], in that it will fail to retrieve some documents that are actually relevant to the search question.

This is particularly problematic when the search question involves terms that are sufficiently tangential to the subject area such that the indexer might have decided to tag it using a different term (but the searcher might consider the same). Essentially, this can be avoided only by an experienced user of controlled vocabulary whose understanding of the vocabulary coincides with the way it is used by the indexer.

Another possibility is that the article is just not tagged by the indexer because indexing exhaustivity is low. For example an article might mention football as a secondary focus, and the indexer might decide not to tag it with "football" because it is not important enough compared to the main focus. But it turns out that for the searcher that article is relevant and hence recall fails. A free text search would automatically pick up that article regardless.

On the other hand free text searches have high exhaustivity (you search on every word) so it has potential for high recall (assuming you solve the problems of synonyms by entering every combination) but will have much lower precision.

Controlled vocabularies are also quickly out-dated and in fast developing fields of knowledge, the authorized terms available might not be available if they are not updated regularly. Even in the best case scenario, controlled language is often not as specific as using the words of the text itself. Indexers trying to choose the appropriate index terms might misinterpret the author, while a free text search is in no danger of doing so, because it uses the author's own words.

The use of controlled vocabularies can be costly compared to free text searches because human experts  or expensive automated systems are necessary to index each entry.  Furthermore, the user has to be familiar with the controlled vocabulary scheme to make best use of the system. But as already mentioned, the control of synonyms, homographs can help increase precision.

Numerous methodologies have been developed to assist in the creation of controlled vocabularies, including [[faceted classification]], which enables a given data record or document to be described in multiple ways.

==Applications==
Controlled vocabularies, such as the [[Library of Congress Subject Headings]],  are an essential component of [[bibliography]], the study and classification of books. They were initially developed in [[library and information science]]. In the 1950s, government agencies  began to develop controlled vocabularies for the burgeoning journal literature in specialized fields; an example is the [[Medical Subject Headings]] (MeSH) developed by the [[United States National Library of Medicine|U.S. National Library of Medicine]]. Subsequently, for-profit firms (called Abstracting and indexing services) emerged to index the fast-growing literature in every field of knowledge. In the 1960s, an online bibliographic database industry developed based on dialup [[X.25]] networking. These services were seldom made available to the public because they were difficult to use; specialist librarians called search intermediaries handled the searching job. In the 1980s, the first [[full text]] databases appeared; these databases contain the full text of the index articles as well as the bibliographic information. Online bibliographic databases have migrated to the Internet and are now publicly available; however, most are proprietary and can be expensive to use. Students enrolled in colleges and universities may be able to access some of these services without charge; some of these services may be accessible without charge at a public library.

In large organizations, controlled vocabularies may be introduced to improve [[technical communication]]. The use of controlled vocabulary ensures that everyone is using the same word to mean the same thing.  This consistency of terms is one of the most important concepts in [[technical writing]] and [[knowledge management]], where effort is expended to use the same word throughout a [[document]] or [[organization]] instead of slightly different ones to refer to the same thing.

Web searching could be dramatically improved by the development of a controlled vocabulary for describing Web pages; the use of such a vocabulary could culminate in a [[Semantic Web]], in which the content of Web pages is described using a machine-readable [[metadata]] scheme. One of the first proposals for such a scheme is the [[Dublin Core]] Initiative. An example of a controlled vocabulary which is usable for [[Web indexing|indexing web pages]] is [[Polythematic Structured Subject Heading System|PSH]].

It is unlikely that a single metadata scheme will ever succeed in describing the content of the entire Web.{{ref|doctorow}} To create a Semantic Web, it may be necessary to draw from two or more metadata systems to describe a Web page's contents. The [[eXchangeable Faceted Metadata Language]] (XFML) is designed to enable controlled vocabulary creators to publish and share metadata systems. XFML is designed on [[faceted classification]] principles.{{ref|pilgrim}}

==See also==
*[[Controlled natural language]]
*[[IMS VDEX|IMS Vocabulary Definition Exchange]]
*[[Nomenclature]]
*[[Ontology (computer science)]]
*[[Terminology]]
*[[Thesaurus]]
*[[Universal Data Element Framework]]
*[[Vocabulary-based transformation]]

==References==
#{{note|warner}} Amy Warner, [http://www.ischool.utexas.edu/~i385e/readings/Warner-aTaxonomyPrimer.html A taxonomy primer].
#{{note|fast}} Karl Fast, Fred Leise and Mike Steckel, [http://boxesandarrows.com/what-is-a-controlled-vocabulary/]
#{{note|doctorow}} Cory Doctorow, [http://www.well.com/~doctorow/metacrap.htm Metacrap].
#{{note|pilgrim}} Mark Pilgrim, [http://petervandijck.com/xfml/ eXchangeable Faceted Metadata Language].
#[http://www.imresources.fit.qut.edu.au/vocab/ Controlled Vocabularies] {{Dead link|date=February 2011}} Links to examples of thesauri and classification schemes.
#[http://www.fao.org/aims/kos_list_type.htm Controlled Vocabularies] {{Dead link|date=February 2011}} Links to examples of thesauri and classification schemes used in the domain of Agriculture, Fisheries, Forestry etc.

==External links==
* [http://www.controlledvocabulary.com/ controlledvocabulary.com]  explains how controlled vocabularies are useful in describing images and information for classifying content in electronic databases.
* [http://www.photo-keywords.com/ photo-keywords.com/]  useful guides to creating and editing your own controlled vocabulary suitable for image cataloging.
* [http://www.niso.org/standards/resources/Z39-19.html ANSI/NISO Z39.19 - 2005 Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies]

{{Lexicography}}

[[Category:Searching]]
[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Technical communication]]
[[Category:Semantic Web]]
[[Category:Ontology (information science)]]
[[Category:Controlled vocabularies]]
[[Category:Library science]]
[[Category:Information science]]
>>EOP<<
178<|###|>Search-oriented architecture
{{unreferenced|date=October 2007}}
The use of [[search engine technology]] is the main integration component in an [[information system]]. In a traditional business environment the [[architectural layer]] usually occupied by a [[relational database management system]] (RDBMS) is supplemented or replaced with a search engine or the indexing technology used to build search engines. Queries for information which would usually be performed using [[Structured Query Language]] (SQL) are replaced by keyword or fielded (or field-enabled) searches for structured, [[Semi-structured model|semi-structured]], or unstructured data.

In a typical [[Multitier architecture|multi-tier]] or [[Multitier architecture|N tier]] architecture information is maintained in a data tier where it can be stored and retrieved from a database or file system. The data tier is queried by the logic or business tier when information is needed using a data retrieval language like SQL.

In a '''search-oriented architecture''' the data tier may be replaced or placed behind another tier which contains a search engine and search engine index which is queried instead of the database management system. Queries from the business tier are made in the search engine query language instead of SQL. The search engine itself crawls the relational database management system in addition to other traditional data sources such as web pages or traditional file systems and consolidates the results when queried.

The benefit of adding a search layer to the architecture stack is rapid response time large dynamic datasets made possible by search indexing technology such as an [[inverted index]]. 

== Contrast with ==
* [[Service-oriented architecture]] (SOA)
* [[Service-Oriented Modeling]]

== See also ==
* [[Hibernate search]]
 
[[Category:Software architecture]]
[[Category:Data search engines]]
[[Category:Searching]]
>>EOP<<
184<|###|>Unified Information Access
{{Multiple issues|
{{confusing|date=November 2010}}
{{cleanup|date=November 2010}}
}}

'''Unified Information Access Platforms''' are [[computing platforms]] that integrate large volumes of [[unstructured information|unstructured]], semi-structured, and structured information into a unified environment for processing, analysis and decision-making. These platforms are highly scalable, hybrid architectures that combine elements of database and search technologies in order to make information access dynamic and ad hoc, while offering the reporting and visualization features commonly found in business intelligence applications. While the vision for such integrated platforms has been around for years, only since 20XX have products been released into the market. Companies like [[Applied Relevance]], [[Attivio]], [[BA-Insight]], [[Cambridge Semantics]], [[Endeca Technologies Inc.|Endeca]], [[Exalead]], [[HP Autonomy]], [[PolySpot]], [[MarkLogic]], [[PerfectSearch]], [[Palantir Technologies|Palantir]], [[TopQuadrant]], [[Sinequa]] and [http://www.virtualworks.com VirtualWorks] have recognized the need for this approach.

Unified access applications:
*Create [[Hybrid computer|hybrid]] data structures that combine structured data and data operators with [[Text (literary theory)|text]] and semi-structured operations and analytics. They combine semantic understanding, fuzzy matching, sorting, joins, and various operations such as [[range searching]] within a single architecture, rather than federating a query to multiple sources in multiple forms.
*Leverage these hybrid structures to provide real-time access through ad hoc queries to multiple sources of information, including information across a spectrum of [[File format|format]]s (e.g. rich media) through a single [[Interface (computer science)|interface]].
*Handle sparse matrices of unpredictable content.
*Optimize interactions for consumption and decisions, [[Process (computing)|processing]] queries faster than traditional database and/or BI applications and implementing visual consumption metaphors.
*Scale to [[terabyte]]s.
*Provide reporting tools that are BI-like, or integrate easily with BI applications and reporting tools.

== References ==
* Worldwide Search and Discovery 2009 Vendor Shares and Update on Market Trends, IDC #223926, July, 2010 by Susan Feldman and Hadley Reynolds.
* Building the Intelligent Enterprise: The Case for Unified Access and Analytics
Susan Feldman, Jul 2009 - Doc # 219467   
<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->
{{Reflist}}

== External links ==
* Video: [http://www.attivio.com/poweringbusiness/videos/63-attivio/869-unified-information-access-in-4-minutes.html "Unified Information Access in 4 Minutes"]
* http://www.computerworld.com/s/article/9180280/Five_Advantages_of_Unified_Information_Access_UIA
* http://www.eweek.com/c/a/Enterprise-Applications/How-to-Use-Unified-Information-Access-to-Get-Most-Value-from-Your-Data/

[[Category:Searching]]
>>EOP<<
190<|###|>Bayesian search theory
'''Bayesian search theory''' is the application of [[Bayesian statistics]] to the search for lost objects. It has been used several times to find lost sea vessels, for example the [[USS Scorpion (SSN-589)|USS ''Scorpion'']]. It also played a key role in the recovery of the flight recorders in the [[Air France Flight 447]] disaster of 2009. 

==Procedure==

The usual procedure is as follows:

# Formulate as many reasonable hypotheses as possible about what may have happened to the object. 
# For each hypothesis, construct a [[probability density function]] for the location of the object.
# Construct a function giving the probability of actually finding an object in location&nbsp;X when searching there if it really is in location&nbsp;X. In an ocean search, this is usually a function of water depth  in shallow water chances of finding an object are good if the search is in the right place. In deep water chances are reduced.
# Combine the above information coherently to produce an overall probability density map. (Usually this simply means multiplying the two functions together.) This gives the probability of finding the object by looking in location&nbsp;X, for all possible locations&nbsp;X. (This can be visualized as a [[contour map]] of probability.)
# Construct a search path which starts at the point of highest probability and 'scans' over high probability areas, then intermediate probabilities, and finally low probability areas.
# Revise all the probabilities continuously during the search. For example, if the hypotheses for location&nbsp;X imply the likely disintegration of the object and the search at location&nbsp;X has yielded no fragments, then the probability that the object is somewhere around there is greatly reduced (though not usually to zero) while the probabilities of its being at other locations is correspondingly increased. The revision process is done by applying [[Bayes' theorem]].

In other words, first search where it most probably will be found, then search where finding it is less probable, then search where the probability is even less (but still possible due to limitations on fuel, range, water currents, etc.), until insufficient hope of locating the object at acceptable cost remains.

The advantages of the Bayesian method are that all information available is used coherently (i.e., in a "leak-proof" manner) and the method automatically produces estimates of the cost for a given success probability. That is, even before the start of searching, one can say, hypothetically, "there is a 65% chance of finding it in a 5-day search. That probability will rise to 90% after a 10-day search and 97% after 15&nbsp;days" or a similar statement. Thus the economic viability of the search can be estimated before committing resources to a search.

Apart from the [[USS Scorpion (SSN-589)|USS ''Scorpion'']], other vessels located by Bayesian search theory include the [[MV Derbyshire|MV&nbsp;''Derbyshire'']], the largest British vessel ever lost at sea, and the [[SS Central America|SS&nbsp;''Central America'']]. It also proved successful in the search for a lost [[hydrogen bomb]] following the [[1966 Palomares B-52 crash]] in Spain, and the recovery in the Atlantic Ocean of the crashed [[Air France Flight 447]].

Bayesian search theory is incorporated into the CASP (Computer Assisted Search Program) mission planning software used by the [[United States Coast Guard]] for [[search and rescue]]. This program was later adapted for inland search by adding terrain and ground cover factors for use by the [[United States Air Force]] and [[Civil Air Patrol]].

==Mathematics==

Suppose a grid square has a probability ''p'' of containing the wreck and that the probability of successfully detecting the wreck if it is there is ''q''. If the square is searched and no wreck is found, then, by Bayes' theorem, the revised probability of the wreck being in the square is given by

: <math>  p' = \frac{p(1-q)}{(1-p)+p(1-q)} = p \frac{1-q}{1-pq} < p.</math>
For each other grid square, if its prior probability is ''r'', its posterior probability is given by

: <math> r' = r \frac{1}{1- pq} > r. </math>


==Optimal Distribution of Search Effort==

The classical book on this subject, based on probabilistic information, by [[Lawrence D. Stone]], won the 1975 [[Frederick W. Lanchester Prize]] by the [[Operations Research Society of America]].



<!-- Material on USS Scorpion, moved from Bayesian inference

In May 1968, the [[U.S. Navy]]'s [[nuclear submarine]] [[USS Scorpion (SSN-589)|USS ''Scorpion'' (SSN-589)]] failed to arrive as expected at her home port of [[Norfolk, Virginia]]. The command officers of the U.S. Navy were nearly convinced that the vessel had been lost off the [[East Coast of the United States|Eastern Seaboard]],  but an extensive search there failed to discover the remains of the ''Scorpion''.

Then, a Navy deep-water expert, [[John Craven USN|John P. Craven]], suggested that the USS ''Scorpion'' had sunk elsewhere. Craven organised a search southwest of the [[Azores]] based on a controversial approximate triangulation by [[hydrophone]]s. He was allocated only a single ship, the [[USNS Mizar (AGOR-11)|''Mizar'']], and he took advice from a firm of consultant mathematicians in order to maximise his resources. A Bayesian search methodology was adopted. Experienced submarine commanders were interviewed to construct hypotheses about what could have caused the loss of the ''Scorpion''.

The sea area was divided up into grid squares and a probability assigned to each square, under each of the hypotheses, to give a number of probability grids, one for each hypothesis. These were then added together to produce an overall probability grid. The probability attached to each square was then the probability that the wreck was in that square. A second grid was constructed with probabilities that represented the probability of successfully finding the wreck if that square were to be searched and the wreck were to be actually there. This was a known function of water depth. The result of combining this grid with the previous grid is a grid which gives the probability of finding the wreck in each grid square of the sea if it were to be searched.


==Optimal Distribution of Search Effort==

The classical book on this subject by [[Lawrence D. Stone]] won the 1975 [[Lancaster Prize]] by the American Operations Research Society.

-->

==See also==
* [[Bayesian inference]]
* [[Search games]]

== References ==
* [[Stone, Lawrence D.]], ''The Theory of Optimal Search'', published by the [[Operations Research Society of America]], 1975
* [[Stone, Lawrence D.]], In search of Air France Flight 447. Institute of Operations Research and the Management Sciences, 2011
* Iida, Koji., '' Studies on the Optimal Search Plan'', Vol.&nbsp;70, Lecture Notes in Statistics, Springer-Verlag, 1992.
* De Groot, Morris H., ''Optimal Statistical Decisions'', Wiley Classics Library, 2004.
* Richardson, Henry R; and Stone, Lawrence D. Operations Analysis during the underwater search for ''Scorpion''. ''Naval Research Logistics Quarterly'', June&nbsp;1971, Vol.&nbsp;18, Number&nbsp;2. Office of Naval Research.
* Stone, Lawrence D. Search for the SS ''Central America'': Mathematical Treasure Hunting. Technical Report, Metron Inc. Reston, Virginia.
* [[Bernard Koopman|Koopman, B.O.]] ''Search and Screening'', Operations Research Evaluation Group Report 56, Center for Naval Analyses, Alexandria, Virginia. 1946.
* Richardson, Henry R; and Discenza, J.H. The United States Coast Guard computer-assisted search planning system (CASP). ''Naval Research Logistics Quarterly''. Vol.&nbsp;27 number&nbsp;4. pp.&nbsp;659680. 1980.
* [[Ross, Sheldon M.]], ''An Introduction to Stochastic Dynamic Programming'', Academic Press. 1983.

[[Category:Bayesian statistics|Search theory]]
[[Category:Searching]]
[[Category:Search algorithms]]
[[Category:Operations research]]
>>EOP<<
196<|###|>Category:Concordances (publishing)
{{Cat main|Concordance (publishing)}}
[[Category:Biblical studies]]
[[Category:Indexing]]
[[Category:Linguistics]]
[[Category:Reference works]]
[[Category:Searching]]
[[Category:Hypertext]]
>>EOP<<
202<|###|>Tversky index
The '''Tversky index''', named after [[Amos Tversky]],<ref>{{cite journal |last=Tversky |first=Amos |title=Features of Similarity |journal=Psychological Reviews |volume=84 |number=4 |year=1977 |pages=327352 |url=http://www.cogsci.ucsd.edu/~coulson/203/tversky-features.pdf}}</ref> is an asymmetric [[similarity measure]] on [[set theory|sets]] that compares a variant to a prototype. The Tversky index can be seen as a generalization of [[Dice's coefficient]] and [[Tanimoto coefficient]].

For sets ''X'' and ''Y'' the Tversky index is a number between 0 and 1 given by 

<math>S(X, Y) = \frac{| X \cap Y |}{| X \cap Y | + \alpha | X - Y | + \beta | Y - X |} </math>,

Here, <math>X - Y</math> denotes the  [[Complement (set theory)| relative complement ]] of Y in X.

Further, <math>\alpha, \beta \ge 0 </math> are parameters of the Tversky index.  Setting <math>\alpha = \beta = 1 </math> produces the Tanimoto coefficient; setting <math>\alpha = \beta = 0.5 </math> produces Dice's coefficient. 

If we consider ''X'' to be the prototype and ''Y'' to be the variant, then <math>\alpha</math> corresponds to the weight of the prototype and <math>\beta</math> corresponds to the weight of the variant. Tversky measures with <math>\alpha + \beta = 1</math> are of special interest.<ref>http://www.daylight.com/dayhtml/doc/theory/theory.finger.html</ref>

Because of the inherent asymmetry, the Tversky index does not meet the criteria for a similarity metric. However, if symmetry is needed a variant of the original formulation has been proposed using '''max''' and '''min''' functions <ref>Jimenez, S., Becerra, C., Gelbukh, A. [http://aclweb.org/anthology/S/S13/S13-1028.pdf SOFTCARDINALITY-CORE: Improving Text Overlap with Distributional Measures for Semantic Textual Similarity]. Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, p.194-201, June 78, 2013, Atlanta, Georgia, USA.</ref>
.

<math>S(X,Y)=\frac{| X \cap Y |}{| X \cap Y |+\beta\left(\alpha a+(1-\alpha)b\right)}</math>,

<math>a=\min\left(|X-Y|,|Y-X|\right) </math>,

<math>b=\max\left(|X-Y|,|Y-X|\right) </math>,

This formulation also re-arranges parameters <math>\alpha </math> and <math>\beta </math>. Thus, <math> \alpha </math> controls the balance between <math> |X - Y| </math> and <math> |Y - X| </math> in the denominator. Similarly, <math>\beta</math> controls the effect of the symmetric difference <math> |X\,\triangle\,Y\,| </math> versus <math> | X \cap Y | </math> in the denominator.

==Notes==
{{reflist}}

[[Category:Index numbers]]
[[Category:String similarity measures]]
[[Category:Measure theory]]
>>EOP<<
208<|###|>JaroWinkler distance
{{About|the measure|other uses|Jaro (disambiguation){{!}}Jaro}}

{{Original research|date=May 2013}}
{{Notability|date=May 2013}}

In [[computer science]] and [[statistics]], the '''JaroWinkler distance''' (Winkler, 1990) is a measure of similarity between two [[String (computer science)|strings]].  It is a variant of the '''Jaro distance''' metric (Jaro, 1989, 1995), a type of string [[edit distance]], and was developed in the area of [[record linkage]] (duplicate detection) (Winkler, 1990). The higher the JaroWinkler distance for two strings is, the more similar the strings are.  The JaroWinkler distance metric is designed and best suited for short strings such as person names.  The score is normalized such that 0 equates to no similarity and 1 is an exact match.

== Definition ==

The Jaro distance <math>d_j</math> of two given strings <math>s_1</math> and <math>s_2</math> is

: <math>d_j = \left\{

\begin{array}{l l}
  0 & \text{if }m = 0\\
  \frac{1}{3}\left(\frac{m}{|s_1|} + \frac{m}{|s_2|} + \frac{m-t}{m}\right) & \text{otherwise} \end{array} \right.</math>

Where:

* <math>m</math> is the number of ''matching characters'' (see below);
* <math>t</math> is half the number of ''transpositions'' (see below).

Two characters from <math>s_1</math> and <math>s_2</math> respectively, are considered ''matching'' only if they are the same and not farther than <math>\left\lfloor\frac{\max(|s_1|,|s_2|)}{2}\right\rfloor-1</math>.

Each character of <math>s_1</math> is compared with all its matching
characters in <math>s_2</math>. The number of matching (but different sequence order) characters
divided by 2 defines the number of ''transpositions''.
For example, in comparing CRATE with TRACE, only 'R'   'A'   'E'  are the matching characters, i.e. m=3. Although 'C', 'T' appear in both strings, they are farther than 1, i.e., floor(5/2)-1=1. Therefore, t=0 . In DwAyNE versus DuANE the matching letters are already in the same order D-A-N-E, so no transpositions are needed.

JaroWinkler distance uses a [[prefix]] scale <math>p</math> which gives more favourable ratings to strings that match from the beginning for a set prefix length <math>\ell</math>.  Given two strings <math>s_1</math> and <math>s_2</math>, their JaroWinkler distance <math>d_w</math> is:

: <math>d_w = d_j + (\ell p (1 - d_j))</math>

where:

* <math>d_j</math> is the Jaro distance for strings <math>s_1</math> and <math>s_2</math>
* <math>\ell</math> is the length of common prefix at the start of the string up to a maximum of 4 characters
* <math>p</math> is a constant [[scaling factor]] for how much the score is adjusted upwards for having common prefixes.  <math>p</math> should not exceed 0.25, otherwise the distance can become larger than 1.  The standard value for this constant in Winkler's work is <math>p = 0.1</math>

Although often referred to as a ''distance metric'', the JaroWinkler distance is actually not a [[metric (mathematics)|metric]] in the mathematical sense of that term because it does not obey the [[triangle inequality]] [http://richardminerich.com/tag/jaro-winkler/].

In some implementations of Jaro-Winkler, the prefix bonus <math>\ell p (1 - d_j)</math> is only added when the compared strings have a Jaro distance above a set "boost threshold" <math>b_t</math>. The boost threshold in Winkler's implementation was 0.7.

: <math>d_w = \left\{

\begin{array}{l l}
  d_j & \text{if }d_j < b_t\\
  d_j + (\ell p (1 - d_j)) & \text{otherwise} \end{array} \right.</math>

== Example ==

''Note that Winkler's "reference" C code differs in at least two ways from published accounts of the JaroWinkler metric. First is his use of a typo table (adjwt) and also some optional additional tolerance for long strings.''

Given the strings <math>s_1</math> ''MARTHA'' and <math>s_2 </math> ''MARHTA'' we find:

* <math>m = 6</math>
* <math>|s_1| = 6</math>
* <math>|s_2| = 6</math>
* There are mismatched characters T/H and H/T leading to <math>t = \frac{2}{2} = 1</math>

We find a Jaro score of:

<math>d_j = \frac{1}{3}\left(\frac{6}{6} + \frac{6}{6} + \frac{6-1}{6}\right) = 0.944</math>

To find the JaroWinkler score using the standard weight <math>p = 0.1</math>, we continue to find:

* <math>\ell = 3</math>

Thus:

: <math>d_w = 0.944 + (3 * 0.1 (1 - 0.944)) = 0.961</math>

Given the strings <math>s_1</math> ''DWAYNE'' and <math>s_2</math> ''DUANE'' we find:

* <math>m = 4</math>
* <math>|s_1| = 6</math>
* <math>|s_2| = 5</math>
* <math>t = 0</math>

We find a Jaro score of:

: <math>d_j = \frac{1}{3}\left(\frac{4}{6} + \frac{4}{5} + \frac{4-0}{4}\right) = 0.822</math>

To find the JaroWinkler score using the standard weight <math>p = 0.1</math>, we continue to find:

* <math>\ell = 1</math>

Thus:

: <math>d_w = 0.822 + (1 * 0.1 (1 - 0.822)) = 0.84</math>

Given the strings <math>s_1</math> ''DIXON'' and <math>s_2</math> ''DICKSONX'' we find:

{{elucidate|date=March 2013}}

{| class="wikitable"
|-
|
| D
| I
| X
| O
| N
|-
| D
| <span style="background: #ffcc33">1
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">0
| 0
|-
| I
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">1
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">0
|-
| C
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">0
|-
| K
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">0
|-
| S
| 0
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">0
|-
| O
| 0
| 0
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">1
| <span style="background: #ffcc33">0
|-
| N
| 0
| 0
| 0
| <span style="background: #ffcc33">0
| <span style="background: #ffcc33">1
|-
| X
| 0
| 0
| 0
| 0
| <span style="background: #ffcc33">0
|}

* <math>m = 4</math>  Note that the two ''X''s are not considered matches because they are outside the match window of 3.

* <math>|s_1| = 5</math>
* <math>|s_2| = 8</math>
* <math>t = 0</math>

We find a Jaro score of:

: <math>d_j = \frac{1}{3}\left(\frac{4}{5} + \frac{4}{8} + \frac{4-0}{4}\right) = 0.767</math>

To find the JaroWinkler score using the standard weight <math>p = 0.1</math>, we continue to find:

* <math>\ell = 2</math>

Thus:

: <math>d_w = 0.767 + (2 * 0.1 (1 - 0.767)) = 0.814</math>

== See also ==

* [[Levenshtein distance]]
* [[Record linkage]]
* [[Census]]

== References ==

* {{cite journal | last=Cohen |first=W. W. |last2=Ravikumar |first2=P. |last3=Fienberg |first3=S. E. |year=2003 |title=A comparison of string distance metrics for name-matching tasks |journal=KDD Workshop on Data Cleaning and Object Consolidation |volume=3 |pages=73-8 |url=https://www.cs.cmu.edu/afs/cs/Web/People/wcohen/postscript/kdd-2003-match-ws.pdf}}
* {{cite journal | author = [[Matthew A. Jaro|Jaro, M. A.]] | title = Advances in record linkage methodology as applied to the 1985 census of Tampa Florida | journal = Journal of the American Statistical Association | year = 1989 | volume = 84 | issue = 406 |pages=41420| url = | doi = 10.1080/01621459.1989.10478785 }}
* {{cite journal |author=Jaro, M. A. |title=Probabilistic linkage of large public health data file  |journal= Statistics in Medicine |year=1995 |volume=14 |issue=57 |pages=4918  |pmid=7792443 |doi=10.1002/sim.4780140510}}
* {{cite journal

  | author = [[William E. Winkler|Winkler, W. E.]]
  | title = String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage
  | journal = Proceedings of the Section on Survey Research Methods
  | publisher = American Statistical Association
  | pages = 354359
  | year = 1990
  | url = http://www.amstat.org/sections/srms/Proceedings/papers/1990_056.pdf }}

* {{cite journal | author = [[William E. Winkler|Winkler, W. E.]] | title = Overview of Record Linkage and Current Research Directions | journal = Research Report Series, RRS | year = 2006 | volume = | issue = | url = http://www.census.gov/srd/papers/pdf/rrs2006-02.pdf}}

== External links ==

* [http://web.archive.org/web/20100227020019/http://www.census.gov/geo/msb/stand/strcmp.c strcmp.c - Original C Implementation by the author of the algorithm]

{{DEFAULTSORT:Jaro-Winkler distance}}

[[Category:String similarity measures]]
>>EOP<<
214<|###|>Most frequent k characters
{{multiple issues|
{{Third-party|date=March 2014}}
{{Notability|date=March 2014}}
}}

In [[information theory]], '''MostFreqKDistance''' is a [[string metric]] technique for quickly estimating how [[Similarity measure|similar]] two [[Order theory|ordered sets]] or [[String (computer science)|strings]] are. The scheme was invented by {{harvs|first=Sadi Evren|last=SEKER|authorlink=Sadi Evren SEKER|year=2014|txt}},<ref name="mfkc"/> and initially used in [[text mining]] applications like [[author recognition]].<ref name="mfkc">{{citation
 | last1 = SEKER | first1 = Sadi E. | author1-link = Sadi Evren SEKER
 | last2 = Altun | first2 = Oguz
 | last3 = Ayan | first3 = Ugur
 | last4 = Mert | first4 = Cihan
 | contribution = A Novel String Distance Function based on Most Frequent K Characters
 | volume = 4
 | issue = 2
 | pages = 177183
 | publisher = [[International Association of Computer Science and Information Technology Press (IACSIT Press)]]
 | title = [[International Journal of Machine Learning and Computing (IJMLC)]]
 | contribution-url = http://arxiv.org/abs/1401.6596
 | year = 2014}}</ref>
Method is originally based on a hashing function MaxFreqKChars <ref name="hashfunc">{{citation
 | last1 = Seker | first1 = Sadi E. | author1-link = Sadi Evren SEKER
 | last2 = Mert | first2 = Cihan
 | contribution = A Novel Feature Hashing For Text Mining
 | url = http://journal.ibsu.edu.ge/index.php/jtst/article/view/428
 | pages = 3741
 | publisher = [[International Black Sea University]]
 | title = Journal of Technical Science and Technologies
 | ISSN = 2298-0032
 | volume = 2
 | issue = 1
 | year = 2013}}</ref> classical [[author recognition]] problem and idea first came out while studying on [[data stream mining]].<ref name="author">{{citation
 | last1 = Seker | first1 = Sadi E. | author1-link = Sadi Evren SEKER
 | last2 = Al-Naami | first2 = Khaled
 | last3 = Khan | first3 = Latifur
 | contribution = Author attribution on streaming data
 | doi = 10.1109/IRI.2013.6642511
 | url = http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6642511
 | pages = 497503
 | publisher = [[IEEE]]
 | title = Information Reuse and Integration (IRI), 2013 IEEE 14th International Conference on, San Francisco, USA, Aug 14-16, 2013
 | year = 2013}}</ref> Algorithm is suitable for coding in most of the programming languages like [[Java (programming language)|Java]], [[Tcl]], [[Python (programming language)|Python]] or [[J (programming language)|J]]. <ref>
{{Citation |title=Rosetta Code: Most frequent k chars distance , code sources for Python, Java, Tcl and J |accessdate=16 Oct 2014 |url=http://rosettacode.org/wiki/Most_frequent_k_chars_distance}}
</ref>


==Definition==
Method has two steps.
* [[Hash function|Hash]] input strings str1 and str2 separately using MostFreqKHashing and output hstr1 and hstr2 respectively
* Calculate string distance (or string similarity coefficient) of two hash outputs, hstr1 and hstr2 and output an integer value

===Most frequent K hashing===
The first step of algorithm is calculating the hashing based on the most frequent k characters. The hashing algorithm has below steps:
<syntaxhighlight lang="Java">
String function MostFreqKHashing (String inputString, int K)
    def string outputString
    for each distinct character
        count occurrence of each character
    for i := 0 to K
        char c = next most freq ith character  (if two chars have same frequency than get the first occurrence in inputString)
        int count = number of occurrence of the character
        append to outputString, c and count
    end for
    return outputString
</syntaxhighlight>

Above function, simply gets an input string and an integer K value and outputs the most frequent K characters from the input string. The only condition during the creation of output string is adding the first occurring character first, if the frequencies of two characters are equal. Similar to the most of [[hashing function]]s, ''Most Frequent K Hashing'' is also a [[one way function]].

===Most frequent K distance===
The second step of algorithm works on two outputs from two different input strings and outputs the similarity coefficient (or distance metric).
<syntaxhighlight lang="Java">
int function MostFreqKSimilarity (String inputStr1, String inputStr2, int limit)
    def int similarity
    for each c = next character from inputStr1
        lookup c in inputStr2
        if c is null
             continue
             similarity += frequency of c in inputStr1
    return limit-similarity
</syntaxhighlight>
Above function, simply gets two input strings, previously outputted from the <code>MostFreqKHashing</code> function. From the most frequent k hashing function, the characters and their frequencies are returned. So, the similarity function calculates the similarity based on characters and their frequencies by checking if the same character appears on both strings. The limit is usually taken to be 10 and in the end the function returns the result of the subtraction of the sum of similarities from limit.

In some implementations, the distance metric is required instead of similarity coefficient. In order to convert the output of above similarity coefficient to distance metric, the output can be subtracted from any constant value (like the maximum possible output value). For the case, it is also possible to implement a [[wrapper function]] over above two functions.

===String distance wrapper function===
In order to calculate the distance between two strings, below function can be implemented
<syntaxhighlight lang="Java">
int function MostFreqKSDF (String inputStr1, String inputStr2, int K, int maxDistance)
    return maxDistance - MostFreqKSimilarity(MostFreqKHashing(inputStr1, K), MostFreqKHashing(inputStr2, K))
</syntaxhighlight>

Any call to above string distance function will supply two input strings and a maximum distance value. The function will calculate the similarity and subtract that value from the maximum possible distance. It can be considered as a simple [[additive inverse]] of similarity.

==Examples==
Let's consider maximum 2 frequent hashing over two strings research and seeking.
MostFreqKHashing('research', 2) = r2e2
because we have 2 'r' and 2 'e' characters with the highest frequency and we return in the order they appear in the string.
MostFreqKHashing('seeking', 2) = e2s1
Again we have character 'e' with highest frequency and rest of the characters have same frequency of 1, so we return the first character of equal frequencies, which is 's'.
Finally we make the comparison:
MostFreqKSimilarity('r2e2', 'e2s1') = 2
We simply compared the outputs and only character occurring in both input is character 'e' and the occurrence in both input is 2.
Instead running the sample step by step as above, we can simply run by using the string distance wrapper function as below:
MostFreqKSDF('research', 'seeking', 2) = 2

Below table holds some sample runs between example inputs for K=2:
{|class="wikitable"
|-
! Inputs
! Hash Outputs
! SDF Output (max from 10)
|-
|'night'
'nacht'
|n1i1
n1a1
|9
|-
|'my'
'a'
|m1y1
a1NULL0
|10
|-
|research
research	
|r2e2
r2e2	
|6
|-
|aaaaabbbb
ababababa	
|a5b4
a5b4	
|1
|-
|significant
capabilities	
|i3n2
i3a2	
|7
|}

Method is also suitable for bioinformatics to compare the genetic strings like in [[FASTA format]].

Str1 = LCLYTHIGRNIYYGSYLYSETWNTGIMLLLITMATAFMGYVLPWGQMSFWGATVITNLFSAIPYIGTNLV

Str2 = EWIWGGFSVDKATLNRFFAFHFILPFTMVALAGVHLTFLHETGSNNPLGLTSDSDKIPFHPYYTIKDFLG

MostFreqKHashing(str1, 2) = L9T8

MostFreqKHashing(str2, 2) = F9L8

MostFreqKSDF(str1, str2, 2, 100) = 83

==Algorithm complexity and comparison==
The motivation behind algorithm is calculating the similarity between two input strings. So, the hashing function should be able to reduce the size of input and at the same time keep the characteristics of the input. Other hashing algorithms like [[MD5]] or [[SHA-1]], the output is completely unrelated with the input and those hashing algorithms are not suitable for string similarity check.

On the other hand string similarity functions like [[Levenshtein distance]] have the algorithm complexity problem.

Also algorithms like [[Hamming distance]], [[Jaccard coefficient]] or [[Tanimoto coefficient]] have relatively low algorithm complexity but the success rate in [[text mining]] studies are also low.

===Time complexity===
The calculation of time complexity of 'most frequent k char string similarity' is quite simple. In order to get the maximum frequent K characters from a string, the first step is sorting the string in a lexiconical manner. After this sort, the input with highest occurrence can be achieved with a simple pass in linear time complexity. Since major classical sorting algorithms are working in O(nlogn) complexity like [[merge sort]] or [[quick sort]], we can sort the first string in O(nlogn) and second string on O(mlogm) times. The total complexity would be O(nlog n ) + O (m log m) which is O(n log n) as the upper bound [[worst case analysis]].

===Comparison===
Below table compares the complexity of algorithms:
{|class="wikitable"
|-
! Algorithm
! Time Complexity
|-
| [[Levenshtein distance]]
| O(nm) = O(n^2)
|-
| [[Jaccard index]]
| O(n+m) = O(n)
|-
| MostFreqKSDF
| O(nlogn+mlogm) = O(n log n)
|}

For the above table, n is the length of first string and m is the length of second string.

==Success on text mining==
The success of string similarity algorithms are compared on a study. The study is based on IMDB62 dataset which is holding 1000 comment entries in [[Internet Movie Database]] from each 62 people. The data set is challenged for three string similarity functions and the success rates are as below:

{|class="wikitable"
|-
! Algorithm
! Running Time
! Error (RMSE)
! Error (RAE)
|-
|[[Levenshtein distance]]
|3647286.54 sec
|29
|0.47
|-
|[[Jaccard index]]
|228647.22 sec
|45
|0.68
|-
|MostFreqKSDF
|2712323.51 sec
|32
|0.49
|}

The running times for [[author recognition]] are in seconds and the error rates are [[root mean square error]] (RMSE) and [[relative absolute error]] (RAE).

Above table shows, the 'most frequent k similarity' is better than [[Levenshtein distance]] by time and [[Jaccard index]] by success rate.

For the time performance and the success rates, the bitwise similarity functions like [[Dice's coefficient|SrensenDice index]], [[Tversky index]] or [[Hamming Distance]] are all in the same category with similar success rates and running times. There are obviously slight differences but the idea behind bitwise operation, looses the string operations like deletion or addition. For example a single bit addition to the front of one of the input strings would yield a catastrophic result on the similarity for bitwise operators while Levenshtein distance is successfully catching.

Unfortunately, [[big data]] studies requires a faster algorithm with still acceptable success. Here the 'max frequent k characters' is an easy and simple algorithm (as in [[Occams razor]]), which is straight forward to implement.

==See also==
[http://rosettacode.org/wiki/Most_frequent_k_chars_distance RosettaCode,Code reposistory of Most Frequent K Chars Distance Algorithm in Java, Python, TCL or J languages] (Retrieved Oct. 16 2014)
<div class= style="-moz-column-count:2; column-count:2;">
* [[agrep]]
* [[Approximate string matching]]
* [[Bitap algorithm]]
* [[DamerauLevenshtein distance]]
* [[diff]]
* [[MinHash]]
* [[Dynamic time warping]]
* [[Euclidean distance]]
* [[Fuzzy string searching]]
* [[Hamming weight]]
* [[Hirschberg's algorithm]]
* [[Sequence homology|Homology of sequences in genetics]]
* [[HuntMcIlroy algorithm]]
* [[Jaccard index]]
* [[JaroWinkler distance]]
* [[Levenshtein distance]]
* [[Longest common subsequence problem]]
* [[Lucene]] (an open source search engine that implements edit distance)
* [[Manhattan distance]]
* [[Metric space]]
* [[NeedlemanWunsch algorithm]]
* [[Optimal matching]] algorithm
* [[Sequence alignment]]
* Similarity space on [[Numerical taxonomy]]
* [[SmithWaterman algorithm]]
* [[Srensen similarity index]]
* [[String distance metric]]
* [[String similarity function]]
* [[Wagner-Fischer algorithm]]
* [[Locality-sensitive hashing]]
</div>

==References==
{{reflist}}

[[Category:String similarity measures]]
[[Category:Dynamic programming]]
[[Category:Articles with example pseudocode]]
[[Category:Quantitative linguistics]]
[[Category:Hash functions]]
[[Category:Hashing]]
>>EOP<<
220<|###|>Scopus
{{Use dmy dates|date=August 2013}}
{{other uses}}
{{infobox bibliographic database
| title = Scopus
| image = [[File:Scopus_type_logo.jpg]]
| caption = Scopus logo
| producer = [[Elsevier]]
| country = 
| history = 
| languages = English
| providers = 
| cost = Subscription
| disciplines= 
| depth = 
| formats = 
| temporal = 1995-present
| geospatial = Worldwide
| number = 55 million
| updates = 
| p_title = 
| p_dates = 
| ISSN = 
| web = http://www.scopus.com
| titles = 
}}
'''Scopus''' is a [[bibliographic database]] containing [[Abstract (summary)|abstracts]] and [[citation]]s for [[academic journal]] [[Article (publishing)|articles]]. It covers nearly 22,000 titles from over 5,000 publishers, of which 20,000 are [[peer review|peer-reviewed]] journals in the scientific, technical, medical, and social sciences (including arts and humanities).<ref>{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview |title=Scopus Content Overview |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}</ref> It is owned by [[Elsevier]] and is available online by [[subscription business model|subscription]]. Searches in Scopus also incorporate searches of patent databases.<ref>{{cite journal |doi=10.1001/jama.2009.1307 |title=Comparisons of Citations in Web of Science, Scopus, and Google Scholar for Articles Published in General Medical Journals |year=2009 |last1=Kulkarni |first1=A. V. |last2=Aziz |first2=B. |last3=Shams |first3=I. |last4=Busse |first4=J. W. |journal=[[JAMA (journal)|JAMA]] |volume=302 |issue=10 |pages=10926 |pmid=19738094}}</ref>

Since Elsevier is the owner of Scopus and is also one of the main international publishers of scientific journals, an independent and international Scopus Content Selection and Advisory Board was established to prevent a potential conflict of interest in the choice of journals to be included in the database and to maintain an open and transparent content coverage policy, regardless of publisher.<ref>{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview#content-policy-and-selection |title=Scopus Content Overview: Content Policy and Selection |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}</ref> The board consists of scientists and subject librarians.

A 2008 study  compared [[PubMed]], Scopus, [[Web of Science]], and [[Google Scholar]] and concluded: <blockquote>"PubMed and Google Scholar are accessed for free [...] Scopus offers about 20% more coverage than Web of Science, whereas Google Scholar offers results of inconsistent accuracy. PubMed remains an optimal tool in biomedical electronic research. Scopus covers a wider journal range [...] but it is currently limited to recent articles (published after 1995) compared with Web of Science. Google Scholar, as for the Web in general, can help in the retrieval of even the most obscure information but its use is marred by inadequate, less often updated, citation information."<ref>{{Cite journal |pmid=17884971 |year=2008 |last1=Falagas |first1=ME |last2=Pitsouni |first2=EI |last3=Malietzis |first3=GA |last4=Pappas |first4=G |title=Comparison of PubMed, Scopus, Web of Science, and Google Scholar: Strengths and weaknesses |volume=22 |issue=2 |pages=33842 |doi=10.1096/fj.07-9492LSF |journal=[[FASEB Journal]]}}</ref></blockquote>

Evaluating ease of use and coverage of Scopus and the Web of Science (WOS), a 2006 study concluded that "Scopus is easy to navigate, even for the novice user. [...] The ability to search both forward and backward from a particular citation would be very helpful to the researcher. The multidisciplinary aspect allows the researcher to easily search outside of his discipline" and "One advantage of WOS over Scopus is the depth of coverage, with the full WOS database going back to 1945 and Scopus going back to 1966. However, Scopus and WOS complement each other as neither resource is all inclusive. [...]".<ref>{{Cite journal |pmid=16522216 |year=2006 |last1=Burnham |first1=JF |title=Scopus database: A review |volume=3 |pages=1 |doi=10.1186/1742-5581-3-1 |pmc=1420322 |journal=Biomedical Digital Libraries}}</ref>

Scopus also offers author profiles which cover affiliations, number of publications and their [[bibliographic]] data, [[references]], and details on the number of citations each published document has received. It has [[alerts|alerting]] features that allows registered users to track changes to a profile and a facility to calculate authors' [[h-index]].

Scopus can be integrated with [[ORCID]].<ref name="Scopus">{{cite web |url=http://orcid.scopusfeedback.com |title=Scopus2Orcid |publisher=Scopus |accessdate=7 May 2014}}</ref>

== References ==
{{reflist|30em}}

== External links ==
* {{Official website|http://www.scopus.com/}}
* [http://www.elsevier.com/online-tools/scopus Scopus information]


{{Reed Elsevier}}

[[Category:Bibliographic databases]]
[[Category:Elsevier]]
[[Category:Citation indices]]
[[Category:Library cataloging and classification]]
[[Category:Scholarly search services]]
>>EOP<<
226<|###|>SCImago Journal Rank
'''SCImago Journal Rank''' (SJR indicator) is a measure of scientific influence of [[academic journal|scholarly journal]]s that accounts for both the number of [[citation]]s received by a journal and the importance or prestige of the journals where such citations come from. The SJR indicator is a variant of the [[centrality|eigenvector centrality measure]] used in network theory. Such measures establish the importance of a node in a network based on the principle that connections to high-scoring nodes contribute more to the score of the node. The SJR indicator, which is inspired by the [[PageRank]] algorithm, has been developed to be used in extremely large and heterogeneous journal citation networks. It is a size-independent indicator and its values order journals by their "average prestige per article" and can be used for journal comparisons in science evaluation processes.

The ''SJR indicator'' is a free journal metric which uses an algorithm similar to [[PageRank]] and provides an alternative to the [[impact factor]] (IF), which is based on data from the [[Science Citation Index]].<ref>{{cite journal | url = http://www.nature.com/news/2008/080102/full/451006a.html | title= Free journal-ranking tool enters citation market | journal = [[Nature (journal)|Nature]] | date= 2 January 2008 | volume= 451 | issue= 6 | doi= 10.1038/451006a | author = Declan Butler | pmid= 18172465 | pages= 6 |accessdate=14 May 2010}}</ref><ref>{{cite journal | url = http://www.fasebj.org/cgi/content/short/22/8/2623 | title = Comparison of SCImago journal rank indicator with journal impact factor | author= Matthew E. Falagas et al | doi = 10.1096/fj.08-107938 | journal = [[The FASEB Journal]] | year = 2008 | issue = 22 | pages = 26232628 | pmid = 18408168 | volume = 22 }}</ref> Average citations per document in a 2-year period, abbreviated as Cites per Doc. (2y), is another index that measures the scientific impact of an average article published in the journal. It is computed using the same formula that journal [[impact factor]] ([[Thomson Reuters]]).

== Rationale ==
If scientific impact is considered related to the number of endorsements, in the form of citations, a journal receives, then prestige can be understood as a combination of the number of endorsements and the prestige or importance of the journals issuing them. The ''SJR indicator'' assigns different values to citations depending on the importance of the journals where they come from. This way, citations coming from highly important journals will be more valuable and hence will provide more prestige to the journals receiving them. The calculation of the ''SJR indicator'' is very similar to the ''[[Eigenfactor]] score'', with the former being based on the [[Scopus]] database and the latter on the ISI [[Web of Science]] database.<ref>{{cite web | title=SCImago Journal & Country Rank (SJR) as an alternative to Thomson Reuters's Impact Factor and EigenFactor | url=http://www.scimagojr.com/news.php?id=41 | date=21 Aug 2008 | accessdate=20 September 2012}}</ref>

== Computation ==
The SJR indicator computation is carried out using an iterative [[algorithm]] that distributes prestige values among the journals until a steady-state solution is reached. The SJR algorithm begins by setting an identical amount of prestige to each journal, then using an iterative procedure, this prestige is redistributed in a process where journals transfer their achieved prestige to each other through citations. The process ends up when the difference between journal prestige values in consecutive iterations do not reach a minimum threshold value any more. The process is developed in two phases, (a) the computation of ''Prestige SJR'' (''PSJR'') for each journal: a size-dependent measure that reflects the whole journal prestige, and (b) the normalization of this measure to achieve a size-independent measure of prestige, the ''SJR indicator''.

== See also ==
* [[Journal Citation Reports]]
* [[Citation index]]
* [[Eigenfactor]]

== References ==
{{reflist}}

== External links ==
* {{official website|http://www.scimagojr.com/}}
* [http://blogs.openaccesscentral.com/blogs/bmcblog/entry/scimago_a_new_source_of SCImago  a new source of journal metrics offering a wealth of free data on open access journals]
* [http://www.earlham.edu/~peters/fos/2008/01/more-on-scimago-journal-rank-v-impact.html More on SCImago Journal Rank v. Impact Factors]

{{DEFAULTSORT:Scimago journal rank}}
[[Category:Citation indices]]
[[Category:Academic publishing]]
>>EOP<<
232<|###|>Query by humming
{{inline|date=August 2012}}
'''Query by humming''' ('''QbH''') is a music retrieval system that branches off the original classification systems of title, artist, composer, and genre. It normally applies to songs or other music with a distinct single theme or melody. The system involves taking a user-hummed [[melody]] (input [[Information retrieval|query]]) and comparing it to an existing [[database]].  The system then returns a ranked list of music closest to the input query. 

One example of this would be a system involving a [[portable media player]] with a built-in [[microphone]] that allows for faster [[Search engine technology|searching]] through [[Digital media|media]] files.

The [[MPEG-7]] standard includes provisions for QbH music searches.

== Examples of QbH systems ==
SoundHound and Midomi are the only commercially available query by humming services available online at Midomi.com or on the mobile app called SoundHound. 
Both are powered by the same backend and are capable of recognizing humming and singing as well as recorded tracks. 
For the singing and humming search, the searchable database is based on Midomi.com's user contributions. Midomi has collected about one million tracks based on user contributions in multiple languages, making it the largest database of its kind by a large margin. The top four languages are: English, Japanese, Chinese and Spanish. 

"Musipedia" is an example of a QbH system that uses a variety of input methods such as humming, tapping the keyboard, keyboard search (a virtual piano keyboard), draw notes, and a contour search, using [[Parsons_code|Parsons Code]] to encode the music pieces.

[[Tunebot]] is a music search engine that uses queries from humming, lyrics, and melody. People can contribute to the database and expand the variety of searchable songs. Tunebot also serves as the back-end for a game called [[Karaoke Callout]], in which players' performances are compared by the engine with songs in the database.

== External links ==
===Online demos===
* [http://www.midomi.com/ Midomi]
* [http://www.soundhound.com/ SoundHound (mobile app)] 
* [http://www.musipedia.org/query_by_humming.0.html QbH system] from Musipedia
* [http://querybyhum.cs.nyu.edu/ QbH research project at NYU]
* [http://www.sloud.com/technology/query_by_humming/ Query by Humming at Sloud Inc], [http://www.sloud.com/ QbH applet (Active X)] 
* [http://www.musicline.de/de/melodiesuche/input Musicline QbH based on technology from Fraunhofer Institut] {{de icon}}
* [http://maart.sourceforge.net/ MaART at Sourceforge]
* [http://tunebot.cs.northwestern.edu/ Tunebot at Northwestern University]

===General info and articles===
* {{Wayback|url=http://mirsystems.info/index.php?id=mirsystems|title=Comprehensive list of Music Information Retrieval systems (apparently last updated ca 2003)|date=20081221191111}}
* [http://www.cs.cornell.edu/zeno/papers/humming/humming.html Query By Humming  Musical Information Retrieval in an Audio Database], paper by Asif Ghias, Jonathan Logan, David Chamberlin, Brian C. Smith; [[ACM Multimedia]] 1995
* [http://cs.nyu.edu/~eugenew/publications/humming-summary.pdf A survey presentation of QBH by Eugene Weinstein, 2006]
* [http://www.dlib.org/dlib/may97/meldex/05witten.html The New Zealand Digital Library MELody inDEX], article by Rodger J. McNab, Lloyd A. Smith, David Bainbridge and Ian H. Witten; [[D-Lib Magazine]] 1997
* [http://deepblue.lib.umich.edu/bitstream/handle/2027.42/35292/10373_ftp.pdf?sequence=1 Name that Tune: A Pilot Study in Finding a Melody from a Sung Query], article by Bryan Pardo, Jonah Shifrin, and William Birmingham, Journal of the American Society for Information Science and Technology, vol. 55 (4), pp. 283-300, 2004

[[Category:Music search engines]]
[[Category:Acoustic fingerprinting]]
>>EOP<<
238<|###|>MusicRadar (service)
{{multiple issues|
{{advert|date=August 2013}}
{{cleanup|reason=Syntax, capitals|date=August 2013}}
{{fanpov|date=August 2013}}
{{Orphan|date=August 2013}}
}}
{{Infobox Website
|name=MusicRadar()
|logo=
|screenshot=
|caption=
|url=http://www.doreso.com/
|commercial=Yes
|type=[[Music]] [[website]]
|registration=Optional
|owner=Shanghai Yinlong Information Technology Co., LTD
|author=Shanghai Yinlong Information Technology Co., LTD
|launch date=January 2013
|current status=
|revenue=}}

'''Music radar''' is a sound-to-sound music search engine, which allows users to obtain more detailed information of music/songs by singing/humming or by recording original music. It is available on [[App Store (iOS)|App Store]]<ref>https://itunes.apple.com/cn/app/yin-le-lei-da/id635262613</ref> for [[iPhone]] and [[Google Play]]<ref>https://play.google.com/store/apps/details?id=com.voicedragon.musicclient.googleplay</ref> for [[Android (operating system)|Android]] mobiles. Music radar was launched by Shanghai Yinlong Information Technology Co., LTD in Jan. 2013.<ref>http://www.doreso.com/</ref>

==Features==
The app (MusicRadar) currently has three ways of searching music: by identifying recorded original music fragment; by humming or singing the melody using microphone; and by direct input of the name of song or singer. Users could share their searching results on [[Facebook]], [[Twitter]] or other SNS website.

==History==
The music radar team got the 1st place on Query by Singing/Humming (QBSH) task, Music Information Retrieval Evaluation eXchange (MIREX) 2012.<ref>http://www.music-ir.org/mirex/wiki/2012:Main_Page</ref> The app was launched for business intention at the end of January 2013, supporting query by singing/humming & audio fingerprinting. After two months, the app has reached its first one million user milestone in April, 2013. In May 2013, Music radar announces that it has integrated deep learning techniques in its query by singing/humming module to promote the recognize rate and reduce the users waiting time. In July 2013, Music radar released China's first cloud based music recognizing openAPI to public.<ref>[http://roll.sohu.com/20130731/n383076514.shtml 20137,,API]</ref>

==References==
{{reflist}}



[[Category:Acoustic fingerprinting]]
[[Category:Music search engines]]
[[Category:IOS software]]
[[Category:Android (operating system) software]]
>>EOP<<
