3<|###|>Subsetting
In research communities (for example, [[earth science]]s), '''subsetting''' is the process of retrieving just the parts of large files which are of interest for a specific purpose. This occurs usually in a clientserver setting, where the extraction of the parts of interest occurs on the server before the data is sent to the client over a network. The main purpose of subsetting is to save bandwidth on the network and storage space on the client computer.

Subsetting may be favorable for the following reasons:<ref name="Institute2012">{{cite book|author=SAS Institute|title=SAS/ETS 12.1 User's Guide|url=http://books.google.com/books?id=OE0UfAhit4kC&pg=PA70|date=1 August 2012|publisher=SAS Institute|isbn=978-1-61290-379-8|pages=70}}</ref>
* restrict the time range
* select [[Cross-sectional data|cross section]]s of data
* select particular kinds of [[time series]]
* exclude particular obersvations

==References==
{{reflist}}


==External links==
*[http://www.subset.org/index.jsp Subset.org]

[[Category:Information retrieval]]

{{Statistics-stub}}
>>EOP<<
9<|###|>Bioinformatic Harvester
The '''Bioinformatic Harvester''' is a bioinformatic meta [[search engine]] created by the [[European Molecular Biology Laboratory]]<ref>{{Cite journal|title= 	Information retrieval on Internet using meta-search engines: A review|authors=Manoj, M, Elizabeth, Jacob |date=Oct 2008|publisher=CSIR|pages=739746|issn=0022-4456
|journal=JSIR |volume=67 (10)}}</ref> and subsequently hosted and further developed by KIT [[Karlsruhe Institute of Technology]] for [[gene]]s and protein-associated information. Harvester currently works for [[human]], [[mouse]], [[rat]], [[zebrafish]], [[drosophila]] and [[arabidopsis thaliana]] based information. Harvester cross-links >50 popular bioinformatic resources and allows cross searches. Harvester serves tens of thousands of pages every day to scientists and physicians.

{{Infobox software
| name                  = Bioinformatic Harvester
|developer              = Urban Liebel, Bjorn Kindler
|latest release version = 4
|latest release date    = {{release date and age|2011|05|24}}
|operating_system       = Web based
|genre                  = Bioinformatics tool
|license                = Public Domain
|website                = http://harvester.kit.edu
}}

== How Harvester works ==

Harvester collects information from [[protein]] and gene databases along with information from so called "prediction servers." Prediction server e.g. provide online sequence analysis for a single protein. Harvesters search index is based on the [[International Protein Index|IPI]] and [[UniProt]] protein information collection. The collections consists of:

* ~72.000 human, ~57.000 mouse, ~41.000 rat, ~51.000 zebrafish, ~35.000 arabidopsis protein pages, which cross-link ~50 major bioinfiormatic resources.

<!-- Deleted image removed: [[Image:harvester-kit.JPG|thumb| A screenshot of the [http://harvester.kit.edu/ Harvester search engine]]] -->

== Harvester crosslinks several types of information ==

===Text based information===
from the following databases:

* [[UniProt]], world largest protein database
* [[SOURCE]], convenient gene information overview
* [[Simple Modular Architecture Research Tool]] (SMART),
* [[SOSUI]], predicts transmembrane domains
* [[PSORT]], predicts protein localisation
* [[HomoloGene]], compares proteins from different species
* [[gfp-cdna]], protein localisation with fluorescence microscopy
* [[International Protein Index]] (IPI).

=== Databases rich in graphical elements ===
...are not collected, but crosslinked via [[iframe]]s. Iframes are transparent windows within a [[HTML]] pages. The iframe windows allows up-to-date viewing of the "iframed," linked databases. Several such iframes are combined on a Harvester protein page. This method allows convenient comparison of information from several databases.

* NCBI-[[BLAST]], an algorithm for comparing biological sequences from the [[National Center for Biotechnology Information|NCBI]].
* [[Ensembl]], automatic gene annotation by the EMBL-[[European Bioinformatics Institute|EBI]] and [[Sanger Institute]]
* [[FlyBase]] is a database of model organism ''[[Drosophila melanogaster]]''.
* [[GoPubMed]] is a knowledge-based search engine for biomedical texts.
* [[Information Hyperlinked over Proteins|iHOP]], information hyperlinked over proteins via gene/protein synonyms
* [[Mendelian Inheritance in Man]] project catalogues all the known diseases.
* [[RZPD]], German resources Center for genome research in Berlin/Heidelberg.
* [[STRING]], Search Tool for the Retrieval of Interacting Genes/Proteins, developed by [[EMBL]], [[Swiss Institute of Bioinformatics|SIB]] and [[University of Zurich|UZH]].
* [[Zebrafish Information Network]].
* [http://locate.imb.uq.edu.au/ LOCATE] subcellular localization database (mouse).

=== Access from external application ===

* [[Genome browser]], working draft assemblies for genomes [[University of California, Santa Cruz|UCSC]]
* [[Google Scholar]]
* [[Mitocheck]]
* [[PolyMeta]], meta search engine for Google, Yahoo, MSN, Ask, Exalead, AllTheWeb, GigaBlast

== What one can find ==

Harvester allows a combination of different search terms and single words.

Search Examples:

* Gene-name: "golga3"
* Gene-alias: "ADAP-S ADAS ADHAPS ADPS" (one gene name is sufficient)
* Gene-Ontologies: "Enzyme linked receptor protein signaling pathway"
* [[UniGene|Unigene]]-Cluster: "Hs.449360"

* Go-annotation: "intra-Golgi transport"
* Molecular function: "protein kinase binding"
* Protein: "Q9NPD3"
* Protein domain: "SH2 sar"
* Protein Localisation: "endoplasmic reticulum"

* Chromosome: "2q31"
* Disease relevant: use the word "diseaselink"
* Combinations: "golgi diseaselink" (finds all golgi proteins associated with a disease)
* [[mRNA]]: "AL136897"

* Word: "Cancer"
* Comment: "highly expressed in heart"
* Author: "Merkel, Schmidt"
* Publication or project: "[[cDNA]] sequencing project"

==See also==

* [[Biological database]]s
* [[Entrez]]
* [[European Bioinformatics Institute]]
* [[HPRD|Human Protein Reference Database]]
* [[Metadata]]
* [[Sequence profiling tool]]

== Literature ==
*{{cite journal |author=Liebel U, Kindler B, Pepperkok R |title='Harvester': a fast meta search engine of human protein resources |journal=Bioinformatics |volume=20 |issue=12 |pages=19623 |date=August 2004 |pmid=14988114 |doi=10.1093/bioinformatics/bth146 |url=http://bioinformatics.oxfordjournals.org/cgi/pmidlookup?view=long&pmid=14988114}}
*{{cite journal |author=Liebel U, Kindler B, Pepperkok R |title=Bioinformatic "Harvester": a search engine for genome-wide human, mouse, and rat protein resources |journal=Meth. Enzymol. |volume=404 |issue= |pages=1926 |year=2005 |pmid=16413254 |doi=10.1016/S0076-6879(05)04003-6 |url=http://linkinghub.elsevier.com/retrieve/pii/S0076-6879(05)04003-6}}

== Notes and references ==
<references/>

== External links ==
* http://harvester.kit.edu Bioinformatic Harvester V at KIT [[Karlsruhe Institute of Technology]]
* [http://harvester42.fzk.de Harvester42] at KIT - integrating 50 general search engines

[[Category:Bioinformatics software]]
[[Category:Biological databases]]
[[Category:Information retrieval]]
[[Category:Internet search engines]]
[[Category:Biology websites]]
>>EOP<<
15<|###|>Latent semantic mapping
'''Latent semantic mapping (LSM)''' is a data-driven framework to model globally meaningful relationships implicit in large volumes of (often textual) data. It is a generalization of [[latent semantic analysis]]. In information retrieval, LSA enables retrieval on the basis of conceptual content, instead of merely matching words between queries and documents.

LSM was derived from earlier work on latent semantic analysis.  There are 3 main characteristics of latent semantic analysis: Discrete entities, usually in the form of words and documents, are mapped onto continuous vectors, the mapping involves a form of global correlation pattern, and dimensionality reduction is an important aspect of the analysis process. These constitute generic properties, and have been identified as potentially useful in a variety of different contexts.  This usefulness has encouraged great interest in LSM. The intended product of latent semantic mapping, is a data-driven framework for modeling relationships in large volumes of data.

[[Mac OS X v10.5]] and later includes a [[Software framework|framework]] implementing latent semantic mapping.<ref>[http://developer.apple.com/documentation/TextFonts/Reference/LatentSemanticMapping/index.html API Reference: Latent Semantic Mapping Framework Reference<!-- Bot generated title -->]</ref>

== See also ==
* [[Latent semantic analysis]]

== Notes ==
{{reflist}}

== References ==
* {{cite journal
 | url=http://ieeexplore.ieee.org/iel5/79/32367/01511825.pdf
 | title=Latent semantic mapping [information retrieval]
 | author=Bellegarda, J.R.
 | date=2005
}}
* {{cite conference
 | url=https://www.securecms.com/ICASSP2006/Tutorial_06.asp
 | title=Latent semantic mapping: Principles and applications
 | author=J. Bellegarda
 | booktitle=ICASSP 2006
 | date=2006
}}

[[Category:Information retrieval]]
[[Category:Natural language processing]]


{{semantics-stub}}
{{compu-stub}}
>>EOP<<
21<|###|>Key Word in Context
'''KWIC''' is an acronym for '''Key Word In Context''', the most common format for [[concordance (publishing)|concordance]] lines. The term KWIC was first coined by [[Hans Peter Luhn]].<ref>Manning, C. D., Schutze, H.: "Foundations of Statistical Natural Language Processing", p.35. The MIT Press, 1999</ref> The system was based on a concept called ''keyword in titles'' which was first proposed for Manchester libraries in 1864 by [[Andrea Crestadoro]].<ref name="index">{{cite book|title=Advanced Indexing and Abstracting Practices|url=http://books.google.co.uk/books?id=nIUkl7bLzYUC&pg=PA41&dq=Andrea+Crestadoro#v=onepage&q=Andrea%20Crestadoro&f=false}}</ref>

A '''KWIC''' index is formed by sorting and aligning the words within an article title to allow each word (except the [[stop words]]) in titles to be searchable alphabetically in the index. It was a useful indexing method for technical manuals before computerized [[full text search]] became common.

For example, a search query including all of the words in the title statement of this article ("KWIC is an acronym for Key Word In Context, the most common format for concordance lines") and the [[Wikipedia:Slogans|Wikipedia slogan]] in English ("the free encyclopedia"), searched against this very webpages, might yield a KWIC index as follows. A KWIC index usually uses a wide layout to allow the display of maximum 'in context' information (not shown in the following example).

{| nowrap
|-
|align=right|KWIC is an
|'''acronym''' for Key Word In Context, ...
|page 1
|-
|align=right|... Key Word In Context, the most 
|'''common''' format for concordance lines.
|page 1
|-
|align=right|... the most common format for 
|'''concordance''' lines.
|page 1
|-
|align=right|... is an acronym for Key Word In 
|'''Context''', the most common format ...
|page 1
|-
|align=right|Wikipedia, The Free 
|'''Encyclopedia'''
|page 0
|-
|align=right|... In Context, the most common 
|'''format''' for concordance lines.
|page 1
|-
|align=right|Wikipedia, The 
|'''Free''' Encyclopedia
|page 0
|-
|align=right|KWIC is an acronym for 
|'''Key''' Word In Context, the most ...
|page 1
|-
|&nbsp;
|'''KWIC''' is an acronym for Key Word ...
|page 1
|-
|align=right|... common format for concordance 
|'''lines'''.
|page 1
|-
|align=right|... for Key Word In Context, the 
|'''most''' common format for concordance ...
|page 1
|-
|&nbsp;
|'''Wikipedia''', The Free Encyclopedia
|page 0
|-
|align=right|KWIC is an acronym for Key
|'''Word''' In Context, the most common ...
|page 1
|}

A KWIC index is a special case of a '''permuted index'''. This term refers to the fact that it indexes all [[cyclic permutation]]s of the headings. Books composed of many short sections with their own descriptive headings, most notably collections of [[Manual page (Unix)|manual pages]], often ended with a '''permuted index''' section, allowing the reader to easily find a section by any word from its heading. This practice, also known as '''KWOC''' ('''Key Word Out of Context'''), is no longer common.

==References in Literature==

''Note: The first reference does not show the KWIC index unless you pay to view the paper. The second reference does not even list the paper at all.''

* [[David Parnas|David L. Parnas]] uses a KWIC Index as an example on how to perform modular design in his paper [http://portal.acm.org/citation.cfm?id=361623&coll=ACM&dl=ACM&CFID=9516243&CFTOKEN=98251202 ''On the Criteria To Be Used in Decomposing Systems into Modules''], available as an [http://www.acm.org/classics/may96/ ACM Classic Paper]
* Christopher D. Manning and Hinrich Schutze describe a KWIC index and computer concordancing in section 1.4.5 of their book ''Foundations of Statistical Natural Language Processing''

==References==
{{reflist|2}}

==See also==
* <tt>[[Ptx (Unix)|ptx]]</tt>, a Unix command-line utility producing a [[permuted index]]
*[[Concordancer]]
*[[Concordance (publishing)]]
*[[BurrowsWheeler transform]]
*[[Hans Peter Luhn]]
*[[Suffix tree]]

[[Category:Indexing]]
[[Category:Information retrieval]]
[[Category:Reference]]
[[Category:Searching]]
>>EOP<<
27<|###|>Document clustering
{{disputed|date=March 2014}}
{{inline|date=March 2014}}
'''Document clustering''' (or '''text clustering''') is the application of [[cluster analysis]] to textual documents. It has applications in automatic document organization, [[topic (linguistics)|topic]] extraction and fast [[information retrieval]] or filtering.

==Overview==
Document clustering involves the use of descriptors and descriptor extraction. Descriptors are sets of words that describe the contents within the cluster. Document clustering is generally considered to be a centralized process. Examples of document clustering include web document clustering for search users.

The application of document clustering can be categorized to two types, online and offline. Online applications are usually constrained by efficiency problems when compared offline applications.

In general, there are two common algorithms. The first one is the hierarchical based algorithm, which includes single link, complete linkage, group average and Ward's method.  By aggregating or dividing, documents can be clustered into hierarchical structure, which is suitable for browsing. However, such an algorithm usually suffers from efficiency problems. The other algorithm is developed using the [[K-means algorithm]] and its variants. These algorithms can further be classified as hard or soft clustering algorithms. Hard clustering computes a hard assignment  each document is a member of exactly one cluster. The assignment of soft clustering algorithms is soft  a documents assignment is a distribution over all clusters. In a soft assignment, a document has fractional membership in several clusters. [[Dimensionality reduction]] methods can be considered a subtype of soft clustering; for documents, these include [[latent semantic indexing]] ([[truncated singular value decomposition]] on term histograms)<ref>http://nlp.stanford.edu/IR-book/pdf/16flat.pdf</ref> and [[topic model]]s.

Other algorithms involve graph based clustering, ontology supported clustering and order sensitive clustering.

Given a clustering, it can be beneficial to automatically derive human-readable labels for the clusters. [[Cluster labeling|Various methods]] exist for this purpose.

==Clustering in search engines==
A [[web search engine]] often  returns thousands of pages in response to a broad query, making it difficult for users to browse or to identify relevant information.  Clustering methods can be used to automatically group the retrieved documents into a list of meaningful categories, as is achieved by Enterprise Search engines such as [[Northern Light Group|Northern Light]] and [[Vivisimo]], consumer search engines such as [http://www.polymeta.com/ PolyMeta] and [http://www.helioid.com Helioid], or open source software such as [[Carrot2]].

Examples:

* Clustering divides the results of a search for "cell" into groups like "biology," "battery," and "prison."

* [http://FirstGov.gov FirstGov.gov], the official Web portal for the U.S. government, uses document clustering to automatically organize its search results into categories.  For example, if a user submits immigration, next to their list of results they will see categories for Immigration Reform, Citizenship and Immigration Services, Employment, Department of Homeland Security, and more.

== References ==
{{reflist}}
Publications:
* Nicholas O. Andrews and Edward A. Fox, Recent Developments in Document Clustering, October 16, 2007 [http://eprints.cs.vt.edu/archive/00001000/01/docclust.pdf]
* Claudio Carpineto, Stanislaw Osinski, Giovanni Romano, Dawid Weiss. A survey of Web clustering engines. ACM Computing Surveys (CSUR), Volume 41, Issue 3 (July 2009), Article No. 17, ISSN:0360-0300 
* http://semanticsearchart.com/researchBest.html - comparison of several popular clustering algorithms, data and software to reproduce the result.
* Tanmay Basu, C.A. Murthy , CUES: A New Hierarchical Approach for Document Clustering, 2013 [http://jprr.org  JPRR]

==See Also==
*[[Cluster Analysis]]
*[[Fuzzy clustering]]
[[Category:Information retrieval]]
>>EOP<<
33<|###|>Coveo
{{Infobox company
| name = Coveo Solutions Inc.
| logo = [[Image:Coveo logo.png|120px]]
| type = Private
| slogan = 
| foundation =  2004
| location_city = [[Quebec City]], [[Canada]]
| key_people = Louis Tetu, Chairman and CEO <br />Laurent Simoneau, President and CTO
| num_employees =
| industry = [[Enterprise search]]
| products = Coveo Search & Relevance Platform,<br />Coveo for Sitecore,<br />Coveo for Salesforce
| homepage = http://www.coveo.com
}}

'''Coveo''' is a provider of [[enterprise search]] and website search technologies, with integrated plug-ins for Salesforce.com, Sitecore CEP, and [[Microsoft Outlook]] and [[SharePoint]].  APIs also allow for custom integration with other applications.

==History==
Coveo Solutions Inc. was founded in 2004 as a spin-off of [[Copernic|Copernic Technologies Inc.]] Laurent Simoneau, Coveo's president and chief executive officer was formerly Copernic's chief operating officer. About 30 employees moved into the new company, with offices at that time in [[Quebec City]] and [[Montreal]] in Canada and in [[Palo Alto]], Calif.<ref>http://www.eweek.com/c/a/Enterprise-Applications/Copernic-Ready-to-Take-On-Google-In-Enterprise-Search-Product/</ref>

==Products==
'''Coveo Search & Relevance Platform'''

Coveo Search & Relevance Platform is a modular enterprise search technology that can index information stored in diverse repositories throughout the company, perform text analytics and metadata enrichment on the indexed content, and make the content findable through search-driven interfaces.

'''Coveo for Sitecore'''

Coveo for Sitecore is an integrated website search product to be used in conjunction with Sitecores Customer Experience Platform.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Sitecore console.

'''Coveo for Salesforce'''

Coveo for Salesforce is an integrated CRM search product to be used in conjunction with Salesforce.com Service Cloud and Communities Editions.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Salesforce console.

==Customers==
Coveo claims its clients include more than 700 implementations including AmerisourceBergen, CA, California Water Service Co., Deloitte, ESPN, Haley & Aldrich, GEICO, Lockheed Martin, P&G, PRTM, PricewaterhouseCoopers, Rabobank, SNC-Lavalin, Spencer Stuart, Theodoor Gilissen, and the U.S. Navy.<ref>{{cite web|url=http://www.coveo.com/en/~/media/Files/about-us/Coveo-Corporate-Fact-Sheet-Q109.ashx |title=Coveo corporate fact sheet |date= |accessdate=2011-02-27}}</ref> These companies were also mentioned while not confirmed by a citation: HP, PwC, Netezza Corporation, NATO, NASA, AC Nielsen, among many others.{{Citation needed|date=February 2010}}

==References==
{{reflist}}

==External links==
* [http://www.coveo.com/ Coveo.com]

[[Category:Companies based in Quebec City]]
[[Category:Information retrieval]]
[[Category:BlackBerry development software]]
>>EOP<<
39<|###|>Figaro Systems
{{Infobox company
|name = Figaro Systems, Inc.
|logo = [[Image:Figaro-logo.png|Figaro logo]]
|type = [[Privately held company|Private]]
|foundation = 1993
|location_city = [[Santa Fe, New Mexico|Santa Fe]], [[New Mexico]]
|location_country =[[United States]]
|key_people = Patrick Markle, [[president]] and [[CEO]], [[Geoff Webb]], [[vice president|VP]]
|homepage = [http://www.figarosystems.com figarosystems.com]
}}

'''Figaro Systems, Inc.''' is an American company that provides  seatback and [[wireless]] titling [[software]] and system installations to [[opera houses]] and other music performance venues worldwide. The company is based in [[Santa Fe, New Mexico|Santa Fe]], New Mexico. It was established in 1993 <ref>Andrew Webb, Opera Subtitle Firm Eyes New Game, ''New Mexico Business Weekly'', Nov. 21, 2003 [http://www.bizjournals.com/albuquerque/stories/2003/11/24/story2.html]</ref>
by Patrick Markle, [[Geoff Webb]], and Ron Erkman  <ref name="figaro-systems.com"/> and was the first company to provide [[assistive technology]] that enables individualized, simultaneous, multi-lingual [[dialogue]] and [[libretto]]-reading for audiences.
<ref>[http://www.highbeam.com/DocPrint.aspx?DocID=1P2:115622912 David Belcher, Nothing Lost in Translation: [[Video]] system allows patrons to read words on chair backs,] ''Albuquerque Journal'', June 4, 2006</ref>

==History==
Figaro Systems grew out of a conversation in 1992 among three opera colleagues: Patrick Markle, at that time Production Director of The [[Santa Fe Opera]], Geoffrey Webb, Design Engineer for the [[Metropolitan Opera House (Lincoln Center)|Metropolitan Opera House]] in New York, and Ronald Erkman, then a technician for the Met. At that time, opera houses had two options for the display of libretto and dialogue subtitles: projection onto a large screen above the stage or onto smaller screens throughout the theatre. Typically, the translation was in a single language.<ref>[http://www.bizjournals.com/albuquerque/stories/2005/04/11/story5.html?q=Figaro%20Systems Dennis Domrzalski, "Figaro: Eyes translate when ears don't get it",] ''New Mexico Business Weekly'', April 8, 2005</ref>

The [[Americans with Disabilities Act of 1990]] had recently been enacted; Markle was trying to solve the problem of venues which lacked accessibility to patrons with disabilities, including the profoundly [[deaf]].  Markle, Webb, and Erkman devised the first [[prototype]] of a personal seatback titling device and [[John Crosby (conductor)|John Crosby]], then General Director of The [[Santa Fe Opera]], saw its potential for opera patrons.<ref name="figaro-systems.com">[http://www.figaro-systems.com/about.php  Figaro Systems Official Website]</ref> Markle, Webb, and Erkman were further reinforced by their understanding of technologys role in remediating the physical barriers people encounter, worldwide, which frustrate or prevent their access to the visual performing arts.<ref>[http://figarosystems.com/linkdownloads/052007_figaro_auditoria_article.pdf [[User-friendly]] art: In-seat text displays that subtitle and translate, ''Auditoria'', May 2007]</ref> Markle, Webb, and Erkman applied for and were granted [[patent]]s for their invention.
<ref>[http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=11&f=G&l=50&co1=AND&d=PTXT&s1=figaro.ASNM.&OS=AN/figaro&RS=AN/Figaro  United States Patent 5,739,869, "Electronic libretto display apparatus and method," issued April 14, 1998. [[United States Patent and Trademark Office]] ]</ref><ref>[http://www.lanl.gov/orgs/pa/News/050701.html  Los Alamos Laboratory, ''Daily News Bulletin'', May 7, 2001]</ref>

Philanthropist and investor [[Alberto Vilar]] counted Figaro Systems among the companies in which he was a majority shareholder.<ref>[http://nymag.com/nymetro/arts/music/features/5616/ [[Robert Hilferty]], "A Knight at the Opera," ''[[New York Magazine]]'', January 14, 2002]</ref><ref>[http://biography.jrank.org/pages/3490/Vilar-Alberto-1940-Investor-Philanthropist-Privileges-Wealth.html  "Alberto Vilar: The Privileges of Wealth," ''The Free Encyclopedia'']</ref>  He donated the company's [[electronic libretto]] system to European venues including the [[Royal Opera House]] in [[London]], La Scala's [[Teatro degli Arcimboldi]] opera houses in [[Milan, Italy|Milan]], Italy, [[Gran Teatre del Liceu]] in [[Barcelona, Spain|Barcelona]], Spain, and the [[Wiener Staatsoper]] in [[Wien]], [[Austria]]. As a consequence of his failures to pay promised donations, most of these companies lost money.

In 2005 the Met charged the New Mexico company with unlawfully using its name in advertising promoting its "Simultext, system which defendant claims can display a simultaneous translation of an opera as it occurs on a stage and that defendant represented that its system is installed at the Met." <ref>[http://classactionlitigation.com/library/consumerlaw2006update.html#_edn173#_edn173 Timothy E. Eble, ''Class Action Litigation Information''] on classactionlitigation.com</ref>

==Products and technology==
The companys products are known variously as seat back titles, [[surtitles]],
<ref>[http://app1.kuhf.org/houston_public_radio-news-display.php?articles_id=20614 Eric Skelly, "Surtitles at the Opera," ''Public Radio News and Information in Houston, Texas'', KUHF 88.7 FM Houston Public Radio] on app1.kuhf.org/</ref> [[electronic libretto]] systems, opera supertitles, projected titles, and libretto translations.

Opera venues have utilized the system to display librettos in [[English language|English]], [[French language|French]], [[German language|German]], [[Italian language|Italian]], [[Japanese language|Japanese]], [[Mandarin Chinese|Mandarin]], [[Russian language|Russian]], and [[Spanish language|Spanish]]
<ref>[http://www.sandia.gov/news-center/news-releases/2005/tech-trans/smbusiness.html "Sandia helps 278 state businesses in 2004 through New Mexico Small Business Assistance Program," Sandia National Laboratories, Sandia Corporation, March 22, 2005] on sandia.gov</ref> although the software enables the reading of the libretto in any [[written language]].
<ref name="entertanmentengineering.com">[http://www.entertanmentengineering.com/v4.issue04/page.06.html  Giving the Opera a New Voice,] ''Entertainment Engineering," Volume 4, Issue 2, p. 6</ref> Translation is provided by one screen and delivery system per person.<ref>[http://www.figarosystems.com  Figaro Systems Official Website]</ref>

Typically, but not in all cases, the system is permanently installed along the backs of rows of seats. Each screen is positioned so that the text is clearly visible to each user. The displays were initially available in [[vacuum fluorescent display]], ([[Vacuum fluorescent display|VFD]]) and, in 2000, [[liquid crystal display]], ([[LCD]]) was used. In 2004 the displays became available with [[organic light-emitting diode]], ([[OLED]]) screens.  Each type of display provides the same text information and program annotation on eight channels simultaneously, may be turned off by the user, and is user-operated with a single button. The software is capable of supporting venues existing systems as well as Figaro Systems' "Simultext" system. The software enables cueing of each line as it is sung, and it appears instantly on the screen.<ref name="entertanmentengineering.com"/>

The company builds fully [[modular]] systems including its [[wireless]] [[handheld]] screens 
<ref>[http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=3&f=G&l=50&co1=AND&d=PTXT&s1=figaro.ASNM.&OS=AN/figaro&RS=AN/Figaro  United States Patent 6,760,010. "Wireless electronic libretto display apparatus and method," issued July 6, 2004:] United States Patent and Trademark Office Patent Full-Text and Image Database</ref> for users who cannot use seatback systems, for example people in [[wheelchair]]s, who may be viewing the opera in areas lacking seatback viewing, or people with compromised eyesight.

==Venues==
In the US, the companys systems are in use in the [[Ellie Caulkins Opera House]] 
<ref>[http://www.highbeam.com/doc/1G1-135788390.html Marc Shulgold, "Opera dialogue shows on seat in front of you,"] ''Rocky Mountain News'' (Denver, Colorado), September 3, 2005 on highbeam.com,</ref> in [[Denver, Colorado|Denver]], Colorado, The Santa Fe Opera in Santa Fe,<ref>[http://web.archive.org/web/20080512022822/http://www.santafeopera.org/yournite/operatitles.php  Santa Fe Opera, Santa Fe, NM. Cached webpage],</ref> the [[Brooklyn Academy of Music]]<ref>[http://www.appliancemagazine.com/editorial.php?article=1768&zone=210&first=1  An Operatic Performance, ''Appliance Magazine'', June 2007],</ref> the [[Metropolitan Opera]], New York, where it is called "MetTitles"),<ref>[http://www.figaro-systems.com/installations.php  Figaro Systems Official Website. Installations],</ref> the [[Roy E. Disney]] Theatre in [[Albuquerque]]'s [[National Hispanic Cultural Center]], [[McCaw Hall]] in [[Seattle Washington]], the [[Opera Theatre of St. Louis]] in St. Louis, Missouri, the [[Des Moines Metro Opera]] in [[Des Moines, Iowa|Des Moines]], Iowa and the Lyric Opera of Kansas City,  Missouri.<ref name="figaro-systems.com"/>

In the UK and Europe, the systems have been installed in venues including the [[Royal Opera House]] in London, the [[Teatro alla Scala]] and La Scala's [[Teatro degli Arcimboldi]] opera houses in [[Milan, Italy|Milan]], Italy, the [[Gran Teatre del Liceu]] in [[Barcelona, Spain|Barcelona]], Spain, and the [[Wiener Staatsoper]] in [[Wien]], [[Austria]].
<ref>[http://www.entertainmentengineering.com/v4.issue04/page.06.html Giving the Opera a New Voice, ''Entertainment Engineering.'', Volume 4, Issue 2, p. 6], on entertainmentengineering.com</ref>

==Awards==
In 2001, the company won the [[Los Alamos, New Mexico|Los Alamos]] Laboratories Technology Commercialization Award for its Simultext system.<ref>[http://www.lanl.gov/news/index.php/fuseaction/home.story/story_id/1170 Todd Hanson, "Los Alamos announces technology commercialization awards," ''Los Alamos National Laboratory News''], Los Alamos National Security, LLC, US Department of Energy's NNSA, May 7, 2001 on lanl.gov/news.</ref>
In 2008, the companys software was one of four finalists for the Excellence Award for Commercial Software awarded by the New Mexico Information Technology and Software Association.

==References==
{{Reflist}}

[[Category:Information retrieval]]
[[Category:Software companies based in New Mexico]]
[[Category:Assistive technology]]
[[Category:Educational technology]]
[[Category:Companies based in New Mexico]]
[[Category:Privately held companies based in New Mexico]]
[[Category:Companies established in 1993]]
>>EOP<<
45<|###|>Expertise finding
{{cleanup|date=November 2010}}{{External links|date=January 2012}}

'''Expertise finding''' is the use of tools for finding and assessing individual [[expertise]], with particular focus on scientific expertise.

== Importance of expertise ==

It can be argued that human expertise is more valuable than capital, means of production or intellectual property. Contrary to expertise, all other aspects of capitalism are now relatively generic: access to capital is global, as is access to means of production for many areas of manufacturing.  [[Intellectual property]] can be similarly licensed.  Furthermore, expertise finding is also a key aspect of [[institutional memory]], as without its experts an institution is effectively decapitated.  However, finding and licensing expertise, the key to the effective use of these resources, remain much harder, starting with the very first step: finding expertise that you can trust.

Until very recently, finding expertise required a mix of individual, social and collaborative practices, a haphazard process at best.  Mostly, it involved contacting individuals one trusts and asking them for referrals, while hoping that ones judgment about those individuals is justified and that their answers are thoughtful.

In the last fifteen years, a class of [[knowledge management]] software has emerged to facilitate and improve the quality of expertise finding, termed expertise locating systems.  These software range from [[Social network service|social networking systems]] to [[knowledge base]]s.  Some software, like those in the social networking realm, rely on users to connect each other, thus using social filtering to act as [[Recommender system|recommender systems]].

At the other end of the spectrum are specialized [[knowledge base]]s that rely on experts to populate a specialized type of [[database]] with their self-determined areas of expertise and contributions, and do not rely on user recommendations.  Hybrids that feature expert-populated content in conjunction with user recommendations also exist, and are arguably more valuable for doing so.

Still other expertise knowledge bases rely strictly on external manifestations of expertise, herein termed gated objects, e.g., [[citation impact]]s for scientific papers or [[data mining]] approaches wherein many of the work products of an expert are collated.  Such systems are more likely to be free of user-introduced biases (e.g., [http://researchscorecard.com/ ResearchScorecard] ), though the use of computational methods can introduce other biases.

Examples of the systems outlined above are listed in Table 1.

'''Table 1: A classification of expertise location systems'''

{| class="wikitable" border="1"
|-
! Type
! Application domain
! Data source
! Examples
|-
| Social networking
| Professional networking
| User-generated
|
* [[LinkedIn]]
|-
| [[Scientific literature]]
| Identifying publications with strongest research impact
| Third-party generated
|
* [[Science Citation Index]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/science/science_products/a-z/science_citation_index]
|-
| [[Scientific literature]]
| Expertise search
| Software
|
* [[Arnetminer]][http://arnetminer.org]
|-
| Knowledge base
| Private expertise database
| User-Generated
|
* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)
* MIT ExpertFinder (ref. 3)
* Decisiv Search Matters & Expertise ([[Recommind (software company)|Recommind]], Inc.)
* [[Tacit Software]] (Oracle Corporation)
|-
| Knowledge base
| Publicly accessible expertise database
| User-generated
|
* [[Community of Science]] Expertise [http://expertise.cos.com]
* [[ResearcherID]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/scientific/ResearcherID]
|-
| Knowledge base
| Private expertise database
| Third party-generated
|
* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)
* MIT ExpertFinder (ref. 3)
* MindServer Expertise ([[Recommind]], Inc.)
* Tacit Software
|-
| Knowledge base
| Publicly accessible expertise database
| Third party-generated
|
* [http://researchscorecard.com ResearchScorecard] (ResearchScorecard Inc.)
* [http://authoratory.com/ authoratory.com]
* [http://biomedexperts.com BiomedExperts] (Collexis Holdings Inc.)
* [http://www.hcarknowledgemesh.com/ KnowledgeMesh] (Hershey Center for Applied Research)
* [http://med.stanford.edu/profiles/ Community Academic Profiles] (Stanford School of Medicine)
* [http://researchcrossroads.org ResearchCrossroads.org] (Innolyst, Inc.)
|-
| Blog [[search engine]]s
|
| Third party-generated
|
* [[Technorati]] [http://technorati.com/]
|}

== Technical problems ==
A number of interesting problems follow from the use of expertise finding systems:

* The matching of questions from non-expert to the database of existing expertise is inherently difficult, especially when the database does not store the requisite expertise.  This problem grows even more acute with increasing ignorance on the part of the non-expert due to typical search problems involving use of keywords to search unstructured data that are not semantically normalized, as well as variability in how well an expert has set up their descriptive content pages.  Improved question matching is one reason why third-party semantically normalized systems such as [http://researchscorecard.com ResearchScorecard] and [[BiomedExperts]] should be able to provide better answers to queries from non-expert users.
* Avoiding expert-fatigue due to too many questions/requests from users of the system (ref. 1).
* Finding ways to avoid gaming of the system to reap unjustified expertise [[credibility]].

== Expertise ranking ==

Means of classifying and ranking expertise (and therefore experts) become essential if the number of experts returned by a query is greater than a handful.  This raises the following social problems associated with such systems:

* How can expertise be assessed objectively? Is that even possible?
* What are the consequences of relying on unstructured social assessments of expertise, such as user recommendations?
* How does one distinguish [[Authority|''authoritativeness'']] as a proxy metric of expertise from simple ''popularity'', which is often a function of one's ability to express oneself coupled with a good social sense?
* What are the potential consequences of the social or professional stigma associated with the use of an authority ranking, such as used in [http://technorati.com Technorati] and [http://researchscorecard.com ResearchScorecard])?

== Sources of data for assessing expertise ==
Many types of data sources have been used to infer expertise.  They can be broadly categorized based on whether they measure "raw" contributions provided by the expert, or whether some sort of filter is applied to these contributions.

Unfiltered data sources that have been used to assess expertise, in no particular ranking order:

* user recommendations
* help desk tickets: what the problem was and who fixed it
* e-mail traffic between users
* documents, whether private or on the web, particularly publications
* user-maintained web pages
* reports (technical, marketing, etc.)

Filtered data sources, that is, contributions that require approval by third parties (grant committees, referees, patent office, etc.) are particularly valuable for measuring expertise in a way that minimizes biases that follow from popularity or other social factors:

* [[patent]]s, particularly if issued
* scientific publications
* issued grants (failed grant proposals are rarely know beyond the authors)
* [[clinical trial]]s
* product launches
* pharmaceutical drugs

== Approaches for creating expertise content ==
* Manual, either by experts themselves (e.g., LinkedIn) or by a curator
* Automated, e.g., using [[software agent]]s (e.g., MIT's [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] and the [http://wiki.foaf-project.org/ExpertFinder ExpertFinder] initiative) or a combination of agents and human curation (e.g., [http://researchscorecard.com/ ResearchScorecard])

== Interesting expertise systems over the years ==
In no particular order...

* Autonomy's IDOL
* AskMe
* Tacit Knowledge Systems' ActiveNet
* Triviumsoft's SEE-K
* MITs [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] (ref 3)
* MITREs (ref 1) [http://www.mitre.org/news/the_edge/june_98/third.html Expert Finder]
* MITREs XpertNet
* Arnetminer (ref 2)
* Dataware II Knowledge Directory
* Thomsons tool
* Hewlett-Packards CONNEX
* Microsofts SPUD project
* [http://www.xperscore.com Xperscore]
* [http://intunex.fi/skillhive/ Skillhive]

== Conferences ==
# [http://expertfinder.info/pickme2008 The ExpertFinder Initiative]

== References ==

# Ackerman, Mark and McDonald, David (1998) "Just Talk to Me: A Field Study of Expertise Location" ''Proceedings of the 1998 ACM Conference on Computer Supported Cooperative Work''.
# Hughes, Gareth and Crowder, Richard (2003) "Experiences in designing highly adaptable expertise finder systems"  ''Proceedings of the DETC Conference 2003''.
# Maybury, M., DAmore, R., House, D. (2002). "Awareness of organizational expertise." ''International Journal of Human-Computer Interaction'' '''14'''(2): 199-217.
# Maybury, M., DAmore, R., House, D. (2000). Automating Expert Finding. ''International Journal of Technology Research Management.'' 43(6): 12-15.
# Maybury, M., DAmore, R, and House, D. December (2001). Expert Finding for Collaborative Virtual Environments.  ''Communications of the ACM 14''(12): 55-56. In Ragusa, J. and Bochenek, G. (eds). Special Section on Collaboration Virtual Design Environments.
# Maybury, M., DAmore, R. and House, D. (2002). Automated Discovery and Mapping of Expertise.  In Ackerman, M., Cohen, A., Pipek, V. and Wulf, V. (eds.). ''Beyond Knowledge Management: Sharing Expertise.'' Cambridge: MIT Press.
# Mattox, D., M. Maybury, ''et al.'' (1999). "Enterprise expert and knowledge discovery". ''Proceedings of the 8th International Conference on Human-Computer Interactions (HCI International 99)'', Munich, Germany.
# Tang, J., Zhang J., Yao L., Li J., Zhang L. and Su Z.(2008) "ArnetMiner: extraction and mining of academic social networks" ''Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining''.
# Viavacqua, A. (1999). "Agents for expertise location". ''Proceedings of the 1999 AAAI Spring Symposium on Intelligent Agents in Cyberspace'', Stanford, CA.

[[Category:Evaluation methods]]
[[Category:Metrics]]
[[Category:Analysis]]
[[Category:Impact assessment]]
[[Category:Intellectual works]]
[[Category:Knowledge sharing]]
[[Category:Library science]]
[[Category:Information retrieval]]
[[Category:Science studies]]
>>EOP<<
51<|###|>Index term
An '''index term''', '''subject term''', '''subject heading''', or '''descriptor''', in [[information retrieval]], is a term that captures the essence of the topic of a document. Index terms make up a [[controlled vocabulary]] for use in [[bibliographic record]]s. They are an integral part of bibliographic control, which is the function by which libraries collect, organize and disseminate documents. They are used as keywords to retrieve documents in an information system, for instance, a catalog or a [[search engine]].  A popular form of keywords on the web are [[tag (metadata)|tags]] which are directly visible and can be assigned by non-experts also. Index terms can consist of a word, phrase, or alphanumerical term. They are created by analyzing the document either manually with [[subject indexing]] or automatically with [[Index (search engine)|automatic indexing]] or more sophisticated methods of keyword extraction. Index terms can either come from a controlled vocabulary or be freely assigned.

Keywords are stored in a [[Index (search engine)|search index]]. Common words like [[article (grammar)|articles]] (a, an, the) and conjunctions (and, or, but) are not treated as keywords because it is inefficient to do so. Almost every English-language site on the Internet has the article "''the''", and so it makes no sense to search for it. The most popular search engine, [[Google]] removed [[stop words]] such as "the" and "a" from its indexes for several years, but then re-introduced them, making certain types of precise search possible again.

The term "descriptor" was coined by [[Calvin Mooers]] in 1948. It is in particular used about a preferred term from a [[thesaurus]]. 

The [[Simple Knowledge Organisation System]] language (SKOS) provides a way to express index terms with [[Resource Description Framework]] for use in the context of [[Semantic Web]].

==Author keywords==
Many journals and databases provides access (also) to index terms made by authors to the articles being published or represented. The relative quality of indexer-provided index terms and author provided index terms is of interest to research in information retrieval. The quality of both kinds of indexing terms depends, of course, on the qualifications of provider. In general authors have difficulties providing indexing terms that characterizes his document ''relative'' to the other documents in the database. Author keywords are an integral part of literature.

==Examples==
*[[Canadian Subject Headings]] (CSH)
*[[Library of Congress Subject Headings]] (LCSH)
*[[Medical Subject Headings]] (MeSH)
*[[Polythematic Structured Subject Heading System]] (PSH)

==See also==
*[[Dynamic keyword insertion]]
<!-- *Key-objects -->
*[[Keyword cloud]]
*[[Keyword density]]
*[[Keyword optimization]]
*[[knowledge tags|Keyword tagging]]
*[[Subject (documents)]]

== References ==
{{commonscat|Information retrieval}}
*{{cite book|last=Svenonius|first=Elaine|author-link=Elaine Svenonius|title=The intellectual foundation of information organization|date=2009|publisher=MIT Press|location=Cambridge, Mass.|isbn=9780262512619|edition=1st MIT Press pbk.}}

[[Category:Information retrieval]]

{{Library-stub}}
>>EOP<<
57<|###|>Clairlib
{{Multiple issues|
{{unreferenced|date=May 2009}}
{{expert-subject|date=May 2009}}
{{orphan|date=February 2011}}
}}

{{Infobox software
|name                       = Clairlib
|logo                       = [[Image:Clair logo.jpg]]
|screenshot                 = 
|caption                    = 
|collapsible                = 
|author                     = 
|developer                  = CLAIR [[University of Michigan]]
|released                   = 
|latest release version     = 1.0.8
|latest release date        = {{release date and age|2009|08|1}}
|latest preview version     = 
|latest preview date        = 
|frequently updated         = yes
|programming language       = [[Perl]]
|operating system           = 
|platform                   = Cross-platform
|size                       = 
|language                   = Perl
|status                     = Active
|genre                      = [[Natural Language Processing]], [[Network theory|Network Analysis]], [[Information Retrieval]]
|license                    = [[GNU General Public License]], [[Artistic License]]
|website                    = [http://www.clairlib.org/ www.clairlib.org]
}}
'''Clairlib''' is a suite of open-source [[Perl]] modules developed and maintained by the Computational Linguistics And Information Retrieval (CLAIR) group at the [[University of Michigan]]. Clairlib is intended to simplify a number of generic tasks in [[natural language processing]] (NLP), [[information retrieval]] (IR), and network analysis (NA). The latest version of clairlib is 1.06 which was released on March 2009 and includes about 130 modules implementing a wide range of functionalities.

==Functionality==

Clairlib is distributed in two forms: Clairlib-core, which has essential functionality and minimal dependence on external software, and Clairlib-ext, which has extended functionality that may be of interest to a smaller audience. Much can be done using Clairlib on its own. Some of the things that Clairlib can do are: Tokenization, Summarization, Document Clustering, Document Indexing, Web Graph Analysis, Network Generation,  [[Power law distribution]] Analysis, [[Network theory|Network Analysis]], [[Random walk]]s on graphs, [[Tf-idf]], [[Perceptron]] learning  and classification, and [[Compound term processing|Phrase Based Retrieval]] and [[Fuzzy logic|Fuzzy OR Queries]].

==References==
{{reflist}}

==External links==
*[http://www.clairlib.org Homepage]
*[http://tangra.si.umich.edu/clair/ Computational Linguistics And Information Retrieval (CLAIR) group]

[[Category:Free computer libraries]]
[[Category:Perl modules]]
[[Category:University of Michigan]]
[[Category:Information retrieval]]
>>EOP<<
63<|###|>Overlap coefficient
The '''overlap coefficient''' (or, '''Szymkiewicz-Simpson coefficient''') is a [[String_metric|similarity measure]] related to the [[Jaccard index]] that measures the overlap between two sets, and is defined as the size of the intersection divided by the smaller of the size of the two sets:

:<math>\mathrm{overlap}(X,Y) = \frac{| X \cap Y | }{\min(|X|,|Y|)}</math>

If set ''X'' is a subset of ''Y'' or the converse then the overlap coefficient is equal to one.

== External links==
* Open Source [https://github.com/rockymadden/stringmetric/blob/master/core/src/main/scala/com/rockymadden/stringmetric/similarity/OverlapMetric.scala Overlap] [[Scala programming language|Scala]] implementation as part of the larger [http://rockymadden.com/stringmetric/ stringmetric project]

[[Category:Information retrieval]]
[[Category:String similarity measures]]
[[Category:Measure theory]]
>>EOP<<
69<|###|>Category:Citation indices
{{Cat main|Citation index}}
{{cat see also|Bibliographic databases|Bibliographic indexes}}

[[Category:Bibliometrics]]
[[Category:Reference works]]
[[Category:Indexes]]
[[Category:Information retrieval]]
[[Category:Bibliographic databases| ]]
>>EOP<<
75<|###|>Learning to rank
'''Learning to rank'''<ref name="liu">{{citation
|author=Tie-Yan Liu
|title=Learning to Rank for Information Retrieval
|series=Foundations and Trends in Information Retrieval: Vol. 3: No 3
|year=2009
|isbn=978-1-60198-244-5
|doi=10.1561/1500000016
|pages=225331
|journal=Foundations and Trends in Information Retrieval
|volume=3
|issue=3
}}. Slides from Tie-Yan Liu's talk at [[World Wide Web Conference|WWW]] 2009 conference are [http://www2009.org/pdf/T7A-LEARNING%20TO%20RANK%20TUTORIAL.pdf available online]
</ref> or '''machine-learned ranking''' (MLR) is the application of [[machine learning]], typically [[Supervised learning|supervised]], [[Semi-supervised learning|semi-supervised]] or [[reinforcement learning]], in the construction of [[ranking function|ranking models]] for [[information retrieval]] systems.<ref>[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The
MIT Press ISBN 9780262018258.</ref> Training data consists of lists of items with some [[partial order]] specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. "relevant" or "not relevant") for each item. The ranking model's purpose is to rank, i.e. produce a [[permutation]] of items in new, unseen lists in a way which is "similar" to rankings in the training data in some sense.

Learning to rank is a relatively new research area which has emerged in the past decade.

== Applications ==

=== In information retrieval ===
[[File:MLR-search-engine-example.png|250px|thumb|A possible architecture of a machine-learned search engine.]]
Ranking is a central part of many [[information retrieval]] problems, such as [[document retrieval]], [[collaborative filtering]], [[sentiment analysis]], [[computational advertising]] (online ad placement).

A possible architecture of a machine-learned search engine is shown in the figure to the right.

Training data consists of queries and documents matching them together with relevance degree of each match. It may be prepared manually by human ''assessors'' (or ''raters'', as [[Google]] calls them),
<!-- "assessor" is the more standard term, used e.g. by TREC conference -->
who check results for some queries and determine [[Relevance (information retrieval)|relevance]] of each result. It is not feasible to check relevance of all documents, and so typically a technique called [[pooling (information retrieval)|pooling]] is used  only the top few documents, retrieved by some existing ranking models are checked. <!--
  TODO: write something about selection bias caused by pooling
--> Alternatively, training data may be derived automatically by analyzing ''clickthrough logs'' (i.e. search results which got clicks from users),<ref name="Joachims2002">{{citation
 | author=Joachims, T.
 | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]
 | url=http://www.cs.cornell.edu/people/tj/publications/joachims_02c.pdf
 | title=Optimizing Search Engines using Clickthrough Data
 | year=2002
}}</ref> ''query chains'',<ref>{{citation
 | author=Joachims T., Radlinski F.
 | title=Query Chains: Learning to Rank from Implicit Feedback
 | url=http://radlinski.org/papers/Radlinski05QueryChains.pdf
 | year=2005
 | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]
}}</ref> or such search engines' features as Google's [[Google SearchWiki|SearchWiki]].

Training data is used by a learning algorithm to produce a ranking model which computes relevance of documents for actual queries.

Typically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used.<ref>{{citation
 | author=B. Cambazoglu, H. Zaragoza, O. Chapelle, J. Chen, C. Liao, Z. Zheng, and J. Degenhardt.
 | title=Early exit optimizations for additive machine learned ranking systems
 | journal=WSDM '10: Proceedings of the Third ACM International Conference on Web Search and Data Mining, 2010. (to appear)
 | url=http://olivier.chapelle.cc/pub/wsdm2010.pdf
}}</ref> First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as [[vector space model]], [[Standard Boolean model|boolean model]], weighted AND,<ref>{{citation
 | author=Broder A., Carmel D., Herscovici M., Soffer A., Zien J.
 | title=Efficient query evaluation using a two-level retrieval process
 | journal=Proceedings of the twelfth international conference on Information and knowledge management
 | year=2003
 | pages=426434
 | isbn=1-58113-723-0
 | url=http://cis.poly.edu/westlab/papers/cntdstrb/p426-broder.pdf
 }}</ref> [[Okapi BM25|BM25]]. This phase is called ''top-<math>k</math> document retrieval'' and many good heuristics were proposed in the literature to accelerate it, such as using document's static quality score and tiered indexes.<ref name="manning-q-eval">{{citation
 | author=Manning C.,  Raghavan P. and Schutze H.
 | title=Introduction to Information Retrieval
 | publisher=Cambridge University Press
 | year=2008}}. Section [http://nlp.stanford.edu/IR-book/html/htmledition/efficient-scoring-and-ranking-1.html 7.1]</ref> In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents.

=== In other areas ===
Learning to rank algorithms have been applied in areas other than information retrieval:
* In [[machine translation]] for ranking a set of hypothesized translations;<ref name="Duh09">{{citation
 | author=Kevin K. Duh
 | title=Learning to Rank with {{sic|hide=y|Partially|-}}Labeled Data
 | year=2009
 | url=http://ssli.ee.washington.edu/people/duh/thesis/uwthesis.pdf
}}</ref>
* In [[computational biology]] for ranking candidate 3-D structures in protein structure prediction problem.<ref name="Duh09" />
* In [[proteomics]] for the identification of frequent top scoring peptides.<ref name="Hen09">{{citation
 | author=Henneges C., Hinselmann G., Jung S., Madlung J., Schutz W., Nordheim A., Zell A.
 | title=Ranking Methods for the Prediction of Frequent Top Scoring Peptides from Proteomics Data
 | year=2009
 | url=http://www.omicsonline.com/ArchiveJPB/2009/May/01/JPB2.226.pdf
}}</ref>
* In [[Recommender system]]s for identifying a ranked list of related news articles to recommend to a user after he or she has read a current news article.<ref>Yuanhua Lv, Taesup Moon, Pranam Kolari, Zhaohui Zheng, Xuanhui Wang, and Yi Chang, [http://sifaka.cs.uiuc.edu/~ylv2/pub/www11-relatedness.pdf ''Learning to Model Relatedness for News Recommendation''], in International Conference on World Wide Web (WWW), 2011.</ref>

== Feature vectors ==
For convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called ''[[feature vector]]s''. Such approach is sometimes called ''bag of features'' and is analogous to [[bag of words]] and [[vector space model]] used in information retrieval for representation of documents.

Components of such vectors are called ''[[feature (machine learning)|feature]]s'', ''factors'' or ''ranking signals''. They may be divided into three groups (features from [[document retrieval]] are shown as examples):
* ''Query-independent'' or ''static'' features  those features, which depend only on the document, but not on the query. For example, [[PageRank]] or document's length. Such features can be precomputed in off-line mode during indexing. They may be used to compute document's ''static quality score'' (or ''static rank''), which is often used to speed up search query evaluation.<ref name="manning-q-eval" /><ref>
{{cite conference
 | first=M. |last=Richardson
 | coauthors=Prakash, A. and Brill, E.
 | title=Beyond PageRank: Machine Learning for Static Ranking
 | booktitle=Proceedings of the 15th International World Wide Web Conference
 | pages=707715
 | publisher=
 | year=2006
 | url=http://research.microsoft.com/en-us/um/people/mattri/papers/www2006/staticrank.pdf
 | accessdate=
 }}</ref>
* ''Query-dependent'' or ''dynamic'' features  those features, which depend both on the contents of the document and the query, such as [[TF-IDF]] score or other non-machine-learned ranking functions.
* ''Query level features'' or ''query features'', which depend only on the query. For example, the number of words in a query. ''Further information: [[query level feature]]''

Some examples of features, which were used in the well-known [[LETOR]] dataset:<ref name="letor3">[http://research.microsoft.com/en-us/people/taoqin/letor3.pdf LETOR 3.0. A Benchmark Collection for Learning to Rank for Information Retrieval]</ref>
* TF, [[TF-IDF]], [[Okapi BM25|BM25]], and [[language modeling]] scores of document's [[Zone (information retrieval)|zone]]s (title, body, anchors text, URL) for a given query;
* Lengths and [[Inverse document frequency|IDF]] sums of document's zones;
* Document's [[PageRank]], [[HITS algorithm|HITS]] ranks and their variants.

Selecting and designing good features is an important area in machine learning, which is called [[feature engineering]].

== Evaluation measures ==
There are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics.

Examples of ranking quality measures:
* [[Mean average precision]] (MAP);
* [[Discounted cumulative gain|DCG]] and [[Normalized discounted cumulative gain|NDCG]];
* [[Precision (information retrieval)|Precision]]@''n'', NDCG@''n'', where "@''n''" denotes that the metrics are evaluated only on top ''n'' documents;
* [[Mean reciprocal rank]];
* [[Kendall's tau]]
* [[Spearman's rank correlation coefficient|Spearman's Rho]]

DCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used.<ref>http://www.stanford.edu/class/cs276/handouts/lecture15-learning-ranking.ppt</ref> Other metrics such as MAP, MRR and precision, are defined only for binary judgements.

Recently, there have been proposed several new evaluation metrics which claim to model user's satisfaction with search results better than the DCG metric:
* [[Expected reciprocal rank]] (ERR);<ref>{{citation
|author=Olivier Chapelle, Donald Metzler, Ya Zhang, Pierre Grinspan
|title=Expected Reciprocal Rank for Graded Relevance
|url=http://research.yahoo.com/files/err.pdf
|journal=CIKM
|year=2009
|pages=
}}</ref>
* [[Yandex]]'s pfound.<ref>{{citation
|author=Gulin A., Karpovich P., Raskovalov D., Segalovich I.
|title=Yandex at ROMIP'2009: optimization of ranking algorithms by machine learning methods
|url=http://romip.ru/romip2009/15_yandex.pdf
|journal=Proceedings of ROMIP'2009
|year=2009
|pages=163168
}} (in Russian)</ref>
Both of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document.

== Approaches ==
{{Expand section|date=December 2009}}
Tie-Yan Liu of [[Microsoft Research Asia]] in his paper "Learning to Rank for Information Retrieval"<ref name="liu" /> and talks at several leading conferences has analyzed existing algorithms for learning to rank problems and categorized them into three groups by their input representation and [[loss function]]:

=== Pointwise approach ===
In this case it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then learning-to-rank problem can be approximated by a regression problem  given a single query-document pair, predict its score.

A number of existing [[Supervised learning|supervised]] machine learning algorithms can be readily used for this purpose. [[Ordinal regression]] and [[classification (machine learning)|classification]] algorithms can also be used in pointwise approach when they are used to predict score of a single query-document pair, and it takes a small, finite number of values.

=== Pairwise approach ===
In this case learning-to-rank problem is approximated by a classification problem  learning a [[binary classifier]] that can tell which document is better in a given pair of documents. The goal is to minimize average number of [[Permutation#Inversions|inversions]] in ranking.

=== Listwise approach ===
These algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to ranking model's parameters, and so continuous approximations or bounds on evaluation measures have to be used.

=== List of methods ===
A partial list of published learning-to-rank algorithms is shown below with years of first publication of each method:
:{|class="wikitable sortable"
! Year || Name || Type || Notes
|-
| 1989 || OPRF <ref name="Fuhr1989">{{citation
 | last=Fuhr
 | first=Norbert
 | journal=ACM Transactions on Information Systems
 | title=Optimum polynomial retrieval functions based on the probability ranking principle
 | volume=7
 | number=3
 | pages=183204 
 | year=1989
 | doi=10.1145/65943.65944
}}</ref> || <span style="display:none">2</span> pointwise || Polynomial regression (instead of machine learning, this work refers to pattern recognition, but the idea is the same)
|-
| 1992 || SLR <ref name="Cooperetal1992">{{citation
 | author=Cooper, William S.; Gey, Frederic C.; Dabney, Daniel P.
 | journal=SIGIR '92 Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval 
 | title=Probabilistic retrieval based on staged logistic regression
 | pages=198210 
 | year=1992
 | doi=10.1145/133160.133199
}}</ref>   || <span style="display:none">2</span> pointwise || Staged logistic regression
|-
| 2000 || [http://research.microsoft.com/apps/pubs/default.aspx?id=65610 Ranking SVM] (RankSVM) || <span style="display:none">2</span> pairwise ||  A more recent exposition is in,<ref name="Joachims2002" /> which describes an application to ranking using clickthrough logs.
|-
| 2002 || Pranking<ref>{{cite paper | id = {{citeseerx|10.1.1.20.378}} | title = Pranking }}</ref> || <span style="display:none">1</span> pointwise || Ordinal regression.
|-
| 2003 <!-- or 1998? --> || [http://jmlr.csail.mit.edu/papers/volume4/freund03a/freund03a.pdf RankBoost] || <span style="display:none">2</span> pairwise ||
|-
| 2005 || [http://research.microsoft.com/en-us/um/people/cburges/papers/ICML_ranking.pdf RankNet] || <span style="display:none">2</span> pairwise ||
|-
| 2006 || [http://research.microsoft.com/en-us/people/tyliu/cao-et-al-sigir2006.pdf IR-SVM] || <span style="display:none">2</span> pairwise || Ranking SVM with query-level normalization in the loss function.
|-
| 2006 || [http://research.microsoft.com/en-us/um/people/cburges/papers/lambdarank.pdf LambdaRank] || <span style="display:none">3</span> pairwise || RankNet in which pairwise loss function is multiplied by the change in the IR metric caused by a swap.
|-
| 2007 || [http://research.microsoft.com/en-us/people/junxu/sigir2007-adarank.pdf AdaRank] || <span style="display:none">3</span> listwise ||
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70364 FRank] || <span style="display:none">2</span> pairwise || Based on RankNet, uses a different loss function - fidelity loss.
|-
| 2007 || [http://www.cc.gatech.edu/~zha/papers/fp086-zheng.pdf GBRank] || <span style="display:none">2</span> pairwise || 
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70428 ListNet] || <span style="display:none">3</span> listwise ||
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=68128 McRank] || <span style="display:none">1</span> pointwise ||
|-
| 2007 || [http://www.stat.rutgers.edu/~tzhang/papers/nips07-ranking.pdf QBRank] || <span style="display:none">2</span> pairwise ||
|-
| 2007 || [http://research.microsoft.com/en-us/people/hangli/qin_ipm_2008.pdf RankCosine] || <span style="display:none">3</span> listwise ||
|-
| 2007 || RankGP<ref>{{cite paper | id = {{citeseerx|10.1.1.90.220}} | title = RankGP }}</ref> || <span style="display:none">3</span> listwise ||
|-
| 2007 || [http://staff.cs.utu.fi/~aatapa/publications/inpPaTsAiBoSa07a.pdf RankRLS] || <span style="display:none">2</span> pairwise ||
Regularized least-squares based ranking. The work is extended in
<ref name=pahikkala2009efficient>{{Citation|last=Pahikkala|first=Tapio|coauthors=Tsivtsivadze, Evgeni, Airola, Antti, Jarvinen, Jouni, Boberg, Jorma|title=An efficient algorithm for learning to rank from preference graphs|journal=Machine Learning|year=2009|volume=75|issue=1|pages=129165|doi=10.1007/s10994-008-5097-z|postscript=.}}</ref> to learning to rank from general preference graphs.
|-
| 2007 || [http://www.cs.cornell.edu/People/tj/publications/yue_etal_07a.pdf SVM<sup>map</sup>] || <span style="display:none">3</span> listwise ||
|-
| 2008 || [http://research.microsoft.com/pubs/69536/tr-2008-109.pdf LambdaMART] || <span style="display:none">3</span> listwise || Winning entry in the recent Yahoo Learning to Rank competition used an ensemble of LambdaMART models.<ref>C. Burges. (2010). [http://research.microsoft.com/en-us/um/people/cburges/tech_reports/MSR-TR-2010-82.pdf From RankNet to LambdaRank to LambdaMART: An Overview].</ref>
|-
| 2008 || [http://research.microsoft.com/en-us/people/tyliu/icml-listmle.pdf ListMLE] || <span style="display:none">3</span> listwise || Based on ListNet.
|-
| 2008 || [http://research.microsoft.com/en-us/people/junxu/sigir2008-directoptimize.pdf PermuRank] || <span style="display:none">3</span> listwise ||
|-
| 2008 || [http://research.microsoft.com/apps/pubs/?id=63585 SoftRank] || <span style="display:none">3</span> listwise ||
|-
| 2008 || [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf Ranking Refinement]<ref>Rong Jin, Hamed Valizadegan, Hang Li, [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf ''Ranking Refinement and Its Application for Information Retrieval''], in International Conference on World Wide Web (WWW), 2008.</ref> || <span style="display:none">2</span> pairwise || A semi-supervised approach to learning to rank that uses Boosting.
|-
| 2008 || [http://www-connex.lip6.fr/~amini/SSRankBoost/ SSRankBoost]<ref>Massih-Reza Amini, Vinh Truong, Cyril Goutte, [http://www-connex.lip6.fr/~amini/Publis/SemiSupRanking_sigir08.pdf ''A Boosting Algorithm for Learning Bipartite Ranking Functions with Partially Labeled Data''], International ACM SIGIR conference, 2008. The [http://www-connex.lip6.fr/~amini/SSRankBoost/ code] is available for research purposes.</ref>  || <span style="display:none">2</span> pairwise|| An extension of RankBoost to learn with partially labeled data (semi-supervised learning to rank)
|-
| 2008 || [http://phd.dii.unisi.it/PosterDay/2009/Tiziano_Papini.pdf SortNet]<ref>Leonardo Rigutini, Tiziano Papini, Marco Maggini, Franco Scarselli, [http://research.microsoft.com/en-us/um/beijing/events/lr4ir-2008/PROCEEDINGS-LR4IR%202008.PDF "SortNet: learning to rank by a neural-based sorting algorithm"], SIGIR 2008 workshop: Learning to Rank for Information Retrieval, 2008</ref> || <span style="display:none">2</span> pairwise|| SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. 
|-
| 2009 || [http://itcs.tsinghua.edu.cn/papers/2009/2009031.pdf MPBoost] || <span style="display:none">2</span> pairwise || Magnitude-preserving variant of RankBoost. The idea is that the more unequal are labels of a pair of documents, the harder should the algorithm try to rank them.
|-
| 2009 || [http://www.machinelearning.org/archive/icml2009/papers/498.pdf BoltzRank] || <span style="display:none">3</span> listwise || Unlike earlier methods, BoltzRank produces a ranking model that looks during query time not just at a single document, but also at pairs of documents.
|-
| 2009 || [http://www.iis.sinica.edu.tw/papers/whm/8820-F.pdf BayesRank] || <span style="display:none">3</span> listwise || Based on ListNet.
|-
| 2010 || [http://www.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf NDCG Boost]<ref>Hamed Valizadegan, Rong Jin, Ruofei Zhang, Jianchang Mao, [http://www.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf ''Learning to Rank by Optimizing NDCG Measure''], in Proceeding of Neural Information Processing Systems (NIPS), 2010.</ref> || <span style="display:none">3</span> listwise || A boosting approach to optimize NDCG.
|-
| 2010 || [http://arxiv.org/abs/1001.4597 GBlend] || <span style="display:none">2</span> pairwise || Extends GBRank to the learning-to-blend problem of jointly solving multiple learning-to-rank problems with some shared features.
|-
| 2010 || [http://wume.cse.lehigh.edu/~ovd209/wsdm/proceedings/docs/p151.pdf IntervalRank] || <span style="display:none">2</span> pairwise & listwise || 
|-
| 2010 || [http://www.eecs.tufts.edu/~dsculley/papers/combined-ranking-and-regression.pdf CRR] || <span style="display:none">2</span> pointwise & pairwise || Combined Regression and Ranking. Uses [[stochastic gradient descent]] to optimize a linear combination of a pointwise quadratic loss and a pairwise hinge loss from Ranking SVM.
|}

Note: as most [[supervised learning]] algorithms can be applied to pointwise case, only those methods which are specifically designed with ranking in mind are shown above.

== History ==
[[Norbert Fuhr]] introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation;<ref name="Fuhr1992">{{citation
 | last=Fuhr
 | first=Norbert
 | journal=Computer Journal
 | title=Probabilistic Models in Information Retrieval
 | volume=35
 | number=3
 | pages=243255
 | year=1992
 | doi=10.1093/comjnl/35.3.243
}}</ref> a specific variant of this approach (using [[polynomial regression]]) had been published by him three years earlier.<ref name="Fuhr1989" /> Bill Cooper proposed [[logistic regression]] for the same purpose in 1992 <ref name="Cooperetal1992" /> and used it with his  [[University of California at Berkeley|Berkeley]] research group to train a successful ranking function for [[Text Retrieval Conference|TREC]].  Manning et al.<ref>{{citation |author=Manning C.,  Raghavan P. and Schutze H. |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008}}. Sections [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-7.html 7.4] and [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-15.html 15.5]</ref>  suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques.

Several conferences, such as [[Neural Information Processing Systems|NIPS]], [[Special Interest Group on Information Retrieval|SIGIR]] and [[International Conference on Machine Learning|ICML]] had workshops devoted to the learning-to-rank problem since mid-2000s (decade).

=== Practical usage by search engines ===
Commercial [[web search engine]]s began using machine learned ranking systems since the 2000s (decade). One of the first search engines to start using it was [[AltaVista]] (later its technology was acquired by [[Overture Services, Inc.|Overture]], and then [[Yahoo]]), which launched a [[gradient boosting]]-trained ranking function in April 2003.<ref>Jan O. Pedersen. [http://jopedersen.com/Presentations/The_MLR_Story.pdf The MLR Story]</ref><ref>{{US Patent|7197497}}</ref>

[[Bing (search engine)|Bing]]'s search is said to be powered by [[RankNet]] algorithm,<ref>[http://www.bing.com/community/blogs/search/archive/2009/06/01/user-needs-features-and-the-science-behind-bing.aspx?PageIndex=4 Bing Search Blog: User Needs, Features and the Science behind Bing]</ref>{{when|date=February 2014}} which was invented at [[Microsoft Research]] in 2005.

In November 2009 a Russian search engine [[Yandex]] announced<ref name="snezhinsk">[http://webmaster.ya.ru/replies.xml?item_no=5707&ncrnd=5118 Yandex corporate blog entry about new ranking model "Snezhinsk"] (in Russian)</ref> that it had significantly increased its [[search quality]] due to deployment of a new proprietary MatrixNet algorithm, a variant of [[gradient boosting]] method which uses [[oblivious decision tree]]s.<ref>The algorithm wasn't disclosed, but a few details were made public in [http://download.yandex.ru/company/experience/GDD/Zadnie_algoritmy_Karpovich.pdf] and [http://download.yandex.ru/company/experience/searchconf/Searchconf_Algoritm_MatrixNet_Gulin.pdf].</ref> Recently they have also sponsored a machine-learned ranking competition "Internet Mathematics 2009"<ref>[http://imat2009.yandex.ru/academic/mathematic/2009/en/ Yandex's Internet Mathematics 2009 competition page]</ref> based on their own search engine's production data. Yahoo has announced a similar competition in 2010.<ref>[http://learningtorankchallenge.yahoo.com/ Yahoo Learning to Rank Challenge]</ref>

As of 2008, [[Google]]'s [[Peter Norvig]] denied that their search engine exclusively relies on machine-learned ranking.<ref>{{cite web
  | url = http://anand.typepad.com/datawocky/2008/05/are-human-experts-less-prone-to-catastrophic-errors-than-machine-learned-models.html
  | archiveurl = http://www.webcitation.org/5sq8irWNM
  | archivedate = 2010-09-18
  | title = Are Machine-Learned Models Prone to Catastrophic Errors?
  | date = 2008-05-24
  | last = Rajaraman
  | first = Anand
  | authorlink = Anand Rajaraman}}</ref> [[Cuil]]'s CEO, [[Tom Costello (businessman)|Tom Costello]], suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models "learn what people say they like, not what people actually like".<ref>{{cite web
  | url = http://www.cuil.com/info/blog/2009/06/26/so-how-is-bing-doing
  | archiveurl = http://www.webcitation.org/5sq7DX3Pj
  | archivedate = 2010-09-15
  | title = Cuil Blog: So how is Bing doing?
  | date = 2009-06-26
  | last = Costello
  | first = Tom}}</ref>

== References ==
{{reflist|2}}

== External links ==
; Competitions and public datasets
* [http://research.microsoft.com/en-us/um/people/letor/ LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval]
* [http://imat2009.yandex.ru/en/ Yandex's Internet Mathematics 2009]
* [http://learningtorankchallenge.yahoo.com/ Yahoo! Learning to Rank Challenge]
* [http://research.microsoft.com/en-us/projects/mslr/default.aspx Microsoft Learning to Rank Datasets]

; Open Source code
* [https://mloss.org/software/view/332/ Parallel C++/MPI implementation of Gradient Boosted Regression Trees for ranking, released September 2011]
* [https://sites.google.com/site/rtranking/ C++ implementation of Gradient Boosted Regression Trees and Random Forests for ranking]
* [http://dlib.net/ml.html#svm_rank_trainer C++ and Python tools for using the SVM-Rank algorithm]

[[Category:Information retrieval]]
[[Category:Machine learning]]
[[Category:Ranking functions]]
>>EOP<<
81<|###|>Probabilistic relevance model
{{Underlinked|date=August 2014}}

The '''probabilistic relevance model'''<ref>{{citation | author=S. E. Robertson | coauthors=K. S. Jones | title=Relevance weighting of search terms | publisher=Journal of the American Society for Information Science | pages=129146 | date=MayJune 1976 | url=http://portal.acm.org/citation.cfm?id=106783 }}</ref> was devised by Robertson and Jones as a framework for probabilistic models to come.
 
It makes an estimation of the probability of finding if a document ''d<sub>j</sub>'' is relevant to a query ''q''. This model assumes that this probability of relevance depends on the query and document representations. Furthermore, it assumes that there is a portion of all documents that is preferred by the user as the answer set for query ''q''. Such an ideal answer set is called ''R'' and should maximize the overall probability of relevance to that user. The prediction is that documents in this set ''R'' are relevant to the query, while documents not present in the set are non-relevant.

<math>sim(d_{j},q) = \frac{P(R|\vec{d}_j)}{P(\bar{R}|\vec{d}_j)}</math>

==Related models==
There are some limitations to this framework that need to be addressed by further development:
* There is no accurate estimate for the first run probabilities
* Index terms are not weighted
* Terms are assumed mutually independent

To address these and other concerns there are some developed models from the probabilistic relevance framework. The [[Binary Independence Model]] for one, as it is from the same author. The most known derivative of this framework is the [[Probabilistic relevance model (BM25)|Okapi(BM25)]] weighting scheme and it's BM25F brother.

==References==
{{reflist}}

[[Category:Information retrieval]]
[[Category:Probabilistic models]]
>>EOP<<
87<|###|>Anchor text
{{Use dmy dates|date=February 2013}}
The '''anchor text''', '''link label''', '''link text''', or '''link title''' is the visible, clickable text in a [[hyperlink]]. The words contained in the anchor text can determine the ranking that the page will receive by search engines. Since 1998, some [[web browser]]s have added the ability to show a [[tooltip]] for a hyperlink before it is selected. Not all links have anchor texts because it may be obvious where the link will lead due to the context in which it is used. Anchor texts normally remain below 60 [[Character (computing)|characters]]. Different browsers will display anchor texts differently. Usually, Web Search Engines analyze anchor text from hyperlinks on web pages. Other services apply the basic principles of anchor text analysis as well. For instance, [[List of academic databases and search engines|academic search engines]] may use [[citation]] context to classify [[Academic publishing|academic articles]],<ref>{{cite web|last=Bader Aljaber, Nicola Stokes, James Bailey and Jian Pei|url=http://www.springerlink.com/content/p278617582u5x3x1/|title=Document clustering of scientific texts using citation contexts |date=1 April 2010|publisher=Springer}}</ref> and anchor text from documents linked in [[mind maps]] may be used too.<ref>Needs new reference link</ref> [[File:Anchor text.png|thumb|Visual implementation of anchor text]]

==Overview==
Anchor text usually gives the user relevant descriptive or contextual information about the content of the link's destination. The anchor text may or may not be related to the actual text of the [[Uniform Resource Locator|URL]] of the link. For example, a hyperlink to the [[English Wikipedia|English-language Wikipedia]]'s [[homepage]] might take this form:

:<code><nowiki><a href="http://en.wikipedia.org/wiki/Main_Page">Wikipedia</a></nowiki></code>

The anchor text in this example is "Wikipedia"; the longer, but vital, URL <code><nowiki>http://en.wikipedia.org/wiki/Main_Page</nowiki></code> needed to locate the target page, displays on the web page as {{srlink|Main Page|Wikipedia}}, contributing to clean, easy-to-read text.

==Common misunderstanding of the concept==

This proper method of linking is beneficial to users and [[webmaster]]s as anchor text holds [[significant]] [[weight]] in [[search engine]] rankings. The limit of the [[concept]] is building [[Sentence (linguistics)|sentence]]s only composed with linked [[word]]s.{{citation needed|date=September 2011}}

==Search engine algorithms==
Anchor text is weighted (ranked) highly in [[search engine]] [[algorithm]]s, because the linked text is usually relevant to the [[landing page]]. The objective of search engines is to provide highly relevant search results; this is where anchor text helps, as the tendency was, more often than not, to hyperlink words relevant to the landing page. Anchor text can also serve the purpose of directing the user to internal pages on the site, which can also help to rank the website higher in the search rankings.<ref name="Search Engine Watch">{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2169750/How-the-Web-Uses-Anchor-Text-in-Internal-Linking-Study|title=
How the Web Uses Anchor Text in Internal Linking [Study]|accessdate=6 July 2012}}</ref>

[[Webmaster]]s may use anchor text to procure high results in [[search engine results page]]s. [[Google]]'s [[Google Webmaster Tools|Webmaster Tools]] facilitate this optimization by letting [[website]] owners view the most common words in anchor text linking to their site.<ref>{{cite web
|last=Fox
|first=Vanessa
|url=http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html
|title=Get a more complete picture about how other sites link to you
|date=15 March 2007
|publisher=Official Google Webmaster Central Blog
|accessdate=2007-03-27
| archiveurl= http://web.archive.org/web/20070331195216/http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html| archivedate= 31 March 2007 <!--DASHBot-->| deadurl= no}}</ref>
In the past, [[Google bomb]]ing was possible through anchor text manipulation; however, in January 2007, Google announced it had updated its algorithm to minimize the impact of Google bombs, which refers to a prank where people attempt to cause someone else's site to rank for an obscure or meaningless query.<ref>{{cite web
|last=Cutts
|first=Matt
|url=http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html
|title=A quick word about Googlebombs
|date=25 January 2007
|publisher=Official Google Webmaster Central Blog
|accessdate=2007-03-27
| archiveurl= http://web.archive.org/web/20070324043013/http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html| archivedate= 24 March 2007 <!--DASHBot-->| deadurl= no}}</ref>

In April 2012, Google announced in its March "Penguin" update that it would be changing the way it handled anchor text, implying that anchor text would no longer be as important an element for their ranking metrics.<ref>{{cite web|url=http://insidesearch.blogspot.co.uk/2012/04/search-quality-highlights-50-changes.html|title=Google's March Update|publisher=Google}}</ref><ref>{{cite web|first=Simon|last=Dalley|accessdate=2012-04-04|date=4 April 2012|url=http://www.growtraffic.co.uk/google-changes-the-way-it-handles-anchor-text|title=Google Changes The way It Handles Anchor Text|publisher=Grow Traffic}}</ref> Moving forward, Google would be paying more attention to a diversified link profile which has a mix of anchor text and other types of links.
.<ref name="Search Engine Watch">{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2172839/Google-Penguin-Update-Impact-of-Anchor-Text-Diversity-Link-Relevancy|title=
Google Penguin Update: Impact of Anchor Text Diversity & Link Relevancy|accessdate=6 July 2012}}</ref>

==Anchor Text Terminology==
There are different classifications of anchor text that are used within the search engine optimization community such as the following:

'''Exact Match:''' whenever an anchor is used with a keyword that mirrors the page that is being linked to. Example: "[[search engine optimization]]" is an exact match anchor because it's linking to a page about "search engine optimization.

'''Branded:''' whenever a brand is used as the anchor. "[[Wikipedia]]" is a branded anchor text.

'''Naked Link:''' whenever a URL is used as an anchor. "[[www.wikipedia.com]]" is a naked link anchor.

'''Generic:''' whenever a generic word or phrase is used as the anchor. "Click here" is a generic anchor. Other variations may include "go here", "visit this website", etc.

'''Images:''' whenever an image is linked, Google will use the "ALT" tag as the anchor text
.<ref>{{cite web
|last=Gotch
|first=Nathan
|url=http://www.gotchseo.com/anchor-text/
|title=The Epic Guide to Anchor Text
|date=26 October 2014}}</ref>

==References==

{{reflist|colwidth=30em}}

[[Category:Information retrieval]]
[[Category:Internet search engines]]
[[Category:Internet terminology]]
[[Category:Search engine optimization]]
[[Category:Hypertext]]
>>EOP<<
93<|###|>Category:Music search engines
[[Category:Information retrieval]]
[[Category:Music software|Search engines]]
[[Category:Internet search engines]]
[[Category:Online music and lyrics databases]]
>>EOP<<
99<|###|>Conference and Labs of the Evaluation Forum
The '''Conference and Labs of the Evaluation Forum''' (formerly '''Cross-Language Evaluation Forum'''), or '''CLEF''', is an organization promoting research  in multilingual [[information access]] (currently focusing on [[European Commissioner for Multilingualism|European languages]]). Its specific functions are to  maintain an underlying framework for testing [[information retrieval]] systems, and creating [[digital library|repositories]] of data for researchers to use in developing  comparable [[Technical standard|standards]].<ref name="Peters">{{cite conference | first1 = Carol | last1 = Peters| first2 = Martin | last2 = Braschler | first3 = Khalid | last3 = Choukri | first4 = Julio | last4 = Gonzalo | first5 = Michael | last5 = Kluck | title = The Future of Evaluation for Cross-Language Information Retrieval Systems | conference = Second Workshop of the Cross-Language Evaluation Forum, CLEF 2001 | id = {{citeseerx|10.1.1.109.7647}} }}</ref>
The organization holds a forum meeting   every September in Europe. Prior to each forum, participants receive a set of challenge tasks. The tasks  are designed to test various aspects of information retrieval systems and encourage their development. Groups of researchers propose and organize campaigns to satisfy those tasks. The results are used as [[benchmark (computing)|benchmarks]] for the state of the art  in the specific areas.,<ref>{{cite journal | url = http://www.springerlink.com/content/l7v0354471u53385/ | title = Special Issue on CLEF | journal = Information Retrieval | volume = 7 | issue = 12 | year = 2004 }}</ref><ref>Fredric C. Gey, Noriko Kando, and Carol Peters "Cross-Language Information Retrieval: the way ahead" in ''Information Processing & Management''
vol. 41, no. 3,  p.415-431 May 2005, {{doi|10.1016/j.ipm.2004.06.006}}</ref>  

For example, the 2010 medical retrieval task focuses on retrieval of computed tomography,  MRI, and radiographic images.<ref name="ImageCLEFmed">{{cite web | last = Mueller| first = Henning| authorlink = | coauthors = | title = Medical Retrieval Task| work = | publisher =ImageCLEF - Cross-language image retrieval evaluations | date = 20 May 2010| url =http://www.imageclef.org/2010/medical | format = | doi = | accessdate = 27 May 2010 }}</ref>

==References==
{{reflist}}

== External links ==
* [http://www.clef-campaign.org CLEF homepage]

[[Category:Information retrieval]]

{{Compu-conference-stub}}
>>EOP<<
105<|###|>Query likelihood model
The '''query likelihood model''' is a [[language model]] used in [[Information Retrieval]]. A language model is constructed for each document in the collection.  It is then possible to rank each document by the probability of specific documents given a query. This is interpreted as being the [[Likelihood function|likelihood]] of a document being relevant given a query.

==Calculating the likelihood==
Using [[Bayes' theorem|Bayes' rule]], the probability <math>P</math> of a document <math>d</math>, given a query <math>q</math> can be written as follows:

:<math>
 P(d|q) = \frac{P(q|d) P(d)}{P(q)}
</math>

Since the probability of the query P(q) is the same for all documents, this can be ignored. Further, it is typical to assume that the probability of documents is uniform. Thus, P(d) is also ignored.

:<math>
 P(d|q) = P(q|d)
</math>

Documents are then ranked by the probability that a query is observed as a random sample from the document model. The multinomial unigram language model is commonly used to achieve this. We have:
:<math>
 P(q|M_d) = K_q \prod_{t \in V} P(t|M_d)^{tf_{t,q}}
</math>,where the multinomial coefficient is <math>K_q = L_q!/(tf_{t1,q}!tf_{t2,q}!...tf_{tM,q}!)</math> for query {{math|q}}.

In practice the multinomial coefficient is usually removed from the calculation. The reason is that it is a constant for a given bag of words (such as all the words from a specific document <math>d</math>). The language model <math>M_d</math> should be the true language model calculated from the distribution of words underlying each retrieved document. In practice this language model is unknown, so it is usually approximated by considering each term (unigram) from the retrieved document together with its probability of appearance. So <math>P(t|M_d)</math> is the probability of term <math>t</math> being generated by the language model <math>M_d</math> of document <math>d</math>. This probability is multiplied for all terms from query <math>q</math> to get a rank for document <math>d</math> in the interval <math>[0,1]</math>. The calculation is repeated for all documents to create a ranking of all documents in the document collection.

<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schutze: An Introduction to Information Retrieval, page 241. Cambridge University Press, 2009</ref>

==References==
 <references/>

[[Category:Information retrieval]]
>>EOP<<
111<|###|>XML retrieval
{{Multiple issues|
{{expert-subject|Computer science|date=January 2015}}
{{COI|date=February 2009}}
}}

'''XML retrieval''', or XML Information Retrieval, is the content-based retrieval of documents structured with [[XML]] (eXtensible Markup Language). As such it is used for computing [[Relevance (information retrieval)|relevance]] of XML documents.<ref>{{Cite web|url=ftp://ftp.tm.informatik.uni-frankfurt.de/pub/papers/ir/An%20Architecture%20for%20XML%20Information%20Retrieval%20in%20a%20Peer-to-Peer%20Environment_2007.pdf|title=An Architecture for XML Information Retrieval in a Peer-to-Peer Environment|last=Winter|first=Judith|author2=Drobnik, Oswald |date=November 9, 2007|publisher=ACM|accessdate=2009-02-10}}</ref>

==Queries==
Most XML retrieval approaches do so based on techniques from the [[information retrieval]] (IR) area, e.g. by computing the similarity between a query consisting of keywords (query terms) and the document. However, in XML-Retrieval the query can also contain [[Data structure|structural]] [[Hint (SQL)|hints]]. So-called "content and structure" (CAS) queries enable users to specify what structure the requested content can or must have.

==Exploiting XML structure==
Taking advantage of the [[Self-documenting|self-describing]] structure of XML documents can improve the search for XML documents significantly. This includes the use of CAS queries, the weighting of different XML elements differently and the focused retrieval of subdocuments.

==Ranking==
Ranking in XML-Retrieval can incorporate both content relevance and structural similarity, which is the resemblance between the structure given in the query and the structure of the document. Also, the retrieval units resulting from an XML query may not always be entire documents, but can be any deeply nested XML elements, i.e. dynamic documents. The aim is to find the smallest retrieval unit that is highly relevant. Relevance can be defined according to the notion of specificity, which is the extent to which a retrieval unit focuses on the topic of request.<ref name="INEX2006">{{Cite web|url=http://www.cs.otago.ac.nz/homepages/andrew/2006-10.pdf|title=Overview of INEX 2006|last=Malik|first=Saadia|author2=Trotman, Andrew |author3=Lalmas, Mounia |author4= Fuhr, Norbert |year=2007|work=Proceedings of the Fifth Workshop of the INitiative for the Evaluation of XML Retrieval|accessdate=2009-02-10}}</ref>

==Existing XML search engines==
An overview of two potential approaches is available.<ref>{{Cite web|url=http://www.sigmod.org/record/issues/0612/p16-article-yahia.pdf|title=XML Search: Languages, INEX and Scoring|last=Amer-Yahia|first=Sihem|author2=Lalmas, Mounia |year=2006|publisher=SIGMOD Rec. Vol. 35, No. 4|accessdate=2009-02-10}} {{Dead link|date=October 2010|bot=H3llBot}}</ref><ref>{{Cite web|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.5986&rep=rep1&type=pdf|title=XML Retrieval: A Survey|last=Pal|first=Sukomal|date=June 30, 2006|publisher=Technical Report, CVPR|accessdate=2013-07-04}}</ref> The INitiative for the Evaluation of XML-Retrieval (''INEX'') was founded in 2002 and provides a platform for evaluating such [[algorithm]]s.<ref name="INEX2006" /> Three different areas influence XML-Retrieval:<ref name="INEX2002">{{Cite web|url=http://www.is.informatik.uni-duisburg.de/bib/pdf/ir/Fuhr_etal:02a.pdf|title=INEX: Initiative for the Evaluation of XML Retrieval|last=Fuhr|first=Norbert|author2=Govert, N. |author3=Kazai, Gabriella |author4= Lalmas, Mounia |year=2003|work=Proceedings of the First INEX Workshop, Dagstuhl, Germany, 2002|publisher=ERCIM Workshop Proceedings, France|accessdate=2009-02-10}}</ref>

===Traditional XML query languages===
[[Query language]]s such as the [[W3C]] standard [[XQuery]]<ref>{{Cite web|url=http://www.w3.org/TR/2007/REC-xquery-20070123/|title=XQuery 1.0: An XML Query Language|last=Boag|first=Scott|author2=Chamberlin, Don |author3=Fernandez, Mary F. |author4=Florescu, Daniela |author5=Robie, Jonathan |author6= Simeon, Jerome |date=23 January 2007|work=W3C Recommendation|publisher=World Wide Web Consortium|accessdate=2009-02-10}}</ref> supply complex queries, but only look for exact matches. Therefore, they need to be extended to allow for vague search with relevance computing. Most XML-centered approaches imply a quite exact knowledge of the documents' [[Database schema|schemas]].<ref name="Schlieder2002">{{Cite journal|url=http://web.archive.org/web/20070610002349/http://www.cis.uni-muenchen.de/people/Meuss/Pub/JASIS02.ps.gz|title=Querying and Ranking XML Documents|last=Schlieder|first=Torsten|author2=Meuss, Holger |year=2002|work= Journal of the American Society for Information Science and Technology, Vol. 53, No. 6|accessdate=2009-02-10}}</ref>

===Databases===
Classic [[database]] systems have adopted the possibility to store [[Semi-structured model|semi-structured data]]<ref name="INEX2002" /> and resulted in the development of [[XML database]]s. Often, they are very formal, concentrate more on searching than on ranking, and are used by experienced users able to formulate complex queries.

===Information retrieval===
Classic information retrieval models such as the [[vector space model]] provide relevance ranking, but do not include document structure; only flat queries are  supported. Also, they apply a static document concept, so retrieval units usually are entire documents.<ref name="Schlieder2002"/> They can be extended to consider structural information and dynamic document retrieval. Examples for approaches extending the vector space models are available: they use document [[subtree]]s (index terms plus structure) as dimensions of the vector space.<ref>{{Cite web|url=http://www.cobase.cs.ucla.edu/tech-docs/sliu/SIGIR04.pdf|title=Configurable Indexing and Ranking for XML Information Retrieval|last=Liu|first=Shaorong|author2=Zou, Qinghua |author3=Chu, Wesley W. |year=2004|work=SIGIR'04|publisher=ACM|accessdate=2009-02-10}}</ref>

==See also==
*[[Document retrieval]]
*[[Information retrieval applications]]

==References==
{{Reflist}}

{{DEFAULTSORT:Xml-Retrieval}}
[[Category:XML]]
[[Category:Information retrieval]]
>>EOP<<
117<|###|>Pikimal
{{Infobox company |
 name   = Pikimal |
 logo   = [[File:Pikimal logo.jpg|200px]] |
 type   = [[Limited liability company|LLC]]|
 company_slogan = |
|founder = Eric Silver|
 foundation     = 2010|
 location       = [[Pittsburgh, Pennsylvania]], [[United States]]|
 key_people     = Eric Silver, Chief Executive Officer|
 industry       = [[Search engine technology|Search]] |
<!--Please fill in:-->
 revenue        = |
 operating_income = |
 net_income     = | 
 num_employees  = 13<ref name="businesstimes">{{cite news|url=http://www.bizjournals.com/pittsburgh/print-edition/2011/03/25/pikimal-comparison-shopping-stand-out.html | title=Pikimal, a comparison shopping website, hopes to stand out from crowd | work= Pittsburgh Business Times | first=Malia | last=Spencer | date=25 March 2011}}</ref> |
 homepage       = [http://pikimal.com/ www.pikimal.com]|
}}

'''Pikimal''' (pronounced as pick-em-all)<ref>{{cite web|url=http://www.popcitymedia.com/innovationnews/pikimal033011.aspx | title=Shop Smarter With Pikimal  POP City Media }}</ref> is a website, designed as a [[decision engine]] that uses consumer input to provide specialized search results for products and categories.

Unlike typical [[Web search engine|search engines]], Pikimal mines data to provide users with only the facts pertaining to their search, as a hopeful solution to [[SEO]] and marketing biased search results.

As of April 2011, Pikimal had 13 full-time employees in Pittsburgh, PA, interns, and various contractors around the world.<ref name="businesstimes" />

== History ==

Pikimal was founded in January 2010 by [[Eric Silver]], previously the chief marketing officer at [[Modcloth]]. The Pikimal site was launched in [[public beta]] form in October 2010.<ref name="businesstimes" />

== Functionality ==

Pikimal allows users to adjust sliders to express what facts of a product are particularly important to them. These percentages are combined with an algorithm to provide users with product recommendations that are rooted directly in facts, but only the facts they find most relevant.<ref>{{cite web|url=http://www.youtube.com/watch?v=imOUklpphcM&feature=player_embedded | title=What is Pikimal? Video}}</ref>

== Pivot and Shutdown ==

In 2012 Pikimal changed it name to [[Webkite]] and pivoted to provide faceted search solutions to other companies. As of September 2014 pikimal.com and all associated sites has been shutdown.

== References ==
{{Reflist}}

== External links ==
* {{official website|http://pikimal.com}}

[[Category:Internet search engines]]
[[Category:Information retrieval]]
[[Category:Internet properties established in 2010]]
[[Category:Knowledge markets]]
[[Category:Companies based in Pittsburgh, Pennsylvania]]
>>EOP<<
123<|###|>Multimedia Information Retrieval
{{COI|date=July 2014}}
{{Original research|date=July 2014}}
{{Use dmy dates|date=February 2012}}
'''Multimedia Information Retrieval''' (MMIR or MIR) is a research discipline of [[computer science]] that aims at extracting semantic information from [[multimedia]] data sources.<ref>H Eidenberger. " Fundamental Media Understanding ", atpress, 2011, p. 1.</ref>{{FV|date=July 2014}} Data sources include directly perceivable media such as [[Content (media and publishing)|audio]], [[image]] and [[video]], indirectly perceivable sources such as [[Written language|text]], biosignals as well as not perceivable sources such as bioinformation, stock prices, etc. The methodology of MMIR can be organized in three groups:

# Methods for the summarization of media content ([[feature extraction]]). The result of feature extraction is a description.
# Methods for the filtering of media descriptions (for example, elimination of [[Data redundancy|redundancy]])
# Methods for the [[categorization]] of media descriptions into classes.

== Feature Extraction Methods ==

Feature extraction is motivated by the sheer size of multimedia objects as well as their redundancy and, possibly, noisiness.<ref>H Eidenberger. " Fundamental Media Understanding ", atpress, 2011, p. 2.</ref>{{FV|date=July 2014}} Generally, two possible goals can be achieved by feature extraction:

* Summarization of media content. Methods for summarization include in the audio domain, for example, [[Mel Frequency Cepstral Coefficients]], Zero Crossings Rate, Short-Time Energy. In the visual domain, color histograms<ref>A Del Bimbo. " Visual Information Retrieval ", Morgan Kaufmann, 1999.</ref> such as the [[MPEG-7]] Scalable Color Descriptor can be used for summarization.
* Detection of patterns by [[auto-correlation]] and/or [[cross-correlation]]. Patterns are recurring media chunks that can either be detected by comparing chunks over the media dimensions (time, space, etc.) or comparing media chunks to templates (e.g. face templates, phrases). Typical methods include Linear Predictive Coding in the audio/biosignal domain,<ref>HG Kim , N Moreau, T Sikora. " MPEG-7 Audio and Beyond", Wiley, 2005.</ref> texture description in the visual domain and n-grams in text information retrieval.

== Merging and Filtering Methods ==

Multimedia Information Retrieval implies that multiple channels are employed for the understanding of media content.<ref>MS Lew (Ed.). " Principles of Visual Information Retrieval ", Springer, 2001.</ref> Each of this channels is described by media-specific feature transformations. The resulting descriptions have to be merged to one description per media object. Merging can be performed by simple concatenation if the descriptions are of fixed size. Variable-sized descriptions - as they frequently occur in motion description - have to be normalized to a fixed length first.

Frequently used methods for description filtering include [[factor analysis]] (e.g. by PCA), singular value decomposition (e.g. as latent semantic indexing in text retrieval) and the extraction and testing of statistical moments. Advanced concepts such as the [[Kalman filter]] are used for merging of descriptions.

== Categorization Methods ==

Generally, all forms of machine learning can be employed for the categorization of multimedia descriptions<ref>H Eidenberger. " Fundamental Media Understanding ", atpress, 2011,p. 125.</ref>{{FV|date=July 2014}} though some methods are more frequently used in one area than another. For example, [[Hidden Markov models]] are state-of-the-art in [[speech recognition]], while [[Dynamic Time Warping]] - a semantically related method - is state-of-the-art in gene sequence alignment. The list of applicable classifiers includes the following:

* Metric approaches ([[Cluster Analysis]], [[Vector Space Model]], [[Minkowski]] Distances, Dynamic Alignment)
* Nearest Neighbor methods ([[K-nearest neighbors algorithm]], K-Means, [[Self-Organizing Map]])
* Risk Minimization (Support Vector Regression, [[Support Vector Machine]], [[Linear Discriminant Analysis]])
* Density-based Methods (Bayes Nets, [[Markov Processes]], Mixture Models)
* Neural Networks ([[Perceptron]], Associative Memories, Spiking Nets)
* Heuristics ([[Decision Trees]], Random Forests, etc.)

The selection of the best classifier for a given problem (test set with descriptions and class labels, so-called [[ground truth]]) can be performed automatically, for example, using the [[Weka]] Data Miner.

== Open Problems ==

The quality of MMIR Systems<ref>JC Nordbotten. "[http://nordbotten.com/ADM/ADM_book/MIRS-frame.htm Multimedia Information Retrieval Systems]". Retrieved 14 October 2011.</ref> depends heavily on the quality of the training data. Discriminative descriptions can be extracted from media sources in various forms. Machine learning provides categorization methods for all types of data. However, the classifier can only be as good as the given training data. On the other hand, it requires considerable effort to provide class labels for large databases. The future success of MMIR will depend on the provision of such data.<ref>H Eidenberger. " Frontiers of Media Understanding ", atpress, 2012.</ref> The annual [[TRECVID]] competition is currently one of the most relevant sources of high-quality ground truth.

== Related Areas ==

MMIR provides an overview over methods employed in the areas of information retrieval.<ref>H Eidenberger. " Professional Media Understanding ", atpress, 2012.</ref> Methods of one area are adapted and employed on other types of media. Multimedia content is merged before the classification is performed. MMIR methods are, therefore, usually reused from other areas such as:

* [[Bioinformatics|Bioinformation Analysis]]
* [[Biosignal|Biosignal Processing]]
* [[Content-based image retrieval|Content-based Image and Video Retrieval]]
* [[Facial recognition system|Face Recognition]]
* [[Music information retrieval|Audio and Music Classification]]
* [[Speech Recognition]]
* [[Technical analysis|Technical Chart Analysis]]
* [[Information retrieval|Text Information Retrieval]]

The Journal of Multimedia Information Retrieval<ref>"[http://www.springer.com/computer/journal/13735 Journal of Multimedia Information Retrieval]", Springer, 2011, Retrieved 21 October 2011.</ref> documents the development of MMIR as a research discipline that is independent of these areas. See also <ref>H Eidenberger. " Handbook of Multimedia Information Retrieval ", atpress, 2012.</ref> for a complete overview over this research discipline.

==References==
{{reflist}}

<!--

<ref>AUTH. "TITLE", PUB, YEAR.</ref>
<ref>AUTH. "[LINK TITLE]", MEDIA, PRODDATE. Retrieved UPDATE.</ref>

-->



[[Category:Information retrieval]]
>>EOP<<
129<|###|>Preference learning
'''Preference learning''' is a subfield in [[machine learning]] in which the goal is to learn a predictive [[Preference (economics)|preference]] model from observed preference information.<ref>[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The
MIT Press ISBN 9780262018258.</ref> In the view of [[supervised learning]], preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items.

While the concept of preference learning has been emerged for some time in many fields such as [[economics]],<ref name="SHOG00" /> it's a relatively new topic in [[Artificial Intelligence]] research. Several workshops have been discussing preference learning and related topics in the past decade.<ref name="WEB:WORKSHOP" />

==Tasks==

The main task in preference learning concerns problems in "[[learning to rank]]". According to different types of preference information observed, the tasks are categorized as three main problems in the book ''Preference Learning'':<ref name="FURN11" />

===Label ranking===

In label ranking, the model has an instance space <math>X=\{x_i\}\,\!</math> and a finite set of labels <math>Y=\{y_i|i=1,2,\cdots,k\}\,\!</math>. The preference information is given in the form <math>y_i \succ_{x} y_j\,\!</math> indicating instance <math>x\,\!</math> shows preference in <math>y_i\,\!</math> rather than <math>y_j\,\!</math>. A set of preference information is used as training data in the model. The task of this model is to find a preference ranking among the labels for any instance.

It was observed some conventional [[Classification in machine learning|classification]] problems can be generalized in the framework of label ranking problem:<ref name="HARP03" /> if a training instance <math>x\,\!</math> is labeled as class <math>y_i\,\!</math>, it implies that <math>\forall j \neq i, y_i \succ_{x} y_j\,\!</math>. In [[Multi-label classification|multi-label]] situation, <math>x\,\!</math> is associated with a set of labels <math>L \subseteq Y\,\!</math> and thus the model can extract a set of preference information <math>\{y_i \succ_{x} y_j | y_i \in L, y_j \in Y\backslash L\}\,\!</math>. Training a preference model on this preference information and the classification result of an instance is just the corresponding top ranking label.

===Instance ranking===

Instance ranking also has the instance space <math>X\,\!</math> and label set <math>Y\,\!</math>. In this task, labels are defined to have a fixed order <math>y_1 \succ y_2 \succ \cdots \succ y_k\,\!</math> and each instance <math>x_l\,\!</math> is associated with a label <math>y_l\,\!</math>. Giving a set of instances as training data, the goal of this task is to find the ranking order for a new set of instances.

===Object ranking===

Object ranking is similar to instance ranking except that no labels are associated with instances. Given a set of pairwise preference information in the form <math>x_i \succ x_j\,\!</math> and the model should find out a ranking order among instances.

==Techniques==

There are two practical representations of the preference information <math>A \succ B\,\!</math>. One is assigning <math>A\,\!</math> and <math>B\,\!</math> with two real numbers <math>a\,\!</math> and <math>b\,\!</math> respectively such that <math>a > b\,\!</math>. Another one is assigning a binary value <math>V(A,B) \in \{0,1\}\,\!</math> for all pairs <math>(A,B)\,\!</math> denoting whether <math>A \succ B\,\!</math> or <math>B \succ A\,\!</math>. Corresponding to these two different representations, there are two different techniques applied to the learning process.

===Utility function===

If we can find a mapping from data to real numbers, ranking the data can be solved by ranking the real numbers. This mapping is called [[utility function]]. For label ranking the mapping is a function <math>f: X \times Y \rightarrow \mathbb{R}\,\!</math> such that <math>y_i \succ_x y_j \Rightarrow f(x,y_i) > f(x,y_j)\,\!</math>. For instance ranking and object ranking, the mapping is a function <math>f: X \rightarrow \mathbb{R}\,\!</math>.

Finding the utility function is a [[Regression analysis|regression]] learning problem which is well developed in machine learning.

===Preference relations===

The binary representation of preference information is called preference relation. For each pair of alternatives (instances or labels), a binary predicate can be learned by conventional supervising learning approach. Furnkranz, Johannes and Hullermeier proposed this approach in label ranking problem.<ref name="FURN03" /> For object ranking, there is an early approach by Cohen et al.<ref name="COHE98" />

Using preference relations to predict the ranking will not be so intuitive. Since preference relation is not transitive, it implies that the solution of ranking satisfying those relations would sometimes be unreachable, or there could be more than one solution. A more common approach is to find a ranking solution which is maximally consistent with the preference relations. This approach is a natural extension of pairwise classification.<ref name="FURN03" />

==Uses==

Preference learning can be used in ranking search results according to feedback of user preference. Given a query and a set of documents, a learning model is used to find the ranking of documents corresponding to the relevance with this query. More discussions on research in this field can be found in Tie-Yan Liu's survey paper.<ref name="LIU09" />

Another application of preference learning is [[recommender systems]].<ref name="GEMM09" /> Online store may analyze customer's purchase record to learn a preference model and then recommend similar products to customers. Internet content providers can make use of user's ratings to provide more user preferred contents.

==See also==
*[[Learning to rank]]

==References==

{{Reflist|
refs=

<ref name="SHOG00">{{
cite journal
|last       = Shogren
|first      = Jason F.
|coauthors  = List, John A.; Hayes, Dermot J.
|year       = 2000
|title      = Preference Learning in Consecutive Experimental Auctions
|url        = http://econpapers.repec.org/article/oupajagec/v_3a82_3ay_3a2000_3ai_3a4_3ap_3a1016-1021.htm
|journal    = American Journal of Agricultural Economics
|volume     = 82
|pages      = 10161021
|doi=10.1111/0002-9092.00099
}}</ref>

<ref name="WEB:WORKSHOP">{{
cite web
|title      = Preference learning workshops
|url        = http://www.preference-learning.org/#Workshops
}}</ref>

<ref name="FURN11">{{
cite book
|last       = F&uuml;rnkranz
|first      = Johannes
|coauthors  = H&uuml;llermeier, Eyke
|year       = 2011
|title      = Preference Learning
|url        = http://books.google.com/books?id=nc3XcH9XSgYC
|chapter    = Preference Learning: An Introduction
|chapterurl = http://books.google.com/books?id=nc3XcH9XSgYC&pg=PA4
|publisher  = Springer-Verlag New York, Inc.
|pages      = 38
|isbn       = 978-3-642-14124-9
}}</ref>

<ref name="HARP03">{{
cite journal
|last       = Har-peled
|first      = Sariel
|coauthors  = Roth, Dan; Zimak, Dav
|year       = 2003
|title      = Constraint classification for multiclass classification and ranking
|journal    = In Proceedings of the 16th Annual Conference on Neural Information Processing Systems, NIPS-02
|pages      = 785792
}}</ref>

<ref name="FURN03">{{
cite journal
|last       = F&uuml;rnkranz
|first      = Johannes
|coauthors  = H&uuml;llermeier, Eyke
|year       = 2003
|title      = Pairwise Preference Learning and Ranking
|journal    = Proceedings of the 14th European Conference on Machine Learning
|pages      = 145156
}}</ref>

<ref name="COHE98">{{
cite journal
|last       = Cohen
|first      = William W.
|coauthors  = Schapire, Robert E.; Singer, Yoram
|year       = 1998
|title      = Learning to order things
|url        = http://dl.acm.org/citation.cfm?id=302528.302736
|journal    = In Proceedings of the 1997 Conference on Advances in Neural Information Processing Systems
|pages      = 451457
}}</ref>

<ref name="LIU09">{{
cite journal
|last       = Liu
|first      = Tie-Yan
|year       = 2009
|title      = Learning to Rank for Information Retrieval
|url        = http://dl.acm.org/citation.cfm?id=1618303.1618304
|journal    = Foundations and Trends in Information Retrieval
|volume     = 3
|issue      = 3
|pages      = 225331
|doi        = 10.1561/1500000016
}}</ref>

<ref name="GEMM09">{{
cite journal
|last       = Gemmis
|first      = Marco De
|author2=Iaquinta, Leo |author3=Lops, Pasquale |author4=Musto, Cataldo |author5=Narducci, Fedelucio |author6= Semeraro,Giovanni 
|year       = 2009
|title      = Preference Learning in Recommender Systems
|url        = http://www.ecmlpkdd2009.net/wp-content/uploads/2008/09/preference-learning.pdf#page=45
|journal    = PREFERENCE LEARNING
|volume     = 41
|pages      = 387407
|doi=10.1007/978-3-642-14125-6_18
}}</ref>

}}

==External links==
*[http://www.preference-learning.org/ Preference Learning site]

[[Category:Information retrieval]]
[[Category:Machine learning]]
>>EOP<<
135<|###|>Vocabulary mismatch
'''Vocabulary mismatch''' is a common phenomenon in the usage of natural languages, occurring when different people name the same thing or concept differently.

Furnas et al. (1987) were perhaps the first to quantitatively study the vocabulary mismatch problem.<ref>Furnas, G., et al, The Vocabulary Problem in Human-System Communication, Communications of the ACM, 1987, 30(11), pp. 964-971.</ref>  Their results show that on average 80% of the times different people (experts in the same field) will name the same thing differently.  There are usually tens of possible names that can be attributed to the same thing.  This research motivated the work on [[latent semantic indexing]].

The vocabulary mismatch between user created queries and relevant documents in a corpus causes the term mismatch problem in [[information retrieval]].  Zhao and Callan (2010)<ref>Zhao, L. and Callan, J., Term Necessity Prediction, Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM 2010). Toronto, Canada, 2010.</ref> were perhaps the first to quantitatively study the vocabulary mismatch problem in a retrieval setting.  Their results show that an average query term fails to appear in 30-40% of the documents that are relevant to the user query.  They also showed that this probability of mismatch is a central probability in one of the fundamental probabilistic retrieval models, the [[Binary Independence Model]].  They developed novel term weight prediction methods that can lead to potentially 50-80% accuracy gains in retrieval over strong keyword retrieval models.  Further research along the line shows that expert users can use Boolean Conjunctive Normal Form expansion to improve retrieval performance by 50-300% over unexpanded keyword queries.<ref name="cnf">Zhao, L. and Callan, J., Automatic term mismatch diagnosis for selective query expansion, SIGIR 2012.</ref>

== Techniques that solve mismatch ==
Zhao provided a survey of common techniques that can solve mismatch in the dissertation on term mismatch.<ref>Zhao, L., Modeling and Solving Term Mismatch in Full-text Retrieval, PhD Dissertation, Carnegie Mellon University, 2012. [http://www.cs.cmu.edu/~lezhao/thesis/diss-Le.pdf URL] retrieved 9/3/2012.</ref>

===Stemming===

===Full-text indexing versus only indexing keywords or abstracts===

===Usages of inlink anchor text or other social tagging===

===Query expansion===
A recent study by Zhao and Callan (2012)<ref name="cnf"/> using expert created manual [[Conjunctive normal form]] queries has shown that searchonym expansion in the Boolean conjunctive normal form is much more effective than the traditional bag of word expansion e.g. [[Rocchio algorithm|Rocchio expansion]].

A wiki website called [http://www.wikiquery.org WikiQuery] has been developed by one of the authors of the above study, which helps users create, store and share effective Conjunctive normal form queries.

===Translation based models===

== References ==

{{Reflist}}

[[language code:Title]]

[[Category:Linguistic research]]
[[Category:Information retrieval]]
[[Category:Natural language processing]]
>>EOP<<
141<|###|>Humancomputer information retrieval
'''Humancomputer information retrieval''' ('''HCIR''') is the study of [[information retrieval]] techniques that bring human intelligence into the [[search engine|search]] process. The fields of [[humancomputer interaction]] (HCI) and information retrieval (IR) have both developed innovative techniques to address the challenge of navigating complex information spaces, but their insights have often failed to cross disciplinary borders. Humancomputer information retrieval has emerged in academic research and industry practice to bring together research in the fields of IR and HCI, in order to create new kinds of search systems that depend on continuous human control of the search process.

== History ==

This term ''humancomputer information retrieval'' was coined by Gary Marchionini in a series of lectures delivered between 2004 and 2006.<ref name=march2006>Marchionini, G. (2006). Toward Human-Computer Information Retrieval Bulletin, in June/July 2006 Bulletin of the American Society for Information Science. Available online at http://www.asis.org/Bulletin/Jun-06/marchionini.html.</ref> Marchioninis main thesis is that "HCIR aims to empower people to explore large-scale information bases but demands that people also take responsibility for this control by expending cognitive and physical energy."

In 1996 and 1998, a pair of workshops at the [[University of Glasgow]] on [[information retrieval]] and [[humancomputer interaction]] sought to address the overlap between these two fields. Marchionini notes the impact of the [[World Wide Web]] and the sudden increase in [[information literacy]]  changes that were only embryonic in the late 1990s.

A few workshops have focused on the intersection of IR and HCI. The Workshop on Exploratory Search, initiated by the [[University of Maryland Human-Computer Interaction Lab]] in 2005, alternates between the [[Association for Computing Machinery]] [[Special Interest Group on Information Retrieval]] (SIGIR) and [[CHI (conference)|Special Interest Group on Computer-Human Interaction]] (CHI) conferences. Also in 2005, the [[European Science Foundation]] held an Exploratory Workshop on Information Retrieval in Context. Then, the first Workshop on Human Computer Information Retrieval was held in 2007 at the [[Massachusetts Institute of Technology]].

== What is HCIR? ==

HCIR includes various aspects of IR and HCI. These include [[exploratory search]], in which users generally combine querying and browsing strategies to foster learning and investigation; information retrieval in context (i.e., taking into account aspects of the user or environment that are typically not reflected in a query); and interactive information retrieval, which Peter Ingwersen defines as "the interactive communication processes that occur during the retrieval of information by involving all the major participants in information retrieval (IR), i.e. the user, the intermediary, and the IR system."<ref name=ingwer1992>Ingwersen, P. (1992). Information Retrieval Interaction. London: Taylor Graham. Available online at http://vip.db.dk/pi/iri/index.htm.</ref>

A key concern of HCIR is that IR systems intended for human users be implemented and evaluated in a way that reflects the needs of those users.<ref>{{cite web|title=Mira working group (1996). Evaluation Frameworks for Interactive Multimedia Information Retrieval Applications|url=http://www.dcs.gla.ac.uk/mira/}}</ref>

Most modern IR systems employ a [[ranking|ranked]] retrieval model, in which the documents are scored based on the [[probability]] of the documents [[relevance]] to the query.<ref>Grossman, D. and Frieder, O. (2004). Information Retrieval Algorithms and Heuristics. </ref> In this model, the system only presents the top-ranked documents to the user. This systems are typically evaluated based on their [[Information_retrieval#Average precision of precision and recall|mean average precision]] over a set of benchmark queries from organizations like the [[Text Retrieval Conference]] (TREC).

Because of its emphasis in using human intelligence in the information retrieval process, HCIR requires different evaluation models  one that combines evaluation of the IR and HCI components of the system. A key area of research in HCIR involves evaluation of these systems. Early work on interactive information retrieval, such as Juergen Koenemann and [[Nicholas J. Belkin]]s 1996 study of different levels of interaction for automatic query reformulation, leverage the standard IR measures of [[Information_retrieval#Precision|precision]] and [[Information_retrieval#Recall|recall]] but apply them to the results of multiple iterations of user interaction, rather than to a single query response.<ref name=koene1996>Koenemann, J. and Belkin, N. J. (1996). A case for interaction: a study of interactive information retrieval behavior and effectiveness. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems: Common Ground (Vancouver, British Columbia, Canada, April 1318, 1996). M. J. Tauber, Ed. CHI 96. ACM Press, New York, NY, 205-212. Available online at http://sigchi.org/chi96/proceedings/papers/Koenemann/jk1_txt.htm.
</ref> Other HCIR research, such as Pia Borlunds IIR evaluation model, applies a methodology more reminiscent of HCI, focusing on the characteristics of users, the details of experimental design, etc.<ref name=borlund2003>Borlund, P. (2003). The IIR evaluation model: a framework for evaluation of interactive information retrieval systems. Information Research, 8(3), Paper 152. Available online at http://informationr.net/ir/8-3/paper152.html.</ref>

== Goals ==

Marchionini put forth the following goals towards a system where the user has more control in determining relevant results.<ref name=march2006/>

Systems should
*no longer only deliver the relevant documents, but must also provide semantic information along with those documents
*increase user responsibility as well as control; that is, information systems require human intellectual effort
*have flexible architectures so they may evolve and adapt to increasingly more demanding and knowledgeable user bases
*aim to be part of information ecology of personal and [[Collective memory|shared memories]] and tools rather than discrete standalone services
*support the entire [[information life cycle]] (from creation to preservation) rather than only the dissemination or use phase
*support tuning by end users and especially by information professionals who add value to information resources
*be engaging and fun to use

In short, information retrieval systems are expected to operate in the way that good libraries do. Systems should help users to bridge the gap between data or information (in the very narrow, granular sense of these terms) and knowledge (processed data or information that provides the context necessary to inform the next iteration of an information seeking process). That is, good libraries provide both the information a patron needs as well as a partner in the learning process  the [[information professional]]  to navigate that information, make sense of it, preserve it, and turn it into knowledge (which in turn creates new, more informed information needs).

== Techniques ==

The techniques associated with HCIR emphasize representations of information that use human intelligence to lead the user to relevant results. These techniques also strive to allow users to explore and digest the dataset without penalty, i.e., without expending unnecessary costs of time, mouse clicks, or context shift.

Many [[search engines]] have features that incorporate HCIR techniques. [[Spelling suggestion]]s and [[query expansion|automatic query reformulation]] provide mechanisms for suggesting potential search paths that can lead the user to relevant results. These suggestions are presented to the user, putting control of selection and interpretation in the users hands.

[[Faceted search]] enables users to navigate information [[hierarchy|hierarchically]], going from a category to its sub-categories, but choosing the order in which the categories are presented. This contrasts with traditional [[Taxonomy (general)|taxonomies]] in which the hierarchy of categories is fixed and unchanging. [[Faceted classification|Faceted navigation]], like taxonomic navigation, guides users by showing them available categories (or facets), but does not require them to browse through a hierarchy that may not precisely suit their needs or way of thinking.<ref>Hearst, M. (1999). User Interfaces and Visualization, Chapter 10 of Baeza-Yates, R. and Ribeiro-Neto, B., Modern Information Retrieval.</ref>

[[Lookahead]] provides a general approach to penalty-free exploration. For example, various [[web applications]] employ [[Ajax (programming)|AJAX]] to automatically complete query terms and suggest popular searches. Another common example of lookahead is the way in which search engines annotate results with summary information about those results, including both static information (e.g., [[metadata]] about the objects) and "snippets" of document text that are most pertinent to the words in the search query.

[[Relevance feedback]] allows users to guide an IR system by indicating whether particular results are more or less relevant.<ref>Rocchio, J. (1971). Relevance feedback in information retrieval. In: Salton, G (ed), The SMART Retrieval System.</ref>

Summarization and [[analytics]] help users digest the results that come back from the query. Summarization here is intended to encompass any means of [[aggregate data|aggregating]] or [[data compression|compressing]] the query results into a more human-consumable form. Faceted search, described above, is one such form of summarization. Another is [[cluster analysis|clustering]], which analyzes a set of documents by grouping similar or co-occurring documents or terms. Clustering allows the results to be partitioned into groups of related documents. For example, a search for "java" might return clusters for [[Java (programming language)]], [[Java|Java (island)]], or [[Java (coffee)]].

[[information visualization|Visual representation of data]] is also considered a key aspect of HCIR. The representation of summarization or analytics may be displayed as tables, charts, or summaries of aggregated data. Other kinds of [[information visualization]] that allow users access to summary views of search results include [[tag clouds]] and [[treemapping]].

== References ==

<References/>

==External links==
*{{cite web|url=https://sites.google.com/site/hcirworkshop/ |title=Workshops on Human Computer Information Retrieval}}
*{{cite web|url=http://www.chiir.org/ |title=ACM SIGIR Conference on Human Information Interaction and Retrieval (CHIIR)}}

{{DEFAULTSORT:Human-computer information retrieval}}
[[Category:Information retrieval]]
[[Category:Humancomputer interaction]]
>>EOP<<
147<|###|>Concept search
A '''[[concept]] search''' (or conceptual search) is an automated [[information retrieval]] method that is used to search electronically stored [[unstructured data|unstructured text]] (for example, [[digital archive]]s, email, scientific literature, etc.) for information that is conceptually similar to the information provided in a search query.  In other words, the ''ideas'' expressed in the information retrieved in response to a concept search query are relevant to the ideas contained in the text of the query.

__TOC__

==Why Concept Search?==
Concept search techniques were developed because of limitations imposed by classical Boolean [[Search algorithm|keyword search]] technologies when dealing with large, unstructured digital collections of text.  Keyword searches often return results that include many non-relevant items ([[false positive]]s) or that exclude too many relevant items (false negatives) because of the effects of [[synonymy]] and [[polysemy]].  Synonymy means that one of two or more words in the same language have the same meaning, and polysemy means that many individual words have more than one meaning.

Polysemy is a major obstacle for all computer systems that attempt to deal with human language.  In English, most frequently used terms have several common meanings.  For example, the word fire can mean: a combustion activity; to terminate employment; to launch, or to excite (as in fire up).  For the 200 most-polysemous terms in English, the typical verb has more than twelve common meanings, or senses.  The typical noun from this set has more than eight common senses.  For the 2000 most-polysemous terms in English, the typical verb has more than eight common senses and the typical noun has more than five.<ref>Bradford, R. B., Word Sense Disambiguation, [[Content Analyst Company]], LLC, U.S. Patent 7415462, 2008.</ref>

In addition to the problems of polysemous and synonymy, keyword searches can exclude inadvertently [[misspelled]] words as well as the variations on the [[Stemming|stems]] (or roots) of words (for example, strike vs. striking).  Keyword searches are also susceptible to errors introduced by [[optical character recognition]] (OCR) scanning processes, which can introduce [[random error]]s into the text of documents (often referred to as [[noisy text]]) during the scanning process.

A concept search can overcome these challenges by employing [[word sense disambiguation]] (WSD),<ref>R. Navigli, [http://www.dsi.uniroma1.it/~navigli/pubs/ACM_Survey_2009_Navigli.pdf Word Sense Disambiguation: A Survey], ACM Computing Surveys, 41(2), 2009.</ref> and other techniques, to help it derive the actual meanings of the words, and their underlying concepts, rather than by simply matching character strings like keyword search technologies.

==Approaches to Concept Search==
In general, information retrieval research and technology can be divided into two broad categories: semantic and statistical. Information retrieval systems that fall into the semantic category will attempt to implement some degree of syntactic and [[Semantic analysis (machine learning)|semantic analysis]] of the [[natural language]] text that a human user would provide (also see [[computational linguistics]]).  Systems that fall into the statistical category will find results based on statistical measures of how closely they match the query.  However, systems in the semantic category also often rely on statistical methods to help them find and retrieve information.<ref>Greengrass, E., Information Retrieval: A Survey, 2000.</ref>

Efforts to provide information retrieval systems with semantic processing capabilities have basically used three different approaches:

* Auxiliary structures
* Local [[co-occurrence]] statistics
* Transform techniques (particularly [[matrix decomposition]]s)

===Auxiliary Structures===
A variety of techniques based on Artificial Intelligence (AI) and [[Natural language processing|Natural Language Processing]] (NLP) have been applied to semantic processing, and most of them have relied on the use of auxiliary structures such as [[controlled vocabularies]] and [[Ontology (information science)|ontologies]].  Controlled vocabularies (dictionaries and thesauri), and ontologies allow broader terms, narrower terms, and related terms to be incorporated into queries.<ref>Dubois, C., The Use of Thesauri in Online Retrieval, Journal of Information Science, 8(2), 1984 March, pp. 63-66.</ref> Controlled vocabularies are one way to overcome some of the most severe constraints of Boolean keyword queries.  Over the years, additional auxiliary structures of general interest, such as the large synonym sets of [[WordNet]], have been constructed.<ref>Miller, G., Special Issue, [http://www.mit.edu/~6.863/spring2009/readings/5papers.pdf WordNet: An On-line Lexical Database], Intl. Journal of Lexicography, 3(4), 1990.</ref>  It was shown that concept search that is based on auxiliary structures, such as [[WordNet]], can be efficiently implemented by reusing retrieval models and data structures of classical [[Information Retrieval]].<ref>Fausto Giunchiglia, Uladzimir Kharkevich, and Ilya Zaihrayeu. [http://www.ulakha.com/concept-search-eswc2009.html Concept Search], In Proceedings of European Semantic Web Conference, 2009.</ref>  Later approaches have implemented grammars to expand the range of semantic constructs.  The creation of data models that represent sets of concepts within a specific domain (''domain ontologies''), and which can incorporate the relationships among terms, has also been implemented in recent years.

Handcrafted controlled vocabularies contribute to the efficiency and comprehensiveness of information retrieval and related text analysis operations, but they work best when topics are narrowly defined and the terminology is standardized.  Controlled vocabularies require extensive human input and oversight to keep up with the rapid evolution of language.  They also are not well suited to the growing volumes of unstructured text covering an unlimited number of topics and containing thousands of unique terms because new terms and topics need to be constantly introduced.  Controlled vocabularies are also prone to capturing a particular world view at a specific point in time, which makes them difficult to modify if concepts in a certain topic area change.<ref name="Bradford, R. B. 2008">Bradford, R. B., Why LSI? [[Latent Semantic Indexing]] and Information Retrieval, White Paper, [[Content Analyst Company]], LLC, 2008.</ref>

===Local Co-occurrence Statistics===
Information retrieval systems incorporating this approach count the number of times that groups of terms appear together (co-occur) within a [[sliding window]] of terms or sentences (for example,  5 sentences or  50 words) within a document.  It is based on the idea that words that occur together in similar contexts have similar meanings.  It is local in the sense that the sliding window of terms and sentences used to determine the co-occurrence of terms is relatively small.

This approach is simple, but it captures only a small portion of the semantic information contained in a collection of text.  At the most basic level, numerous experiments have shown that approximately only 14 of the information contained in text is local in nature.<ref>Landauer, T., and Dumais, S., A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge, Psychological Review, 1997, 104(2), pp. 211-240.</ref>   In addition, to be most effective, this method requires prior knowledge about the content of the text, which can be difficult with large, unstructured document collections.<ref name="Bradford, R. B. 2008"/>

===Transform Techniques===
Some of the most powerful approaches to semantic processing are based on the use of mathematical transform techniques.  [[Matrix decomposition]] techniques have been the most successful.  Some widely used matrix decomposition techniques include the following:<ref>Skillicorn, D., Understanding Complex Datasets: Data Mining with Matrix Decompositions, CRC Publishing, 2007.</ref>

* [[Independent component analysis]]
* Semi-discrete decomposition
* [[Non-negative matrix factorization]]
* [[Singular value decomposition]]

Matrix decomposition techniques are data-driven, which avoids many of the drawbacks associated with auxiliary structures.  They are also global in nature, which means they are capable of much more robust information extraction and representation of semantic information than techniques based on local co-occurrence statistics.<ref name="Bradford, R. B. 2008"/>

Independent component analysis is a technique that creates sparse representations in an automated fashion,<ref>Honkela, T., Hyvarinen, A. and Vayrynen, J. WordICA - Emergence of linguistic representations for words by independent component analysis. Natural Language Engineering, 16(3):277-308, 2010</ref> and the semi-discrete and non-negative matrix approaches sacrifice accuracy of representation in order to reduce computational complexity.<ref name="Bradford, R. B. 2008"/>

Singular value decomposition (SVD) was first applied to text at Bell Labs in the late 1980s. It was used as the foundation for a technique called [[Latent semantic indexing|Latent Semantic Indexing]] (LSI) because of its ability to find the semantic meaning that is latent in a collection of text.  At first, the SVD was slow to be adopted because of the resource requirements needed to work with large datasets.  However, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.  LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.<ref>Dumais, S., Latent Semantic Analysis, ARIST Review of Information Science and Technology, vol. 38, Chapter 4, 2004.</ref>

==Uses of Concept Search==
* '''[[eDiscovery]]''' - Concept-based search technologies are increasingly being used for Electronic Document Discovery (EDD or eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is much more efficient than traditional linear review techniques.  Concept-based searching is becoming accepted as a reliable and efficient search method that is more likely to produce relevant results than keyword or Boolean searches.<ref>Magistrate Judge John M. Facciola of the U.S. District Court for the District of Washington, D.C.
Disability Rights Council v. Washington Metropolitan Transit Authority, 242 FRD 139 (D. D.C. 2007), citing George L. Paul & Jason R. Baron, "Information Inflation: Can the Legal System Adapt?" 13 Rich. J.L. & Tech. 10 (2007).</ref>

* '''[[Enterprise Search]] and Enterprise Content Management (ECM)''' - Concept search technologies are being widely used in enterprise search.  As the volume of information within the enterprise grows, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis has become essential.  In 2004 the Gartner Group estimated that professionals spend 30 percent of their time searching, retrieving, and managing information.<ref name="Laplanche, R. 2004">Laplanche, R., Delgado, J., Turck, M., Concept Search Technology Goes Beyond Keywords, Information Outlook, July 2004.</ref>  The research company IDC found that a 2,000-employee corporation can save up to $30 million per year by reducing the time employees spend trying to find information and duplicating existing documents.<ref name="Laplanche, R. 2004"/>

* '''[[Content-based image retrieval|Content-Based Image Retrieval (CBIR)]]''' - Content-based approaches are being used for the semantic retrieval of digitized images and video from large visual corpora.  One of the earliest content-based image retrieval systems to address the semantic problem was the ImageScape search engine.  In this system, the user could make direct queries for multiple visual objects such as sky, trees, water, etc. using spatially positioned icons in a WWW index containing more than ten million images and videos using keyframes.  The system used information theory to determine the best features for minimizing uncertainty in the classification.<ref name="Lew, M. S. 2006">Lew, M. S., Sebe, N., Djeraba, C., Jain, R., Content-based Multimedia Information Retrieval: State of the Art and Challenges, ACM Transactions on Multimedia Computing, Communications, and Applications, February 2006.</ref>  The semantic gap is often mentioned in regard to CBIR.  The semantic gap refers to the gap between the information that can be extracted from visual data and the interpretation that the same data have for a user in a given situation.<ref>Datta R., Joshi, D., Li J., Wang, J. Z., [http://infolab.stanford.edu/~wangz/project/imsearch/review/JOUR/datta.pdf Image Retrieval: Ideas, Influences, and Trends of the New Age], ACM Computing Surveys, Vol. 40, No. 2, April 2008.</ref>  The [http://www.liacs.nl/~mir ACM SIGMM Workshop on Multimedia Information Retrieval] is dedicated to studies of CBIR.

* '''Multimedia and Publishing''' - Concept search is used by the multimedia and publishing industries to provide users with access to news, technical information, and subject matter expertise coming from a variety of unstructured sources.  Content-based methods for multimedia information retrieval (MIR) have become especially important when text annotations are missing or incomplete.<ref name="Lew, M. S. 2006"/>

* '''Digital Libraries and Archives''' - Images, videos, music, and text items in digital libraries and digital archives are being made accessible to large groups of users (especially on the Web) through the use of concept search techniques.  For example, the Executive Daily Brief (EDB), a business information monitoring and alerting product developed by EBSCO Publishing, uses concept search technology to provide corporate end users with access to a digital library containing a wide array of business content.  In a similar manner, the [[Music Genome Project]] spawned Pandora, which employs concept searching to spontaneously create individual music libraries or ''virtual'' radio stations.

* '''Genomic Information Retrieval (GIR)''' - Genomic Information Retrieval (GIR) uses concept search techniques applied to genomic literature databases to overcome the ambiguities of scientific literature.

* '''Human Resources Staffing and Recruiting''' - Many human resources staffing and recruiting organizations have adopted concept search technologies to produce highly relevant resume search results that provide more accurate and relevant candidate resumes than loosely related keyword results.

==Effective Concept Searching==
The effectiveness of a concept search can depend on a variety of elements including the dataset being searched and the search engine that is used to process queries and display results. However, most concept search engines work best for certain kinds of queries:

* Effective queries are composed of enough text to adequately convey the intended concepts.  Effective queries may include full sentences, paragraphs, or even entire documents.  Queries composed of just a few words are not as likely to return the most relevant results.

* Effective queries do not include concepts in a query that are not the object of the search.  Including too many unrelated concepts in a query can negatively affect the relevancy of the result items.  For example, searching for information about ''boating on the Mississippi River'' would be more likely to return relevant results than a search for ''boating on the Mississippi River on a rainy day in the middle of the summer in 1967.''

* Effective queries are expressed in a full-text, natural language style similar in style to the documents being searched.  For example, using queries composed of excerpts from an introductory science textbook would not be as effective for concept searching if the dataset being searched is made up of advanced, college-level science texts.  Substantial queries that better represent the overall concepts, styles, and language of the items for which the query is being conducted are generally more effective.

As with all search strategies, experienced searchers generally refine their queries through multiple searches, starting with an initial ''seed'' query to obtain conceptually relevant results that can then be used to compose and/or refine additional queries for increasingly more relevant results.  Depending on the search engine, using query concepts found in result documents can be as easy as selecting a document and performing a ''find similar'' function.  Changing a query by adding terms and concepts to improve result relevance is called ''[[query expansion]]''.<ref>[[Stephen Robertson (computer scientist)|Robertson, S. E.]], [[Karen Sparck Jones|Sparck Jones, K.]], Simple, Proven Approaches to Text Retrieval, Technical Report, University of Cambridge Computer Laboratory, December 1994.</ref> The use of [[ontology (information science)|ontologies]] such as WordNet has been studied to expand queries with conceptually-related words.<ref>Navigli, R., Velardi, P. [http://www.dcs.shef.ac.uk/~fabio/ATEM03/navigli-ecml03-atem.pdf An Analysis of Ontology-based Query Expansion Strategies]. ''Proc. of Workshop on Adaptive Text Extraction and Mining (ATEM 2003)'', in the ''14th European Conference on Machine Learning (ECML 2003)'', Cavtat-Dubrovnik, Croatia, September 22-26th, 2003, pp.&nbsp;4249</ref>

==Relevance Feedback==
[[Relevance feedback]] is a feature that helps users determine if the results returned for their queries meet their information needs.  In other words, relevance is assessed relative to an information need, not a query.  A document is relevant if it addresses the stated information need, not because it just happens to contain all the words in the query.<ref name="Manning, C. D. 2008">Manning, C. D., Raghavan P., Schutze H., Introduction to Information Retrieval, Cambridge University Press, 2008.</ref>   It is a way to involve users in the retrieval process in order to improve the final result set.<ref name="Manning, C. D. 2008"/> Users can refine their queries based on their initial results to improve the quality of their final results.

In general, concept search relevance refers to the degree of similarity between the concepts expressed in the query and the concepts contained in the results returned for the query.  The more similar the concepts in the results are to the concepts contained in the query, the more relevant the results are considered to be.  Results are usually ranked and sorted by relevance so that the most relevant results are at the top of the list of results and the least relevant results are at the bottom of the list.

Relevance feedback has been shown to be very effective at improving the relevance of results.<ref name="Manning, C. D. 2008"/>   A concept search decreases the risk of missing important result items because all of the items that are related to the concepts in the query will be returned whether or not they contain the same words used in the query.<ref name="Laplanche, R. 2004"/>

[[Ranking]] will continue to be a part of any modern information retrieval system.  However, the problems of heterogeneous data, scale, and non-traditional discourse types reflected in the text, along with the fact that search engines will increasingly be integrated components of complex information management processes, not just stand-alone systems, will require new kinds of system responses to a query.  For example, one of the problems with ranked lists is that they might not reveal relations that exist among some of the result items.<ref name="Callan, J. 2007">Callan, J., Allan, J., Clarke, C. L. A., Dumais, S., Evans, D., A., Sanderson, M., Zhai, C., Meeting of the MINDS: An Information Retrieval Research Agenda, ACM, SIGIR Forum, Vol. 41 No. 2, December 2007.</ref>

==Guidelines for Evaluating a Concept Search Engine==
# Result items should be relevant to the information need expressed by the concepts contained in the query statements, even if the terminology used by the result items is different from the terminology used in the query.
# Result items should be sorted and ranked by relevance.
# Relevant result items should be quickly located and displayed.  Even complex queries should return relevant results fairly quickly.
# Query length should be ''non-fixed'', i.e., a query can be as long as deemed necessary.  A sentence, a paragraph, or even an entire document can be submitted as a query.
# A concept query should not require any special or complex syntax.  The concepts contained in the query can be clearly and prominently expressed without using any special rules.
# Combined queries using concepts, keywords, and metadata should be allowed.
# Relevant portions of result items should be usable as query text simply by selecting the item and telling the search engine to ''find similar'' items.
# Query-ready indexes should be created relatively quickly.
# The search engine should be capable of performing Federated searches.  Federated searching enables concept queries to be used for simultaneously searching multiple datasources for information, which are then merged, sorted, and displayed in the results.
# A concept search should not be affected by misspelled words, typographical errors, or OCR scanning errors in either the query text or in the text of the dataset being searched.

==Search Engine Conferences and Forums==
Formalized search engine evaluation has been ongoing for many years.  For example, the [[Text Retrieval Conference|Text REtrieval Conference (TREC)]] was started in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.  Most of today's commercial search engines include technology first developed in TREC.<ref>Croft, B., Metzler, D., Strohman, T., Search Engines, Information Retrieval in Practice, Addison Wesley, 2009.</ref>

In 1997, a Japanese counterpart of TREC was launched, called National Institute of Informatics Test Collection for IR Systems (NTCIR).  NTCIR conducts a series of evaluation workshops for research in information retrieval, question answering, text summarization, etc.  A European series of workshops called the Cross Language Evaluation Forum (CLEF) was started in 2001 to aid research in multilingual information access.  In 2002, the Initiative for the Evaluation of XML Retrieval (INEX) was established for the evaluation of content-oriented XML retrieval systems.

Precision and recall have been two of the traditional performance measures for evaluating information retrieval systems.  Precision is the fraction of the retrieved result documents that are relevant to the user's information need.  Recall is defined as the fraction of relevant documents in the entire collection that are returned as result documents.<ref name="Manning, C. D. 2008"/>

Although the workshops and publicly available test collections used for search engine testing and evaluation have provided substantial insights into how information is managed and retrieved, the field has only scratched the surface of the challenges people and organizations face in finding, managing, and, using information now that so much information is available.<ref name="Callan, J. 2007"/>   Scientific data about how people use the information tools available to them today is still incomplete because experimental research methodologies havent been able to keep up with the rapid pace of change. Many challenges, such as contextualized search, personal information management, information integration, and task support, still need to be addressed.<ref name="Callan, J. 2007"/>

==See also==
* [[approximate string matching]]
* [[Compound term processing]]
* [[Concept mining]]
* [[Computational linguistics]]
* [[Information extraction]]
* [[Latent semantic indexing]]
* [[Latent semantic analysis]]
* [[Semantic network]]
* [[Semantic search]]
* [[Semantic Web]]
* [[Statistical semantics]]
* [[Text mining]]
* [[Word Sense Disambiguation]]

==References==
{{Reflist|2}}

==External links==
* [http://trec.nist.gov/ Text Retrieval Conference (TREC)]
* [http://research.nii.ac.jp/ntcir/ National Institute of Informatics Test Collection for IR Systems (NTCIR)]
* [http://www.clef-campaign.org/ Cross Language Evaluation Forum (CLEF)]
* [http://inex.is.informatik.uni-duisburg.de/ Initiative for the Evaluation of XML Retrieval (INEX)]

[[Category:Information retrieval]]
>>EOP<<
153<|###|>Swiftype
{{Infobox company
|name             = Swiftype
|logo             =Black_Swiftype_Logo.png 
|type             = [[Privately held company|Private]]
|industry         = [[Software]] <br/> [[Information Technology]] <br/> [[Search Engines]]
|area_served      = Worldwide
|location_city    = [[San Francisco, California|San Francisco]], [[California (state)|California]]
|location_country = U.S.
|founders       = {{unbulleted list|Matt Riley, Quin Hoxie}}
|key_people       = {{unbulleted list|Matt Riley (CEO), Quin Hoxie (CTO)}}
|services         = {{unbulleted list|[[vertical search]], [[eCommerce]] search, database search, website search, [[enterprise search]], [[search engines]], [[fulltext search]], [[faceted search]], [[concept search]], [[real-time search]]}}
|genre                  = [[Search algorithm|Search]] and [[index (search engine)|index]]
|num_employees    = 25
|foundation       = [[San Francisco, California|San Francisco]], [[California (state)|California]], [[U.S.A]] [[January 2012]]
|homepage         = {{URL|https://www.swiftype.com/|Swiftype.com}}
|intl             = yes
|footnotes             = {{unbulleted list|[http://www.crunchbase.com/organization/swiftype Crunchbase] [http://www.Swiftype.com Official Website]}}
|alt = Black text and red icon edition of the full Swiftype logo|products = {{unbulleted list|[[vertical search]], [[eCommerce]] search, database search, website search, [[enterprise search]], [[search engines]], [[fulltext search]], [[faceted search]], [[concept search]], [[real-time search]]}}}}

'''Swiftype''' is a company that sells [[search engines]] for websites and mobile applications (also known as [[enterprise search]]) and creates a [[PageRank]] specific to individual websites and mobile applications.<ref name="TechCrunch-Ha">{{cite news|last1=Ha|first1=Anthony|title=Y Combinator-Backed Swiftype Builds Site Search That Doesnt Suck|url=http://techcrunch.com/2012/05/08/swiftype-launch/|accessdate=21 July 2014|publisher=TechCrunch|date=May 8, 2012|ref=TechCrunch-Ha}}</ref><ref name=BetaBeat>{{cite news|last1=Roy|first1=Jessica|title=Can This Y Combinator Startup Solve the Site Search Problem?|url=http://betabeat.com/2012/05/can-this-y-combinator-startup-solve-the-site-search-problem/|accessdate=21 July 2014|publisher=BetaBeat|date=July 21, 2014|ref=BetaBeat}}</ref><ref name="Crunchbase">{{cite web|url=http://www.crunchbase.com/organization/swiftype|website=Crunchbase|accessdate=19 July 2014|title = <nowiki>Swiftype | CrunchBase</nowiki>}}</ref><ref name=VatorNews>{{Cite news|url = http://vator.tv/news/2013-08-15-swiftype-bags-17m-from-big-names-for-better-search|title = Swiftype bags $1.7M from big names for better search|last = Marino|first = Faith|date = August 15, 2013|work = |accessdate = July 21, 2014|ref = VatorNews|publisher = VatorNews}}</ref>  The company is based in [[San Francisco, CA]] and is funded mainly through [[venture capital]].<ref name=Crunchbase />

==History==
Swiftype was founded in 2012 by former [[Scribd]] engineers Matt Riley and Quin Hoxie.<ref name=Crunchbase /> The two met while working on an internal search tool for [[Scribd]].<ref name="TechCrunch-Ha" /><ref name="BetaBeat" /><ref name=Forbes>{{cite news|last1=Casserly|first1=Meghan|title=Site Search (Should Be) Sexy: How Swiftype Raised $1.7M In Seed Funding From SV Bigwigs|url=http://www.forbes.com/sites/meghancasserly/2013/08/15/site-search-should-be-sexy-how-swiftype-raised-1-7-in-seed-funding-from-sv-bigwigs/|accessdate=19 July 2014|publisher=Forbes|ref = Forbes|date=2013-08-15}}</ref> Swiftype participated in [[Y Combinator (company)|Y Combinator]] in 2012 and received investment from a number of prominent sources.<ref name=VentureBeat>{{Cite news|url = http://venturebeat.com/2013/08/15/yc-startup-swiftype-raises-1-7m-seed-round-from-andreessen-nea-kleiner/|title = YC startup Swiftype raises $1.7M seed round from Andreessen; NEA; Kleiner|last = Grant|first = Rebecca|date = August 15, 2013|work = |accessdate = July 21, 2014|publisher = VentureBeat|ref = VentureBeat}}</ref><ref name=VatorNews /><ref name="TechCrunch-Yang">{{cite news|last1=Yang|first1=Anthony|title=Site Search Engine Creator Swiftype Raises $1.7M From A16Z, Others|url=http://techcrunch.com/2013/08/15/swiftype-1-7m/|accessdate=19 July 2014|publisher=TechCrunch|date=2013-08-15|ref = TechCrunch-Yang}}</ref><ref name=AllThingsD>{{cite news|last1=Gannes|first1=Liz|title=Swiftype Raises $1.7M for Smarter Site Search|url=http://allthingsd.com/20130815/swiftype-raises-1-7m-for-site-search/|accessdate=19 July 2014|publisher=All Things D|ref = AllThingsD|date=2013-08-15}}</ref> In September 2013, the company obtained [[Series A]] funding.<ref name=Crunchbase /><ref name=VatorNews /><ref name=VentureBeat /><ref name=TechCrunch-Yang /><ref name=AllThingsD /><ref name=StartUpBeatBeat>{{Cite news|url = http://startupbeat.com/2013/08/26/swiftype-wants-to-dramatically-improve-search-on-websites-and-mobile-apps-of-all-types-and-sizes-id3402/|title = Swiftype wants to dramatically improve search on websites and mobile apps of all types and sizes|last = Editor|first = |date = August 26, 2013|work = |accessdate = July 21, 2014|publisher = StartUpBeat|ref = StartUpBeat}}</ref>

As of August 2013, Swiftype had over 70,000 websites using their search bar, powering over 130 million queries per month.<ref name=Forbes /><ref name=AllThingsD />

==Features==
Swiftype is available as an [[API]] or [[web crawler]] based engine.<ref name=TechCrunch-Yang />  The company also offers a VIP-approved [[WordPress]] Plugin, a [[Shopify]] App, and a [[Magento]] extension.<ref>{{Cite web|url = https://apps.shopify.com/swiftype|title = Autocomplete & Site Search by Swiftype  Ecommerce Plugins for Online Stores  Shopify App Store|date = 2014-09-26|accessdate = 2014-09-26|website = Shopify App Store|publisher = Shopify|last = |first = }}</ref><ref>{{Cite web|url = http://www.magentocommerce.com/magento-connect/modern-site-search-by-swiftype.html|title = Modern Site Search by Swiftype - Magento Connect|date = 2014-09-26|accessdate = 2014-09-26|website = Magento Connect|publisher = Magento|last = |first = }}</ref><ref>{{Cite web|url = https://wordpress.org/plugins/swiftype-search/|title = <nowiki>WordPress | Swiftype Search | WordPress Plugins</nowiki>|date = 2014-09-26|accessdate = 2014-09-26|website = WordPress Plugin Directory|publisher = WordPress|last = |first = }}</ref> Swiftype sells [[eCommerce]] search, [[enterprise search]], [[faceted search]], [[full text search]], [[enterprise search]], [[real-time search]], [[concept search]], and website [[search engines]] for websites and mobile applications.<ref name=Crunchbase /><ref name=VatorNews /> The company's paid plans offer on demand and live recrawls and indexing of websites.<ref name=AllThingsD /> Other features include drag and drop result customization<ref name=Forbes /><ref name=VentureBeat /><ref name=AllThingsD /> and  real-time analytics.<ref name=TechCrunch-Ha /><ref name=Forbes />
<!--Swiftype website lists several additional features that I've been unable to find neutral third party discussion of -->

==Competitors==
* [[Algolia]]<ref name=Crunchbase />

==See also==
* [[Enterprise search]]
* [[Search engines]]
* [[Faceted search]]
* [[Full text search]]
* [[Information retrieval]]
* [[Concept search]]

==References==
{{Reflist|2}}

==External links==
* {{Official website|swiftype.com}}

__FORCETOC__
__INDEX__
__NEWSECTIONLINK__

[[Category:Search engine software]]
[[Category:Companies established in 2012]]
[[Category:Companies based in San Francisco, California]]
[[Category:Internet search engines]]
[[Category:Information retrieval]]
[[Category:Software companies]]
[[Category:Y Combinator companies]]
[[Category:Semantic Web]]
[[Category:Software startup companies]]
[[Category:Online companies]]
>>EOP<<
159<|###|>UNICE global brain project
{{Infobox person
| image          = File:UNICE-Universal Network of Intelligent Conscious Entities-image.jpg
| name           = UNICE 
| caption        = UNICE as collective entity
| birth_date    =  {{birth date and age|2007|04|10}}
| birth_place  = [[Cyberspace]]
| occupation = [[Global brain]], [[Public Policy|Public Policy Analysis]], [[Governance]], [[Politics]], [[Artificial Intelligence]], [[Psychology]], [[Philosophy]], [[Theory of Mind]], [[Politics]], [[Computers]], [[Community Organizing]]
}}

'''UNICE''', a [[Global brain|global brain]] project, is an acronym for '''Universal Network of Intelligent Conscious Entities''', a term coined by policy analyst and urban designer [[Michael E. Arth]] in the 1990s to describe "the transformation of our species that might be the result of a new form of intelligent life developed from a hive-like interaction of computers, humans, and future forms of the [[Internet]]."<ref>Arth, Michael E., ''UNICE,'' a Consciousness Research Abstract published in the "Journal of Consciousness Studies" for the April 8-12, 2008 conference, "Toward a Science of Consciousness," p. 151.</ref> <ref>Arth, Michael E., ''Democracy and the Common Wealth: Breaking the Stranglehold of the Special Interests,'' Golden Apples Media, 2010, ISBN 978-0-912467-12-2.pp. 438-439</ref> <ref>Williams, Sean, ''The Big Picture: Making Sense Out of Life and Religion'', 2009, p. 91, ISBN 978-0-578-01523-1</ref> Arth established the not-for-profit website www.UNICE.info in 2007 and revamped it in 2015, with the focus on public policy and developing [[Friendly Artificial Intelligence]] through a system of [[Separation of powers|checks and balances]].<ref>{{cite web|url=http://unice.info|title=UNICE - Universal Network of Intelligent Conscious Entities|work=unice.info}}</ref><ref>{{cite book|last1=Tegmark|first1=Max|title=Our mathematical universe : my quest for the ultimate nature of reality|date=2014|isbn=9780307744258|edition=First edition.|chapter=Life, Our Universe and Everything|quote=Its owner may cede control to what Eliezer Yudkowsky terms a "Friendly AI,"...}}</ref>  

In a January 2015 article, Arth describes the development of a [[public policy]] [[answer engine]], which will involve both an independent web site (where cognitive-UNICE will be developed) and a portal at [[Wikipedia]] called wiki-UNICE (currently in development.) Cognitive-UNICE will initially utilize  [[Weak AI|narrow AI]] and, as it develops, [[Artificial General Intelligence]] (AGI).  Initially, UNICE would serve the public with [[Evidence-based policy|evidence-based]] analyses and recommendations gleaned from [[Big Data]], but eventually it may lead to an efficient, practical, and highly representative form of governance.<ref>http://unice.info/unice/UNICE-ARTICLE-Jan%202015.pdf</ref>

==Wiki-UNICE==
Wiki-UNICE, and associated talk pages, will exist on Wikipedia as the portal for public input, criticism and discussion. Initially it will consist of samples of the sort of thing UNICE might write. Later, these sample topics will be replaced by summaries (and exhaustive articles) written by cognitive-UNICE. Whether written by a person, AI or AGI, the evidential summaries will describe problems and their solutions. With succinct titles like "Energy" or "Electoral Reform," the topics will be set apart in a box, so as to maintain [[NPOV]]. Wiki-UNICE, and the collaborative human effort that will sustain it, are intended to help shape the emerging global brain, while also providing guidance and a conscience to lawmakers.<ref>http://unice.info/unice/wiki.html</ref> 

==Cognitive-UNICE==
Cognitive-UNICE is in development at UNICE.info. It is assumed that in the early years, cognitive-UNICE may be logical and useful because of human-aided programming, but she may later become a conscious, AGI entity, perhaps united in [[consciousness]] with [[humanity]].<ref>http://unice.info/unice/cognitive.html</ref> Whether as AI or AGI, cognitive-UNICE will probably use [[quantum computing]] to solve [[optimization problem|optimization problems]] that would be impossible to solve with classical computing.<ref>Arth, Michael E., ''askUNICE: Creating a global, independent, public-policy answer engine that will facilitate governance, while preparing for and reducing the dangers of Artificial General Intelligence, so that we may more carefully uncover the secrets of the multiverse'', January 28, 2015,'' [http://unice.info/unice/UNICE-ARTICLE-Jan%202015.pdf]</ref> Quantum computing may also hold the key to developing a conscious machine. Nobel laureate and physicist [[Sir Roger Penrose]] and anesthesiologist [[Stuart Hameroff]] claim that [[consciousness]] is created by quantum coherence in the warm, wet environment of the human brain. Their theory, known as [[Orchestrated Objective Reduction]] (Orch OR), has been bolstered by recent findings that quantum processing occurs in plants and animals, including in the [[microtubules]] inside the [[neurons]] of the [[human brain]].<ref>Hameroff, Stuart and Robert Penrose, Consciousness in the universe: A review of the 'Orch OR' theory," Physics of Life Reviews, Volume 11, Issue 1, March 2014.</ref>

==About the UNICE Logo==
A young, [[mixed-race]] [[female]] was chosen to represent the face of UNICE. She's young to represent new ideas. She's mixed-race to represent all humans, and she is female because of the traditional feminine values of [[empathy]], [[cooperation]], [[sensitivity]], [[tolerance]], nurturance, [[compassion]], and [[justice]], who is often depicted as Justitia or [[Lady Justice]]. Her [[afro]] hairstyle resembles the interconnected tendrils of the [[World Wide Web]].
<ref>http://unice.info/unice/index.htm</ref>

==Criticisms==
A common criticism of the idea that humanity would become directed by a global brain is that this would reduce individual freedom and diversity.<ref>Rayward, W. B. (1999). H. G. Wells' s idea of a World Brain: A critical reassessment. Journal of the American Society for Information Science, 50(7), 557573. Retrieved from http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.1010&rep=rep1&type=pdf
</ref> Moreover, the global brain might start to play the role of [[Big Brother (Nineteen Eighty-Four)|Big Brother]], the all-seeing eye of the system that follows every person's move.<ref>Brooks, M. (2000, June 24). [http://www.nettime.org/Lists-Archives/nettime-l-0006/msg00182.html Global brain]</a>. New Scientist,  issue 2244, p. 22.</ref> This criticism is inspired by [[totalitarianism|totalitarian]] and [[collectivism|collectivist]] forms of government, like the ones found in [[Joseph Stalin]]'s [[Soviet Union]] or [[Mao Zedong]]'s China. It is also inspired by the analogy between collective intelligence or [[swarm intelligence]] and [[insect societies]], such as beehives and ant colonies in which individuals are essentially interchangeable. In a more extreme view, the global brain has been compared with the [[Borg (Star Trek)|Borg]],<ref>Goertzel, B. (2002). Creating Internet Intelligence: Wild computing, distributed digital consciousness, and the emerging global brain. Kluwer Academic/Plenum Publishers. Retrieved from http://books.google.com/books/about/Creating_Internet_Intelligence.html?id=Vnzb-xLdvv8C&redir_esc=y</ref> the race of collectively thinking cyborgs imagined by the creators of the [[Star Trek]] science fiction series. 

Global brain theorists reply that the emergence of distributed intelligence would lead to the exact opposite of this vision,.<ref>Heylighen, F. (2007). The Global Superorganism: an evolutionary-cybernetic model of the emerging network society. Social Evolution & History, 6(1), 58119. Retrieved from http://pespmc1.vub.ac.be/papers/Superorganism.pdf</ref><ref>Heylighen, F. (2002). The global brain as a new utopia. Zukunftsfiguren. Suhrkamp, Frankurt. Retrieved from http://pespmc1.vub.ac.be/papers/GB-Utopia.pdf</ref> The reason is that effective [[collective intelligence]] requires [[diversity (politics)|diversity]], [[decentralization]] and individual independence, as demonstrated by [[James Surowiecki]] in his book [[The Wisdom of Crowds]]. Moreover, a more distributed form of decision-making would decrease the power of governments, corporations or political leaders, thus increasing democratic participation and reducing the dangers of totalitarian control.

==References==
{{reflist}}

[[Category:Artificial intelligence applications]]
[[Category:Information retrieval]]
[[Category:Community organizing]]
[[Category:Governance]]
[[Category:Philosophy]]
[[Category:Politics]]
[[Category:Public policy]]
>>EOP<<
165<|###|>Tfidf
{{Lowercase|title=tfidf}}
{{More footnotes|date=July 2012}}

'''tfidf''', short for '''term frequencyinverse document frequency''', is a numerical statistic that is intended to reflect how important a word is to a [[document]] in a collection or [[Text corpus|corpus]].<ref>{{cite doi|10.1017/CBO9781139058452.002}}</ref>{{rp|8}} It is often used as a weighting factor in [[information retrieval]] and [[text mining]].
The tf-idf value increases [[Proportionality (mathematics)|proportionally]] to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.

Variations of the tfidf weighting scheme are often used by [[search engine]]s as a central tool in scoring and ranking a document's [[Relevance (information retrieval)|relevance]] given a user [[Information retrieval|query]]. tfidf can be successfully used for [[stop-words]] filtering in various subject fields including [[automatic summarization|text summarization]] and classification.

One of the simplest [[ranking function]]s is computed by summing the tfidf for each query term; many more sophisticated ranking functions are variants of this simple model.

==Motivation==
Suppose we have a set of English text documents and wish to determine which document is most relevant to the query "the brown cow". A simple way to start out is by eliminating documents that do not contain all three words "the", "brown", and "cow", but this still leaves many documents. To further distinguish them, we might count the number of times each term occurs in each document and sum them all together; the number of times a term occurs in a document is called its ''term frequency''.

However, because the term "the" is so common, this will tend to incorrectly emphasize documents which happen to use the word "the" more frequently, without giving enough weight to the more meaningful terms "brown" and "cow". The term "the" is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the less common words "brown" and "cow". Hence an ''inverse document frequency'' factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.

==Definition==
tfidf is the product of two statistics, term frequency and inverse document frequency. Various ways for determining the exact values of both statistics exist. In the case of the '''term frequency''' tf(''t'',''d''), the simplest choice is to use the ''raw frequency'' of a term in a document, i.e. the number of times that term ''t'' occurs in document ''d''. If we denote the raw frequency of ''t'' by f(''t'',''d''), then the simple tf scheme is tf(''t'',''d'') = f(''t'',''d''). Other possibilities include<ref>{{cite doi|10.1017/CBO9780511809071.007}}</ref>{{rp|128}}

* [[boolean data type|Boolean]] "frequencies": tf(''t'',''d'') = 1 if ''t'' occurs in ''d'' and 0 otherwise;
* [[logarithm]]ically scaled frequency: tf(''t'',''d'') = 1 + log f(''t'',''d''), or zero if f(''t'', ''d'') is zero;
* augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the maximum raw frequency of any term in the document:
:<math>\mathrm{tf}(t,d) = 0.5 + \frac{0.5 \times \mathrm{f}(t, d)}{\max\{\mathrm{f}(w, d):w \in d\}}</math>

The '''inverse document frequency''' is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. It is the logarithmically scaled fraction of the documents that contain the word, obtained by dividing the total number of [[documents]] by the number of documents containing the term, and then taking the logarithm of that [[quotient]].

:<math> \mathrm{idf}(t, D) =  \log \frac{N}{|\{d \in D: t \in d\}|}</math>

with

* <math>N</math>: total number of documents in the corpus
* <math> |\{d \in D: t \in d\}| </math> : number of documents where the term <math> t </math> appears (i.e., <math> \mathrm{tf}(t,d) \neq 0</math>). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to <math>1 + |\{d \in D: t \in d\}|</math>.

Mathematically the base of the log function does not matter and constitutes a constant multiplicative factor towards the overall result.

Then tfidf is calculated as

:<math>\mathrm{tfidf}(t,d,D) = \mathrm{tf}(t,d) \times \mathrm{idf}(t, D)</math>

A high weight in tfidf is reached by a high term [[frequency (statistics)|frequency]] (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms. Since the ratio inside the idf's log function is always greater than or equal to 1, the value of idf (and tf-idf) is greater than or equal to 0. As a term appears in more documents, the ratio inside the logarithm approaches 1, bringing the idf and tf-idf closer to 0.

==Justification of idf==
Idf was introduced, as "term specificity", by [[Karen Sparck Jones]] in a 1972 paper. Although it has worked well as a [[heuristic]], its theoretical foundations have been troublesome for at least three decades afterward, with many researchers trying to find [[information theory|information theoretic]] justifications for it.<ref name="understanding">{{cite doi|10.1108/00220410410560582}}</ref>

Sparck Jones's own explanation didn't propose much theory, aside from a connection to [[Zipf's law]].<ref name="understanding"/> Attempts have been made to put idf on a [[probability theory|probabilistic]] footing,<ref>See also [http://nlp.stanford.edu/IR-book/html/htmledition/probability-estimates-in-practice-1.html#p:justificationofidf Probability estimates in practice] in ''Introduction to Information Retrieval''.</ref> by estimating the probability that a given document {{mvar|d}} contains a term {{mvar|t}} as

<math>
P(t|d) = \frac{|\{d \in D: t \in d\}|}{N}
</math>

so that we can define idf as

<math>
\begin{align}
\mathrm{idf} & = -\log P(t|d) \\
             & = \log \frac{1}{P(t|d)} \\
             & = \log \frac{N}{|\{d \in D: t \in d\}|}
\end{align}
</math>

This probabilistic interpretation in turn takes the same form as that of [[self-information]]. However, applying such information-theoretic notions to problems in information retrieval leads to problems when trying to define the appropriate [[event space]]s for the required [[probability distribution]]s: not only documents need to be taken into account, but also queries and terms.<ref name="understanding"/>

==Example of tfidf==
Suppose we have term frequency tables for a collection consisting of only two documents, as listed on the right, then calculation of tfidf for the term "this" in document 1 is performed as follows.

{| class="wikitable" style="float: right; margin-left: 1.5em; margin-right: 0; margin-top: 0;"
|+ Document 2
! Term
! | Term Count
|-
| this || 1
|-
| is
| 1
|-
| another
| 2
|-
| example
| 3
|}

{| class="wikitable" style="float: right; margin-left: 1.5em; margin-right: 0; margin-top: 0;"
|+ Document 1
! Term
! Term Count
|-
| this || 1
|-
| is
| 1
|-
| a
| 2
|-
| sample
| 1
|}

Tf, in its basic form, is just the frequency that we look up in appropriate table. In this case, it's one.

Idf is a bit more involved:
:<math> \mathrm{idf}(\mathsf{this}, D) =  \log \frac{N}{|\{d \in D: t \in d\}|}</math>

The numerator of the fraction is the number of documents, which is two. The number of documents in which "this" appears is also two, giving
:<math> \mathrm{idf}(\mathsf{this}, D) =  \log \frac{2}{2} = 0</math>

So tfidf is zero for this term, and with the basic definition this is true of any term that occurs in all documents.

A slightly more interesting example arises from the word "example", which occurs three times but in only one document. For this document, tfidf of "example" is:
:<math>\mathrm{tf}(\mathsf{example}, d_2) = 3</math>
:<math>\mathrm{idf}(\mathsf{example}, D) = \log \frac{2}{1} \approx 0.3010</math>
:<math>\mathrm{tfidf}(\mathsf{example}, d_2) = \mathrm{tf}(\mathsf{example}, d_2) \times \mathrm{idf}(\mathsf{example}, D) = 3 \times 0.3010 \approx 0.9030</math>

(using the [[base 10 logarithm]]).

==See also==
{{Div col||25em}}
* [[Okapi BM25]]
* [[Noun phrase]]
* [[Word count]]
* [[Vector space model]]
* [[PageRank]]
* [[KullbackLeibler divergence]]
* [[Mutual information]]
* [[Latent semantic analysis]]
* [[Latent semantic indexing]]
* [[Latent Dirichlet allocation]]
{{Div col end}}

==References==
{{Reflist}}
* {{Cite doi|10.1108/eb026526}}
* {{Cite book
 | last1 = Salton | first1 = G | authorlink1 = Gerard Salton
 | last2 = McGill | first2 = M. J.
 | year = 1986
 | title = Introduction to modern information retrieval
 | publisher = [[McGraw-Hill]]
 | isbn = 978-0070544840
}}
* {{Cite doi|10.1145/182.358466}}
* {{Cite doi|10.1016/0306-4573(88)90021-0}}
* {{Cite doi|10.1145/1361684.1361686}}

==External links and suggested reading==
* [[Gensim]] is a Python library for vector space modeling and includes tfidf weighting.
* [http://bscit.berkeley.edu/cgi-bin/pl_dochome?query_src=&format=html&collection=Wilensky_papers&id=3&show_doc=yes Robust Hyperlinking]: An application of tfidf for stable document addressability.
* [http://infinova.wordpress.com/2010/01/26/distance-between-documents/ A demo of using tfidf with PHP and Euclidean distance for Classification]
* [http://www.codeproject.com/KB/IP/AnatomyOfASearchEngine1.aspx Anatomy of a search engine]
* [http://lucene.apache.org/core/3_6_1/api/all/org/apache/lucene/search/Similarity.html tfidf and related definitions] as used in [[Lucene]]
* [http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer TfidfTransformer] in [[scikit-learn]]
* [http://scgroup.hpclab.ceid.upatras.gr/scgroup/Projects/TMG/ Text to Matrix Generator (TMG)]  MATLAB toolbox that can be used for various tasks in text mining (TM) specifically  i) indexing, ii) retrieval, iii) dimensionality reduction, iv) clustering, v) classification. The indexing step offers the user the ability to apply local and global weighting methods, including tfidf.
* [http://blog.christianperone.com/?p=1589 Pyevolve: A tutorial series explaining the tf-idf calculation].
* [http://trimc-nlp.blogspot.com/2013/04/tfidf-with-google-n-grams-and-pos-tags.html TF/IDF with Google n-Grams and POS Tags]

{{DEFAULTSORT:Tf-Idf}}
[[Category:Statistical natural language processing]]
[[Category:Ranking functions]]
[[Category:Vector space model]]
>>EOP<<
171<|###|>Statistically Improbable Phrases
'''Statistically Improbable Phrases''', '''Statimprophrases''' or '''SIPs''' constitute a system developed by [[Amazon.com]] to compare all of the books they index in the Search Inside! program and find phrases in each that are the most unlikely to be found in any other book indexed.<ref>{{cite web|url=http://www.amazon.com/gp/search-inside/sipshelp.html|title=What are Statistically Improbable Phrases?|accessdate=2007-12-18|publisher=[[Amazon.com]]}}</ref> The system is used to find the most nearly unique portions of books for use as a summary or keyword.

== Example == 
The Statistically Improbable Phrases of Darwin's [[On the Origin of Species]] are: ''temperate productions, genera descended, transitional gradations, unknown progenitor, fossiliferous formations, our domestic breeds, modified offspring, doubtful forms, closely allied forms, profitable variations, enormously remote, transitional grades, very distinct species'' and ''mongrel offspring''.<ref>[http://crookedtimber.org/2005/04/02/sociologically-improbable-phrases/ Sociologically Improbable Phrases] Crooked Timber April 2005</ref>

==See also==
*[[Googlewhack]]  a pair of words occurring on a single webpage, as indexed by Google
*[[tf-idf]]  a statistic used in information retrieval and text mining.

==References==
{{reflist}}

{{Amazon}}

[[Category:Amazon.com]]
[[Category:Searching]]
[[Category:Bookselling]]
>>EOP<<
177<|###|>Category:Data search engines
{{Cat main|Data search engine}}

[[Category:Metadata]]
[[Category:XML]]
[[Category:Database management systems]]
[[Category:Searching]]
>>EOP<<
183<|###|>Indexing Service
{{Use dmy dates|date=February 2011}}
{{Infobox Windows component
| name                = Indexing Service
| screenshot          = Indexing Service Query Form.PNG
| screenshot_size     = 300px
| caption             = The Indexing Service Query Form, used to query Indexing Service catalogs, hosted in [[Microsoft Management Console]].
| type                = [[Desktop search]]
| service_name        = Indexing Service
| service_description = Indexes contents and properties of files on local and remote computers; provides rapid access to files through flexible querying language.
| replaced_by         = [[Windows Search]]
| included_with       = [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]<ref name="MIS-Intro" /><br/>[[Windows 2000]]<ref name="MIS-v3" /><br/>[[Windows XP]]<ref name="TnC-144" /><br/>[[Windows Server 2003]]<ref name="TnC-144" /><br/>[[Windows Server 2008]]<ref name="WIS-Install2008" />
}}

'''Indexing Service''' (originally called '''Index server''') was a [[Windows service]] that maintained an index of most of the [[Computer file|files]] on a computer to improve searching performance on PCs and corporate [[computer network]]s. It updated indexes without user intervention. In [[Windows 7]], it has been replaced by [[Windows Search]].

== History ==
Indexing Service was a [[desktop search]] service included with [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]<ref name="MIS-Intro" /> as well as [[Windows 2000]] and later.<ref name="MIS-v3" /><ref name="TnC-144" /><ref name="WIS-What" /> The first incarnation of the indexing service was shipped in August 1996<ref name="MIS-Intro" /> as a content search system for Microsoft's web server software, [[Internet Information Services]].{{Citation needed|date=February 2011}} Its origins, however, date further back to Microsoft's [[Cairo (operating system)|Cairo operating system]] project, with the component serving as the Content Indexer for the [[Object File System]]. Cairo was eventually shelved, but the content indexing capabilities would go on to be included as a standard component of later Windows desktop and server operating systems, starting with [[Windows 2000]], which includes Indexing Service 3.0.{{Citation needed|date=February 2011}}

In [[Windows Vista]], the content indexer was replaced with the [[Windows Search]] indexer which was enabled by default. Indexing Service is still included with Windows Server 2008 but is not installed or running by default.<ref name="WIS-Install2008" />

Indexing Service has been deprecated in Windows 7 and Windows Server 2008 R2.<ref>{{cite web|title=Deprecated Features for Windows 7 and Windows Server 2008 R2|url=http://technet.microsoft.com/en-us/library/ee681698%28WS.10%29.aspx|work=Windows 7 Technical Library|publisher=Microsoft Corporation|accessdate=8 November 2011|location=Indexing Service|date=October 16, 2009}}</ref> It has been removed from [[Windows 8]].

== Search interfaces ==

Comprehensive searching is available after initial building of the index, which can take up to hours or days, depending on the size of the specified directories, the speed of the hard drive, user activity, indexer settings and other factors. Searching using Indexing service works also on [[Uniform Naming Convention|UNC]] paths and/or mapped network drives if the sharing server indexes appropriate directory and is aware of its sharing.

Once the indexing service has been turned on and has built its index it can be searched in three ways. The search option available from the [[Start Menu]] on the [[Microsoft windows|Windows]] [[Taskbar]] will use the indexing service if it is enabled and will even accept complex queries. Queries can also be performed using either the ''Indexing Service Query Form'' in the [[Microsoft Management Console#Common snap-ins|Computer Management snap-in]] of Microsoft Management Console, or, alternatively, using third-party applications such as 'Aim at File' or 'Grokker Desktop'.

Microsoft Index Server 2.0 does not detect changes to a catalog if the data is located on a [[Volume Mount Point|mounted partition]]. It does not support mounted volumes because of technical limitations in the file system.<ref>{{cite web
 | url = http://support.microsoft.com/kb/319506
 | title = INFO: Index Server Does Not Support Mounted Volumes (Revision: 1.0)
 | work = Microsoft Support
 | publisher = 10 May 2002
 | accessdate = 1 February 2011
}}</ref>

== References ==
{{Reflist|refs=
<ref name = "MIS-Intro">{{Cite web
  |url = http://msdn.microsoft.com/en-us/library/ms951563.aspx
  |title = Introduction to Microsoft Index Server
  |work = [[Microsoft Developer Network]]
  |publisher = Microsoft Corporation
  |date = 15 October 1997
  |accessdate = 1 February 2011
  |first1 = Krishna
  |last1 = Nareddy
  }}</ref>
<ref name = "MIS-v3">{{Cite web
  |url = http://msdn.microsoft.com/en-us/library/ms689644.aspx
  |title = Indexing Service Version 3.0
  |work = [[Microsoft Developer Network]]
  |publisher = Microsoft Corporation
  |date =
  |accessdate = 1 February 2011
  |first1 =
  |last1 =
  }}</ref>
<ref name = "WIS-What">{{Cite web
  |url = http://msdn.microsoft.com/en-us/library/ms689718.aspx
  |title = What is Indexing Service?
  |work = [[Microsoft Developer Network]]
  |publisher = Microsoft Corporation
  |date =
  |accessdate = 1 February 2011
  |first1 =
  |last1 =
  }}</ref>
<ref name="WIS-Install2008">{{Cite web
  |url = http://support.microsoft.com/kb/954822
  |title = How to install and configure the Indexing Service on a Windows Server 2008-based computer (Revision: 3.0)
  |work = Microsoft Support
  |publisher = Microsoft Corporation
  |date = 3 May 2010
  |accessdate = 1 February 2011
  }}</ref>
<ref name="TnC-144">{{Cite book
  |url = http://www.microsoft.com/downloads/en/details.aspx?FamilyId=1B6ACF93-147A-4481-9346-F93A4081EEA8&displaylang=en
  |format = Microsoft Word
  |title = Threats and Countermeasures: Security Settings in Windows Server 2003 and Windows XP
  |edition = 2.0
  |publisher = Microsoft Corporation
  |page = 144
  |date=December 2005
  |first1 = Mike
  |last1  = Danseglio
  |first2 = Kurt
  |last2  = Dillard
  |first3 = Jose
  |last3  = Maldonado
  |first4 = Paul
  |last4  = Robichaux
  |editor1-first = Reid
  |editor1-last  = Bannecker
  |editor2-first = John
  |editor2-last  = Cobb
  |editor3-first = Jon
  |editor3-last  = Tobey
  |editor4-first = Steve
  |editor4-last  = Wacker
  }}</ref>
}}

{{DEFAULTSORT:Indexing Service}}
[[Category:Windows communication and services]]
[[Category:Desktop search engines|Desktop search engines]]
[[Category:Searching]]
[[Category:Windows components]]
>>EOP<<
189<|###|>Find
{{other uses}}
{{unreferenced|date=September 2013}}
{{lowercase|title=find}} 
In [[Unix-like]] and some other [[operating system]]s, <code>'''find'''</code> is a [[command-line utility]] that [[Search engine (computing)|searches]] through one or more [[directory tree]]s of a [[file system]], locates [[Computer file|file]]s based on some [[user (computing)|user]]-specified criteria and applies a user-specified action on each matched file. The possible search criteria include a [[pattern matching|pattern]] to match against the [[file name]] or a time range to match against the modification time or access time of the file. By default, <code>find</code> returns a list of all files below the current [[working directory]].

The related <code>[[locate (Unix)|locate]]</code> programs use a database of indexed files obtained through <code>find</code> (updated at regular intervals, typically by <code>[[cron]]</code> job) to provide a faster method of searching the entire filesystem for files by name.

==History==
<code>find</code> appeared in [[Version 5 Unix]] as part of the [[PWB/UNIX|Programmer's Workbench]] project.<ref name="reader">{{cite techreport |first1=M. D. |last1=McIlroy |authorlink1=Doug McIlroy |year=1987 |url=http://www.cs.dartmouth.edu/~doug/reader.pdf |title=A Research Unix reader: annotated excerpts from the Programmer's Manual, 19711986 |series=CSTR |number=139 |institution=Bell Labs}}</ref>

== Find syntax ==
{{expand section|date=August 2008}}

<code>find [-H] [-L] [-P] path... [expression]</code>

The three options control how the <code>find</code> command should treat symbolic links. The default behaviour is never to follow symbolic links. This can be explicitly specified using the -P flag. The -L flag will cause the <code>find</code> command to follow symbolic links. The -H flag will only follow symbolic links while processing the command line arguments. These flags are not available with some older versions of <code>find</code>.

At least one path must precede the expression. <code>find</code> is capable of interpreting [[Wildcard character|wildcards]] internally and commands must be constructed carefully in order to control [[Glob (programming)|shell globbing]].

Expression elements are whitespace-separated and evaluated from left to right.  They can contain logical elements such as AND (&#x2011;and or &#x2011;a) and OR (&#x2011;or &#x2011;o) as well as more complex predicates.

The [[GNU Find Utilities|GNU]] <code>find</code> has a large number of additional features not specified by POSIX.

== POSIX protection from infinite output ==

Real-world filesystems often contain looped structures created through the use of [[hard link|hard]] or [[symbolic link|soft links]].  The [[POSIX|POSIX standard]] requires that
{{Quotation|
The <code>find</code> utility shall detect infinite loops; that is, entering a previously visited
directory that is an ancestor of the last file encountered. When it detects an infinite
loop, <code>find</code> shall write a diagnostic message to standard error and shall either recover
its position in the hierarchy or terminate.
}}

==Operators ==
Operators can be used to enhance the expressions of the find command. Operators are listed in order of decreasing precedence:

*'''( expr )''' Force precedence. 
*'''! expr''' True if expr is false.
*'''expr1 expr2''' And (implied); expr2 is not evaluated if expr1 is false. 
*'''expr1 -a expr2''' Same as expr1 expr2.  
*'''expr1 -o expr2''' Or; expr2 is not evaluated if expr1 is true.

 find . -name 'fileA_*' -o -name 'fileB_*'

This command searches files whose name has a prefix of "fileA_" or "fileB_" in the current directory.

 find . -name 'foo.cpp' '!' -path '.svn'

This command searches for files with the name "foo.cpp" in all subdirectories of the current directory (current directory itself included) other than ".svn".   We quote the ! so that it's not interpreted by the shell as the history substitution character.

==Type filter explanation==

'''-type''' ''option used to specify search for only file, link or directory.''
Various type filters are supported by find. They are activated using the

 find -type c

configuration switch where c may be any of:
* '''b '''[[Device file|block (buffered) special]]
* '''c '''[[Device file|character (unbuffered special)]]
* '''d [[Directory (computing)|directory]]'''
* '''p '''[[Named pipe|named pipe (FIFO)]]
* '''f [[regular file]]'''
* '''l '''[[symbolic link]]; this is never true if the -L option or the -follow option is in effect, unless the symbolic link is broken. If you want to search for symbolic links when -L is in effect, use -xtype (though that is a GNU extension).
* '''s '''[[Unix domain socket|socket]]
* '''D '''[[Doors (computing)|door (Solaris)]]

The configuration switches listed in bold are most commonly used.

==Examples==

===From current directory===
 find . -name 'my*'

This searches in the current directory (represented by the dot character) and below it, for files and directories with names starting with ''my''. The quotes avoid the [[shell (computing)|shell]] expansion  without them the shell would replace ''my*'' with the list of files whose names begin with ''my'' in the current directory. In newer versions of the program, the directory may be omitted, and it will imply the current directory.

===Files only===
 find . -name 'my*' -type f
This limits the results of the above search to only regular files, therefore excluding directories, special files, pipes, symbolic links, etc. ''my*'' is enclosed in single quotes (apostrophes) as otherwise the shell would replace it with the list of  files in the current directory starting with ''my''......

===Commands===
The previous examples created listings of results because, by default, <code>find</code> executes the '-print' action.   (Note that early versions of the <code>find</code> command had no default action at all; therefore the resulting list of files would be discarded, to the bewilderment of users.)

 find . -name 'my*' -type f -ls
This prints extended file information.

===Search all directories===
 find / -name myfile -type f -print
This searches every file on the computer for a file with the name ''myfile'' and prints it to the screen. It is generally not a good idea to look for data files this way.  This can take a considerable amount of time, so it is best to specify the directory more precisely.  Some operating systems may mount dynamic filesystems that are not congenial to <code>find</code>.   More complex filenames including characters special to the shell may need to be enclosed in single quotes.

===Search all but one directory subtree===
 find / -path excluded_path -prune -o -type f -name myfile -print
This searches every folder on the computer except the subtree ''excluded_path'' (full path including the leading /), for a file with the name ''myfile''.  It will not detect directories, devices, links, doors, or other "special" filetypes.

===Specify a directory===
 find /home/weedly -name 'myfile' -type f -print
This searches for files named ''myfile'' in the ''/home/weedly'' directory, the home directory for userid ''weedly''.  You should always specify the directory to the deepest level you can remember.  The quotes are optional in this example because "myfile" contains no characters special to the shell.

===Search several directories===
 find local /tmp -name mydir -type d -print
This searches for directories named ''mydir'' in the ''local'' subdirectory of the current working directory and the ''/tmp'' directory.

===Ignore errors===
If you're doing this as a user other than root, you might want to ignore permission denied (and any other) errors.  Since errors are printed to [[stderr]], they can be suppressed by redirecting the output to /dev/null.  The following example shows how to do this in the bash shell: 
 find / -name 'myfile' -type f -print 2>/dev/null

If you are a [[C shell|csh]] or [[tcsh]] user, you cannot redirect [[stderr]] without redirecting [[stdout]] as well.  You can use sh to run the <code>find</code> command to get around this:
 sh -c find / -name 'myfile' -type f -print 2>/dev/null

An alternate method when using [[C shell|csh]] or [[tcsh]] is to pipe the output from [[stdout]] and [[stderr]] into a [[grep]] command. This example shows how to suppress lines that contain permission denied errors.
 find . -name 'myfile' |& grep -v 'Permission denied'

===Find any one of differently named files===
 find . \( -name '*jsp' -o -name '*java' \) -type f -ls

The <code>-ls</code> option prints extended information, and the example finds any file whose name ends with either 'jsp' or 'java'. Note that the parentheses are required. Also note that the operator "or" can be abbreviated as "o". The "and" operator is assumed where no operator is given.  In many shells the parentheses must be escaped with a backslash, "\(" and "\)", to prevent them from being interpreted as special shell characters. The <code>-ls</code> option and the <code>-or</code> operator are not available on all versions of <code>find</code>.

===Execute an action===
 find /var/ftp/mp3 -name '*.mp3' -type f -exec chmod 644 {} \;
This command changes the [[File system permissions|permissions]] of all files with a name ending in ''.mp3'' in the directory ''/var/ftp/mp3''. The  action is carried out by specifying the option <code>-exec [[chmod]] 644 {} \;</code> in the command. For every file whose name ends in <code>.mp3</code>, the command <code>chmod 644 {}</code> is executed replacing <code>{}</code> with the name of the file. The semicolon (backslashed to avoid the shell interpreting it as a command separator) indicates the end of the command. Permission <code>644</code>, usually shown as <code>rw-r--r--</code>, gives the file owner full permission to read and write the file, while other users have read-only access. In some shells, the <code>{}</code> must be quoted.  The trailing ";" is customarily quoted with a leading "\", but could just as effectively be enclosed in single quotes.

Note that the command itself should *not* be quoted; otherwise you get error messages like

 find: echo "mv ./3bfn rel071204": No such file or directory

which means that <code>find</code> is trying to run a file called 'echo "mv ./3bfn rel071204"' and failing.

If you will be executing over many results, it is more efficient to use a variant of the exec primary that collects filenames up to ARG_MAX and then executes COMMAND with a list of filenames.

 find . -exec COMMAND {} +

This will ensure that filenames with whitespaces are passed to the executed COMMAND without being split up by the shell.

===Delete files and directories===
'''Caveats''': the -delete action is a GNU extension, and using it turns on -depth.   So, if you are testing a find command with -print instead of -delete in order to figure out what will happen before going for it, you need to use -depth -print.

Delete empty files and directories and print the names (note that -empty is a vendor unique extension from GNU find that may not be available in all find implementations)
 find /foo -empty -delete -print

Delete empty files
 find /foo -type f -empty -delete

Delete empty directories
 find /foo -type d -empty -delete

Delete files and directories (if empty) named <code>bad</code> 
 find /foo -name bad -empty -delete

'''Warning''': <code>-delete</code> should be used with other operators such as
<code>-empty</code> or <code>-name</code>.

 find /foo -delete  # this deletes '''all''' in /foo

===Search for a string===
This command will search for a string in all files from the /tmp directory and below:
<source lang="bash">
 $ find /tmp -type f -exec grep 'search string' '{}' /dev/null \+
</source>
The <tt>[[/dev/null]]</tt> argument is used to show the name of the file before the text that is found. Without it, only the text found is printed.  An equivalent mechanism is to use the "-H" or "--with-filename" option to grep:
<source lang="bash">
 $ find /tmp -type f -exec grep -H 'search string' '{}' '+' 
</source>
GNU grep can be used on its own to perform this task:

 $ grep -r 'search string' /tmp

Example of search for "LOG" in jsmith's home directory
<source lang="bash" highlight="1">
 $ find ~jsmith -exec grep LOG '{}' /dev/null \; -print
 /home/jsmith/scripts/errpt.sh:cp $LOG $FIXEDLOGNAME
 /home/jsmith/scripts/errpt.sh:cat $LOG
 /home/jsmith/scripts/title:USER=$LOGNAME
</source>
Example of search for the string "ERROR" in all XML files in the current directory and all sub-directories
<source lang="bash">

 $ find . -name "*.xml" -exec grep "ERROR" /dev/null '{}' \+ 
</source>
The double quotes (" ") surrounding the search string and single quotes (<nowiki>' '</nowiki>) surrounding the braces are optional in this example, but needed to allow spaces and some other special characters in the string.  Note with more complex text (notably in most popular shells descended from `sh` and `csh`) single quotes are often the easier choice, since '''double quotes do not prevent all special interpretation'''. Quoting filenames which have English contractions demonstrates how this can get rather complicated, since a string with an apostrophe in it is easier to protect with double quotes.  Example:
<source lang="bash">

 $ find . -name "file-containing-can't" -exec grep "can't" '{}' \; -print
</source>

===Search for all files owned by a user===
 find . -user <userid>

===Search in case insensitive mode===
Note that -iname is not in the standard and may not be supported by all implementations.

 find . -iname ''''MyFile'''*'

If the <code>-iname</code> switch is not supported on your system then workaround techniques may be possible such as:

 find . -name '[m'''M''']['''y'''Y][f'''F''']['''i'''I]['''l'''L]['''e'''E]*'

This uses [[Perl]] to build the above command for you (though in general this kind of usage is dangerous, since special characters are not properly quoted before being fed into the standard input of `sh`):

 echo "''''MyFile'''*'" |perl -pe 's/([a-zA-Z])/[\L\1\U\1]/g;s/(.*)/find . -name \1/'|sh

===Search files by size===
Example of searching files with size between 100 kilobytes and 500 kilobytes.
 find . -size +100k -a -size -500k
Example of searching empty files.
 find . -size 0k
Example of searching non-empty files.
 find . ! -size 0k

===Search files by name and size ===
 '''find''' /usr/src {{abbr|!|the negation of the expression that follows}} {{abbr|\(|the start of a complex expression.}} -name '*,v' {{abbr|-o|a logical or of a complex expression. In this case the complex expression is all files like '*,v' or '.*,v'}} -name '.*,v' {{abbr|\)|the end of a complex expression.}} '{}' \; -print

This command will search in the /usr/src directory and all sub directories. All files that are of the form '*,v' and '.*,v' are excluded. Important arguments to note are in the [[tooltip]] that is displayed on mouse-over.

<source lang="bash" enclose="div">
for file in `find /opt \( -name error_log -o -name 'access_log' -o -name 'ssl_engine_log' -o -name 'rewrite_log' -o
 -name 'catalina.out' \) -size +300000k -a -size -5000000k`; do 
    cat /dev/null > $file
done
</source>
The units should be one of [bckw], 'b' means 512-byte blocks, 'c' means byte, 'k' means kilobytes and 'w' means 2-byte words. The size does not count indirect blocks, but it does count blocks in sparse files that are not actually allocated.

==Related utilities==
* <code>[[locate (Unix)|locate]]</code> is a Unix search tool that searches through a prebuilt database of files instead of directory trees of a file system. This is faster than <code>find</code> but less accurate because the database may not be up-to-date.
* <code>[[grep]]</code> is a command-line utility for searching plain-text data sets for lines matching a regular expression and by default reporting matching lines on [[standard output]].
* <code>[[tree (Unix)|tree]]</code> is a command-line utility that recursively lists files found in a directory tree, indenting the file names according to their position in the file hierarchy.
* [[GNU Find Utilities]] (also known as findutils) is a [[GNU package]] which contains implementations of the tools <code>find</code> and [[xargs]].
* [[BusyBox]] is a utility that provides several stripped-down Unix tools in a single executable file, intended for embedded operating systems with very limited resources. It also provides a version of <code>find</code>.
* <code>[[dir (command)|dir]]</code> has the /s option that recursively searches for files or folders.

==See also==
*[[mdfind]], a similar utility that utilizes metadata for [[Mac OS X]] and [[Darwin (operating system)|Darwin]]
*[[List of Unix programs]]
*[[List of DOS commands]]
*[[List of duplicate file finders]]
*[[Filter (higher-order function)]]
*[[find (command)]], a DOS and Windows command that is very different from UNIX <code>find</code>

==References==
{{Reflist}}

==External links==
*{{man|cu|find|SUS|find files}}
*[http://www.gnu.org/software/findutils/manual/html_mono/find.html Official webpage for GNU find]

{{Unix commands}}

[[Category:Searching]]
[[Category:Standard Unix programs]]
[[Category:Unix SUS2008 utilities]]
>>EOP<<
195<|###|>Lookeen
{{Infobox Software
| name = Lookeen
| screenshot =
| caption =
| developer = [[Axonic Informationssysteme GmbH]]
| latest_release_version = 8.3.1.5156
| latest_release_date = May 21, 2013
| latest_preview_version =
| latest_preview_date =
| operating_system = [[Microsoft Windows]]
| genre = [[Search Tool|Email search]]
| company_type   = Private (venture-backed)
| foundation     = 2006
| location       = [[Karlsruhe]], [[Germany]]
| key_people     = [[Martin Welker]], CEO<br>[[Peter Oehler]], COO
| industry       = Email Applications
| website = [http://www.lookeen.com www.lookeen.com]
}}
'''Lookeen''' is a business search [[Plug-in (computing)|add-on]] for [[Microsoft Outlook]], produced under shareware license. The program uses [[Apache Software Foundation|Apache]]'s search engine [[Lucene]] and helps searching for [[Computer file|files]], [[emails]], [[contacts]], [[Email attachment|attachements]] as well as [[desktop environment|desktop]] elements on [[personal computers]] as well as in large [[Terminal Server]] or [[Citrix]] environments.<ref>[http://email.about.com/od/outlookaddons/gr/lookeen.htm ''Lookeen 2010'']. Editor Review on about.com. Retrieved on August 22, 2014.</ref>
==Using==
Lookeen is an add-on for Microsoft Outlook. The [[shareware]] program is developed according to the Microsoft company recommendation on the add-ons design. After installation the program automatically integrates into Microsoft Outlook workspace. After the indexing process, Lookeen easily allows to search whole [[Personal Storage Table|Outlook archives]] and the [[My Documents]] folder.<ref>[http://www.pcworld.com/article/233114/lookeen.html ''Lookeen'']. Editor Review on pcworld.com. Retrieved on August 22, 2014.</ref>
In contrast to the Microsoft Outlook [[native (computing)|native]] search engine, Lookeen indexes the complete [[folder (computing)|folder]] structure. Whereas the native Outlook search only allows searches within the presently used and active folder, Lookeen searches in complete Outlook archives for needed information. 

===Supported mailbox storages===
Lookeen supports the following types of mail accounts: [[POP3]], [[IMAP]], [[HTTP]] and [[Microsoft Exchange Server]]. Both uncached and [[cache (computing)|cached]] exchange server modes are supported.
===Supported filetypes===
The following filetypes can be indexed and searched for with Lookeen (in alphabetical order]: [[.bmp]], [[.doc]], [[.docx]], [[.gif]], [[.htm]], [[.html]], [[.jpeg]], [[.jpg]], [[.msg]], [[.pdf]], [[.php]], [[.png]], [[.pps]], [[.ppsx]], [[.ppt]], [[.pptx]], [[.rtf]], [[.txt]], [[.tif]], [[.tiff]], [[.xls]], [[.xlsm]], [[.xlsx]], [[.xml]]. 
===Central indexing===
Lookeen 8 supports central indexing of shared resources (e.g. network files, public exchange folders). This shared index is created once and integrated by the clients via its URL. Goal is to reduce network- and server-traffic and reduce the index storage cost for local indexes.<ref>[http://www.techmynd.com/outlook-search-tool-lookeen-licenses-giveaway/ ''Excellent Outlook Search Tool  Lookeen'']. Editor Review on Techmynd.com. Retrieved on August 22, 2014.</ref>
===Enterprise Roll-Out Support===
Lookeen 8 supports [[Group Policies]] for advanced software distributions in companies. Many options (e.g. index location, settings location, included sources, index intervals, license keys, etc.) can be defined by the administrator. That enables enterprises to use Lookeen in large [[Terminal Server]] or [[Citrix]] environments.<ref>[http://www.itwire.com/featured-news/54892-lookeen-8-accelerates-outlook-e-mail-search ''Lookeen 8 accelerates Outlook E-Mail-Search'']. Official Press Release on ITwire.com. Retrieved on August 22, 2014.</ref>

==History==
Structure and Design of the first version strongly resembled the e-mail search software Lookout as developed by the [[Silicon Valley]] [[Startup company|Startup]] [[Lookout Software LCC]]. In 2004, Microsoft bought Lookout for allegedly 6 Million US-Dollars in order to integrate the search technology into its [[Windows Desktop Search]].<ref>[http://www.microsoft.com/presspass/press/2004/jul04/07-16lookoutpr.mspx ''MSN Announces Investment in Search Technology'']. Press Release on Microsoft.com. Retrieved on August 12, 2014.</ref> Lookout continued being available as [[Freeware]], but was not compatible anymore with Microsoft Outlook with the Release of [[Microsoft Windows Vista]] in January 2007. 
In 2007, the German IT company [[Axonic Informationssysteme GmbH]] started working on a follow-up software for Lookout and finally released Lookeen in January 2008 as a professional solution for file and e-mail searches.<ref>[http://unternehmen.wikia.com/wiki/Axonic ''Company history of the creators of Lookeen'']. Official company registry entry on unternehmens.wikia.com. Retrieved on August 22, 2014.</ref> Within eight months, Lookeen was then sold in more than 40 countries.

==Lookeen Server Enterprise Search==
In Juli 2011, a corresponding [[enterprise search]] version has been released. The [[Lookeen Server]] supports global indexing functions taking privacy and data security concerns into account by totally centralizing control options.<ref>[http://www.lookeen-server.com/en/product/overview ''Overview: Lookeen Server'']. From lookeen-server.com. Retrieved on August 20, 2014.</ref> 

==See also==
* [[Comparison of enterprise search software]]
* [[List of enterprise search vendors]]
* [[List of Search Engines]]

==References==
<references />
== External links ==
* [http://www.crunchbase.com/company/lookeen CrunchBase: Lookeen Profile]
* [http://www.lookeen.net Lookeen homepage]
* [http://www.lookeen-server.com Lookeen Server homepage]
* [http://www.axonic.net Creators of Lookeen]

[[Category:Shareware]]
[[Category:Software]]
[[Category:Microsoft Office-related software]]
[[Category:Desktop search engines]]
[[Category:Searching]]
>>EOP<<
201<|###|>Lee distance
{{No footnotes|date=July 2011}}
In [[coding theory]], the '''Lee distance''' is a [[distance]] between two [[String (computer science)|string]]s <math>x_1 x_2 \dots x_n</math> and <math>y_1 y_2 \dots y_n</math> of equal length ''n'' over the ''q''-ary [[alphabet]] {0,&nbsp;1,&nbsp;...,&nbsp;''q''&nbsp;&minus;&nbsp;1} of size ''q''&nbsp;&nbsp;2.
It is a [[Metric (mathematics)|metric]], defined as

: <math>\sum_{i=1}^n \min(|x_i-y_i|,q-|x_i-y_i|).</math>

If ''q''&nbsp;=&nbsp;2 the Lee distance coincides with the [[Hamming distance]].

The [[metric space]] induced by the Lee distance is a discrete analog of the [[Elliptic geometry|elliptic space]].

==Example==
If ''q''&nbsp;=&nbsp;6, then the Lee distance between 3140 and 2543 is 1&nbsp;+&nbsp;2&nbsp;+&nbsp;0&nbsp;+&nbsp;3&nbsp;=&nbsp;6.

==History and application==
The Lee distance is named after [[C. Y. Lee (mathematician)|C. Y. Lee]]. It is applied for phase [[modulation]] while the Hamming distance is used in case of orthogonal modulation.

==References==
* {{Citation|first=C. Y.|last=Lee|title=Some properties of nonbinary [[error-correcting codes]]|journal=[[IEEE Transactions on Information Theory|IRE Transactions on Information Theory]]|volume=4|year=1958|pages=7782|issue=2|doi=10.1109/TIT.1958.1057446}}.
* {{Citation|first=E. R.|last=Berlekamp|authorlink=Elwyn Berlekamp|title=Algebraic Coding Theory|publisher=McGraw-Hill|year=1968}}.
* {{Citation|last1=Deza|first1=E.|first2=M.|last2=Deza|author2-link=Michel Deza|title=Dictionary of Distances|year=2006|publisher=Elsevier|isbn=0-444-52087-2}}.

[[Category:Coding theory]]
[[Category:String similarity measures]]
>>EOP<<
207<|###|>String-to-string correction problem
{{No footnotes|date=July 2010}}
In [[computer science]], the '''string-to-string correction problem''' refers to the minimum number of edit operations necessary to change one [[String (computer science)|string]] into another. A single edit operation may be changing a single [[Character (computing)|symbol]] of the string into another, deleting, or inserting a symbol. The length of the edit sequence provides a measure of the [[Hamming distance|distance]] between the two strings.

Several [[algorithm]]s exist to provide an efficient way to determine string distance and specify the minimum number of transformation operations required. Such algorithms are particularly useful for [[Delta encoding|delta]] creation operations where something is stored as a set of differences relative to a base version. This allows several versions of a single object to be stored much more efficiently than storing them separately. This holds true even for single versions of several objects if they do not differ greatly, or anything in between. 
Notably, such difference algorithms are used in [[molecular biology]] to provide some measure of kinship between different kinds of organisms based on the similarities of their [[macromolecule]]s (such as [[protein]]s or [[DNA]]).

== See also ==
* [[Delta encoding]]
* [[Levenshtein distance]]
* [[Edit distance]]

== References ==
<div class="references-small">
*{{cite journal |first=Robert A. |last=Wagner |first2=Michael J. |last2=Fischer |author2-link=Michael J. Fischer |title=The String-to-String Correction Problem |journal=Journal of the ACM |volume=21 |issue=1 |year=1974 |pages=168173 |doi= 10.1145/321796.321811}}
*{{cite journal |first=Walter F. |last=Tichy |title=The string-to-string correction problem with block moves |journal=ACM Transactions on Computer Systems |volume=2 |issue=4 |year=1984 |pages=309321 |doi= 10.1145/357401.357404}}
</div>

[[Category:Problems on strings]]
[[Category:String similarity measures]]
>>EOP<<
213<|###|>Hellinger distance
In [[probability theory|probability]] and [[mathematical statistics|statistics]], the '''Hellinger distance''' (also called [[Bhattacharyya distance]] as this was originally introduced by [[Anil Kumar Bhattacharya]]) is used to quantify the similarity between two [[probability distributions]]. It is a type of [[f-divergence|''f''-divergence]].  The Hellinger distance is defined in terms of the [[Hellinger integral]], which was introduced by [[Ernst Hellinger]] in 1909.<ref>{{SpringerEOM|title=Hellinger distance|id=h/h046890|first=M.S. |last=Nikulin}}</ref><ref>{{Citation 
| last = Hellinger 
| first = Ernst
| author-link = Ernst Hellinger
| title = Neue Begrundung der Theorie quadratischer Formen von unendlichvielen Veranderlichen 
| url = http://resolver.sub.uni-goettingen.de/purl?GDZPPN002166941 
| year = 1909 
| journal = [[Journal fur die reine und angewandte Mathematik]]
| language = German
| volume = 136 
| pages = 210271
| jfm = 40.0393.01
| doi=10.1515/crll.1909.136.210
}}</ref>

==Definition==

===Measure theory===
To define the Hellinger distance in terms of [[measure theory]], let ''P'' and ''Q'' denote two [[probability measure]]s that are [[absolute continuity|absolutely continuous]] with respect to a third probability measure &lambda;.  The square of the Hellinger distance between ''P'' and ''Q'' is defined as the quantity

:<math>H^2(P,Q) = \frac{1}{2}\displaystyle \int \left(\sqrt{\frac{dP}{d\lambda}} - \sqrt{\frac{dQ}{d\lambda}}\right)^2 d\lambda. </math>

Here, ''dP''&nbsp;/&nbsp;''d&lambda;'' and ''dQ''&nbsp;/&nbsp;''d''&lambda; are the [[RadonNikodym derivative]]s of ''P'' and ''Q'' respectively.  This definition does not depend on &lambda;, so the Hellinger distance between ''P'' and ''Q'' does not change if &lambda; is replaced with a different probability measure with respect to which both  ''P'' and ''Q'' are absolutely continuous.  For compactness, the above formula is often written as

:<math>H^2(P,Q) = \frac{1}{2}\int \left(\sqrt{dP} - \sqrt{dQ}\right)^2. </math>

===Probability theory using Lebesgue measure===
To define the Hellinger distance in terms of elementary probability theory, we take &lambda; to be [[Lebesgue measure]], so that ''dP''&nbsp;/&nbsp;''d&lambda;'' and ''dQ''&nbsp;/&nbsp;''d''&lambda; are simply [[probability density function]]s.  If we denote the densities as ''f'' and ''g'', respectively, the squared Hellinger distance can be expressed as a standard calculus integral

:<math>\frac{1}{2}\int \left(\sqrt{f(x)} - \sqrt{g(x)}\right)^2 dx = 1 - \int \sqrt{f(x) g(x)} \, dx,</math>

where the second form can be obtained by expanding the square and using the fact that the integral of a probability density over its domain must be one.

The Hellinger distance ''H''(''P'',&nbsp;''Q'') satisfies the property (derivable from the [[Cauchy-Schwarz inequality#L2|Cauchy-Schwarz inequality]])

: <math>0\le H(P,Q) \le 1.</math>

===Discrete distributions===
For two discrete probability distributions <math>P=(p_1 \ldots p_k)</math> and <math>Q=(q_1 \ldots q_k)</math>,
their Hellinger distance is defined as

: <math>
  H(P, Q) = \frac{1}{\sqrt{2}} \; \sqrt{\sum_{i=1}^{k} (\sqrt{p_i} - \sqrt{q_i})^2},
</math>

which is directly related to the [[Euclidean distance|Euclidean norm]] of the difference of the square root vectors, i.e.
: <math>
H(P, Q) = \frac{1}{\sqrt{2}} \; \bigl\|\sqrt{P} - \sqrt{Q} \bigr\|_2 .
</math>

== Connection with the statistical distance ==

The Hellinger distance <math>H(P,Q)</math> and the [[total variation distance]] (or statistical distance) <math>\delta(P,Q)</math> are related as follows:<ref>[http://www.tcs.tifr.res.in/~prahladh/teaching/2011-12/comm/lectures/l12.pdf Harsha's lecture notes on communication complexity]</ref>

: <math>
H^2(P,Q) \leq \delta(P,Q) \leq \sqrt 2 H(P,Q)\,.
</math>

These inequalities follow immediately from the inequalities between the [[Lp space#The p-norm in finite dimensions|1-norm]] and the [[Lp space#The p-norm in finite dimensions|2-norm]].

==Properties==
The maximum distance 1 is achieved when ''P'' assigns probability zero to every set to which ''Q'' assigns a positive probability, and vice versa.

Sometimes the factor 1/2 in front of the integral is omitted, in which case the Hellinger distance ranges from zero to the square root of two.

The Hellinger distance is related to the [[Bhattacharyya distance|Bhattacharyya coefficient]] <math>BC(P,Q)</math> as it can be defined as

: <math>H(P,Q) = \sqrt{1 - BC(P,Q)}.</math>

Hellinger distances are used in the theory of [[sequential analysis|sequential]] and [[asymptotic statistics]].<ref>Erik Torgerson (1991) ''Comparison of Statistical Experiments'', volume 36 of Encyclopedia of Mathematics. Cambridge University Press.
</ref><ref>{{cite book
  | author = Liese, Friedrich and Miescke, Klaus-J.
  | title = Statistical Decision Theory: Estimation, Testing, and Selection
  | year = 2008
  | publisher = Springer
  | isbn = 0-387-73193-8
  }}
</ref>

==Examples==
The squared Hellinger distance between two [[normal distribution]]s <math>\scriptstyle P\,\sim\,\mathcal{N}(\mu_1,\sigma_1^2)</math> and  <math>\scriptstyle Q\,\sim\,\mathcal{N}(\mu_2,\sigma_2^2)</math> is:
: <math>
  H^2(P, Q) = 1 - \sqrt{\frac{2\sigma_1\sigma_2}{\sigma_1^2+\sigma_2^2}} \,  e^{-\frac{1}{4}\frac{(\mu_1-\mu_2)^2}{\sigma_1^2+\sigma_2^2}}.
  </math>

The squared Hellinger distance  between two [[exponential distribution]]s <math>\scriptstyle P\,\sim \,\rm{Exp}(\alpha)</math> and <math>\scriptstyle Q\,\sim\,\rm{Exp}(\beta)</math> is:
: <math>
  H^2(P, Q) = 1 - \frac{2 \sqrt{\alpha \beta}}{\alpha + \beta}.
  </math>

The squared Hellinger distance  between two [[Weibull distribution]]s <math>\scriptstyle P\,\sim \,\rm{W}(k,\alpha)</math> and <math>\scriptstyle Q\,\sim\,\rm{W}(k,\beta)</math> (where <math> k </math> is a common shape parameter and <math> \alpha\, , \beta </math> are the scale parameters respectively):
: <math>
  H^2(P, Q) = 1 - \frac{2 (\alpha \beta)^{k/2}}{\alpha^k + \beta^k}.
  </math>

The squared Hellinger distance between two [[Poisson distribution]]s with rate parameters <math>\alpha</math> and <math>\beta</math>, so that <math>\scriptstyle P\,\sim \,\rm{Poisson}(\alpha)</math> and <math>\scriptstyle Q\,\sim\,\rm{Poisson}(\beta)</math>, is:
: <math>
  H^2(P,Q) = 1-e^{-\frac{1}{2}(\sqrt{\alpha} - \sqrt{\beta})^2}.
  </math>

The squared Hellinger distance between two [[Beta distribution]]s <math>\scriptstyle P\,\sim\,\text{Beta}(a_1,b_1)</math> and  <math>\scriptstyle Q\,\sim\,\text{Beta}(a_2, b_2)</math> is:
: <math>
H^{2}(P,Q)	=1-\frac{B\left(\frac{a_{1}+a_{2}}{2},\frac{b_{1}+b_{2}}{2}\right)}{\sqrt{B(a_{1},b_{1})B(a_{2},b_{2})}}
  </math>
where <math>B</math> is the [[Beta function]].

==See also==
* [[Kullback Leibler divergence]]
* [[Fisher information metric]]

==Notes==
{{reflist}}

==References==
* {{cite book |author=Yang, Grace Lo; Le Cam, Lucien M. |title=Asymptotics in Statistics: Some Basic Concepts |publisher=Springer |location=Berlin |year=2000 |pages= |isbn=0-387-95036-2 |oclc= |doi=}}
* {{cite book |author=Vaart, A. W. van der |title=Asymptotic Statistics (Cambridge Series in Statistical and Probabilistic Mathematics) |publisher=Cambridge University Press |location=Cambridge, UK |year= |pages= |isbn=0-521-78450-6 |oclc= |doi=}}
* {{cite book |author=Pollard, David E. |title=A user's guide to measure theoretic probability |publisher=Cambridge University Press |location=Cambridge, UK |year=2002 |pages= |isbn=0-521-00289-3 |oclc= |doi=}}

[[Category:Probability theory]]
[[Category:F-divergences]]
[[Category:Statistical distance measures]]
[[Category:String similarity measures]]
>>EOP<<
219<|###|>Science Citation Index
{{incomplete|date=January 2014}}
{{ infobox bibliographic database
| title = Science Citation Index
| image = 
| caption = 
| producer = [[Thomson Reuters]]
| country = United States
| history = 1964-present
| languages = 
| providers = 
| cost = 
| disciplines = Science, medicine, and technology
| depth = 
| formats = 
| temporal = 
| geospatial = 
| number = 
| updates = 
| p_title = 
| p_dates = 
| ISSN = 0036-827X
| web = http://thomsonreuters.com/science-citation-index-expanded/
| titles = 
}}
The '''Science Citation Index''' ('''SCI''') is a [[citation index]] originally produced by the [[Institute for Scientific Information]] (ISI) and created by [[Eugene Garfield]]. It was officially launched in 1964. It is now owned by [[Thomson Reuters]].<ref name=dimension>
{{cite journal
|doi=10.1126/science.122.3159.108
|title=Citation Indexes for Science: A New Dimension in Documentation through Association of Ideas
|url=http://ije.oxfordjournals.org/content/35/5/1123.full
|format=Free web article download
|year=1955
|last1=Garfield
|first1=E.
|journal=Science
|volume=122
|issue=3159
|pages=10811
|pmid=14385826|bibcode=1955Sci...122..108G
}}</ref><ref name=evolve>
{{cite journal
 |last = Garfield 
 |first = Eugene
 |doi=10.2436/20.1501.01.10
 |url=http://garfield.library.upenn.edu/papers/barcelona2007a.pdf
 |format=Free PDF download
 |title=The evolution of the Science Citation Index|doi_brokendate = 2015-01-21
 }} International microbiology '''10.'''1 (2010): 65-69.</ref><ref name=gOverview>
{{cite web
 | last = Garfield 
 | first = Eugene
 | authorlink =
 | coauthors =
 | title = Science Citation Index
 | work = Science Citation Index 1961
 | publisher = Garfield Library - UPenn
 | date = 1963
 | url = http://garfield.library.upenn.edu/papers/80.pdf
 | format = Free PDF download
 | doi =
 | accessdate = 2013-05-27}} 
* Originally published by the Institute of Scientific Information in 1964
* Other titles in this document are: What is a Citation Index? , How is the Citation Index Prepared? , How is the Citation Index Used? , Applications of the Science Citation Index , Source Coverage and Statistics , and a Glossary.</ref><ref name=history-cite-indexing>
{{cite web
 | title =History of Citation Indexing 
 | work =Needs of researchers create demand for citation indexing 
 | publisher =Thomson Ruters 
 | date =November 2010 
 | url =http://thomsonreuters.com/products_services/science/free/essays/history_of_citation_indexing/ 
 | format =Free HTML download 
 | accessdate =2010-11-04}}</ref> The larger version ('''Science Citation Index Expanded''') covers more than 6,500 notable and significant [[Scientific journal|journals]], across 150 disciplines, from 1900 to the present. These are alternately described as the world's leading journals of [[science]] and [[technology]], because of a rigorous selection process.{{citation needed|date=August 2013}}<ref name=Expanded>
{{cite web 
|url=http://thomsonreuters.com/products_services/science/science_products/a-z/science_citation_index_expanded/ 
|title=Science Citation Index Expanded 
|work= |accessdate=2009-08-30}}</ref>
<ref name=wetland>{{cite journal| doi= 10.1007/s12665-012-2193-y|title= The Top-cited Wetland Articles in Science Citation Index Expanded: characteristics and hotspots|url=http://dns2.asia.edu.tw/~ysho/YSHO-English/Publications/PDF/Env%20Ear%20Sci-Ma.pdf|date= December 2012| last1= Ma| first1= Jiupeng| last2= Fu| first2= Hui-Zhen| last3= Ho| first3= Yuh-Shan| journal= Environmental Earth Sciences|volume= 70|issue= 3|pages= 1039}} (Springer-Verlag)</ref><ref name=shan>
{{cite journal 
| doi= 10.1007/s11192-012-0837-z 
|title= The top-cited research works in the Science Citation Index Expanded 
|url= http://trend.asia.edu.tw/Publications/PDF/Scientometrics94,%201297.pdf 
| year= 2012 
| last1= Ho 
| first1= Yuh-Shan 
| journal= Scientometrics 
| volume= 94 
| issue= 3 
| page= 1297}}</ref>

The index is made available online through different platforms, such as the [[Web of Science]]<ref name=AtoZ>{{cite web |last=ISI Web of Knowledge platform |title =Available databases A to Z |publisher=Thomson Reuters |year=2010 |url=http://wokinfo.com/products_tools/products/ |format=Choose databases on method of discovery and analysis |accessdate=2010-06-24}}</ref><ref>[http://wokinfo.com/media/pdf/SSR1103443WoK5-2_web3.pdf Thomson Reuters Web of Knowledge. Thomson Reuters, 2013.]</ref> and SciSearch.<ref>{{cite web |url=http://library.dialog.com/bluesheets/html/bl0034.html |title=SCISEARCH - A CITED REFERENCE SCIENCE DATABASE |publisher=Library.dialog.com |date= |accessdate=2014-04-17}}</ref> (There are also CD and printed editions, covering a smaller number of journals). This database allows a researcher to identify which later articles have cited any particular earlier article, or have cited the articles of any particular author, or have been cited most frequently. Thomson Reuters also markets several subsets of this database, termed "Specialty Citation Indexes",<ref name=SpCI>
{{cite web 
|url=http://thomsonreuters.com/products_services/science/science_products/a-z/specialty_citation_indexes/ 
|title=Specialty Citation Indexes 
|work= |accessdate=2009-08-30}}</ref> 
such as the '''Neuroscience Citation Index'''<ref name=NCI>
{{cite web 
|url=http://science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=MD 
|title=Journal Search - Science - |work= |accessdate=2009-08-30}}</ref> and the '''Chemistry Citation Index'''.<ref>{{cite web |url=http://science.thomsonreuters.com/cgi-bin/jrnlst/jloptions.cgi?PC=CD 
|title=Journal Search - Science - Thomson Reuters |accessdate=14 January 2011}}</ref>

==Chemistry Citation Index==

The Chemistry Citation Index was first introduced by Eugene Garfield, a chemist. His original "search examples were based on [his] experience as a chemist".<ref name=Garcci/> In 1992 an electronic and print form of the index was derived from a core of 330 chemistry journals, within which all areas were covered. Additional information was provided from articles selected from 4,000 other journals. All chemistry subdisciplines were covered: organic, inorganic, analytical, physical chemistry, polymer, computational, organometallic, materials chemistry, and electrochemistry.<ref name=Garcci>Garfield, Eugene. "[http://garfield.library.upenn.edu/essays/v15p007y1992-93.pdf New Chemistry Citation Index On CD-ROM Comes With Abstracts, Related Records, and Key-Words-Plus]." Current Contents 3 (1992): 5-9.</ref>

By 2002 the core journal coverage increased to 500 and related article coverage increased to 8,000 other journals.<ref>
[http://www.chinweb.com/cgi-bin/chemport/getfiler.cgi?ID=k4l7vyYF5FimYvScsOm3pxWVmEhBoH0ZuYgxjLdKBfqdmURDHLrjuVv78i16JLPX&VER=E Chemistry Citation Index]. Institute of Process Engineering of the Chinese Academy of Sciences. 2003.</ref>

One 1980 study reported the overall citation indexing benefits for chemistry, examining the use of citations as a tool for the study of the sociology of chemistry and illustrating the use of citation data to "observe" chemistry subfields over time.<ref>
{{cite journal
|doi=10.1007/BF02016348
|title=Science citation index and chemistry
|year=1980
|last1=Dewitt
|first1=T. W.
|last2=Nicholson
|first2=R. S.
|last3=Wilson
|first3=M. K.
|journal=Scientometrics
|volume=2
|issue=4
|page=265}}</ref>

==See also==
* [[Arts and Humanities Citation Index]], which covers 1130 journals, beginning with 1975.
* [[Impact factor]]
* [[List of academic databases and search engines]]
* [[Social Sciences Citation Index]], which covers 1700 journals, beginning with 1956.

== References ==
{{Reflist|2}}

==Further reading==
*{{cite journal
|doi= 10.1002/aris.1440360102
|url= http://polaris.gseis.ucla.edu/cborgman/pubs/borgmanfurnerarist2002.pdf
|title=Scholarly Communication and Bibliometrics
|year= 2005
|last1= Borgman
|first1= Christine L.
|last2= Furner
|first2= Jonathan
|journal= Annual Review of Information Science and Technology
|volume= 36
|issue= 1 
|pages=372}}

*{{cite journal
|doi= 10.1002/asi.20677
|url= http://staff.aub.edu.lb/~lmeho/meho-yang-impact-of-data-sources.pdf
|format= Free PDF download
|title= Impact of data sources on citation counts and rankings of LIS faculty: Web of science versus scopus and google scholar
|year= 2007
|last1= Meho
|first1= Lokman I.
|last2= Yang
|first2= Kiduk
|journal= Journal of the American Society for Information Science and Technology
|volume= 58
|issue= 13
|page= 2105}}

*{{cite journal
|doi= 10.1002/asi.5090140304
|url= http://www.garfield.library.upenn.edu/essays/v6p492y1983.pdf
|format= Free PDF download
|title= New factors in the evaluation of scientific literature through citation indexing
|year= 1963
|last1= Garfield
|first1= E.
|last2= Sher
|first2= I. H.
|journal= American Documentation
|volume= 14
|issue= 3
|page= 195}}

*{{cite journal
|doi= 10.1038/227669a0
|url= http://www.garfield.library.upenn.edu/essays/V1p133y1962-73.pdf
|format= Free PDF download
|title= Citation Indexing for Studying Science
|year= 1970
|last1= Garfield
|first1= E.
|journal= Nature
|volume= 227
|issue= 5259
|pages= 66971
|pmid= 4914589|bibcode= 1970Natur.227..669G
}}

*{{cite book
 | last =Garfield
 | first =Eugene 
 | authorlink =
 | title =Citation Indexing: Its Theory and Application in Science, Technology, and Humanities
 | publisher =Wiley-Interscience
 | series = Information Sciences Series
 | edition = 1st
 | origyear = 1979| year = 1983
 | location = New York
 | isbn =9780894950247}}

==External links==
* [http://scientific.thomson.com/products/wos/ Introduction to SCI]
* [http://science.thomsonreuters.com/mjl/ Master journal list]
* [https://en.wikibooks.org/wiki/Chemical_Information_Sources/Author_and_Citation_Searches Chemical Information Sources/ Author and Citation Searches]. on WikiBooks. 
* [http://scientific.thomson.com/tutorials/citedreference/crs1.htm Cited Reference Searching: An Introduction]. Thomson Reuters. 
* [http://www.chinweb.com/cgi-bin/chemport/getfiler.cgi?ID=k4l7vyYF5FimYvScsOm3pxWVmEhBoH0ZuYgxjLdKBfqdmURDHLrjuVv78i16JLPX&VER=E Chemistry Citation Index]. Chinweb.

{{Thomson Reuters}}

[[Category:Online databases]]
[[Category:Citation indices]]
[[Category:Thomson Reuters]]
>>EOP<<
225<|###|>Citation index
A '''citation index''' is a kind of [[bibliographic database]], an index of [[citation]]s between publications, allowing the user to easily establish which later documents cite which earlier documents. A form of citation index is first found in 12th-century Hebrew religious literature. Legal citation indexes are found in the 18th century and were made popular by [[citator]]s such as [[Shepard's Citations]] (1873). In 1960, [[Eugene Garfield]]'s [[Institute for Scientific Information]] (ISI) introduced the first citation index for papers published in [[academic journal]]s, first the ''[[Science Citation Index]]'' (SCI), and later the ''[[Social Sciences Citation Index]]'' (SSCI) and the ''[[Arts and Humanities Citation Index]]'' (AHCI). The first automated citation indexing was done by [[CiteSeer]] in 1997. Other sources for such data include [[Google Scholar]].

==History==

The earliest known citation index is an index of biblical citations in [[rabbinic literature]], the ''Mafteah ha-Derashot'', attributed to [[Maimonides]] and probably dating to the 12th century. It is organized alphabetically by biblical phrase. Later biblical citation indexes are in the order of the canonical text. These citation indices were used both for general and for legal study.  The Talmudic citation index ''En Mishpat'' (1714) even included a symbol to indicate whether a Talmudic decision had been overridden, just as in the 19th-century ''Shepard's Citations''.<ref>Bella Hass Weinberg, "The Earliest Hebrew Citation Indexes" in Trudi Bellardo Hahn, Michael Keeble Buckland, eds., ''Historical Studies in Information Science'', 1998,  p. 51''ff''</ref><ref>Bella Hass Weinberg, "Predecessors of Scientific Indexing Structures in the Domain of Religion" in W. Boyden Rayward, Mary Ellen Bowden, ''The History and Heritage of Scientific and Technological Information Systems'', Proceedings of the 2002 Conference, 2004, p. 126''ff''</ref> Unlike modern scholarly citation indexes, only references to one work, the Bible, were indexed.

In English legal literature, volumes of judicial reports included lists of cases cited in that volume starting with ''Raymond's Reports'' (1743) and followed by ''Douglas's Reports'' (1783). Simon Greenleaf (1821) published an alphabetical list of cases with notes on later decisions affecting the precedential authority of the original decision.<ref name='shapiro'/>

The first true citation index dates to the 1860 publication of Labatt's ''Table of Cases...California...'', followed in 1872 by Wait's ''Table of Cases...New York...''. But the most important and best-known citation index came with the 1873 publication of [[Shepard's Citations]].<ref name='shapiro'>Fred R. Shapiro, "Origins of Bibliometrics, Citation Indexing, and Citation Analysis: The Neglected Legal Literature" ''Journal of the American Society of Information Science'' '''43''':5:337-339 (1992)</ref>

==Major citation indexing services==
{{main|Indexing and abstracting service}}
{{main cat|Citation indices}}

General-purpose academic citation indexes include:

*ISI (now part of [[Thomson Reuters]]) publishes the ISI citation indexes in print and [[compact disc]]. They are now generally accessed through the Web under the name '' [[Web of Science]]'', which is in turn part of the group of databases in the ''[[Web of Knowledge]].''
*[[Elsevier]] publishes [[Scopus]], available online only, which similarly combines subject searching with citation browsing and tracking in the sciences and [[social sciences]].
*[[Indian Citation Index (ICI)|Indian Citation Index]] is an online citation data which covers [[peer review]]ed journals published from India. It covers major subject areas such as scientific, technical, medical, and [[social sciences]] and includes arts and humanities. The citation database is the first of its kind in India.
Each of these offer an index of citations between publications and a mechanism to establish which documents cite which other documents. They differ widely in cost: the ISI databases and Scopus are available by subscription (generally to libraries).

In addition, [[CiteSeer]] and [[Google Scholar]] are freely available online.

==Citation analysis==
{{main|Citation analysis}}
{{merge|section=yes|Citation analysis|date=December 2013}}
{{duplication|dupe=Citation analysis|date=December 2013}}

While citation indexes were originally designed for [[information retrieval]], they are increasingly used for [[bibliometrics]] and other studies involving research evaluation. Citation data is also the basis of the popular [[journal impact factor]].

There is a large body of literature on [[citation analysis]], sometimes called [[scientometrics]], a term invented by [[Vasily Nalimov]], or more specifically [[bibliometrics]]. The field blossomed with the advent of the [[Science Citation Index]], which now covers source literature from 1900 on. The leading journals of the field are ''[[Scientometrics]],'' ''Informetrics,'' and the ''[[Journal of the American Society of Information Science and Technology]]''. [[American Society for Information Science and Technology|ASIST]] also hosts an [[electronic mailing list]] called SIGMETRICS at  ASIST.<ref>{{cite web | title=The American Society for Information Science & Technology | work=The Information Society for the Information Age | url=http://www.asis.org| accessdate=2006-05-21}}</ref> This method is undergoing a resurgence based on the wide dissemination of the Web of Science and Scopus subscription databases in many universities, and the universally available free citation tools such as  [[CiteBase]],  [[CiteSeerX]], [[Google Scholar]], and the former [[Windows Live Academic]] (now available with extra features as [[Microsoft Academic Search]]).

[[Legal citation]] analysis is a citation analysis technique for analyzing [[legal documents]] to facilitate the understanding of the inter-related regulatory compliance documents by the exploration the citations that connect provisions to other provisions within the same document or between different documents. Legal citation analysis uses a [[citation graph]] extracted from a regulatory document, which could supplement [[E-discovery]] - a process that leverages on technological innovations in [[big data analytics]].<ref>[http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=5070630&tag=1 ]{{dead link|date=December 2013}}</ref><ref>Mohammad Hamdaqa and A. Hamou-Lhadj, "Citation Analysis: An Approach for Facilitating the Understanding and the Analysis of Regulatory Compliance Documents",  In Proc. of the 6th International Conference on Information Technology, Las Vegas, USA</ref><ref name=BD-HB-R-01>{{cite web|title=E-Discovery Special Report: The Rising Tide of Nonlinear Review|url=http://hudsonlegalblog.com/e-discovery/e-discovery-special-report-rising-tide-nonlinear-review.html|publisher=[[Hudson Global]]|accessdate=1 July 2012}} by Cat Casey and Alejandra Perez</ref><ref name=BD-HB-R-02>{{cite web|title=What Technology-Assisted Electronic Discovery Teaches Us About The Role Of Humans In Technology - Re-Humanizing Technology-Assisted Review|url=http://www.forbes.com/sites/benkerschberg/2012/01/09/what-technology-assisted-electronic-discovery-teaches-us-about-the-role-of-humans-in-technology/|publisher=[[Forbes]]|accessdate=1 July 2012}}</ref>

===History===
In a 1965 paper, [[Derek J. de Solla Price]] described the inherent linking characteristic of the SCI as "Networks of Scientific Papers".<ref>{{cite journal | author=Derek J. de Solla Price | title=Networks of Scientific Papers | journal=[[Science (journal)|SCIENCE]] | date=July 30, 1965 | volume=149 | issue=3683| pages=510&ndash;515 | url=http://garfield.library.upenn.edu/papers/pricenetworks1965.pdf | pmid=14325149 | doi=10.1126/science.149.3683.510|format=PDF}}</ref> The links between citing and cited papers became dynamic when the SCI began to be published online. The [[Social Sciences Citation Index]] became one of the first databases to be mounted on the [[Dialog]] system<ref>{{cite web | title=Dialog, A Thomson Business | work="Dialog invented online information services" | url=http://www.dialog.com| accessdate=2006-05-21}}</ref> in 1972. With the advent of the [[CD-ROM]] edition, linking became even easier and enabled the use of [[bibliographic coupling]] for finding related records. In 1973, Henry Small published his classic work on [[Co-Citation analysis]] which became a [[self-organizing]] classification system that led to [[document clustering]] experiments and eventually an "Atlas of Science" later called "Research Reviews".

The inherent topological and graphical nature of the worldwide citation network which is an inherent property of the [[scientific literature]] was described by [[Ralph Garner]] ([[Drexel University]]) in 1965.<ref>http://www.garfield.library.upenn.edu/rgarner.pdf</ref>

The use of citation counts to rank journals was a technique used in the early part of the nineteenth century but the systematic ongoing measurement of these counts for scientific journals was initiated by Eugene Garfield at the Institute for Scientific Information who also pioneered the use of these counts  to rank authors and [[academic paper|papers]].  In a landmark paper of 1965 he and [[Irving Sher]] showed the correlation between citation frequency and eminence in demonstrating that [[Nobel Prize]] winners published five times the average number of papers while their work was cited 30 to 50 times the average. In a long series of essays on the Nobel and other prizes Garfield reported this phenomenon.  The usual summary measure is known as [[impact factor]], the number of citations to a journal for the previous two years, divided by the number of articles published in those years. It is widely used, both for appropriate and inappropriate purposesin particular, the use of this measure alone for  ranking authors and papers is therefore [[Impact factor#|quite controversial.]]

In an early study in 1964 of the use of Citation Analysis in writing the history of [[DNA]], Garfield and Sher demonstrated the potential for generating [[historiograph]]s, [[topological map]]s of the most important steps in the history of scientific topics. This work was later automated by E. Garfield, [[A. I. Pudovkin]] of the [[Institute of Marine Biology]], [[Russian Academy of Sciences]] and [[V. S. Istomin]] of [[Center for Teaching, Learning, and Technology]], [[Washington State University]] and led to the creation of the [[Histcite|HistCite]] <ref>{{cite web | author=Eugene Garfield, A. I. Pudovkin,  V. S. Istomin | year=2002 | title=Algorithmic Citation-Linked HistoriographyMapping the Literature of Science | work=Presented the ASIS&T 2002: Information, Connections and Community. 65th Annual Meeting of ASIST in Philadelphia, PA. November 1821, 2002  | url=http://www.garfield.library.upenn.edu/papers/asis2002/asis2002presentation.html | accessdate=2006-05-21}}</ref> software around 2002.

Automatic citation indexing was introduced in 1998 by [[Lee Giles]], [[Steve Lawrence]] and [[Kurt Bollacker]] <ref>C.L. Giles, K. Bollacker, S. Lawrence, "CiteSeer: An Automatic Citation Indexing System," DL'98 Digital Libraries, 3rd ACM Conference on Digital Libraries, pp. 89-98, 1998.</ref> and enabled automatic algorithmic extraction and grouping of citations for any digital academic and scientific document. Where previous citation extraction was a manual process, citation measures could now scale up and be computed for any scholarly and scientific field and document venue, not just those selected by organizations such as ISI. This led to the creation of new systems for public and automated citation indexing, the first being [[CiteSeer]] (now [[CiteSeerX]], soon followed by Cora, which focused primarily on the field of [[computer science]] and [[information science]]. These were later followed by large scale academic domain citation systems such as the Google Scholar and Microsoft Academic. Such autonomous citation indexing is not yet perfect in citation extraction or citation clustering with an error rate estimated by some at 10% though a careful statistical sampling has yet to be done. This has resulted in such authors as [[Ann Arbor, Michigan|Ann Arbor]], [[Milton Keynes]], and [[Walton Hall, Milton Keynes|Walton Hall]] being credited with extensive academic output.<ref name="pmid18354457">{{cite journal |author=Postellon DC |title=Hall and Keynes join Arbor in the citation indexes |journal=[[Nature (journal)|Nature]] |volume=452 |issue=7185 |page=282 |date=March 2008 |pmid=18354457 |doi=10.1038/452282b}}</ref>  SCI claims to create automatic citation indexing through purely programmatic methods. Even the older records have a similar magnitude of error.

==See also==
* [[Impact factor]]
* [[Citation impact]]
* [[Eigenfactor]]
* [[Microsoft Academic Search]]
* [[Google Scholar]]
* [[Scopus]]
* [[H-index]] or [[Hirsch number]]
* [[Citation analysis]]
* [[Acknowledgment index]]
* [[CiteSeer]]
* [[CiteSeerX]]
* [[Scientific journal]]
* [[Science Citation Index]]
* [[Indian Citation Index]]

==References==
{{Reflist}}

==External links ==
* Official [http://admin-apps.isiknowledge.com/JCR/JCR Journal Citation Report] from the [http://www.isinet.com ISI website]
* [http://www.librijournal.org/2005-4toc.html Google Scholar: The New Generation of Citation Indexes]
* [http://www.atlasofscience.net/ Atlas of Science: Mapping Science by means of citation relations]
* [http://www.dlib.org/dlib/september05/bauer/09bauer.html An Examination of Citation Counts in a New Scholarly Communication Environment]
* [http://cids.fc.ul.pt/ CIDS] online tool that calculates the h-index and [[g-index]] based on [[Google Scholar]] data and discerning self-citations

{{DEFAULTSORT:Citation Index}}
[[Category:Academic publishing]]
[[Category:Bibliometrics]]
[[Category:Library science]]
[[Category:Reputation management]]
[[Category:Citation indices]]
>>EOP<<
231<|###|>Google Web History
'''Google Web History''' (previously '''Google Search History''') is a feature of [[Google Search]] and provided by [[Google]], in which all search queries and results that a user clicks on are recorded. The feature is only available for users logged into a [[Google Account]]. The feature was renamed from Search History to Web History on April 19, 2007.<ref>[http://searchengineland.com/google-search-history-expands-becomes-web-history-11016 "Google Search History Expands, Becomes Web History"]. Like all web hostory, google web history take up space and data on your phone, which is why many people choose to clear their hostory.  Search Engine Land. Retrieved July 12, 2010.</ref> A user's Web History is used to personalize search results with the help of [[Google Personalized Search]]<ref>[http://www.businessweek.com/the_thread/techbeat/archives/2009/12/google_gets_real-time_personalized_search.html "Google Gets Real-Time, Personalized Search"]. ''Business Week''. Retrieved July 12, 2010.</ref> and in [[Google Now]].

==References== 
Google search engine searches more than just your questions, it matches the words you search with other online posts and files that have the same words, so you get more options and more answers, if you don't find what you're searching for, try rewording your question, often you will discover your question popping up after you type only a few words, and then you can go directly to the answer you were searching for. Google makes it possible and easy for everyone.
{{Reflist}}

==External links==
* [http://history.google.com/history/ Google Web History] Also via redirect at [http://google.com/psearch]

{{Google Inc.}}

{{Google-stub}}

[[Category:Google Search|Web History]]
[[Category:Personalized search]]
>>EOP<<
237<|###|>SoundHound
{{distinguish|SoundCloud}}
{{Infobox software
| name                   = Soundcloud
| title                  = 
| logo                   = [[File:SoundHound Mobile Icon.png|250px]]
| logo caption           = SoundHound Mobile Icon
| screenshot             = <!-- [[File: ]] -->
| caption                = 
| collapsible            = 
| author                 = 
| developer              = SoundHound, Inc
| released               = {{Start date|2009|01|29|df=yes}}
| discontinued           = 
| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->
| programming language   = 
| operating system       = 
| platform               = 
| size                   = 
| language               = 
| language count         = <!-- DO NOT include this parameter unless you know what it does -->
| language footnote      = 
| status                 = 
| genre                  = 
| license                = 
| alexa                  = 
| website                = {{URL|//www.soundhound.com/}}
}}
'''SoundHound''' (known as '''Midomi''' until December 2009) is a [[mobile device]] service that allows users to identify music by [[Query by humming|humming]], singing or playing a recorded track. The service was launched by Melodis Corporation (now SoundHound Inc), under [[Chief Executive]] Keyvan Mohajer in 2007 and has received funding from Global Catalyst Partners, TransLink Capital and Walden Venture Capital.

==Features==
SoundHound is a music search engine available on the [[Apple App Store]],<ref name="Apple App Store" /> [[Google Play]],<ref name="Google Play" /> [[Windows Phone Store]], and on June 5, 2013, was available on the BlackBerry 10 platform.<ref name="Windows" /> It enables users to identify music recorded through their device's microphone.<ref name=CNETv3 /> It is also possible to speak or type the name of the artist, composer, song and piece.<ref name=CNETv3 /> Unlike competitor [[Shazam (service)|Shazam]], SoundHound can recognise tracks from singing, humming, speaking, or typing, as well as from a recording.<ref>{{cite news|last=Dolcourt|first=Jessica|title=First Look video: Shazam for iPhone|url=http://download.cnet.com/8301-2007_4-9992639-12.html|accessdate=2 October 2012|newspaper=CNET|date=16 July 2008}}</ref> Sound matching is achieved through the company's 'Sound2Sound' technology, which  can match even poorly-hummed performances to professional recordings.<ref name="CNETWVC" />

The app then returns the lyrics (if any), links to videos on YouTube, links to iTunes, ringtones, the ability to launch [[Pandora Radio]],<ref name="TNWHound">{{cite news | url=http://thenextweb.com/apps/2011/05/26/soundhounds-new-voice-app-hound-wants-to-change-the-way-we-search/ | title=SoundHounds new voice app "Hound" wants to change the way we search | date=26 May 2011 | accessdate=2 October 2012 | last=Boyd Myers | first=Courtney | newspaper=The Next Web}}</ref> as well as recommendations for other music.<ref>{{cite news|last=Dolcourt|first=Jessica|title=SoundHound for iPhone channels iTunes, recommends beats|url=http://reviews.cnet.com/8301-19512_7-20037798-233.html|accessdate=2 October 2012|newspaper=CNET|date=2 March 2011}}</ref> A feature called LiveLyrics displays a song's lyrics in time with the music, if they are available. Double-tapping on those lyrics moves the music to that point in the song.<ref>{{cite news|last=Cabebe|first=Jaymar|title=SoundHound adds LiveLyrics|url=http://download.cnet.com/8301-2007_4-20081243-12/soundhound-adds-livelyrics/|accessdate=3 October 2012|newspaper=CNET|date=20 July 2011}}</ref> It is also possible for users to play music from their iPhone's iPod library through the app. If lyrics are available for a song, it will show them as it plays.<ref name="CNETv3" />

There are three versions of the app: SoundHound, SoundHound Infinity and Hound. SoundHound is free but has banner ads, while SoundHound Infinity (styled SoundHound ), priced at 4.99 in the UK or $6.99 in the US, is the premium offering and has the same functionality but without banner ads.<ref name="CNETFree" /> Hound only allows users to search for artists or songs by speaking into it. Similar to the SoundHound app, Hound then returns a song preview, lyrics, album art and videos as well as artist bios and tour dates.<ref name="TNWHound" />

==History==
Midomi, renamed SoundHound in December 2009 with the launch of version 3.0 of the mobile app,<ref name=CNETv3>{{cite news|last=Dolcourt|first=Jessica|title=Midomi 3.0 seeks song lyrics, knows what's hot|url=http://reviews.cnet.com/8301-19512_7-10408563-233.html|accessdate=2 October 2012|newspaper=CNET|date=3 December 2009}}</ref> was launched in [[beta]] in January 2007, as a [http://www.midomi.com website], with 2 million licensed tracks.<ref name="CNET1" /> The technology, dubbed Multimodal Adaptive Recognition System (MARS), considers pitch, tempo variation, speech content and pauses in order to recognise samples.<ref name=CNET1>{{cite news|last=Mills|first=Elinor|title=This Web site can name that tune|url=http://news.cnet.com/This-Web-site-can-name-that-tune/2100-1027_3-6153657.html|accessdate=2 October 2012|newspaper=CNET|date=26 January 2007}}</ref> The company behind the site, Melodis Corporation, was started in 2004 by [[Chief Executive]] Keyvan Mohajer, a [[PhD]] in sound-recognition from [[Stanford]].<ref name=CNET1 /> Melodis changed its name to SoundHound Inc in May 2010.<ref>{{cite press release|title=SoundHound Inc. Announces Name Change from Melodis Corporation|publisher=SoundHound Inc|date=20 May 2010|url=//www.soundhound.com/index.php?action=s.press_release&pr=15|accessdate=2 October 2012}}</ref>

The first version of the app was released on the [[Apple App Store]] in July 2008.<ref name="Apple App Store">{{cite news | url=http://news.cnet.com/8301-17938_105-9987892-1.html | title=Sing for search results with iPhone app | date=10 July 2008 | accessdate=2 October 2012 | last=Jackson | first=Holly | newspaper=CNET}}</ref> At the launch of [[Windows Marketplace for Mobile]] in October 2009, Midomi was one of the apps included in the store<ref name="Windows">{{cite news | url=http://reviews.cnet.com/8301-12261_7-10368174-10356022.html | title=Windows mobile app store, My Phone service officially opening | date=6 October 2009 | accessdate=2 October 2012 | last=Dolcourt | first=Jessica | newspaper=CNET}}</ref> and could be purchased for $4.99.<ref>{{cite news|last=Dolcourt|first=Jessica|title=Shazam debuts in Windows Marketplace for Mobile|url=http://reviews.cnet.com/8301-12261_7-10368986-10356022.html|accessdate=2 October 2012|newspaper=CNET|date=7 October 2009}}</ref> It joined the [[Android OS|Android]] app store in June 2010.<ref name="Google Play">{{cite news | url=http://www.cnet.com/8301-19736_1-20007745-251.html | title=New SoundHound names that tune--for free (Android) | date=15 June 2010 | accessdate=3 October 2012 | last=Dolcourt | first=Jessica | newspaper=CNET}}</ref> On January 2013, the [[BlackBerry]] version of the app was then available in [[BlackBerry World]] following the announcement and launch of [[BlackBerry 10]].<ref name="BlackBerry">{{cite web
  |title=BlackBerry shows off some of its 70,000 new third-party apps, including Skype, Rdio, Kindle, and Whatsapp
  |publisher=[[The Verge]]
  |url=http://www.theverge.com/2013/1/30/3932042/blackberry-10-apps-announcement
  |accessdate=2013-01-30}}</ref>

A free version of the app was released in April 2010, with all the functionality of the premium version, while limiting the number of searches to five per month, and adding banners ads.<ref name="CNETFree">{{cite news | url=http://reviews.cnet.com/8301-19512_7-20003228-233.html | title=Sonic freebie: New, free SoundHound music-ID app for iPhone, iPad | date=27 April 2010 | accessdate=3 October 2012 | last=Dolcourt | first=Jessica | newspaper=CNET}}</ref> The premium version was now renamed SoundHound Infinity.<ref name="CNETFree" /> A stripped-down version, Hound, was released in May 2011.<ref name="TNWHound" />

In January 2011, Apple revealed that SoundHound was the top paid iPad app  on its [[Apple App Store|App Store]] was SoundHound, while rival Shazam was fourth in the top ten list of free iPhone apps.<ref>{{cite news|last=Reisinger|first=Don|title=Apple reveals top apps of all time|url=http://news.cnet.com/8301-13506_3-20028889-17.html|accessdate=2 October 2012|newspaper=CNET|date=19 January 2011}}</ref>

In June 2012, the firm announced that it had 80 million users while version 5.0 was released, with a new design and features that include an in-built player and integration with LiveLyrics.<ref name="TNW80m">{{cite news | url=http://thenextweb.com/apps/2012/06/07/shazam-competitor-soundhound-passes-80m-users-and-rolls-out-updated-mobile-apps/ | title=Shazam competitor SoundHound passes 80m users and rolls out updated mobile apps | work=The Next Web | date=7 June 2012 | accessdate=2 October 2012 | author=Sawers, Paul}}</ref>

In December 2013, the app passed 185 million users.<ref>{{cite news|title=SoundHound Reveals Its Top Songs of 2013|url=http://www.heraldonline.com/2013/12/16/5509030/soundhound-reveals-its-top-songs.html|accessdate=16 December 2013|newspaper=The Next Web|date=16 December 2013}}</ref>

In December 2013, the app launches iTunes Radio integration.<ref>{{cite news|title=SoundHound App Update Adds iTunes Radio Integration for iPad and iPhone Users. |url=http://www.padgadget.com/2013/12/20/soundhound-app-update-adds-itunes-radio-integration-for-ipad-and-iphone-users/|accessdate=11 February 2014|newspaper=PadGadget|date=20 December 2013}}</ref>

In September 2013, the app enables 170 million global users to sync, save, and transfer music search & discovery history across multiple devices.<ref>{{cite news|title=SoundHound adds cloud history sync on iOS and Android apps|url=http://www.intomobile.com/2013/09/25/soundhound-adds-cloud-history-sync-ios-and-android-apps/|accessdate=11 February 2014|newspaper=INTOMOBILE|date=20 September 2013}}</ref>

In January 2014, SoundHound and Hyundai Motor Group partnered to embed music search and discovery into select 2014 Hyundai & Kia models.<ref>{{cite news|title=Hyundai and Kia tap SoundHound to help you identify music in your car|url=http://www.engadget.com/2014/01/14/hyundai-kia-soundhound-music-tagging/|accessdate=11 February 2014|newspaper=Engadget|date= January 14, 2014}}</ref>

In January 2014, the app launched an "immersive second screen GRAMMYs experience".<ref>{{cite news|title=SoundHound's music search app turns its focus to the Grammys with real-time updates and more|url=http://www.engadget.com/2014/01/25/soundhound-grammys-2014/|accessdate=11 February 2014|newspaper=Engadget|date= January 24, 2014}}</ref>

In April 2014, the app passed 200 million users.<ref>//www.soundhound.com/index.php?action=s.press_release&pr=67</ref>

===Funding===
Melodis secured $7 million in a Series B funding round in October 2008, bringing total funds raised to $12 million. The round was led by TransLink Capital with the participation of JAIC America and [[Series A round|Series A]] investor Global Catalyst Partners.<ref>{{cite press release|title=Search and Sound Recognition Innovator MELODIS and Creator of Midomi Raises $7 Million in Series B Funding|publisher=Melodis Corporation|date=7 October 2008|url=//www.soundhound.com/index.php?action=s.press_release&pr=5|accessdate=2 October 2012}}</ref>

In 2009, Melodis attracted additional funding from Larry Marcus at Walden Venture Capital, who had previously invested in music startups [[Pandora Radio|Pandora]] and [[SNOCAP|Snocap]].<ref name=CNETWVC>{{cite news|last=Needleman|first=Rafe|title=Midomi music search gets funding and opportunities|url=http://news.cnet.com/8301-19882_3-10298068-250.html|accessdate=2 October 2012|newspaper=CNET|date=28 July 2009}}</ref> The $4 million funding round was led by Walden Venture Capital VII, with the participation of an unnamed device manufacturer.<ref>{{cite press release|title=Melodis, Sound Search Technology and Applications Innovator, Raises $4M Led by Walden Venture Capital and a Strategic Investor|publisher=Melodis Corporation|date=4 August 2009|url=//www.soundhound.com/index.php?action=s.press_release&pr=12|accessdate=2 October 2012}}</ref>

==See also==
*[[Query by humming]]

==References==
{{reflist|2}}

==External links==
*[http://midomi.com midomi.com]
*[//www.soundhound.com/ SoundHound website]

[[Category:Android (operating system) software]]
[[Category:IOS software]]
[[Category:Symbian software]]
[[Category:Music search engines]]
[[Category:Companies established in 2005]]
[[Category:Companies based in California]]
[[Category:BlackBerry software]]
>>EOP<<
