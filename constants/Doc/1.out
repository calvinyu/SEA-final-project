(dp0
I1
(dp1
S'url'
p2
S'http://en.wikipedia.org/wiki/Proximity search (text)'
p3
sS'text'
p4
S'In [[natural language processing|text processing]], a \'\'\'proximity search\'\'\' looks for documents where two or more separately matching term occurrences are within a specified [[string distance|distance]], where distance is the number of intermediate words or characters. In addition to proximity, some implementations may also impose a constraint on the word order, in that the order in the searched text must be identical to the order of the search query. Proximity searching goes beyond the simple matching of words by adding the constraint of proximity and is generally regarded as a form of advanced search.\n\nFor example, a search could be used to find "red brick house", and match phrases such as "red house of brick" or "house made of red brick". By limiting the proximity, these phrases can be matched while avoiding documents where the words are scattered or spread across a page or in unrelated articles in an anthology.\n\n== Rationale ==\nThe basic linguistic assumption of proximity searching is that the proximity of the words in a document implies a [[semantic relation|relationship]] between the words. Given that authors of documents try to formulate sentences which contain a single idea, or cluster of related ideas within neighboring sentences or organized into paragraphs, there is an inherent, relatively high, probability within the document structure that words used together are related. On the other hand, when two words are on the opposite ends of a book, the probability of a relationship between the words is relatively weak. By limiting search results to only include matches where the words are within the specified maximum proximity, or distance, the search results are assumed to be of higher relevance than the matches where the words are scattered.\n\nCommercial internet search engines tend to produce too many matches (known as recall) for the average search query. Proximity searching is one method of reducing the number of pages matches, and to improve the relevance of the matched pages by using word proximity to assist in ranking. As an added benefit, proximity searching helps combat [[spamdexing]] by avoiding webpages which contain dictionary lists or shotgun lists of thousands of words, which would otherwise rank highly if the search engine was heavily biased toward [[word frequency]].\n\n== Boolean syntax and operators ==\nNote that a proximity search can designate that only some keywords must be within a specified distance. Proximity searching can be used with other search syntax and/or controls to allow more articulate search queries. Sometimes query operators like NEAR, NOT NEAR, FOLLOWED BY, NOT FOLLOWED BY, SENTENCE or FAR are used to indicate a proximity-search limit between specified keywords: for example, "brick NEAR house".\n\n== Usage in commercial search engines ==\nIn regards to implicit/automatic versus explicit proximity search, as of November 2008, most Internet [[search engine]]s only implement an implicit proximity search functionality. That is, they automatically rank those search results higher where the user keywords have a good "overall proximity score" in such results. If only two keywords are in the search query, this has no difference from an explicit proximity search which puts a NEAR operator between the two keywords. However, if three or more than three keywords are present, it is often important for the user to specify which subsets of these keywords expect a proximity in search results. This is useful if the user wants to do a [[prior art]] search (e.g. finding an existing approach to complete a specific task, finding a document that discloses a system that exhibits a procedural behavior collaboratively conducted by several components and links between these components).\n\n[[Web search engine]]s which support proximity search via an explicit proximity operator in their query language include  [[Walhello]], [[Exalead]], [[Yandex]], [[Yahoo!]] and [[Altavista]]:\n* When using the [[Walhello]] search-engine, the proximity can be defined by the number of characters between the keywords.<ref>[http://www.walhello.com/aboutgl.html "About Walhello"], visited 23 December 2009</ref>\n* The search engine Exalead allows the user to specify the required proximity, as the maximum number of words between keywords. The syntax is <tt>(keyword1 NEAR/n keyword2)</tt> where n is the number of words.<ref>[http://www.exalead.com/search/web/search-syntax/#proximity_search "Web Search Syntax"], visited 23 December 2009</ref>\n* [[Yandex]] uses the syntax <tt>keyword1 /n keyword2</tt> to search for two keywords separated by at most <math>n - 1</math> words, and supports a few other variations of this syntax.<ref>[http://help.yandex.ru/search/?id=481939 Yandex help page on query language] (in Russian)</ref>\n* [[Yahoo!]] and [[Altavista]] both support an undocumented NEAR operator.<ref>[http://search.yahoo.com/search?p=site%3Awww.rfc-editor.org+inurl%3Arfc2606+guidance+NEAR+additional "Successful Yahoo! proximity query"] (22 Feb 2010)</ref><ref>[http://search.yahoo.com/search?p=site%3Awww.rfc-editor.org+inurl%3Arfc2606+guidance+NEAR+unused "Unsuccessful Yahoo! proximity query"] (22 Feb 2010)</ref> The syntax is <tt>keyword1 NEAR keyword2</tt>.\n* Google supports AROUND(#).<ref>[http://www.guidingtech.com/16116/google-search-little-known-around-operator/ "GuidingTech: Meet Google Search\'s Little Known AROUND Operator"]</ref>\n\nOrdered search within the [[Google]] and [[Yahoo!]] search engines is possible using the asterisk (*) full-word [[Wildcard character|wildcard]]s: in Google this matches one or more words,<ref>[http://www.google.com/support/websearch/bin/answer.py?answer=136861 "More Google Search Help" visited 23 December 2009]</ref> and an in Yahoo! Search this matches exactly one word.<ref>[http://www.searchengineshowdown.com/features/yahoo/review.html "Review of Yahoo! Search", by Search Engine Showdown, visited 23 December 2009]</ref>  (This is easily verified by searching for the following phrase in both Google and Yahoo!: "addictive * of biblioscopy".)\n\nTo emulate unordered search of the NEAR operator can be done using a combination of ordered searches.  For example, to specify a close co-occurrence of "house" and "dog", the following search-expression could be specified: "house dog" OR "dog house" OR "house * dog" OR "dog * house" OR "house * * dog" OR "dog * * house".\n\n== See also ==\n* [[Compound term processing]]\n* [[Edit distance]]\n* [[Information retrieval]]\n* [[Search engine]]\n* [[Search engine indexing]] - how texts are indexed to support proximity search\n* [[Semantic proximity]]\n\n== Notes ==\n{{Reflist}}\n\n[[Category:Information retrieval]]\n[[Category:Internet search algorithms]]'
p5
sS'title'
p6
S'Proximity search (text)'
p7
ssI130
(dp8
g2
S'http://en.wikipedia.org/wiki/Question answering'
p9
sg4
S'{{other uses|question|answer}}\n{{multiple issues|\n{{cleanup|date=January 2012|reason=extensive use of jargon to define jargon, and inconsistent use of bold and italics font styles}}\n{{cleanup-rewrite|date=January 2012}}\n{{more footnotes|date=February 2014}}\n}}\n\n\'\'\'Question Answering\'\'\' (\'\'\'QA\'\'\') is a computer science discipline within the fields of [[information retrieval]] and [[natural language processing]] (NLP), which is concerned with building systems that automatically answer questions posed by humans in a [[natural language]].\n\nA QA implementation, usually a computer program, may construct its answers by querying a structured [[database]] of knowledge or information, usually a [[knowledge base]]. More commonly, QA systems can pull answers from an unstructured collection of natural language documents<ref>"[https://www.academia.edu/2475776/Versatile_question_answering_systems_seeing_in_synthesis Versatile question answering systems: seeing in synthesis]", Mittal et al., IJIIDS, 5(2), 119-142, 2011  \n</ref>\n\nSome examples of natural language document collections used for QA systems include:\n* a local collection of reference texts\n\n* internal organization documents and web pages\n* compiled [[newswire]] reports\n* a set of [[Wikipedia]] pages\n* a subset of [[World Wide Web]] pages\n\nQA research attempts to deal with a wide range of question types including: fact, list, definition, \'\'How\'\', \'\'Why\'\', hypothetical, semantically constrained, and cross-lingual questions.\n\n* \'\'Closed-domain\'\' question answering deals with questions under a specific domain (for example, medicine or automotive maintenance), and can be seen as an easier task because NLP systems can exploit domain-specific knowledge frequently formalized in [[Ontology (computer science)|ontologies]]. Alternatively, \'\'closed-domain\'\' might refer to a situation where only a limited type of questions are accepted, such as questions asking for [[descriptive knowledge|descriptive]] rather than [[procedural knowledge|procedural]] information. QA systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimers disease <ref>Roser Morante , Martin Krallinger , Alfonso Valencia and  Walter Daelemans. Machine Reading of Biomedical Texts about Alzheimer\xe2\x80\x99s Disease. CLEF 2012 Evaluation Labs and Workshop. September 17 2012</ref>\n* \'\'[[Open domain#References|Open-domain]]\'\' question answering deals with questions about nearly anything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer.\n\n==History==\n\nTwo early QA systems were BASEBALL and LUNAR.{{when|date=November 2012}}{{who|date=November 2012}}{{citation needed|date=November 2012}} BASEBALL answered questions about the US baseball league over a period of one year. LUNAR, in turn, answered questions about the geological analysis of rocks returned by the Apollo moon missions. Both QA systems were very effective in their chosen domains. In fact, LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain posed by people untrained on the system. Further restricted-domain QA systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain. The language abilities of BASEBALL and LUNAR used techniques similar to [[ELIZA]] and [[DOCTOR]], the first [[chatterbot]] programs.\n\n[[SHRDLU]] was a highly successful question-answering program developed by [[Terry Winograd]] in the late 60s and early 70s. It simulated the operation of a robot in a toy world (the "blocks world"), and it offered the possibility to ask the robot questions about the state of the world. Again, the strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program.\n\nIn the 1970s, [[knowledge base]]s were developed that targeted narrower domains of knowledge. The QA systems developed to interface with these [[expert system]]s produced more repeatable and valid responses to questions within an area of knowledge. These [[expert systems]] closely resembled modern QA systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized [[knowledge base]]s, whereas many modern QA systems rely on statistical processing of a large, unstructured, natural language text corpus.\n\nThe 1970s and 1980s saw the development of comprehensive theories in [[computational linguistics]], which led to the development of ambitious projects in text comprehension and question answering. One example of such a system was the Unix Consultant (UC), developed by [[Robert Wilensky]] at [[U.C. Berkeley]] in the late 1980s. The system answered questions pertaining to the [[Unix]] operating system. It had a comprehensive hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning.\n\nRecently, specialized natural language QA systems have been developed, such as [http://bitem.hesge.ch/content/eagli-eagle-eye EAGLi] for health and life scientists.\n\n==Architecture==\nMost modern QA systems use [[natural language]] text documents as their underlying knowledge source.  [[Natural language processing]] techniques are used to both process the question and index or process the text [[Text corpus|corpus]] from which answers are extracted. An increasing number of QA systems use the [[World Wide Web]] as their corpus of text and knowledge. However, many of these tools do not produce a human-like answer, but rather employ "shallow" methods (keyword-based techniques, templates...) to produce a list of documents or a list of document excerpts containing the probable answer highlighted.\n\nIn an alternative QA implementation, human users assemble knowledge in a structured database, called a [[knowledge base]], similar to those employed in the [[expert systems]] of the 1970s. It is also possible to employ a combination of structured databases and natural language text documents in a hybrid QA system. Such a hybrid system may employ data mining algorithms to populate a structured knowledge base that is also populated and edited by human contributors. An example hybrid QA system is the [[Wolfram Alpha]] QA system which employs natural language processing to transform human questions into a form that is processed by a curated knowledge base.\n\nCurrent QA systems<ref>Hirschman, L. & Gaizauskas, R. (2001) [http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=96167 Natural Language Question Answering. The View from Here]. Natural Language Engineering (2001), 7:4:275-300 Cambridge University Press.</ref> typically include a \'\'\'question classifier\'\'\' module that determines the type of question and the type of answer. After the question is analysed, the system typically uses several modules that apply increasingly complex NLP techniques on a gradually reduced amount of text. Thus, a \'\'\'document retrieval module\'\'\' uses [[search engine]]s to identify the documents or paragraphs in the document set that are likely to contain the answer. Subsequently a \'\'\'filter\'\'\' preselects small text fragments that contain strings of the same type as the expected answer. For example, if the question is "Who invented\nPenicillin" the filter returns text that contain names of people. Finally, an \'\'\'answer extraction\'\'\' module looks for further clues in the text to determine if the answer candidate can indeed answer the question.\n\nA \'\'\'multiagent\'\'\' question-answering architecture has been proposed, where each domain is represented by an agent which tries to answer questions taking into account its specific knowledge. The meta\xe2\x80\x93agent controls the cooperation between question answering agents and chooses the most relevant answer(s).<ref>{{vcite journal |author=Galitsky B, Pampapathi R|title=Can many agents answer questions better than one|journal=First Monday |volume = 10| Number=1 |date=2005 | url = http://firstmonday.org/ojs/index.php/fm/article/view/1204/1124\n}}</ref>\n\n==Question answering methods==\nQA is very dependent on a good search [[text corpus|corpus]] - for without documents containing the answer, there is little any QA system can do. It thus makes sense that larger collection sizes generally lend well to better QA performance, unless the question domain is orthogonal to the collection. The notion of [[data redundancy]] in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents,<ref>Lin, J. (2002). The Web as a Resource for Question Answering: Perspectives and Challenges. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC 2002).</ref> leading to two benefits:\n# By having the right information appear in many forms, the burden on the QA system to perform complex NLP techniques to understand the text is lessened.\n# Correct answers can be filtered from [[false positive]]s by relying on the correct answer to appear more times in the documents than instances of incorrect ones.\n\nQuestion answering heavily relies on [[reasoning]]. There are a number of question answering systems designed in [[Prolog]],<ref>{{cite book |last=Galitsky |first=Boris |title=Natural Language Question Answering System: Technique of Semantic Headers |url=http://books.google.com/books?id=LkNmAAAACAAJ |series=International Series on Advanced Intelligence |volume=Volume 2 |year=2003 |publisher=Advanced Knowledge International |location=Australia |isbn=978-0-86803-979-4}}</ref> a [[logic programming]] language associated with [[artificial intelligence]].\n\n===Open domain question answering===\nIn [[information retrieval]], an open domain question answering system aims at returning an answer in response to the user\xe2\x80\x99s question. The returned answer is in the form of short texts rather than a list of relevant documents. The system uses a combination of techniques from [[computational linguistics]], [[information retrieval]] and [[knowledge representation]] for finding answers.\n\nThe system takes a [[natural language]] question as an input rather than a set of keywords, for example, \xe2\x80\x9cWhen is the national day of China?\xe2\x80\x9d The sentence is then transformed into a query through its [[logical form]]. Having the input in the form of a natural language question makes the system more user-friendly, but harder to implement, as there are various question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a question type to the question is a crucial task, the entire answer extraction process relies on finding the correct question type and hence the correct answer type.\n\nKeyword [[Data extraction|extraction]] is the first step for identifying the input question type. In some cases, there are clear words that indicate the question type directly. i.e. \xe2\x80\x9cWho\xe2\x80\x9d, \xe2\x80\x9cWhere\xe2\x80\x9d or \xe2\x80\x9cHow many\xe2\x80\x9d, these words tell the system that the answers should be of type \xe2\x80\x9cPerson\xe2\x80\x9d, \xe2\x80\x9cLocation\xe2\x80\x9d, \xe2\x80\x9cNumber\xe2\x80\x9d respectively. In the example above, the word \xe2\x80\x9cWhen\xe2\x80\x9d indicates that the answer should be of type \xe2\x80\x9cDate\xe2\x80\x9d. POS tagging and syntactic parsing techniques can also be used to determine the answer type. In this case, the subject is \xe2\x80\x9cChinese National Day\xe2\x80\x9d, the predicate is \xe2\x80\x9cis\xe2\x80\x9d and the adverbial modifier is \xe2\x80\x9cwhen\xe2\x80\x9d, therefore the answer type is \xe2\x80\x9cDate\xe2\x80\x9d. Unfortunately, some interrogative words like \xe2\x80\x9cWhich\xe2\x80\x9d, \xe2\x80\x9cWhat\xe2\x80\x9d or \xe2\x80\x9cHow\xe2\x80\x9d do not give clear answer types. Each of these words can represent more than one type. In situations like this, other words in the question need to be considered. First thing to do is to find the words that can indicate the meaning of the question. A lexical dictionary such as [[WordNet]] can then be used for understanding the context.\n\nOnce the question type has been identified, an [[Information retrieval]] system is used to find a set of documents containing the correct key words. A tagger and NP/Verb Group chunker can be used to verify whether the correct entities and relations are mentioned in the found documents. For questions such as \xe2\x80\x9cWho\xe2\x80\x9d or \xe2\x80\x9cWhere\xe2\x80\x9d, a Named Entity Recogniser is used to find relevant \xe2\x80\x9cPerson\xe2\x80\x9d and \xe2\x80\x9cLocation\xe2\x80\x9d names from the retrieved documents. Only the relevant paragraphs are selected for ranking.\n\nA [[vector space model]] can be used as a strategy for classifying the candidate answers. Check if the answer is of the correct type as determined in the question type analysis stage. Inference technique can also be used to validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate, the more and the closer the better. The answer is then translated into a compact and meaningful representation by parsing. In the previous example, the expected output answer is \xe2\x80\x9c1st Oct.\xe2\x80\x9d\n\n==Issues==\nIn 2002 a group of researchers wrote a roadmap of research in question answering.<ref>Burger, J., Cardie, C., Chaudhri, V., Gaizauskas, R., Harabagiu, S., Israel, D., Jacquemin, C., Lin, C-Y., Maiorano, S., Miller, G., Moldovan, D., Ogden, B., Prager, J., Riloff, E., Singhal, A., Shrihari, R., Strzalkowski, T., Voorhees, E., Weishedel, R. [http://www-nlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper_v2.doc Issues, Tasks and Program Structures to Roadmap Research in Question Answering (QA)].</ref> The following\nissues were identified.<!-- much of the text in this section is copied and pasted from the "roadmap" document; somebody may try and simplify the text -->\n\n;Question classes : Different types of questions (e.g., "What is the capital of [[Liechtenstein]]?" vs. "Why does a [[rainbow]] form?" vs. "Did [[Marilyn Monroe]] and [[Cary Grant]] ever appear in a movie together?") require the use of different strategies to find the answer. Question classes are arranged hierarchically in taxonomies.{{example needed|date=February 2011}}\n\n;Question processing : The same information request can be expressed in various ways, some interrogative ("Who is the King of Lesotho?") and some assertive ("Tell me the name of the King of Lesotho."). A semantic model of question understanding and processing would recognize equivalent questions, regardless of how they are presented. This model would enable the translation of a complex question into a series of simpler questions, would identify ambiguities and treat them in context or by interactive clarification.\n\n;Context and QA : Questions are usually asked within a context and answers are provided within that specific context. The context can be used to clarify a question, resolve ambiguities or keep track of an investigation performed through a series of questions. (For example, the question, "Why did Joe Biden visit Iraq in January 2010?" might be asking why Vice President Biden visited and not President Obama, why he went to Iraq and not Afghanistan or some other country, why he went in January 2010 and not before or after, or what Biden was hoping to accomplish with his visit. If the question is one of a series of related questions, the previous questions and their answers might shed light on the questioner\'s intent.)\n\n;Data sources for QA : Before a question can be answered, it must be known what knowledge sources are available and relevant. If the answer to a question is not present in the data sources, no matter how well the question processing, information retrieval and answer extraction is performed, a correct result will not be obtained.\n\n;Answer extraction : Answer extraction depends on the complexity of the question, on the answer type provided by question processing, on the actual data where the answer is searched, on the search method and on the question focus and context.{{example needed|date=February 2011}}\n\n;Answer formulation : The result of a QA system should be presented in a way as natural as possible. In some cases, simple extraction is sufficient. For example, when the question classification indicates that the answer type is a name (of a person, organization, shop or disease, etc.), a quantity (monetary value, length, size, distance, etc.) or a date (e.g. the answer to the question, "On what day did Christmas fall in 1989?") the extraction of a single datum is sufficient. For other cases, the presentation of the answer may require the use of fusion techniques that combine the partial answers from multiple documents.\n\n;Real time question answering : There is need for developing Q&A systems that are capable of extracting answers from large data sets in several seconds, regardless of the complexity of the question, the size and multitude of the data sources or the ambiguity of the question.\n\n;Multilingual (or cross-lingual) question answering : The ability to answer a question posed in one language using an answer corpus in another language (or even several). This allows users to consult information that they cannot use directly. (See also [[Machine translation]].)\n\n;Interactive QA : It is often the case that the information need is not well captured by a QA system, as the question processing part may fail to classify properly the question or the information needed for extracting and generating the answer is not easily retrieved. In such cases, the questioner might want not only to reformulate the question, but to have a dialogue with the system. In addition, system may also use previously answered questions.<ref>Perera, R. and Nand, P. 2014. [http://link.springer.com/chapter/10.1007%2F978-3-319-11716-4_11 Interaction History Based Answer Formulation for Question Answering.]</ref> (For example, the system might ask for a clarification of what sense a word is being used, or what type of information is being asked for.)\n\n;Advanced reasoning for QA : More sophisticated questioners expect answers that are outside the scope of written texts or structured databases. To upgrade a QA system with such capabilities, it would be necessary to integrate reasoning components operating on a variety of knowledge bases, encoding world knowledge and common-sense reasoning mechanisms, as well as knowledge specific to a variety of domains. [[Evi (software)|Evi]] is an example of such as system.\n\n;Information clustering for QA: Information clustering for question answering systems is a new trend that originated to increase the accuracy of question answering systems through search space reduction. In recent years this was widely researched through development of question answering systems which support information clustering in their basic flow of process.<ref>Perera, R. 2012. [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6305919&isnumber=6305918 IPedagogy: Question Answering System Based on Web Information Clustering.]</ref>\n\n;User profiling for QA : The user profile captures data about the questioner, comprising context data, domain of interest, reasoning schemes frequently used by the questioner, common ground established within different dialogues between the system and the user, and so forth. The profile may be represented as a predefined template, where each template slot represents a different profile feature. Profile templates may be nested one within another.{{example needed|date=February 2011}}\n\n==Progress==\nQA systems have been extended in recent years to encompass additional domains of knowledge<ref>Maybury, M. T. editor. 2004. [http://www.mitpressjournals.org/doi/pdf/10.1162/089120105774321055 New Directions in Question Answering.] AAAI/MIT Press.</ref>  For example, systems have been developed to automatically answer temporal and geospatial questions, questions of definition and terminology, biographical questions, multilingual questions, and questions about the content of audio, images, and video. Current QA research topics include:\n\n* interactivity\xe2\x80\x94clarification of questions or answers\n* answer reuse or caching\n* knowledge representation and reasoning\n* social media analysis with QA systems\n* [[sentiment analysis]]<ref>[http://totalgood.com/bitcrawl/ BitCrawl] by Hobson Lane</ref>\n* utilization of thematic roles<ref>Perera, R. and Perera, U. 2012. [http://www.aclweb.org/anthology/W12-6004 Towards a thematic role based target identification model for question answering.]</ref>\n* semantic resolution: to bridge the gap between syntactically different questions and answer-bearing texts<ref>{{cite conference | author=Bahadorreza Ofoghi, John Yearwood, and Liping Ma | year=2008 | conference=The 30th European Conference on Information Retrieval (ECIR\'08)| pages= 430\xe2\x80\x93437 | publisher=Springer Berlin Heidelberg | title= [http://link.springer.com/chapter/10.1007/978-3-540-78646-7_40 The impact of semantic class identification and semantic role labeling on natural language answer extraction]}}</ref>\n* utilization of linguistic resources,<ref>{{cite journal |author=Bahadorreza Ofoghi, John Yearwood, and Liping Ma|title=[http://onlinelibrary.wiley.com/doi/10.1002/asi.20989/abstract;jsessionid=099F3D167FD0511A48FB1C19C1060676.f02t02?deniedAccessCustomisedMessage=&userIsAuthenticated=false The impact of frame semantic annotation levels, frame\xe2\x80\x90alignment techniques, and fusion methods on factoid answer processing] | journal=Journal of the American Society for Information Science and Technology |volume=60 |issue=2 |pages=247\xe2\x80\x93263 |year =2009}}</ref> such as [[WordNet]], [[FrameNet]], and the similar\n\nIBM\'s question answering system, Watson, defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin.\n<ref>http://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html?_r=0</ref>\n\n==References==\n* Dragomir R. Radev, John Prager, and Valerie Samn. [http://clair.si.umich.edu/~radev/papers/anlp00.pdf Ranking suspected answers to natural language questions using predictive annotation]. In Proceedings of the 6th Conference on Applied Natural Language Processing, Seattle, WA, May 2000.\n* John Prager, Eric Brown, Anni Coden, and Dragomir Radev. [http://clair.si.umich.edu/~radev/papers/sigir00.pdf Question-answering by predictive annotation]. In Proceedings, 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, July 2000.\n*{{cite book | last = Hutchins | first = W. John | authorlink = John Hutchins |author2=Harold L. Somers  | year = 1992 | title = An Introduction to Machine Translation | url = http://www.hutchinsweb.me.uk/IntroMT-TOC.htm | publisher = Academic Press | location = London | isbn = 0-12-362830-X}}\n* L. Fortnow, Steve Homer (2002/2003).   [http://people.cs.uchicago.edu/~fortnow/papers/history.pdf A Short History of Computational Complexity].  In D. van Dalen, J. Dawson, and A. Kanamori, editors, \'\'The History of Mathematical Logic\'\'. North-Holland, Amsterdam.\n\n<references/>\n\n==External links==\n* [http://aclia.lti.cs.cmu.edu/ntcir8 Question Answering Evaluation at NTCIR]\n* [http://trec.nist.gov/data/qamain.html Question Answering Evaluation at TREC]\n* [http://celct.fbk.eu/QA4MRE/ Question Answering Evaluation at CLEF]\n\n{{Computable knowledge}}\n{{Natural Language Processing}}\n\n[[Category:Artificial intelligence applications]]\n[[Category:Natural language processing]]\n[[Category:Computational linguistics]]\n[[Category:Information retrieval]]'
p10
sg6
S'Question answering'
p11
ssI4
(dp12
g2
S'http://en.wikipedia.org/wiki/Divergence-from-randomness model'
p13
sg4
S"In the field of [[information retrieval]], '''divergence from randomness''' is one type of [[probabilistic]] model.\n\nTerm weights are computed by measuring the divergence between a term distribution produced by a random process and the actual term distribution.\n\n==External links==\n*[http://terrier.org/docs/v3.5/dfr_description.html Terrier's DFR Web page]\n*[http://ir.dcs.gla.ac.uk/wiki/DivergenceFromRandomness Glasgow IR group Wiki DFR page]\n\n[[Category:Ranking functions]]\n[[Category:Information retrieval]]\n[[Category:Probabilistic models]]\n\n\n{{comp-sci-stub}}"
p14
sg6
S'Divergence-from-randomness model'
p15
ssI133
(dp16
g2
S'http://en.wikipedia.org/wiki/Temporal information retrieval'
p17
sg4
S'\'\'\'Temporal Information Retrieval (T-IR)\'\'\' is an emerging area of research related to the field of [[information retrieval]] (IR) and a considerable number of sub-areas, positioning itself, as an important dimension in the context of the user information needs.\n\nAccording to [[information theory]] science (Metzger, 2007),<ref name="Metzger2007">{{cite journal |last=Metzger |first=Miriam |title=Making Sense of Credibility on the Web: Models for Evaluating Online Information and Recommendations for Future Research |journal=Journal of the American Society for Information Science and Technology |volume=58 |issue=13 |pages=2078\xe2\x80\x932091 |year =2007 |url=http://dl.acm.org/citation.cfm?id=1315940 |doi=10.1002/asi.20672 }}</ref> timeliness or currency is one of the key five aspects that determine a document\xe2\x80\x99s credibility besides relevance, accuracy, objectivity and coverage. One can provide many examples when the returned search results are of little value due to temporal problems such as obsolete data on weather, outdated information about a given company\xe2\x80\x99s earnings or information on already-happened or invalid predictions.\n\nT-IR, in general, aims at satisfying these temporal needs and at combining traditional notions of document relevance with the so-called temporal relevance. This will enable the return of temporally relevant documents, thus providing a temporal overview of the results in the form of timeliness or similar structures. It also shows to be very useful for query understanding, query disambiguation, query classification, result diversification and so on.\n\nThis page contains a list of the most important research in temporal information retrieval (T-IR) and its related sub-areas. As several of the referred works are related with different research areas a single article can be found in more than one different table. For ease of reading the articles are categorized in a number of different sub-areas referring to its main scope, in detail.\n\n== Temporal dynamics (T-dynamics) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Baeza, Y.\'\'\' (2002). [http://www.dcs.bbk.ac.uk/webDyn2/proceedings/baeza_yates_web_strucutre.pdf/ Web Structure, Dynamics and Page Quality]. In A. Laendar & A. Oliveira (Eds.), \'\'In Lecture Notes in Computer Science - SPIRE2002: 9th International Symposium on String Processing and Information Retrieval\'\' (Vol. 2476/2002, pp.&nbsp;117 \xe2\x80\x93 130). Lisbon, Portugal. September 11\xe2\x80\x9313: Springer Berlin / Heidelberg. || 2002 || SPIRE || T-Dynamics ||\n|-\n|\'\'\'Cho, J., & Garcia-Molina, H.\'\'\' (2003). [http://dl.acm.org/citation.cfm?id=857170 Estimating Frequency of Change]. \'\'In [http://toit.acm.org TOIT: ACM Transactions on Internet Technology]\'\', 3(3), 256 - 290.|| 2003 || TOIT || T-Dynamics ||\n|-\n| \'\'\'Fetterly, D., Manasse, M., Najork, M., & Wiener, J.\'\'\' (2003). [http://dl.acm.org/citation.cfm?id=775246|A Large-Scale Study of the Evolution of Web Pages]]. \'\'In [http://www2003.org/ WWW2003]: Proceedings of the 12th International World Wide Web Conference\'\' (pp.&nbsp;669 \xe2\x80\x93 678). Budapest, Hungary. May 20\xe2\x80\x9324: ACM Press. || 2003 || WWW || T-Dynamics ||\n|-\n| \'\'\'Ntoulas, A., Cho, J., & Olston, C.\'\'\' (2004). [http://dl.acm.org/citation.cfm?id=988674 What\'s New on the Web?: the Evolution of the Web from a Search Engine Perspective]. In [http://www2004.org WWW2004]: Proceedings of the 13th International World Wide Web Conference (pp.&nbsp;1 \xe2\x80\x93 12). New York, NY, United States. May 17\xe2\x80\x9322: ACM Press. || 2004 || WWW || T-Dynamics ||\n|-\n| \'\'\'Vlachos, M., Meek, C., Vagena, Z., & Gunopulos, D.\'\'\' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&nbsp;131 \xe2\x80\x93 142). Paris, France. June 13\xe2\x80\x9318: ACM Press. || 2004 || SIGMOD || T-Dynamics ||\n|-\n| \'\'\'Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., & Grossman, D.\'\'\' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]. \'\'In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology\'\', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||\n|-\n| \'\'\'Jones, R., & Diaz, F.\'\'\' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. \'\'In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]\'\', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||\n|-\n| \'\'\'Bordino, I., Boldi, P., Donato, D., Santini, M., & Vigna, S.\'\'\' (2008). [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4734022&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F4733896%2F4733897%2F04734022.pdf%3Farnumber%3D4734022 Temporal Evolution of the UK Web]. In [http://compbio.cs.uic.edu/adn-icdm08/ ADN2008]: Proceedings of the 1st International Workshop on Analysis of Dynamic Networks associated to [http://icdm08.isti.cnr.it/ ICDM2008]: IEEE International Conference on Data Mining (pp.&nbsp;909 \xe2\x80\x93 918). Pisa, Italy. December 19: IEEE Computer Society Press. || 2008 || ICDM - ADN || T-Dynamics ||\n|-\n| \'\'\'Adar, E., Teevan, J., Dumais, S. T., & Elsas, J. L.\'\'\' (2009). [http://portal.acm.org/citation.cfm?id=1498837 The Web Changes Everything: Understanding the Dynamics of Web Content]. \'\'In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;282 \xe2\x80\x93 291). Barcelona, Spain. February 9\xe2\x80\x9312: ACM Press. || 2009 || WSDM || T-Dynamics ||\n|-\n| \'\'\'Metzler, D., Jones, R., Peng, F., & Zhang, R.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. \'\'In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;700 \xe2\x80\x93 701). Boston, MA, United States. July 19\xe2\x80\x9323: ACM Press. || 2009 || SIGIR || TQ-Understanding ||\n|-\n| \'\'\'Elsas, J. L., & Dumais, S. T.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. \'\'In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;1 \xe2\x80\x93 10). New York, United States. February 3\xe2\x80\x9306: ACM Press. || 2010 || WSDM || T-Dynamics ||\n|-\n| \'\'\'Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. \'\'In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference\'\' (pp.&nbsp;1123 \xe2\x80\x93 1124). Raleigh, United States. April 26\xe2\x80\x9330: ACM Press. || 2010 || WWW || F-IRetrieval ||\n|-\n| \'\'\'Aji, A., Agichtein, E.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=2175298.2175332 Deconstructing Interaction Dynamics in Knowledge Sharing Communities]. \'\'In [http://sbp.asu.edu/sbp2010/sbp10.html]: Third International Conference on Social Computing, Behavioral-Cultural Modeling, & Prediction\'\' (pp.&nbsp;273 \xe2\x80\x93 281). Washington DC, United States. March 30\xe2\x80\x9331: Springer-Verlag. || 2010 || SBP || T-Dynamics ||\n|-\n| \'\'\'Kulkarni, A., Teevan, J., Svore, K. M., & Dumais, S. T.\'\'\' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]. \'\'In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;167 \xe2\x80\x93 176). Hong Kong, China. February 9\xe2\x80\x9312: ACM Press. || 2011 || WSDM || T-Dynamics ||\n|-\n| \'\'\'Campos, R., Dias, G., & Jorge, A. M.\'\'\' (2011). [http://ceur-ws.org/Vol-707/TWAW2011.pdf What is the Temporal Value of Web Snippets?] \'\'In [http://temporalweb.net/page3/page3.html TWAW 2011]: Proceedings of the 1st International Temporal Web Analytics Workshop associated to [http://www.www2011india.com/ WWW2011]: 20th International World Wide Web Conference\'\'. Hyderabad, India. March 28.: CEUR Workshop Proceedings. || 2011 || WWW - TWAW || T-Dynamics ||\n|-\n| \'\'\'Campos, R., Jorge, A., & Dias, G.\'\'\' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. \'\'In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2011 Conference on Research and Development in Information Retrieval\'\', (pp.&nbsp;13 \xe2\x80\x93 16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||\n|-\n| \'\'\'Shokouhi, M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. In [http://www.sigir2011.org/ SIGIR2011]: \'\'In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information\'\' (pp.&nbsp;1171 \xe2\x80\x93 1172). Beijing, China. July 24\xe2\x80\x9328: ACM Press. || 2011 || SIGIR || T-Dynamics ||\n|-\n| \'\'\'Dias, G., Campos, R., & Jorge, A.\'\'\' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] \'\'In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\'. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||\n|-\n| \'\'\'Campos, R., Dias, G., & Jorge, A. M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. \'\'In L. Antunes, & H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence\'\' (Vol. 7026/2011, pp.&nbsp;581 \xe2\x80\x93 596). Lisboa, Portugal. October 10\xe2\x80\x9313: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||\n|-\n| \'\'\'Jatowt, A., & Yeung, C. M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. \'\'In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;1259 \xe2\x80\x93 1264). Glasgow, Scotland, UK. October 24 - 28: ACM Press. || 2011 || CIKM || F-IRetrieval ||\n|-\n| \'\'\'Yeung, C.-m. A., & Jatowt, A.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]. \'\'In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;1231 \xe2\x80\x93 1240). Glasgow, Scotland, UK. October 24\xe2\x80\x9328: ACM Press. || 2011 || CIKM || C-Memory ||\n|-\n| \'\'\'Costa, M., & Silva, M. J., & Couto, F. M.\'\'\' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. \'\'In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference\'\' (pp.&nbsp;757--766). Gold Coast, Australia. July 6\xe2\x80\x9311: ACM Press. || 2014 || SIGIR || T-RModels ||\n|}\n\n== Temporal markup languages (T-MLanguages) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Setzer, A., & Gaizauskas, R.\'\'\' (2000). [ftp://ftp.dcs.shef.ac.uk/home/robertg/papers/lrec00-tempann.pdf Annotating Events and Temporal Information in Newswire Texts]. \'\'In [http://www.xanthi.ilsp.gr/lrec/ LREC2000]: Proceedings of the 2nd International Conference on Language Resources and Evaluation\'\'. Athens, Greece. May 31 - June 2: ELDA. || 2000 || LREC || T-MLanguages ||\n|-\n| \'\'\'Setzer, A.\'\'\' (2001). [http://www.andrea-setzer.org.uk/PAPERS/thesis.pdf Temporal Information in Newswire Articles: An Annotation Scheme and Corpus Study]. Sheffield, UK: University of Sheffield. || 2001 || Phd Thesis || T-MLanguages ||\n|-\n| \'\'\'Ferro, L., Mani, I., Sundheim, B., & Wilson, G.\'\'\' (2001). [http://www.timeml.org/site/terqas/readings/MTRAnnotationGuide_v1_02.pdf TIDES Temporal Annotation Guidelines]. Version 1.0.2. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2001 || Technical Report || T-MLanguages ||\n|-\n| \'\'\'Pustejovsky, J., Casta\xc3\xb1o, J., Ingria, R., Sauri, R., Gaizauskas, R., Setzer, A., et al.\'\'\' (2003). TimeML: Robust Specification of Event and Temporal Expression in Text. \'\'In [http://iwcs.uvt.nl/iwcs5/index.htm IWCS2003]: Proceedings of the 5th International Workshop on Computational Semantics\'\', (pp.&nbsp;28 \xe2\x80\x93 34). Tilburg, Netherlands. January 15\xe2\x80\x9317. || 2003 || IWCS || T-MLanguages ||\n|-\n| \'\'\'Ferro, L., Gerber, L., Mani, I., Sundheim, B., & Wilson, G.\'\'\' (2005). [http://projects.ldc.upenn.edu/ace/docs/English-TIMEX2-Guidelines_v0.1.pdf TIDES 2005 Standard for the Annotation of Temporal Expressions]. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2005 || Technical Report || T-MLanguages ||\n|}\n\n== Temporal taggers (T-taggers) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| [http://www.timeml.org/site/tarsqi/toolkit/manual/ TempEx Module] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] - \'\'\'Mani, I., & Wilson, G.\'\'\' (2000). [[dl.acm.org/citation.cfm?id=1075228|Robust Temporal Processing of News]]. \'\'In [http://www.cse.ust.hk/acl2000/ ACL2000]: Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics\'\' (pp.&nbsp;69 \xe2\x80\x93 76). Hong Kong, China. October 1\xe2\x80\x938: Association for Computational Linguistics. || 2000 || ACL || T-Taggers ||\n|-\n| [http://www.aktors.org/technologies/annie/ Annie] - [http://gate.ac.uk/download/index.html GATE distribution] - \'\'\'Cunningham, H., Maynard, D., Bontcheva, K., & Tablan, V.\'\'\' (2002). [http://eprints.aktors.org/90/01/acl-main.pdf GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applications]. \'\'In [http://www.aclweb.org/mirror/acl2002/ ACL2002]: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics\'\' (pp.&nbsp;168 \xe2\x80\x93 175). Philadelphia, PA, United States. July 6\xe2\x80\x9312: Association for Computational Linguistics. || 2002 || ACL || T-Taggers ||\n|-\n| [http://www.timeml.org/site/tarsqi/modules/gutime/download.html GUTime] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] || 2002 ||  || T-Taggers ||\n|-\n| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - \'\'\'Str\xc3\xb6tgen, J., & Gertz, M.\'\'\' (2010). [http://delivery.acm.org/10.1145/1860000/1859735/p321-strotgen.pdf?ip=188.80.124.88&acc=OPEN&CFID=82473711&CFTOKEN=13661527&__acm__=1337002719_1b05141ffc83e798f400c972756d43ad HeidelTime: High Quality Rule-based Extraction and Normalization of Temporal Expressions]. \'\'In [http://semeval2.fbk.eu/semeval2.php SemEval2010]: Proceedings of the 5th International Workshop on Semantic Evaluation associated to [http://acl2010.org/ ACL2010]: 41st Annual Meeting of the Association for Computational Linguistics\'\', (pp.&nbsp;321 \xe2\x80\x93 324). Uppsala, Sweden. July 11\xe2\x80\x9316.|| 2010 || ACL - SemEval || T-Taggers ||\n|-\n| [http://www.timen.org/ TIMEN] \'\'\'Llorens, H., Derczynski, L., Gaizauskas, R. & Saquete, E.\'\'\' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/128_Paper.pdf TIMEN: An Open Temporal Expression Normalisation Resource]. \'\'In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation\'\'. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||\n|-\n| \'\'\'Chang, A., & Manning, C.\'\'\' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/284_Paper.pdf SUTIME: A Library for Recognizing and Normalizing Time Expressions]. \'\'In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation\'\'. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||\n|-\n| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - \'\'\'Str\xc3\xb6tgen, J., & Gertz, M.\'\'\' (2012). [http://www.springerlink.com/content/64767752451075k8/ Multilingual and cross-domain temporal tagging]. \'\'In [http://www.springerlink.com/content/1574-020x/ LRE]: Language Resources and Evaluation\'\', 1 - 30.|| 2012 || LRE || T-Taggers ||\n|-\n| [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ ManTIME] - \'\'\'Filannino, M., Brown, G. & Nenadic G.\'\'\' (2013). [http://www.aclweb.org/anthology/S/S13/S13-2.pdf#page=89 ManTIME: Temporal expression identification and normalization in the TempEval-3 challenge]. \'\'In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013)\'\', 53 - 57, Atlanta, Georgia, June 14-15, 2013.|| 2013 || ACL - SemEval || T-Taggers || [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ online demo]\n|}\n\n== Temporal indexing (T-indexing) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Alonso, O., & Gertz, M.\'\'\' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. \'\'In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;597 \xe2\x80\x93 598). Seattle, Washington, United States. August 6\xe2\x80\x9311: ACM Press. || 2006 || SIGIR || T-Clustering ||\n|-\n| \'\'\'Berberich, K., Bedathur, S., Neumann, T., & Weikum, G.\'\'\' (2007). [http://dl.acm.org/citation.cfm?id=1277831 A Time Machine for Text Search]. \'\'In [http://www.sigir.org/sigir2007 SIGIR 2007]: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;519 \xe2\x80\x93 526). Amsterdam, Netherlands. July 23\xe2\x80\x9327: ACM Press. || 2007 || SIGIR || W-Archives ||\n|-\n| \'\'\'Jin, P., Lian, J., Zhao, X., & Wan, S.\'\'\' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. \'\'In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application\'\' (pp.&nbsp;220 \xe2\x80\x93 224). Shanghai, China. December 21\xe2\x80\x9322: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||\n|-\n| \'\'\'Song, S., & JaJa, J.\'\'\' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||\n|-\n| \'\'\'Pasca, M.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. \'\'In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing\'\' (pp.&nbsp;1117 \xe2\x80\x93 1121). Fortaleza, Ceara, Brazil. March 16\xe2\x80\x9320: ACM Press. || 2008 || SAC || T-QAnswering ||\n|-\n| \'\'\'Alonso, O., Gertz, M., & Baeza-Yates, R.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. \'\'In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management\'\'. Hong Kong, China. November 2\xe2\x80\x936: ACM Press. || 2009 || CIKM || T-Clustering ||\n|-\n| \'\'\'Arikan, I., Bedathur, S., & Berberich, K.\'\'\' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. \'\'In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining\'\'. Barcelona, Spain. February 9\xe2\x80\x9312: ACM Press. || 2009 || WSDM ||| T-RModels ||\n|-\n| \'\'\'Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.\'\'\' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. \'\'In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval\'\', (pp.&nbsp;41 \xe2\x80\x93 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\n|-\n| \'\'\'Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1871437.1871528 Efficient temporal keyword search over versioned text]. \'\'In [http://www.yorku.ca/cikm10/ CIKM2010]: Proceedings of the 19th ACM international conference on Information and knowledge management\'\', (pp.&nbsp;699-708). Toronto, Canada. October 26-30. ACM Press. || 2010 || CIKM || W-Archives||\n|-\n| \'\'\'Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2009991 Temporal index sharding for space-time efficiency in archive search]. \'\'In [http://www.sigir.org/sigir2011/ SIGIR2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\', (pp.&nbsp;545-554). Beijing, China. July 24-28. ACM Press. || 2011 || SIGIR || T-Indexing||\n|-\n| \'\'\'Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. \'\'In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\', (pp.&nbsp;235 \xe2\x80\x93 243). Portland, United States. August 12-16. ACM Press. || 2012 || SIGIR || W-Archives ||\n|}\n\n== Temporal query understanding (TQ-understanding) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Vlachos, M., Meek, C., Vagena, Z., & Gunopulos, D.\'\'\' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&nbsp;131 \xe2\x80\x93 142). Paris, France. June 13\xe2\x80\x9318: ACM Press. || 2004 || SIGMOD || T-Dynamics ||\n|-\n| \'\'\'Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., & Grossman, D.\'\'\' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]]. \'\'In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology\'\', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||\n|-\n| \'\'\'Jones, R., & Diaz, F.\'\'\' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]]. \'\'In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]\'\', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||\n|-\n| \'\'\'Dakka, W., Gravano, L., & Ipeirotis, P. G.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]]. \'\'In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;1437 \xe2\x80\x93 1438). Napa Valley, California, United States. October 26\xe2\x80\x9330: ACM Press. || 2008 || CIKM || TQ-Understanding ||\n|-\n| \'\'\'Diaz, F.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1498825 Integration of News Content into Web Results]. \'\'In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;182 \xe2\x80\x93 191). Barcelona, Spain. February 9\xe2\x80\x9312: ACM Press. || 2009 || WSDM || TQ-Understanding ||\n|-\n| \'\'\'Metzler, D., Jones, R., Peng, F., & Zhang, R.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. \'\'In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;700 \xe2\x80\x93 701). Boston, MA, United States. July 19\xe2\x80\x9323: ACM Press. || 2009 || SIGIR || TQ-Understanding ||\n|-\n| \'\'\'K\xc3\xb6nig, A.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1572002 Click-Through Prediction for News Queries]. \'\'In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;347 \xe2\x80\x93 354). Boston, MA, United States. July 19\xe2\x80\x9323: ACM Press. || 2009 || SIGIR || TQ-Understanding ||\n|-\n| \'\'\'Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. \'\'In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication\'\' (pp.&nbsp;166 \xe2\x80\x93 175). Suwon, Republic of Korea. January 14\xe2\x80\x9315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\n|-\n| \'\'\'Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: \'\'In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;11 \xe2\x80\x93 20). New York, United States. February 3\xe2\x80\x936: ACM Press. || 2010 || WSDM || T-RModels ||\n|-\n| \'\'\'Kanhabua, N., & N\xc3\xb8rv\xc3\xa5g, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. \'\'In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries\'\'. Glasgow, Scotland. September 6\xe2\x80\x9310: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||\n|-\n| \'\'\'Zhang, R., Konda, Y., Dong, A., Kolari, P., Chang, Y., & Zheng, Z.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1870768 Learning Recurrent Event Queries for Web Search]. \'\'In [http://www.lsi.upc.edu/events/emnlp2010/ EMNLP2010]: Proceedings of the Conference on Empiral Methods in Natural Language Processing\'\' (pp.&nbsp;1129 \xe2\x80\x93 1139). Massachusetts, United States. October 9\xe2\x80\x9311: Association for Computational Linguistics. || 2010 || EMNLP || TQ-Understanding ||\n|-\n| \'\'\'Kulkarni, A., Teevan, J., Svore, K. M., & Dumais, S. T.\'\'\' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]]. \'\'In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;167 \xe2\x80\x93 176). Hong Kong, China. February 9\xe2\x80\x9312: ACM Press. || 2011 || WSDM || T-Dynamics ||\n|-\n| \'\'\'Campos, R.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2010182 Using k-top Retrieved Web Snippets to Date Temporal Implicit Queries based on Web Content Analysis]. \'\'In [http://www.sigir2011.org/%20 SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (p.&nbsp;1325). Beijing, China. July 24\xe2\x80\x9328.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||\n|-\n| \'\'\'Campos, R., Jorge, A., & Dias, G.\'\'\' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. \'\'In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2005 Conference on Research and Development in Information Retrieval\'\', (pp.&nbsp;13 \xe2\x80\x93 16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||\n|-\n| \'\'\'Shokouhi, M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. \'\'In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;1171 \xe2\x80\x93 1172). Beijing, China. July 24\xe2\x80\x9328.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||\n|-\n| \'\'\'Campos, R., Dias, G., Jorge, A., & Nunes, C.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2169103&CFID=102654836&CFTOKEN=48651941 Enriching Temporal Query Understanding through Date Identification: How to Tag Implicit Temporal Queries?] \'\'In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference\'\' (pp.&nbsp;41 \xe2\x80\x93 48). Lyon, France. April 17.: ACM - DL. || 2012 || WWW - TWAW || TQ-Understanding ||\n|-\n| \'\'\'Shokouhi, M., & Radinsky, K.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2348364 Time-Sensitive Query Auto-Completion]. \'\'In [http://www.sigir.org/sigir2012/ SIGIR 2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;601 \xe2\x80\x93 610). Portland, United States. August 12\xe2\x80\x9316.: ACM Press. || 2012 || SIGIR || TQ-Understanding ||\n|-\n| \'\'\'Campos, R., Dias, G., Jorge, A., & Nunes, C.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2398567&dl=ACM&coll=DL&CFID=204979644&CFTOKEN=99312511 GTE: A Distributional Second-Order Co-Occurrence Approach to Improve the Identification of Top Relevant Dates] \'\'In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;2035 \xe2\x80\x93 2039). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || TQ-Understanding ||\n|-\n| \'\'\'Campos, R., Jorge, A., Dias, G., & Nunes, C.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] \'\'In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,\'\' Vol. 1, (pp.&nbsp;1 \xe2\x80\x93 8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||\n|}\n\n== Time-aware retrieval/ranking models (T-RModels) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Li, X., & Croft, B. W.\'\'\' (2003). [http://dl.acm.org/citation.cfm?doid=956863.956951 Time-Based Language Models]. \'\'In CIKM 2003: Proceedings of the 12th International ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;469 \xe2\x80\x93 475). New Orleans, Louisiana, United States. November 2\xe2\x80\x938: ACM Press. || 2003 || CIKM || T-RModels ||\n|-\n| \'\'\'Sato, N., Uehara, M., & Sakai, Y.\'\'\' (2003). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=1232026&contentType=Conference+Publications Temporal Information Retrieval in Cooperative Search Engine]. \'\'In [http://www.dexa.org/previous/dexa2003/cfp/dexa.html DEXA2003]: Proceedings of the 14th International Workshop on Database and Expert Systems Applications\'\' (pp.&nbsp;215 \xe2\x80\x93 220). Prague, Czech Republic. September 1\xe2\x80\x935: IEEE. || 2003 || DEXA || T-RModels ||\n|-\n| \'\'\'Berberich, K., Vazirgiannis, M., & Weikum, G.\'\'\' (2005). [http://projecteuclid.org/DPubS?verb=Display&version=1.0&service=UI&handle=euclid.im/1150474885&page=record Time-Aware Authority Ranking]. \'\'In [http://www.tandf.co.uk/journals/journal.asp?issn=1542-7951&linktype=44 IM: Internet Mathematics]\'\', 2(3), 301 - 332. || 2005 || IM || T-RModels ||\n|-\n| \'\'\'Cho, J., Roy, S., & Adams, R.\'\'\' (2005). [http://dl.acm.org/citation.cfm?id=1066220 Page Quality: In Search of an Unbiased Web Ranking]. In [http://cimic.rutgers.edu/~sigmod05/ SIGMOD2005]: Proceedings of the International Conference on Management of Data (pp.&nbsp;551 \xe2\x80\x93 562). Baltimore, United States. June 13\xe2\x80\x9316: ACM Press. || 2005 || SIGMOD || T-RModels ||\n|-\n| \'\'\'Perki\xc3\xb6, J., Buntine, W., & Tirri, H.\'\'\' (2005). [http://dl.acm.org/citation.cfm?id=1076171 A Temporally Adaptative Content-Based Relevance Ranking Algorithm]. \'\'In [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;647 \xe2\x80\x93 648). Salvador, Brazil. August 15\xe2\x80\x9316: ACM Press. || 2005 || SIGIR || T-RModels ||\n|-\n| \'\'\'Jones, R., & Diaz, F.\'\'\' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. \'\'In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]\'\', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||\n|-\n| \'\'\'Pasca, M.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. \'\'In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing\'\' (pp.&nbsp;1117 \xe2\x80\x93 1121). Fortaleza, Ceara, Brazil. March 16\xe2\x80\x9320: ACM Press. || 2008 || SAC || T-QAnswering ||\n|-\n| \'\'\'Dakka, W., Gravano, L., & Ipeirotis, P. G.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]. \'\'In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;1437 \xe2\x80\x93 1438). Napa Valley, California, United States. October 26\xe2\x80\x9330: ACM Press. || 2008 || CIKM || TQ-Understanding ||\n|-\n| \'\'\'Jin, P., Lian, J., Zhao, X., & Wan, S.\'\'\' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. \'\'In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application\'\' (pp.&nbsp;220 \xe2\x80\x93 224). Shanghai, China. December 21\xe2\x80\x9322: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||\n|-\n| \'\'\'Arikan, I., Bedathur, S., & Berberich, K.\'\'\' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. \'\'In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining\'\'. Barcelona, Spain. February 9\xe2\x80\x9312: ACM Press. || 2009 || WSDM ||| T-RModels ||\n|-\n| \'\'\'Zhang, R., Chang, Y., Zheng, Z., Metzler, D., & Nie, J.-y.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1620899 Search Result Re-ranking by Feedback Control Adjustment for Time-sensitive Query]. \'\'In [http://www.naaclhlt2009.org/ NAACL2009]: Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies\'\', (pp.&nbsp;165 \xe2\x80\x93 168). Boulder, Colorado, United States. May 31 - June 5. || 2009 || NAACL || T-RModels ||\n|-\n| \'\'\'Metzler, D., Jones, R., Peng, F., & Zhang, R.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. \'\'In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;700 \xe2\x80\x93 701). Boston, MA, United States. July 19\xe2\x80\x9323: ACM Press. || 2009 || SIGIR || TQ-Understanding ||\n|-\n| \'\'\'Alonso, O., Gertz, M., & Baeza-Yates, R.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. \'\'In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management\'\'. Hong Kong, China. November 2\xe2\x80\x936: ACM Press. || 2009 || CIKM || T-Clustering ||\n|-\n| \'\'\'Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. \'\'In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication\'\' (pp.&nbsp;166 \xe2\x80\x93 175). Suwon, Republic of Korea. January 14\xe2\x80\x9315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\n|-\n| \'\'\'Elsas, J. L., & Dumais, S. T.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. \'\'In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;1 \xe2\x80\x93 10). New York, United States. February 3\xe2\x80\x9306: ACM Press. || 2010 || WSDM || T-Dynamics ||\n|-\n| \'\'\'Aji, A., Wang, Y., Agichtein, E., Gabrilovich, E.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1871519 Using the Past to Score the Present: Extending Term Weighting Models Through Revision History Analysis] \'\'In [http://www.cikm2010.org/ CIKM 2010]: Proceedings of the 19th ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;629 \xe2\x80\x93 638). Toronto, ON, Canada. October 26 - October 30: ACM Press. || 2010 || CIKM || T-RModels ||\n|-\n| \'\'\'Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: \'\'In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;11 \xe2\x80\x93 20). New York, United States. February 3\xe2\x80\x936: ACM Press. || 2010 || WSDM || T-RModels ||\n|-\n| \'\'\'Berberich, K., Bedathur, S., Alonso, O., & Weikum, G.\'\'\' (2010). [http://www.springerlink.com/content/b193008160713350/ A Language Modeling Approach for Temporal Information Needs]. In C. Gurrin, Y. He, G. Kazai, U. Kruschwitz, S. Little, T. Roelleke, et al. (Eds.), \'\'In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://kmi.open.ac.uk/events/ecir2010/ ECIR 2010]: 32nd European Conference on Information Retrieval\'\' (Vol. 5993/2010, pp.&nbsp;13 \xe2\x80\x93 25). Milton Keynes, UK. March 28\xe2\x80\x9331: Springer Berlin / Heidelberg. || 2010 || ECIR || T-RModels ||\n|-\n| \'\'\'Dong, A., Zhang, R., Kolari, P., Jing, B., Diaz, F., Chang, Y., Zheng, Z., & Zha, H.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1772725&dl=ACM&coll=DL&CFID=204979644&CFTOKEN=99312511 Time is of the Essence: Improving Recency Ranking Using Twitter Data]. \'\'In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference\'\' (pp.&nbsp;331 \xe2\x80\x93 340). Raleigh, United States. April 26\xe2\x80\x9330: ACM Press. || 2010 || WWW || T-RModels ||\n|-\n| \'\'\'Inagaki, Y., Sadagopan, N., Dupret, G., Dong, A., Liao, C., Chang, Y., & Zheng, Z.\'\'\' (2010). [http://labs.yahoo.com/files/aaai10_recencyfeature_2.pdf Session Based Click Features for Recency Ranking]. \'\'In [http://www.aaai.org/Conferences/AAAI/aaai10.php AAAI2010]: Proceedings of the 24th AAAI Conference on Artificial Intelligence\'\' (pp.&nbsp;331 \xe2\x80\x93 340). Atlanta, United States. June 11\xe2\x80\x9315: AAAI Press. || 2010 || AAAI || T-RModels ||\n|-\n| \'\'\'Dai, N., & Davison, B.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1835471 Freshness Matters: In Flowers, Food, and Web Authority]. \'\'In [http://www.sigir2010.org/doku.php SIGIR 2010]: Proceedings of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;114 \xe2\x80\x93 121). Geneve, Switzerland. July 19\xe2\x80\x9323: ACM Press. || 2010 || SIGIR || T-RModels ||\n|-\n| \'\'\'Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.\'\'\' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. \'\'In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval\'\', (pp.&nbsp;41 \xe2\x80\x93 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\n|-\n| \'\'\'Kanhabua, N., & N\xc3\xb8rv\xc3\xa5g, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. \'\'In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries\'\'. Glasgow, Scotland. September 6\xe2\x80\x9310: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||\n|-\n| \'\'\'Efron, M., & Golovchinsky, G.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009984 Estimation Methods for Ranking Recent Information]. \'\'In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;495 \xe2\x80\x93 504). Beijing, China. July 24\xe2\x80\x9328.: ACM Press. || 2011 || SIGIR || T-RModels ||\n|-\n| \'\'\'Dai, N., Shokouhi, M., & Davison, B. D.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009933 Learning to Rank for Freshness and Relevance]. \'\'In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;95 \xe2\x80\x93 104). Beijing, China. July 24\xe2\x80\x9328.: ACM Press. || 2011 || SIGIR || T-RModels ||\n|-\n| \'\'\'Kanhabua, N., Blanco, R., & Matthews, M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2010018&dl=ACM&coll=DL&CFID=102654836&CFTOKEN=48651941 Ranking Related News Predictions]. \'\'In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;755 \xe2\x80\x93 764). Beijing, China. July 24\xe2\x80\x9328: ACM Press. || 2011 || SIGIR || F-IRetrieval ||\n|-\n| \'\'\'Chang, P-T., Huang, Y-C., Yang, C-L., Lin, S-D., & Cheng, P-J.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2348489 Learning-Based Time-Sensitive Re-Ranking for Web Search]. \'\'In Proceedings of the [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval\'\', (pp.&nbsp;1101 \xe2\x80\x93 1102). Portland, United States. August 12 - 16. || 2012 || SIGIR || T-RModels ||\n|-\n| \'\'\'Efron, M.\'\'\' (2012). [http://research.microsoft.com/en-us/people/milads/efrontemporalwsv02.pdf Query-Specific Recency Ranking: Survival Analysis for Improved Microblog Retrieval]. \'\'In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval\'\'. Portland, United States. August 16. || 2012 || SIGIR - TAIA || T-RModels ||\n|-\n| \'\'\'Kanhabua, N., & N\xc3\xb8rv\xc3\xa5g, K.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2398667 Learning to Rank Search Results for Time-Sensitive Queries] \'\'In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;2463 \xe2\x80\x93 2466). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || T-RModels ||\n|-\n| \'\'\'Kim G., and Xing E. P.\'\'\' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. \'\'In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;163 \xe2\x80\x93 172). Rome, Italy. February 4\xe2\x80\x938: ACM Press. || 2013 || WSDM || T-IRetrieval ||\n|-\n| \'\'\'Costa, M., & Silva, M. J., & Couto, F. M.\'\'\' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. \'\'In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference\'\' (pp.&nbsp;757--766). Gold Coast, Australia. July 6\xe2\x80\x9311: ACM Press. || 2014 || SIGIR || T-RModels ||\n|}\n\n== Temporal clustering (T-clustering) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Shaparenko, B., Caruana, R., Gehrke, J., & Joachims, T.\'\'\' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. \'\'In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]\'\' (pp.&nbsp;165 \xe2\x80\x93 174). Houston, United States. November 27\xe2\x80\x9330: IEEE Press. || 2005 || ICDM - TDM || TDT ||\n|-\n| \'\'\'Alonso, O., & Gertz, M.\'\'\' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. \'\'In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;597 \xe2\x80\x93 598). Seattle, Washington, United States. August 6\xe2\x80\x9311: ACM Press. || 2006 || SIGIR || T-Clustering ||\n|-\n| \'\'\'Mori, M., Miura, T., & Shioya, I.\'\'\' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. \'\'In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence\'\' (pp.&nbsp;338 \xe2\x80\x93 342). Hong Kong, China. December 18\xe2\x80\x9322: IEEE Computer Society Press. || 2006 || WIC || TDT ||\n|-\n| \'\'\'Alonso, O., Baeza-Yates, R., & Gertz, M.\'\'\' (2007). Exploratory Search Using Timelines. \'\'In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems\'\'. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||\n|-\n| \'\'\'Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. \'\'In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries\'\' (pp.&nbsp;115 \xe2\x80\x93 124). Austin, United States. June 15\xe2\x80\x9319.: ACM Press. || 2009 || JCDL || F-IRetrieval ||\n|-\n| \'\'\'Campos, R., Dias, G., & Jorge, A.\'\'\' (2009). [http://www.ccc.ipt.pt/~ricardo/ficheiros/KDIR2009.pdf Disambiguating Web Search Results By Topic and Temporal Clustering: A Proposal]. In [http://www.kdir.ic3k.org/ KDIR2009]: Proceedings of the International Conference on Knowledge Discovery and Information Retrieval, (pp.&nbsp;292 \xe2\x80\x93 296). Funchal - Madeira, Portugal. October 6\xe2\x80\x938. || 2009 || KDIR || T-Clustering ||\n|-\n| \'\'\'Alonso, O., Gertz, M., & Baeza-Yates, R.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. \'\'In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management\'\'. Hong Kong, China. November 2\xe2\x80\x936: ACM Press. || 2009 || CIKM || T-Clustering ||\n|-\n| \'\'\'Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. \'\'In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication\'\' (pp.&nbsp;166 \xe2\x80\x93 175). Suwon, Republic of Korea. January 14\xe2\x80\x9315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\n|-\n| \'\'\'Jatowt, A., & Yeung, C. M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. \'\'In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;1259 \xe2\x80\x93 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||\n|-\n| \'\'\'Campos, R., Jorge, A., Dias, G., & Nunes, C.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] \'\'In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,\'\' Vol. 1, (pp.&nbsp;1 \xe2\x80\x93 8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||\n|}\n\n== Temporal text classification (T-classification) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Jong, F., Rode, H., & Hiemstra, D.\'\'\' (2006). [http://doc.utwente.nl/66448/ Temporal Language Models for the Disclosure of Historical Text]. \'\'In [http://www.dans.knaw.nl/en AHC2005]: Proceedings of the XVIth International Conference of the Association for History and Computing\'\' (pp.&nbsp;161 \xe2\x80\x93 168). Amsterdam, Netherlands. September 14\xe2\x80\x9317 || 2005 || AHC || T-Classification ||\n|-\n| \'\'\'Toyoda, M., & Kitsuregawa, M.\'\'\' (2006). [http://dl.acm.org/citation.cfm?id=1135777.1135815 What\'s Really New on the Web? Identifying New Pages from a Series of Unstable Web Snapshots]. \'\'In [http://www2006.org WWW2006]: Proceedings of the 15th International World Wide Web Conference\'\' (pp.&nbsp;233 \xe2\x80\x93 241). Edinburgh, Scotland. May 23\xe2\x80\x9326: ACM Press. || 2006 || WWW || T-Classification ||\n|-\n| \'\'\'Nunes, S., Ribeiro, C., & David, G.\'\'\' (2007). [http://dl.acm.org/citation.cfm?id=1316924 Using Neighbors to Date Web Documents]. \'\'In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 9th ACM International Workshop on Web Information and Data Management associated to [[www2.fc.ul.pt/cikm2007|CIKM2007]]: 16th International Conference on Knowledge and Information Management\'\' (pp.&nbsp;129 \xe2\x80\x93 136). Lisboa, Portugal. November 9: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||\n|-\n| \'\'\'Jatowt, A., Kawai, Y., & Tanaka, K.\'\'\' (2007). [http://dl.acm.org/citation.cfm?id=1316925 Detecting Age of Page Content]. \'\'In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 8th International Workshop on Web Information and Data Management associated to [http://www2.fc.ul.pt/cikm2007 CIKM2007]: 16th International Conference on Knowledge and Information Management\'\' (pp.&nbsp;137 \xe2\x80\x93 144). Lisbon. Portugal. November 9.: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||\n|-\n| \'\'\'Kanhabua, N., & N\xc3\xb8rv\xc3\xa5g, K.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1429902 Improving Temporal Language Models for Determining Time of Non-timestamped Documents]. \'\'In Christensen-Dalsgaard, B., Castelli, D., Jurik, B. A., Lippincott, J. (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2008.org/ ECDL 2008]: 12th European Conference on Research and Advances Technology for Digital Libraries\'\' (Vol. 5173/2008, pp.&nbsp;358 \xe2\x80\x93 370). Aarhus, Denmark. September 14\xe2\x80\x9319: Springer Berlin / Heidelberg. || 2008 || ECDL || T-Classification ||\n|-\n| \'\'\'Jatowt, A., & Yeung, C. M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. \'\'In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;1259 \xe2\x80\x93 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||\n|-\n| \'\'\'Str\xc3\xb6tgen, J., Alonso, O., & Gertz, M.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2169095.2169102&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Identification of Top Relevant Temporal Expressions in Documents]. \'\'In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference\'\' (pp.&nbsp;33 \xe2\x80\x93 40). Lyon, France. April 17: ACM - DL. || 2012 || WWW - TWAW || T-Classification ||\n|-\n| \'\'\'Filannino, M., and Nenadic, G.\'\'\' (2014). [http://www.aclweb.org/anthology/W/W14/W14-4502.pdf Mining temporal footprints from Wikipedia]. \'\'In Proceedings of the First AHA!-Workshop on Information Discovery in Text\'\' (Dublin, Ireland, August 2014), Association for Computational Linguistics and Dublin City University, pp. 7\xe2\x80\x9313. || 2014 || COLING || T-Classification || [http://www.cs.man.ac.uk/~filannim/projects/temporal_footprints/ online demo]\n|}\n\n== Temporal visualization (T-interfaces) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Swan, R., & Allan, J.\'\'\' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. \'\'In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;49 \xe2\x80\x93 56). Athens, Greece. July 24\xe2\x80\x9328: ACM Press. || 2000 || SIGIR || TDT ||\n|-\n| \'\'\'Swan, R., & Jensen, D.\'\'\' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. \'\'In M. Grobelnik, D. Mladenic, & N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\'\' (pp.&nbsp;73 \xe2\x80\x93 80). Boston, Massachusetts, United States. August 20\xe2\x80\x9323: ACM Press. || 2000 || KDD - TM || TDT ||\n|-\n| [http://books.google.com/ngrams Google Ngram Viewer] ||  ||  || T-Interfaces ||\n|-\n| \'\'\'Cousins, S., & Kahn, M.\'\'\' (1991). [http://www.sciencedirect.com/science/article/pii/093336579190005V The Visual Display of Temporal Information]. (E. Keravnou, Ed.) \'\'In AIM: Artificial Intelligence in Medicine\'\', 3(6), 341 - 357. || 1991 || AIM || T-Interfaces ||\n|-\n| \'\'\'Karam, G. M.\'\'\' (1994). [http://dl.acm.org/citation.cfm?id=187157 Visualization Using Timelines]. In T. J. Ostrand (Ed.), \'\'ISSTA1994: Proceedings of the International Symposium on Software Testing and Analysis associated to SIGSOFT: ACM Special Interest Group on Software Engineering\'\' (pp.&nbsp;125 \xe2\x80\x93 137). Seattle, Washington, United States. August 17\xe2\x80\x9319: ACM Press. || 1994 || ISSTA || T-Interfaces ||\n|-\n| \'\'\'Plaisant, C., Miiash, B., Rose, A., Widoff, S., & Shneiderman, B.\'\'\' (1996). [http://dl.acm.org/citation.cfm?id=238493 LifeLines: Visualizing Personal Histories]. \'\'In [http://www.sigchi.org/chi96/proceedings/index.htm CHI1996]: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems\'\' (pp.&nbsp;221 \xe2\x80\x93 227). Vancouver, British Columbia, Canada. April 13\xe2\x80\x9318: ACM Press. || 1996 || CHI || T-Interfaces ||\n|-\n| \'\'\'Toyoda, M., & Kitsuregawa, M.\'\'\' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. \'\'In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia\'\' (pp.&nbsp;151 \xe2\x80\x93 160). Salzburg, Austria. September 6\xe2\x80\x939: ACM Press. || 2005 || HT || W-Archives ||\n|-\n| \'\'\'Efendioglu, D., Faschetti, C., & Parr, T.\'\'\' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]. In \'\'D. Wolber, N. Calder, & ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering\'\' (pp.&nbsp;119 \xe2\x80\x93 120). Palo Alto, California, United States. July 11\xe2\x80\x9314: ACM Press. || 2006 || ICWE || W-Archives ||\n|-\n| \'\'\'Catizone, R., Dalli, A., & Wilks, Y.\'\'\' (2006). [http://www.lrec-conf.org/proceedings/lrec2006/pdf/702_pdf.pdf Evaluating Automatically Generated Timelines from the Web]. \'\'In [http://www.lrec-conf.org/lrec2006/ LREC2006]: Proceedings of the 5th International Conference on Language Resources and Evaluation\'\'. Genoa, Italy. May 24\xe2\x80\x9326: ELDA. || 2006 || LREC || T-Interfaces ||\n|-\n| \'\'\'Mori, M., Miura, T., & Shioya, I.\'\'\' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. \'\'In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence\'\' (pp.&nbsp;338 \xe2\x80\x93 342). Hong Kong, China. December 18\xe2\x80\x9322: IEEE Computer Society Press. || 2006 || WIC || TDT ||\n|-\n| \'\'\'Alonso, O., Baeza-Yates, R., & Gertz, M.\'\'\' (2007). Exploratory Search Using Timelines. \'\'In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems\'\'. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||\n|-\n| \'\'\'Jatowt, A., Kawai, Y., & Tanaka, K.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1367497.1367736 Visualizing Historical Content of Web pages]]. \'\'In [http://www2008.org/ WWW2008]: Proceedings of the 17th International World Wide Web Conference\'\' (pp.&nbsp;1221 \xe2\x80\x93 1222). Beijing, China. April 21\xe2\x80\x9325: ACM Press. || 2008 || WWW || W-Archives ||\n|-\n| \'\'\'Nunes, S., Ribeiro, C., & David, G.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1822292 WikiChanges - Exposing Wikipedia Revision Activity]. \'\'In [http://www.wikisym.org/ws2008/ WikiSym2008]: Proceedings of the 4th International Symposium on Wikis\'\'. Porto, Portugal. September 8\xe2\x80\x9310: ACM Press. || 2008 || WikiSym || T-Interfaces ||\n|-\n| \'\'\'Nunes, S., Ribeiro, C., & David, G.\'\'\' (2009). [http://epia2009.web.ua.pt/onlineEdition/601.pdf Improving Web User Experience with Document Activity Sparklines]. \'\'In L. S. Lopes, N. Lau, P. Mariano, & L. Rocha (Ed.), [http://epia2009.web.ua.pt EPIA2009]: Proceedings of the 14th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence\'\', (pp.&nbsp;601 \xe2\x80\x93 604). Aveiro, Portugal. October 12\xe2\x80\x9315. || 2009 || EPIA || T-Interfaces ||\n|-\n| \'\'\'Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. \'\'In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication\'\' (pp.&nbsp;166 \xe2\x80\x93 175). Suwon, Republic of Korea. January 14\xe2\x80\x9315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\n|-\n| \'\'\'Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.\'\'\' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. \'\'In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval\'\', (pp.&nbsp;41 \xe2\x80\x93 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\n|-\n| \'\'\'Khurana, U., Nguyen V., Cheng H., Ahn, J., Chen X., & Shneiderman, B.\'\'\' (2011). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6113166 Visual Analysis of Temporal Trends in Social Networks Using Edge Color Coding and Metric Timelines]. \'\'In [http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6112285]: Proceedings of the IEEE Social Computing\'\', (pp.&nbsp;549 \xe2\x80\x93 554). Boston, United States. || 2011 || SocialCom || T-Interfaces ||\n|}\n\n== Temporal search engines (T-SEngine) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Alonso, O., & Gertz, M.\'\'\' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. \'\'In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;597 \xe2\x80\x93 598). Seattle, Washington, United States. August 6\xe2\x80\x9311: ACM Press. || 2006 || SIGIR || T-Clustering ||\n|-\n| \'\'\'Alonso, O., Baeza-Yates, R., & Gertz, M.\'\'\' (2007). Exploratory Search Using Timelines. \'\'In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems\'\'. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||\n|-\n| \'\'\'Jin, P., Lian, J., Zhao, X., & Wan, S.\'\'\' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. \'\'In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application\'\' (pp.&nbsp;220 \xe2\x80\x93 224). Shanghai, China. December 21\xe2\x80\x9322: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||\n|-\n| \'\'\'Alonso, O., Gertz, M., & Baeza-Yates, R.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. \'\'In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management\'\'. Hong Kong, China. November 2\xe2\x80\x936: ACM Press. || 2009 || CIKM || T-Clustering ||\n|-\n| \'\'\'Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. \'\'In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication\'\' (pp.&nbsp;166 \xe2\x80\x93 175). Suwon, Republic of Korea. January 14\xe2\x80\x9315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\n|-\n| \'\'\'Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.\'\'\' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. \'\'In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval\'\', (pp.&nbsp;41 \xe2\x80\x93 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\n|}\n\n== Temporal question answering (T-QAnswering) ==\n{| class="wikitable sortable"\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Pasca, M.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. \'\'In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing\'\' (pp.&nbsp;1117 \xe2\x80\x93 1121). Fortaleza, Ceara, Brazil. March 16\xe2\x80\x9320: ACM Press. || 2008 || SAC || T-QAnswering ||\n|}\n\n== Temporal snippets (T-snippets) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Alonso, O., Baeza-Yates, R., & Gertz, M.\'\'\' (2009). [http://www.wssp.info/2009/WSSP2009AlonsoBaezaYatesGertz.pdf Effectiveness of Temporal Snippets]. \'\'In [http://www.wssp.info/2009.html WSSP2009]: Proceedings of the Workshop on Web Search Result Summarization and Presentation associated to [[www2009.org/|WWW2009]]: 18th International World Wide Web Conference\'\'. Madrid, Spain. April 20\xe2\x80\x9324: ACM Press. || 2009 || WWW - WSSP || T-Snippets ||\n|-\n| \'\'\'Alonso, O., Gertz, M., & Baeza-Yates, R.\'\'\' (2011). [http://www.springerlink.com/content/u78qu8x10h613471/ Enhancing Document Snippets Using Temporal Information]. \'\'In R. Grossi, F. Sebastiani, & F. Silvestri (Eds.), Lecture Notes in Computer Science, [http://spire2011.isti.cnr.it/ SPIRE2011]: 18th International Symposium on String Processing and Information Retrieval\'\' (Vol. 7024, pp.&nbsp;26 \xe2\x80\x93 31). Pisa, Italy. October 17\xe2\x80\x9321.: Springer Berlin / Heidelberg. || 2011 || SPIRE || T-Snippets ||\n|-\n| \'\'\'Svore, K. M., Teevan, J., Dumais, S. T., & Kulkarni, A.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2348461 Creating Temporally Dynamic Web Search Snippets]. \'\'In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\', (pp.&nbsp;1045 \xe2\x80\x93 1046). Portland, United States. August 12-16. ACM Press  || 2012 || SIGIR || T-Snippets ||\n|}\n\n== Future information retrieval (F-IRetrieval) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Baeza-Yates, R.\'\'\' (2005). [http://www.dcs.vein.hu/CIR/cikkek/searching_the_future.pdf Searching the Future]. \'\'In S. Dominich, I. Ounis, & J.-Y. Nie (Ed.), MFIR2005: Proceedings of the Mathematical/Formal Methods in Information Retrieval Workshop associated to [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\'. Salvador, Brazil. August 15\xe2\x80\x9319: ACM Press. || 2005 || SIGIR - MFIR || F-IRetrieval ||\n|-\n| \'\'\'Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. \'\'In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries\'\' (pp.&nbsp;115 \xe2\x80\x93 124). Austin, United States. June 15\xe2\x80\x9319.: ACM Press. || 2009 || JCDL || F-IRetrieval ||\n|-\n| \'\'\'Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. \'\'In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication\'\' (pp.&nbsp;166 \xe2\x80\x93 175). Suwon, Republic of Korea. January 14\xe2\x80\x9315: ACM Press. || 2010 || ICIUMC || T-SEngine ||\n|-\n| \'\'\'Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. \'\'In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference\'\' (pp.&nbsp;1123 \xe2\x80\x93 1124). Raleigh, United States. April 26\xe2\x80\x9330: ACM Press. || 2010 || WWW || F-IRetrieval ||\n|-\n| \'\'\'Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.\'\'\' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. \'\'In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval\'\', (pp.&nbsp;41 \xe2\x80\x93 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\n|-\n| \'\'\'Dias, G., Campos, R., & Jorge, A.\'\'\' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] \'\'In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\'. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||\n|-\n| \'\'\'Kanhabua, N., Blanco, R., & Matthews, M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2010018&dl=ACM&coll=DL&CFID=82290723&CFTOKEN=53881602 Ranking Related News Predictions]. \'\'In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;755 \xe2\x80\x93 764). Beijing, China. July 24\xe2\x80\x9328: ACM Press. || 2011 || SIGIR || F-IRetrieval ||\n|-\n| \'\'\'Kanazawa, K., Jatowt, A., & Tanaka, K.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2052362 Improving Retrieval of Future-Related Information in Text Collections]. \'\'In [http://liris.cnrs.fr/~wi-iat11/WI 2011/ WIC2011]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence\'\' (pp.&nbsp;278 \xe2\x80\x93 283). Lyon, France. August 22\xe2\x80\x9327: IEEE Computer Society Press. || 2011 || WIC || F-IRetrieval ||\n|-\n| \'\'\'Campos, R., Dias, G., & Jorge, A. M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. \'\'In L. Antunes, & H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence\'\' (Vol. 7026/2011, pp.&nbsp;581 \xe2\x80\x93 596). Lisboa, Portugal. October 10\xe2\x80\x9313: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||\n|-\n| \'\'\'Jatowt, A., & Yeung, C. M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. \'\'In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;1259 \xe2\x80\x93 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||\n|-\n| \'\'\'Weerkamp, W., & Rijke, M.\'\'\' (2012). [http://research.microsoft.com/en-us/people/milads/taia2012-activities.pdf Activity Prediction: A Twitter-based Exploration]. \'\'In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval\'\'. Portland, United States. August 16. || 2012 || SIGIR - TAIA || F-IRetrieval ||\n|-\n| \'\'\'Radinski, K., & Horvitz, E.\'\'\' (2013). [http://dl.acm.org/citation.cfm?id=2433431 Mining the Web to Predict Future Events]. \'\'In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;255 \xe2\x80\x93 264). Rome, Italy. February 4\xe2\x80\x938: ACM Press. || 2013 || WSDM || F-IRetrieval ||\n|}\n\n== Temporal image retrieval (T-IRetrieval) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Dias, G., Moreno, J. G., Jatowt, A., & Campos, R.\'\'\' (2012). [http://link.springer.com/content/pdf/10.1007%2F978-3-642-34109-0_21 Temporal Web Image Retrieval]. In Calder\xc3\xb3n-Benavides, L., Gonz\xc3\xa1lez-Caro, C., Ch\xc3\xa1vez, E., Ziviani, N. (Eds.), \'\'In Lecture Notes in Computer Science - [http://catic.unab.edu.co/spire/ SPIRE2012]: 19th International Symposium on String Processing and Information Retrieval\'\' (Vol. 7608/2012, pp.&nbsp;199 \xe2\x80\x93 204). Cartagena de Indias, Colombia. October 21\xe2\x80\x9325: Springer Berlin / Heidelberg. || 2012 || SPIRE || T-IRetrieval ||\n|-\n| \'\'\'Palermo, F., Hays, J., & Efros, A.\'\'\' (2012). [http://link.springer.com/content/pdf/10.1007/978-3-642-33783-3_36 Dating Historical Color Images]. In Fitzgibbon, A., Lazebnik, S., Sato, Y., Schmid, C. (Eds.), \'\'In Lecture Notes in Computer Science - [http://eccv2012.unifi.it/ ECCV2012]: 12th European Conference on Computer Vision\'\' (Vol. 7577/2012, pp.&nbsp;499 \xe2\x80\x93 512). Firenze, Italy. October 07\xe2\x80\x9313: Springer Berlin / Heidelberg. || 2012 || ECCV || T-IRetrieval ||\n|-\n| \'\'\'Kim, G., & Xing, E. P.\'\'\' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. \'\'In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;163 \xe2\x80\x93 172). Rome, Italy. February 4\xe2\x80\x938: ACM Press. || 2013 || WSDM || T-IRetrieval ||\n|-\n| \'\'\'Martin, P., Doucet, A., & Jurie, F.\'\'\' (2014). [http://dl.acm.org/citation.cfm?id=2578790 Dating Color Images with Ordinal Classification]. \'\'In [http://www.icmr2014.org/ ICMR2014]: Proceedings of International Conference on Multimedia Retrieval\'\' (pp. 447). Glasgow, United Kingdom. April 01-04: ACM Press. || 2014 || ICMR || T-IRetrieval ||\n|}\n\n== Collective memory (C-memory) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Surowiecki, J.\'\'\' (2004). [http://www.amazon.com/The-Wisdom-Crowds-Collective-Economies/dp/0385503865 The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations]. USA: DoubleDay. || 2004 ||  || C-Memory ||\n|-\n| \'\'\'Hall, D., Jurafsky, D., & Manning, C. D.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1613715.1613763 Studying the History of Ideas using Topic Models]. \'\'In [http://conferences.inf.ed.ac.uk/emnlp08 EMNLP 2008]: Proceedings of the Conference on Empirical Methods in Natural Language Processing\'\' (pp.&nbsp;363 \xe2\x80\x93 371). Waikiki, Honolulu, Hawaii. October 25\xe2\x80\x9327: Association for Computational Linguistics. || 2008 || EMNLP || C-Memory ||\n|-\n| \'\'\'Shahaf, D., & Guestrin, C.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1835884 Connecting the dots between News Articles]]. In [http://www.sigkdd.org/kdd2010/ KDD2010]: Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp.&nbsp;623 \xe2\x80\x93 632). Washington, United States. July 25\xe2\x80\x9328: ACM Press. || 2010 || KDD || C-Memory ||\n|-\n| \'\'\'Takahashi, Y., Ohshima, H., Yamamoto, M., Iwasaki, H., Oyama, S., & Tanaka, K.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=1995980 Evaluating Significance of Historical Entities based on Tempo-spatial Impacts Analysis using Wikipedia Link Structure]]. \'\'In [http://www.ht2011.org/ HT2011]: Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia\'\' (pp.&nbsp;83 \xe2\x80\x93 92). Eindhoven, Netherlands. June 6\xe2\x80\x939: ACM Press. || 2011 || HT || C-Memory ||\n|-\n| \'\'\'Michel, J.-B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., Team, T. G., et al.\'\'\' (2011). [http://www.sciencemag.org/content/331/6014/176 Quantitative Analysis of Culture Using Millions of Digitized Books]. In [http://www.sciencemag.org/ Science], 331(6014), 176 - 182. || 2011 || Science || C-Memory ||\n|-\n| \'\'\'Yeung, C.-m. A., & Jatowt, A.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]]. \'\'In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;1231 \xe2\x80\x93 1240). Glasgow, Scotland, UK. October 24\xe2\x80\x9328: ACM Press. || 2011 || CIKM || C-Memory ||\n|}\n\n== Web archives (W-archives) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| [[List of Web archiving initiatives|List of Web Archive Initiatives]] || 2011 ||  || W-Archives ||\n|-\n| \'\'\'Kahle, B.\'\'\' (1997, 03). [http://www.sciamdigital.com/index.cfm?fa=Products.ViewIssuePreview&ISSUEID_CHAR=00B8E369-1805-4A27-A331-9D727FEAC21&ARTICLEID_CHAR=00B10B9E-5F13-40B2-AA51-0A4D5C41549 Preserving the Internet]. \'\'In [https://www.scientificamerican.com/sciammag/ Scientific American Magazine]\'\', 276(3), pp.&nbsp;72 \xe2\x80\x93 73. || 1997 || SAM || W-Archives ||\n|-\n| \'\'\'Toyoda, M., & Kitsuregawa, M.\'\'\' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. \'\'In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia\'\' (pp.&nbsp;151 \xe2\x80\x93 160). Salzburg, Austria. September 6\xe2\x80\x939: ACM Press. || 2005 || HT || W-Archives ||\n|-\n| \'\'\'Efendioglu, D., Faschetti, C., & Parr, T.\'\'\' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]]. In \'\'D. Wolber, N. Calder, & ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering\'\' (pp.&nbsp;119 \xe2\x80\x93 120). Palo Alto, California, United States. July 11\xe2\x80\x9314: ACM Press. || 2006 || ICWE || W-Archives ||\n|-\n| \'\'\'Jatowt, A., Kawai, Y., Nakamura, S., Kidawara, Y., & Tanaka, K.\'\'\' (2006). [http://dl.acm.org/citation.cfm?id=1149969 Journey to the Past: Proposal of a Framework for Past Web Browser]. \'\'In HT2006: Proceedings of the 17th Conference on Hypertext and Hypermedia\'\' (pp.&nbsp;135 \xe2\x80\x93 144). Odense, Denmark. August 22\xe2\x80\x9325: ACM Press. || 2006 || HT || W-Archives ||\n|-\n| \'\'\'Adar, E., Dontcheva, M., Fogarty, J., & Weld, D. S.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1449756 Zoetrope: Interacting with the Ephemeral Web]]. \'\'In S. B. Cousins, & M. Beaudouin-Lafon (Ed.), [http://www.acm.org/uist/uist2008/ UIST 2008]: Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology\'\' (pp.&nbsp;239 \xe2\x80\x93 248). Monterey, CA, United States. October 19\xe2\x80\x9322: ACM Press. || 2008 || UIST || W-Archives ||\n|-\n| \'\'\'Song, S., & JaJa, J.\'\'\' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||\n|-\n| \'\'\'Gomes, D., Miranda, J., & Costa, M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2042590 A Survey on Web Archiving Initiatives]]. \'\'In [http://www.tpdl2011.org/ TPDL2011]: Proceedings of the 15th international conference on Theory and practice of digital libraries: research and advanced technology for digital libraries\'\' (pp.&nbsp;408 \xe2\x80\x93 420). Berlin, Germany. September 25\xe2\x80\x9329: Springer-Verlag || 2011 || TPDL || W-Archives ||\n|-\n| \'\'\'Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. \'\'In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\', (pp.&nbsp;235 \xe2\x80\x93 243). Portland, United States. August 12-16. ACM Press || 2012 || SIGIR || W-Archives ||\n||\n|-\n| \'\'\'Costa, M., & Silva, M.J.\'\'\' (2012). [http://link.springer.com/chapter/10.1007%2F978-3-642-35063-4_32 Evaluating Web Archive Search Systems]. \'\'In [http://www.wise2012.cs.ucy.ac.cy/ WISE2012]: Proceedings of the 13th International Conference on Web Information System Engineering\'\', (pp.&nbsp;440 - 454). Paphos, Cyprus. November 28-30. Springer-Verlag || 2012 || WISE || W-Archives ||\n|}\n\n== Topic detection and tracking (TDT) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Allan, J., Carbonell, J., Doddington, G., & Yamron, J.\'\'\' (1998). [http://www.cs.pitt.edu/~chang/265/proj10/sisref/1.pdf Topic Detection and Tracking Pilot Study Final Report]. In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, (pp.&nbsp;194 \xe2\x80\x93 218). Lansdowne, Virginia, United States. February. || 1998 || Technical Report || TDT ||\n|-\n| \'\'\'Swan, R., & Allan, J.\'\'\' (1999). [http://dl.acm.org/citation.cfm?id=319956 Extracting Significant Time-Varying Features from Text]]. \'\'In [http://cikmconference.org/1999/ CIKM 1999]]: Proceedings of the 8th International ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;38 \xe2\x80\x93 45). Kansas City, Missouri, United States. November 2\xe2\x80\x936: ACM Press. || 1999 || CIKM || TDT ||\n|-\n| \'\'\'Swan, R., & Jensen, D.\'\'\' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. \'\'In M. Grobelnik, D. Mladenic, & N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\'\' (pp.&nbsp;73 \xe2\x80\x93 80). Boston, Massachusetts, United States. August 20\xe2\x80\x9323: ACM Press. || 2000 || KDD - TM || TDT ||\n|-\n| \'\'\'Swan, R., & Allan, J.\'\'\' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. \'\'In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;49 \xe2\x80\x93 56). Athens, Greece. July 24\xe2\x80\x9328: ACM Press. || 2000 || SIGIR || TDT ||\n|-\n| \'\'\'Makkonen, J., & Ahonen-Myka, H.\'\'\' (2003). [http://www.springerlink.com/content/a5ev5br7wwh5lvyl/ Utilizing Temporal Information in Topic Detection and Tracking]. \'\'In T. Koch, & I. T. Solvberg (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2003.org/ ECDL 2003]: 7th European Conference on Research and Advances Technology for Digital Libraries\'\' (Vol. 2769/2004, pp.&nbsp;393 \xe2\x80\x93 404). Trondheim, Norway. August 17\xe2\x80\x9322: Springer Berlin / Heidelberg. || 2003 || ECDL || TDT ||\n|-\n| \'\'\'Shaparenko, B., Caruana, R., Gehrke, J., & Joachims, T.\'\'\' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. \'\'In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]\'\' (pp.&nbsp;165 \xe2\x80\x93 174). Houston, United States. November 27\xe2\x80\x9330: IEEE Press. || 2005 || ICDM - TDM || TDT ||\n|-\n| \'\'\'Mori, M., Miura, T., & Shioya, I.\'\'\' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. \'\'In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence\'\' (pp.&nbsp;338 \xe2\x80\x93 342). Hong Kong, China. December 18\xe2\x80\x9322: IEEE Computer Society Press. || 2006 || WIC || TDT ||\n|-\n| \'\'\'Kim, P., Myaeng, S.H.\'\'\' (2004). [http://dl.acm.org/citation.cfm?id=1039624 Usefulness of Temporal Information Automatically Extracted from News Articles for Topic Tracking]. \'\'In [http://talip.acm.org/index.htm TALIP]:Journal of ACM Transactions on Asian Language Information Processing\'\' (pp.&nbsp;227 \xe2\x80\x93 242). New York, United States. || 2004 || TALIP || TDT ||\n|}\n\n==References==\n{{reflist}}\n\n[[Category:Information retrieval]]'
p18
sg6
S'Temporal information retrieval'
p19
ssI7
(dp20
g2
S'http://en.wikipedia.org/wiki/European Conference on Information Retrieval'
p21
sg4
S"The '''European Conference on Information Retrieval''' (ECIR) is the main \nEuropean research conference for the presentation of new results in the field of [[information retrieval]] (IR).\nIt is organized by the [[Information Retrieval Specialist Group]] of the [[British Computer Society]] (BCS-IRSG).\n      \nThe event started its life as the ''Annual Colloquium on Information Retrieval Research'' in 1978 and was \nheld in the UK each year until 1998 when it was hosted in Grenoble, France. Since then the venue has\nalternated between the United Kingdom and continental Europe. To mark the metamorphosis\nfrom a small informal colloquium to a major event in the IR research calendar, the \nBCS-IRSG later renamed the event to ''European Conference on Information Retrieval''. In recent years,\nECIR has continued to grow and has become the major European forum for the discussion\nof research in the field of Information Retrieval. \n\nSome of the topics dealt with include:\n* IR models, techniques, and algorithms\n* IR applications\n* IR system architectures\n* Test and evaluation methods for IR\n* [[Natural Language Processing]] for IR\n* Distributed IR\n* Multimedia and cross-media IR\n\n==Time and Location==\n\nTraditionally, the ECIR is held in Spring, near the Easter weekend. Previous locations include\nthe following:\n\n* [[Amsterdam, Netherlands]], 2014 [http://ecir2014.org/]\n* [[Moscow, Russia]], 2013 [http://ecir2013.org/]\n* [[Barcelona, Spain]], 2012 [http://ecir2012.upf.edu/]\n* [[Dublin, Ireland]], 2011 [http://www.ecir2011.dcu.ie/]\n* [[Milton Keynes]], 2010 [http://kmi.open.ac.uk/events/ecir2010/]\n* [[Toulouse]], 2009 [http://ecir09.irit.fr/]\n* [[Glasgow]], 2008 [http://ecir2008.dcs.gla.ac.uk/]\n* [[Rome]], 2007 [http://ecir2007.fub.it/]\n* [[London]], 2006 [http://ecir2006.soi.city.ac.uk/]\n* [[Santiago de Compostela|Santiago]], 2005 [http://www-gsi.dec.usc.es/ecir05/]\n* [[Sunderland, Tyne and Wear|Sunderland]], 2004 [http://ecir04.sunderland.ac.uk/]\n* [[Pisa]], 2003 [http://ecir03.isti.cnr.it/]\n* [[Glasgow]], 2002 [http://irsg.bcs.org/past_ecir.php]*\n* [[Darmstadt]], 2001* (organized by GMD)\n* [[Cambridge]], 2000* (organized by Microsoft Research)\n* [[Glasgow]], 1999*\n* [[Grenoble]], 1998*\n* [[Aberdeen, Scotland|Aberdeen]], 1997*\n* [[Manchester]], 1996*\n* [[Crewe]], 1995* (organized by Manchester Metropolitan University)\n* [[Drymen]], Scotland, 1994* (organized by Strathclyde University)\n* [[Glasgow]], 1993* (organized by Strathclyde University)\n* [[Lancaster, Lancashire|Lancaster]], 1992*\n* [[Lancaster, Lancashire|Lancaster]], 1991*\n* [[Huddersfield]], 1990*\n* [[Huddersfield]], 1989*\n* [[Huddersfield]], 1988*\n* [[Glasgow]], 1987*\n* [[Glasgow]], 1986*\n* [[Bradford]], 1985*\n* [[Bradford]], 1984*\n* [[Sheffield]], 1983*\n* [[Sheffield]], 1982*\n* [[Birmingham]], 1981*\n* [[Leeds]], 1980*\n* [[Leeds]], 1979*\n\n<br /> *as the Annual Colloquium on Information Retrieval Research\n\nFuture locations include:\n* [[Vienna, Austria]], 2015 [http://www.ecir2015.org/]\n\n==External links==\n* [http://irsg.bcs.org/ecir.php Official page at the website of the British Computer Society]\n\n[[Category:Information retrieval]]\n[[Category:Computer science conferences]]"
p22
sg6
S'European Conference on Information Retrieval'
p23
ssI136
(dp24
g2
S'http://en.wikipedia.org/wiki/Pleade'
p25
sg4
S'{{Infobox software\n| name                   = Pleade-infoxbox\n| title                  = Pleade\n| logo                   = [[File:Pleade-logo.png]]\n| logo caption           = Logo de Pleade\n| screenshot             = <!-- [[File: ]] -->\n| caption                = \n| collapsible            = \n| author                 = AJLSM\n| developer              = AJLSM\n| released               = <!-- {{Start date|YYYY|MM|DD|df=yes/no}} -->\n| discontinued           = \n| latest release version = 3.4\n| latest release date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| latest preview version = <!-- 3.5 -->\n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->\n| programming language   = [[Java]], [[XSLT]], [[Apache Cocoon|Cocoon]]\n| operating system       = [[Unix-like]], [[Microsoft Windows]]\n| platform               = \n| size                   = \n| language               = French, English, German, Chinese\n| language count         = <!-- DO NOT include this parameter unless you know what it does -->\n| language footnote      = \n| status                 = Active\n| genre                  = Digital Library\n| license                = GNU General Public License\n| alexa                  = \n| website                = {{URL|http://www.pleade.com/}}\n}}\n\n\'\'\'Pleade\'\'\' is an open source [[search engine]] and browser for [[Finding aid|archival finding aids]] encoded in [[Encoded Archival Description|EAD]] (an XML standard for encoding archival finding aids). Based on the [[SDX]] platform, it is a very flexible web application.\n\n== History ==\nThe software was jointly started by the companies AJLSM and Anaphore and was originally intended for publication and dissemination only of archival research tools like EAD finding aids, but it has become a library portal and a medium for digital libraries.<ref>[http://www.digicult.info/downloads/dc_info_issue6_december_20031.pdf DigiCult.Info issue #6, page 16]</ref>\n\n==Technologies==\nPleade is published in GPL 3. It is based on the [[Apache Cocoon|Apache Cocoon framework]] and it works with the search engine SDX.\n\nIt is able to publish and distribute the following format : [[Encoded Archival Description|EAD]], [[Comma-separated values|CSV]] (internally converted to XML), [[XMLMarc]], [[Text Encoding Initiative|TEI]], [[Dublin Core]]. Support for [[METS]] and [[ALTO (XML)|ALTO]] is under active development.<ref>[http://pleade.com/ Pleade 2012 : les imprim\xc3\xa9s num\xc3\xa9ris\xc3\xa9s et les formats XML METS / ALTO]</ref>\n\n== Features ==\n* Customizable publication ;\n* Customizable index creation ;\n* Customizable search form ;\n* Simple and advanced search among publish documents ;\n* Federate search among different bases (e.g. EAD, METS) ;\n* basket (for database and for images), a search history, printing, etc. ;\n* document viewer supporting : [[JPEG]], [[TIFF]] and for high resolution TIFF and [[JPEG2000]] it use [http://iipimage.sourceforge.net/ IIPImage image server] ;\n* [[OAI-PMH]] repositories and expose them, by default, the format EAD, Dublin Core and [[Dublin Core#Qualified Dublin Core|Qualified DC]] ;\n* The viewer has a Pleade indexing module (paleographic) that can be used to permit correction of the OCR. This tool is a TEI export of data input. A workflow management allows annotators and validation records seized ;\n* Printing resulting and finding aids as PDF documents (with embedded images) ;\n* Compatible with standard archival format : [[Text Encoding Initiative|TEI]], [[BiblioML]] ;\n* Ability to import metadata from an [[Integrated library system|ILS]].\n\n=== Pleade-Entreprise ===\n* Pleade-Entreprise extended features to others XML format, such as [[METS]] and [[ALTO (XML)|ALTO]].\n\n== Examples ==\nThese are examples of websites based on Pleade:\n{{columns-list|2|\n* Archival portals\n** [http://archives-inventaires.loire-atlantique.fr/ Departmental records of Loire-Atlantique (AD 44) (AD 44)]\n** [http://gael.gironde.fr/ GAEL : GAEL: Gironde archives online]\n** [http://odysseo.org/ Odysseo: Resources for the history of immigration]\n** [http://taubira.anaphore.org/ Parliamentary work of Christiane Taubira]\n** [http://archivesetmanuscrits.bnf.fr/ Archives and manuscrits of the BNF French National Library]\n** [http://jubilotheque.upmc.fr/ Jubiloth\xc3\xa8que, UPMC\'s scientific digital library]\n** [http://lbf-ehess.ens-lyon.fr/pages/fonds.html Michel Foucault\'s Library "les Mots et les Choses" ENS]\n\n* Portals documentary\n** [http://www.michael-culture.org/fr/home Michael]\n** [http://www.numerique.culture.fr/mpf/pub-fr/index.html Digital Heritage]\n\n* Digital Libraries\n** Digital Library of Lille\n** Lille III\n** [http://archivesetmanuscrits.bnf.fr/ BNF: Archives and manuscripts (French National Library)]\n}}\n\n== Related resources ==\n* {{Official website|http://pleade.com}}\n* [http://demo.pleade.com Official demo]\n* [http://www.pleadeenpratique.org/ Pleade in practice]\n* [http://www.ajlsm.com/produits/sdx SDX]\n* [http://www.ajlsm.com AJLSM company]\n\n== References ==\n<references/>\n\n[[Category:Digital library software]]\n[[Category:Free software]]\n[[Category:Information retrieval]]\n[[Category:Archival science]]'
p26
sg6
S'Pleade'
p27
ssI10
(dp28
g2
S'http://en.wikipedia.org/wiki/Harshness'
p29
sg4
S"{{Original research|date=September 2007}}\n{{Other uses|Harsh (disambiguation)}}\n'''Harshness''' (also called '''raucousness'''), in [[music information retrieval]], is a Non-Contextual Low-Level Audio Descriptors (NLDs) that represents one dimension of the multi-dimensional [[psychoacoustic]] feature called as musical [[timbre]].\n\nClassical timbre\xe2\x80\x99 NLDs are [[surface roughness|roughness]], [[spectral centroid]], and [[spectral flux]]. While harmonicity and inharmonicity can also be considered NLDs, harshness differs from them, as well as from roughness, once it reckons for a distinguished perceptual audio feature expressed by the summary spectral periodicity. This feature is especially clear in single-[[Pitch (music)|pitch]], single-[[note]], musical audio, where the timbre of two different musical instruments can greatly differ in levels of harshness (e.g., the difference in harshness between a flute and a saxophone is evident). As it is supposed to be, harshness is independent of all others NLDs.\n\n[[Category:Musicology]]\n[[Category:Music technology]]\n[[Category:Information retrieval]]"
p30
sg6
S'Harshness'
p31
ssI139
(dp32
g2
S'http://en.wikipedia.org/wiki/Category:Personalized search'
p33
sg4
S'{{catmore}}\n\n[[Category:Information retrieval]]\n[[Category:Internet search engines| ]]'
p34
sg6
S'Category:Personalized search'
p35
ssI13
(dp36
g2
S"http://en.wikipedia.org/wiki/Mooers' law"
p37
sg4
S'{{For|the observation regarding integrated circuits|Moore\'s law}}\n{{Refimprove|date=September 2011}}\n\n\'\'\'Mooers\' law\'\'\' is an empirical observation of behavior made by American [[computer scientist]] [[Calvin Mooers]] in 1959. The observation is made in relation to [[information retrieval]] and the interpretation of the observation is used commonly throughout the information profession both within and outside its original context.\n\n{{quote|An information retrieval system will tend not to be used whenever it is more painful and troublesome for a customer to have information than for him not to have it.|[[Calvin Mooers]]<ref name="morville">{{cite book|url=http://books.google.com/books?id=xJNLJXXbhusC&printsec=frontcover&dq=isbn:9780596007652&hl=en&sa=X&ei=qvWhT5DfHITs2QX1rNzPCA&ved=0CDAQ6AEwAA#v=onepage&q=mooers\'%20law&f=false |title= Ambient findability |series= O\'Reilly Series. Marketing/Technology & Society |author= Peter Morville |edition= illustrated |publisher= O\'Reilly Media |year= 2005 |page= 44|isbn= 978-0-596-00765-2}}</ref>}}\n\n==Original interpretation==\n\nMooers argued that information is at risk of languishing unused due not only on the effort required to assimilate it but also to any fallout that could arise from the discovery of information that conflicts with the users personal, academic or corporate interests. In interacting with new information, a user runs the risk of proving their work incorrect or even irrelevant. Instead, Mooers argued, users prefer to remain in a state of safety in which new arguments are ignored in an attempt to save potential embarrassment or reprisal from supervisors.<ref>{{cite web|last=Mooers|first=Calvin|title=Mooers Law, or Why some Retrieval Systems are Used and Others Are not|url=http://findarticles.com/p/articles/mi_qa3633/is_199610/ai_n8749122/|work=Business Library|accessdate=25 October 2011}}</ref>\n\n==Out-of-context interpretation==\n\nThe more commonly used interpretation of Mooers\' law is considered to be a derivation of the [[principle of least effort]] first stated by [[George Kingsley Zipf]]. This interpretation focuses on the amount of effort that will be expended to use and understand a particular information retrieval system before the information seeker \'gives up\', and the Law is often paraphrased to increase the focus on the retrieval system:\n\n{{quote|The more difficult and time consuming it is for a customer to use an information system, the less likely it is that he will use that information system.|J. Michael Pemberton}}\n{{quote|Mooers\' Law tells us that information will be used in direct proportion to how easy it is to obtain.|Roger K. Summit <ref name="morville"/>}}\n\nIn this interpretation, "painful and troublesome" comes from \'\'using\'\' the retrieval system.\n\n==References==\n{{reflist}}\n\n*{{cite journal |last=Austin |first=Brice |date=June 2001 |title=Mooers\' Law: In and out of Context |journal=Journal of the American Society for Information Science and Technology |volume=25 |issue=8 |pages=pp 607\xe2\x80\x93609 |url=http://spot.colorado.edu/~norcirc/Mooers.html |accessdate=2007-05-23 |doi=10.1002/asi.1114}}\n\n==External links==\n* [http://special.lib.umn.edu/findaid/xml/cbi00081.xml Calvin N. Mooers Papers, 1930-1992] at the [[Charles Babbage Institute]], University of Minnesota.\n* [http://purl.umn.edu/107510 Oral history interview with Calvin N. Mooers and Charlotte D. Mooers] at the [[Charles Babbage Institute]].  Interview discusses information retrieval and programming language research from World War II through the early 1990s.\n* [http://www.phillyimc.org/en/gasoline-7-17-moors-law-kent-moors-authority Another empirical observation with a similar-sounding name is Moors\' law], named for Kent Moors of Duquesne University, which states crude oil prices double every five years. \n[[Category:Empirical laws]]\n[[Category:Library science]]\n[[Category:Information retrieval]]'
p38
sg6
S"Mooers' law"
p39
ssI142
(dp40
g2
S'http://en.wikipedia.org/wiki/Thesaurus (information retrieval)'
p41
sg4
S'{{about|thesauri used to support indexing, tagging or searching for information|thesauri used in general/literary applications|Thesaurus|the Clare Fischer album|Thesaurus (album)}}\n\nIn the context of [[information retrieval]], a \'\'\'thesaurus\'\'\' (plural: "thesauri") is a form of controlled vocabulary that seeks to dictate semantic manifestations of [[metadata]] in the indexing of content objects. A thesaurus serves to minimise semantic ambiguity by ensuring uniformity and consistency in the storage and retrieval of the manifestations of content objects. ANSI/NISO Z39.19-2005 defines a content object as "any item that is to be described for inclusion in an information retrieval system, website, or other source of information".<ref>ANSI & NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.11</ref> The thesaurus aids the assignment of preferred terms to convey semantic metadata associated with the content object.<ref>ANSI & NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.12</ref>\n\nA thesaurus serves to guide both an indexer and a searcher in selecting the same preferred term or combination of preferred terms to represent a given subject. [[ISO 25964]], the international standard for information retrieval thesauri, defines a thesaurus as a \xe2\x80\x9ccontrolled and structured vocabulary in which concepts are represented by terms, organized so that relationships between concepts are made explicit, and preferred terms are accompanied by lead-in entries for synonyms or quasi-synonyms.\xe2\x80\x9d\n\nA thesaurus is composed by at least three elements: 1-a list of words (or terms), 2-the relationship amongst the words (or terms), indicated by their hierarchical relative position (e.g. parent/broader term; child/narrower term, synonym, etc.), 3-a set of rules on how to use the thesaurus.\n\n== History ==\nWherever there have been large collections of information, whether on paper or in computers, scholars have faced a challenge in pinpointing the items they seek. The use of classification schemes to arrange the documents in order was only a partial solution. Another approach was to index the contents of the documents using words or terms, rather than classification codes. In the 1940s and 1950s some pioneers, such as [[Calvin Mooers]], Charles L. Bernier, [http://pubs.acs.org/cen/priestley/recipients/1951crane.html Evan J. Crane] and [[Hans Peter Luhn]], collected up their index terms in various kinds of list that they called a \xe2\x80\x9cthesaurus\xe2\x80\x9d (by analogy with the well known thesaurus developed by [[Peter Roget]]).<ref>Roberts, N. The pre-history of the information retrieval thesaurus. \'\'Journal of Documentation\'\', 40(4), 1984, p.271-285.</ref> The first such list put seriously to use in information retrieval was the thesaurus developed in 1959 at the E I Dupont de Nemours Company.<ref>Aitchison, J. and Dextre Clarke, S. The thesaurus: a historical viewpoint, with a look to the future. \'\'Cataloging & Classification Quarterly\'\', 37 (3/4), 2004, p.5-21.</ref><ref>Krooks, D.A. and Lancaster, F.W. The evolution of guidelines for thesaurus construction. \'\'Libri\'\', 43(4), 1993, p.326-342.</ref>\n\nThe first two of these lists to be published were the \'\'Thesaurus of ASTIA Descriptors\'\' (1960) and the \'\'Chemical Engineering Thesaurus\'\' of the American Institute of Chemical Engineers (1961), a descendant of the Dupont thesaurus. More followed, culminating in the influential \'\'Thesaurus of Engineering and Scientific Terms\'\' (TEST) published jointly by the Engineers Joint Council and the US Department of Defense in 1967. TEST did more than just serve as an example; its Appendix 1 presented \'\'Thesaurus rules and conventions\'\' that have guided thesaurus construction ever since.\nHundreds of thesauri have been produced since then, perhaps thousands. The most notable innovations since TEST have been:\n(a)\tExtension from monolingual to multilingual capability; and \n(b)\tAddition of a conceptually organized display to the basic alphabetical presentation.\n\nHere we mention only some of the national and international standards that have built steadily on the basic rules set out in TEST:\n\n* [[UNESCO]] \'\'Guidelines for the establishment and development of monolingual thesauri\'\'. 1970 (followed by later editions in 1971 and 1981)\n* DIN 1463 \'\'Guidelines for the establishment and development of monolingual thesauri\'\'. 1972 (followed by later editions)\n* ISO 2788 \'\'Guidelines for the establishment and development of monolingual thesauri\'\'. 1974 (revised 1986)\n* ANSI \'\'American National Standard for Thesaurus Structure, Construction, and Use\'\'. 1974 (revised 1980 and superseded by ANSI/NISO Z39.19-1993)\n* ISO 5964 \'\'Guidelines for the establishment and development of multilingual thesauri\'\'. 1985\n* ANSI/NISO Z39.19 \'\'Guidelines for the construction, format, and management of monolingual thesauri\'\'. 1993 (revised 2005 and renamed \'\'Guidelines for the construction, format, and management of monolingual controlled vocabularies\'\'.)\n* ISO 25964 \'\'Thesauri and interoperability with other vocabularies\'\'. Part 1 (\'\'Thesauri for information retrieval\'\' published 2011; Part 2 (\'\'Interoperability with other vocabularies\'\') published 2013.\n\nThe most clearly visible trend across this history of thesaurus development has been from the context of small-scale isolation to a networked world.<ref>Dextre Clarke, Stella G. and Zeng, Marcia Lei. [http://www.niso.org/publications/isq/2012/v24no1/clarke/ From ISO 2788 to ISO 25964: the evolution of thesaurus standards towards interoperability and data modeling] \'\'Information standards quarterly\'\', 24(1), 2012, p.20-26.</ref> Access to information was notably enhanced when thesauri crossed the divide between monolingual and multilingual applications. More recently, as can be seen from the titles of the latest ISO and NISO standards, there is a recognition that thesauri need to work in harness with other forms of vocabulary or knowledge organization system, such as subject heading schemes, classification schemes, taxonomies and ontologies. The official website for ISO 25964 gives more information, including a reading list.<ref>\'\'[http://www.niso.org/schemas/iso25964/ ISO 25964 \xe2\x80\x93 the international standard for thesauri and interoperability with other vocabularies.]\'\' National Information Standards Organization, 2013.</ref>\n\n== Purpose ==\nIn information retrieval, a thesaurus can be used as a form of controlled vocabulary to aid in the indexing of appropriate metadata for information bearing entities. A thesaurus helps with expressing the manifestations of a concept in a prescribed way, to aid in improving [[precision and recall]]. This means that the semantic conceptual expressions of information bearing entities are easier to locate due to uniformity of language. Additionally, a thesaurus is used for maintaining a hierarchical listing of terms; usually single words or bound phrases that aid the indexer in narrowing the terms and limiting semantic ambiguity.\n\nThe [[Art and Architecture Thesaurus|Art & Architecture Thesaurus]], for example, is used by countless museums around the world, to catalogue their collections. [[AGROVOC]], the thesaurus of the UN\xe2\x80\x99s [[Food and Agriculture Organization]], is used to index and/or search its AGRIS database of worldwide literature on agricultural research.\n\n== Structure ==\nInformation retrieval thesauri are formally organized so that existing relationships between concepts are made clear. For example, \xe2\x80\x9ccitrus fruits\xe2\x80\x9d might be linked to the broader concept of \xe2\x80\x9cfruits\xe2\x80\x9d, and the narrower ones of \xe2\x80\x9coranges\xe2\x80\x9d, \xe2\x80\x9clemons\xe2\x80\x9d, etc. When the terms are displayed online, the links between them make it very easy to surf around the thesaurus, selecting useful terms for a search. When a single term could have more than one meaning, like tables (furniture) or tables (data), these are listed separately so that the user can choose which concept to search for and avoid retrieving irrelevant results. For any one concept, all the known synonyms are listed, such as \xe2\x80\x9cmad cow disease\xe2\x80\x9d, \xe2\x80\x9cbovine spongiform encephalopathy\xe2\x80\x9d, \xe2\x80\x9cBSE\xe2\x80\x9d, etc. The idea is to guide all the indexers and all the searchers to use the same term for the same concept, so that search results will be as complete as possible. If the thesaurus is multilingual, equivalent terms in other languages are shown too. Following international standards, concepts are generally arranged hierarchically within facets or grouped by themes or topics. Unlike a general thesaurus used for literary purposes, information retrieval thesauri typically focus on one discipline, subject or field of study.\n\n== See also ==\n* [[Controlled vocabulary]]\n* [[ISO 25964]]\n* [[Thesaurus]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.niso.org/schemas/iso25964/ Official site for ISO 25964] \n* [http://www.taxonomywarehouse.com/ Taxonomy Warehouse]\n\n[[Category:Information retrieval]]\n[[Category:Thesauri]]'
p42
sg6
S'Thesaurus (information retrieval)'
p43
ssI16
(dp44
g2
S'http://en.wikipedia.org/wiki/Category:Data management'
p45
sg4
S"{{catdiffuse}}\n\n'''[[Data management]]''' comprises all the disciplines related to managing data as a valuable resource.\n\n\n{{Commons cat|Data management}}\n\n[[Category:Computer data|Management]]\n[[Category:Data|Management]]\n[[Category:Project management]]\n[[Category:Information retrieval]]\n[[Category:Information technology management]]"
p46
sg6
S'Category:Data management'
p47
ssI145
(dp48
g2
S'http://en.wikipedia.org/wiki/Schoolr'
p49
sg4
S'{{orphan|date=April 2010}}\n\'\'\'Schoolr\'\'\' is a [[front and back ends|front-end]] academic directory that features frequently used third-party search engines and resources from [[Google]], [[Wikipedia]], [[Reference.com]], [[Acronym Finder]], [[Wolfram Alpha]], [[Yahoo! Babel Fish]], and the [[University of North Carolina]].<ref>[http://www.lifehack.org/articles/technology/schoolr-google-wikipedia-dictionarycom-and-more-on-one-site.html "Schoolr: Google, Wikipedia, Dictionary.com, and more on one site]", \'\'Stepcase Lifehack\'\', March 19, 2007</ref>\n\nSchoolr went live on December 3, 2006<ref>[http://lifehacker.com/290027/schoolr-search-start-page "Schoolr search start page]", \'\'Lifehacker\'\', August 16, 2007</ref> and was developed by Sasan Aghdasi at the [[University of Victoria]].\n\n== References ==\n<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->\n{{Reflist}}\n\n== External links ==\n* [http://www.schoolr.com/ Schoolr]\n\n{{Internet search}}\n\n{{DEFAULTSORT:Schoolr}}\n[[Category:Information retrieval]]\n[[Category:Internet search engines]]\n[[Category:Internet terminology]]'
p50
sg6
S'Schoolr'
p51
ssI19
(dp52
g2
S'http://en.wikipedia.org/wiki/Information retrieval applications'
p53
sg4
S'Areas where [[information retrieval]] techniques are employed include (the entries are in alphabetical order within each category):\n\n==General applications of information retrieval==\n* [[Digital libraries]]\n*  [[Information filtering]]\n** [[Recommender systems]]\n*  Media search\n** Blog search\n** [[Image retrieval]]\n** [[Music information retrieval|Music retrieval]]\n** News search\n** Speech retrieval\n** Video retrieval\n* [[Search engines]]\n** [[Desktop search]]\n** [[Enterprise search]]\n** [[Federated search]]\n** [[Mobile search]]\n** [[Social search]]\n** [[Web search engine|Web search]]\n\n==Domain specific applications of information retrieval==\n* Expert search finding\n* Genomic information retrieval\n* [[Geographic information retrieval]]\n*  Information retrieval for chemical structures\n* Information retrieval in [[software engineering]]\n* [[Legal information retrieval]]\n* [[Vertical search]]\n\n==Other retrieval methods==\nMethods/Techniques in which [[information retrieval]] techniques are employed include:\n* [[Adversarial information retrieval]]\n* [[Automatic summarization]]\n**[[Multi-document summarization]]\n* [[Compound term processing]]\n* [[Cross-language information retrieval|Cross-lingual retrieval]]\n* [[Document classification]]\n* [[Spam filtering]]\n* [[Question answering]]\n\n== See also ==\n* [[Information retrieval]]\n\n{{DEFAULTSORT:Information Retrieval Applications}}\n[[Category:Information retrieval|*]]'
p54
sg6
S'Information retrieval applications'
p55
ssI148
(dp56
g2
S'http://en.wikipedia.org/wiki/Gerard Salton Award'
p57
sg4
S'The \'\'\'Gerard Salton Award\'\'\' is presented by the [[Association for Computing Machinery]] (ACM) [[Special Interest Group on Information Retrieval]] (SIGIR) every three years to an individual who has made "significant, sustained and continuing contributions to research in [[information retrieval]]". SIGIR also co-sponsors (with [[SIGWEB]]) the [[Vannevar Bush Award]], for the best paper at the [[Joint Conference on Digital Libraries]].\n\n==Chronological honorees and lectures==\n* 1983 - [[Gerard Salton]], [[Cornell University]] : "About the future of automatic information retrieval."\n* 1988 - [[Karen Sp\xc3\xa4rck Jones]], [[University of Cambridge]] : "A look back and a look forward."\n* 1991 - [[Cyril Cleverdon]], [[Cranfield Institute of Technology]] : "The significance of the Cranfield tests on index languages."\n* 1994 - William S. Cooper, [[University of California, Berkeley]] : "The formalism of probability theory in IR: a foundation or an encumbrance?"\n* 1997 - [[Tefko Saracevic]], [[Rutgers University]] : "Users lost (summary): reflections on the past, future, and limits of information science." \n* 2000 - [[Stephen Robertson (computer scientist)|Stephen E. Robertson]], [[City University, London|City University London]] : "On theoretical argument in information retrieval."<BR>\'\'\'For ...\'\'\' \'\'"Thirty years of significant, sustained and continuing contributions to research in information retrieval. Of special importance are the theoretical and empirical contributions to the development, refinement, and evaluation of probabilistic models of information retrieval."\'\'\n* 2003 - [[W. Bruce Croft]], [[University of Massachusetts Amherst]] : "Information retrieval and computer science: an evolving relationship."<BR>\'\'\'For ...\'\'\' \'\'"More than twenty years of significant, sustained and continuing contributions to research in information retrieval. His contributions to the theoretical development and practical use of [[Bayesian inference]] networks and [[language modelling]] for retrieval, and to their evaluation through extensive experiment and application, are particularly important. The Center for Intelligent Information Retrieval which he founded illustrates the strong synergies between fundamental research and its application to a wide range of practical information management problems."\'\'\n* 2006 - [[C. J. van Rijsbergen]], [[University of Glasgow]] : \t"Quantum haystacks."\n* 2009 - [[Susan Dumais]], [[Microsoft Research]] : "An Interdisciplinary Perspective on Information Retrieval."\n* 2012 - [[Norbert Fuhr]], [[University of Duisburg-Essen]]: "Information Retrieval as Engineering Science."\n\n==External links==\n* [http://www.acm.org/sigir/ ACM SIGIR homepage]\n* [http://www.sigir.org/awards/awards.html ACM SIGIR awards]\n\n[[Category:Association for Computing Machinery]]\n[[Category:Computer science awards]]\n[[Category:Information retrieval]]'
p58
sg6
S'Gerard Salton Award'
p59
ssI22
(dp60
g2
S'http://en.wikipedia.org/wiki/Audio mining'
p61
sg4
S'{{unreferenced|date=January 2012}}\n\'\'\'Audio mining\'\'\' is a technique by which the content of an audio signal can be automatically analysed and searched. It is most commonly used in the field of [[speech recognition|automatic speech recognition]], where the analysis tries to identify any speech within the audio. The audio will typically be processed by a speech recognition system in order to identify word or [[phoneme]] units that are likely to occur in the spoken content. This information may either be used immediately in pre-defined searches for keywords or phrases (a real-time "word spotting" system), or the output of the speech recogniser may be stored in an index file. One or more audio mining index files can then be loaded at a later date in order to run searches for keywords or phrases.\n\nThe results of a search will normally be in terms of hits, which are regions within files that are good matches for the chosen keywords. The user may then be able to listen to the audio corresponding to these hits in order to verify if a correct match was found.\n\nAudio mining systems used in the field of speech recognition are often divided into two groups: those that use [[Large Vocabulary Continuous Speech Recogniser]]s (LVCSR) and those that use phonetic recognition. \n\nMusical audio mining (also known as [[Music information retrieval]]) relates to the identification of perceptually important characteristics of a piece of music such as melodic, harmonic or rhythmic structure. Searches can then be carried out to find pieces of music that are similar in terms of their melodic, harmonic and/or rhythmic characteristics.\n\n==See also==\n* [[Speech Analytics]]\n\n\n[[Category:Speech recognition]]\n[[Category:Information retrieval]]\n[[Category:Computational linguistics]]'
p62
sg6
S'Audio mining'
p63
ssI151
(dp64
g2
S'http://en.wikipedia.org/wiki/Latent semantic indexing'
p65
sg4
S'\'\'\'Latent semantic indexing\'\'\' (\'\'\'LSI\'\'\') is an indexing and retrieval method that uses a mathematical technique called [[singular value decomposition]] (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text.  LSI is based on the principle that words that are used in the same contexts tend to have similar meanings.  A key feature of LSI is its ability to extract the conceptual content of a body of text by establishing associations between those terms that occur in similar contexts.<ref>Deerwester, S., et al, Improving Information Retrieval with Latent Semantic Indexing, Proceedings of the 51st Annual Meeting of the American Society for Information Science 25, 1988, pp. 36\xe2\x80\x9340.</ref>\n\nLSI is also an application of [[correspondence analysis]], a multivariate statistical technique developed by [[Jean-Paul Benz\xc3\xa9cri]]<ref>{{ cite book\n | author = Benz\xc3\xa9cri, J.-P.\n | publisher=Dunod |location= Paris, France\n | year = 1973\n | title = L\'Analyse des Donn\xc3\xa9es. Volume II. L\'Analyse des Correspondences\n }}</ref> in the early 1970s, to a [[contingency table]] built from word counts in documents.\n\nCalled Latent Semantic Indexing because of its ability to correlate semantically related terms that are latent in a collection of text, it was first applied to text at Bellcore in the late 1980s.   The method, also called [[latent semantic analysis]] (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches.  Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don\xe2\x80\x99t share a specific word or words with the search criteria.\n\n__TOC__\n\n== Benefits of LSI ==\n\nLSI overcomes two of the most problematic constraints of Boolean keyword queries:  multiple words that have similar meanings ([[synonymy]]) and words that have more than one meaning ([[polysemy]]).  Synonymy is often the cause of [[vocabulary mismatch|mismatches in the vocabulary]] used by the authors of documents and the users of information retrieval systems.<ref>{{cite doi|10.1145/32206.32212}}</ref><ref>{{cite doi|10.1145/1871437.1871474}}</ref>   As a result, Boolean or keyword queries often return irrelevant results and miss information that is relevant.\n\nLSI is also used to perform automated document categorization.  In fact, several experiments have demonstrated that there are a number of correlations between the way LSI and humans process and categorize text.<ref>Landauer, T., et al., Learning Human-like Knowledge by Singular Value Decomposition: A Progress Report, M. I. Jordan, M. J. Kearns & S. A. Solla (Eds.), Advances in Neural Information Processing Systems 10, Cambridge: MIT Press, 1998, pp. 45\xe2\x80\x9351.</ref>    Document categorization is the assignment of documents to one or more predefined categories based on their similarity to the conceptual content of the categories.<ref>{{cite doi|10.1145/288627.288651}}</ref>   LSI uses \'\'example\'\' documents to establish the conceptual basis for each category.  During categorization processing, the concepts contained in the documents being categorized are compared to the concepts contained in the example items, and a category (or categories) is assigned to the documents based on the similarities between the concepts they contain and the concepts that are contained in the example documents.\n\nDynamic clustering based on the conceptual content of documents can also be accomplished using LSI.  Clustering is a way to group documents based on their conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster.  This is very useful when dealing with an unknown collection of unstructured text.\n\nBecause it uses a strictly mathematical approach, LSI is inherently independent of language.  This enables LSI to elicit the semantic content of information written in any language without requiring the use of auxiliary structures, such as dictionaries and thesauri.  LSI can also perform cross-linguistic concept searching and example-based categorization.  For example, queries can be made in one language, such as English, and conceptually similar results will be returned even if they are composed of an entirely different language or of multiple languages.\n\nLSI is not restricted to working only with words.  It can also process arbitrary character strings.  Any object that can be expressed as text can be represented in an LSI vector space.<ref>Zukas, Anthony, Price, Robert J., Document Categorization Using Latent Semantic Indexing, White Paper, [[Content Analyst Company]], LLC</ref>   For example, tests with MEDLINE abstracts have shown that LSI is able to effectively classify genes based on conceptual modeling of the biological information contained in the titles and abstracts of the MEDLINE citations.<ref>{{cite doi|10.1093/bioinformatics/bth464}}</ref>\n\nLSI automatically adapts to new and changing terminology, and has been shown to be very tolerant of noise (i.e., misspelled words, typographical errors, unreadable characters, etc.).<ref>{{cite doi|10.1007/11427995_68}}</ref>   This is especially important for applications using text derived from Optical Character Recognition (OCR) and speech-to-text conversion.  LSI also deals effectively with sparse, ambiguous, and contradictory data.\n\nText does not need to be in sentence form for LSI to be effective.  It can work with lists, free-form notes, email, Web-based content, etc.  As long as a collection of text contains multiple terms, LSI can be used to identify patterns in the relationships between the important terms and concepts contained in the text.\n\nLSI has proven to be a useful solution to a number of conceptual matching problems.<ref>Ding, C., A Similarity-based Probability Model for Latent Semantic Indexing, Proceedings of the 22nd International ACM SIGIR Conference on Research and Development in Information Retrieval, 1999, pp. 59\xe2\x80\x9365.</ref><ref>Bartell, B., Cottrell, G., and Belew, R., Latent Semantic Indexing is an Optimal Special Case of Multidimensional Scaling, Proceedings, ACM SIGIR Conference on Research and Development in Information Retrieval, 1992, pp. 161\xe2\x80\x93167.</ref>  The technique has been shown to capture key relationship information, including causal, goal-oriented, and taxonomic information.<ref>Graesser, A., and Karnavat, A., Latent Semantic Analysis Captures Causal, Goal-oriented, and Taxonomic Structures, Proceedings of CogSci 2000, pp. 184\xe2\x80\x93189.</ref>\n\n== LSI timeline ==\n\n\'\'\'Mid-1960s\'\'\' \xe2\x80\x93 Factor analysis technique first described and tested (H. Borko and M. Bernick)\n\n\'\'\'1988\'\'\' \xe2\x80\x93 Seminal paper on LSI technique published (Deerwester et al.)\n\n\'\'\'1989\'\'\' \xe2\x80\x93 Original patent granted (Deerwester et al.)\n\n\'\'\'1992\'\'\' \xe2\x80\x93 First use of LSI to assign articles to reviewers<ref>Dumais, S., and Nielsen, J., Automating the Assignment of Submitted Manuscripts to Reviewers, Proceedings of the Fifteenth Annual International Conference on Research and Development in Information Retrieval, 1992, pp. 233\xe2\x80\x93244.</ref>  (Dumais and Nielsen)\n\n\'\'\'1994\'\'\' \xe2\x80\x93 Patent granted for the cross-lingual application of LSI (Landauer et al.)\n\n\'\'\'1995\'\'\' \xe2\x80\x93 First use of LSI for grading essays (Foltz, et al., Landauer et al.)\n\n\'\'\'1999\'\'\' \xe2\x80\x93 First implementation of LSI technology for intelligence community for analyzing unstructured text (SAIC).\n\n\'\'\'2002\'\'\' \xe2\x80\x93 LSI-based product offering to intelligence-based government agencies (SAIC)\n\n\'\'\'2005\'\'\' \xe2\x80\x93 First vertical-specific application \xe2\x80\x93 publishing \xe2\x80\x93 EDB (EBSCO, [[Content Analyst Company]])\n\n== Mathematics of LSI ==\n\nLSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text.  In general, the process involves constructing a weighted term-document matrix, performing a \'\'\'Singular Value Decomposition\'\'\' on the matrix, and using the matrix to identify the concepts contained in the text.\n\n=== Term-document matrix ===\n\nLSI begins by constructing a term-document matrix, <math>A</math>, to identify the occurrences of the <math>m</math> unique terms within a collection of <math>n</math> documents.  In a term-document matrix, each term is represented by a row, and each document is represented by a column, with each matrix cell, <math>a_{ij}</math>, initially representing the number of times the associated term appears in the indicated document, <math>\\mathrm{tf_{ij}}</math>.  This matrix is usually very large and very sparse.\n\nOnce a term-document matrix is constructed, local and global weighting functions can be applied to it to condition the data.  The weighting functions transform each cell, <math>a_{ij}</math> of <math>A</math>, to be the product of a local term weight, <math>l_{ij}</math>, which describes the relative frequency of a term in a document, and a global weight, <math>g_i</math>, which describes the relative frequency of the term within the entire collection of documents.\n\nSome common local weighting functions <ref>\nBerry, M. W., and Browne, M., Understanding Search Engines: Mathematical Modeling and Text Retrieval, Society for Industrial and Applied Mathematics, Philadelphia, (2005).</ref> are defined in the following table.\n\n{| style="width:60%" cellpadding="25" cellspacing="5" align="center"\n|-\n|  style="width:22%" | \'\'\'Binary\'\'\' ||\n| <math>l_{ij} = 1</math> if the term exists in the document, or else <math>0</math>\n|-\n|  style="width:22%" | \'\'\'TermFrequency\'\'\' ||\n| <math>l_{ij} = \\mathrm{tf}_{ij}</math>, the number of occurrences of term <math>i</math> in document <math>j</math>\n|-\n|  style="width:22%" | \'\'\'Log\'\'\' ||\n| <math>l_{ij} = \\log(\\mathrm{tf}_{ij} + 1)</math>\n|-\n|  style="width:22%" | \'\'\'Augnorm\'\'\' ||\n| <math>l_{ij} = \\frac{\\Big(\\frac{\\mathrm{tf}_{ij}}{\\max_i(\\mathrm{tf}_{ij})}\\Big) + 1}{2}</math>\n|}\n\nSome common global weighting functions are defined in the following table.\n\n{| style="width:60%" cellpadding="25" cellspacing="5" align="center"\n|-\n| style="width:22%" | \'\'\'Binary\'\'\' ||\n| <math>g_i = 1</math>\n|-\n| style="width:22%" | \'\'\'Normal\'\'\' ||\n| <math>g_i = \\frac{1}{\\sqrt{\\sum_j \\mathrm{tf}_{ij}^2}}</math>\n|-\n| style="width:22%" | \'\'\'GfIdf\'\'\' ||\n| <math>g_i = \\mathrm{gf}_i / \\mathrm{df}_i</math>, where <math>\\mathrm{gf}_i</math> is the total number of times term <math>i</math> occurs in the whole collection, and <math>\\mathrm{df}_i</math> is the number of documents in which term <math>i</math> occurs.\n|-\n| style="width:22%" | \'\'\'Idf\'\'\' ||\n| <math>g_i = \\log_2 \\frac{n}{1+ \\mathrm{df}_i}</math>\n|-\n| style="width:22%" | \'\'\'Entropy\'\'\' ||\n| <math>g_i = 1 + \\sum_j \\frac{p_{ij} \\log p_{ij}}{\\log n}</math>, where <math>p_{ij} = \\frac{\\mathrm{tf}_{ij}}{\\mathrm{gf}_i}</math>\n|}\n\nEmpirical studies with LSI report that the Log Entropy weighting functions work well, in practice, with many data sets.<ref>Landauer, T., et al., Handbook of Latent Semantic Analysis, Lawrence Erlbaum Associates, 2007.</ref>  In other words, each entry <math>a_{ij}</math> of <math>A</math> is computed as:\n\n:<math>g_i = 1 + \\sum_j \\frac{p_{ij} \\log p_{ij}}{\\log n}</math>\n\n:<math>a_{ij} = g_i \\ \\log (\\mathrm{tf}_{ij} + 1)</math>\n\n=== Rank-reduced singular value decomposition ===\n\nA rank-reduced, [[singular value decomposition]] is performed on the matrix to determine patterns in the relationships between the terms and concepts contained in the text.  The SVD forms the foundation for LSI.<ref>Berry, Michael W., Dumais, Susan T., O\'Brien, Gavin W., Using Linear Algebra for Intelligent Information Retrieval, December 1994, SIAM Review 37:4 (1995), pp. 573\xe2\x80\x93595.</ref>   It computes the term and document vector spaces by approximating the single term-frequency matrix, <math>A</math>, into three other matrices\xe2\x80\x94 an \'\'\'\'\'m\'\'\'\'\' by \'\'\'\'\'r\'\'\'\'\'  term-concept vector matrix <math>T</math>, an \'\'\'\'\'r\'\'\'\'\' by \'\'\'\'\'r\'\'\'\'\' singular values matrix <math>S</math>, and a \'\'\'\'\'n\'\'\'\'\' by \'\'\'\'\'r\'\'\'\'\' concept-document vector matrix, <math>D</math>, which satisfy the following relations:\n\n<math>A \\approx TSD^T</math>\n\n<math>T^T T = I_r \\quad D^T D = I_r </math>\n\n<math>S_{1,1} \\geq S_{2,2} \\geq \\ldots \\geq  S_{r,r} > 0 \\quad S_{i,j} = 0 \\; \\text{where} \\; i \\neq j</math>\n\nIn the formula, \'\'\'A\'\'\' is the supplied \'\'\'\'\'m\'\'\'\'\' by \'\'\'\'\'n\'\'\'\'\' weighted matrix of term frequencies in a collection of text where \'\'\'\'\'m\'\'\'\'\' is the number of unique terms, and \'\'\'\'\'n\'\'\'\'\' is the number of documents.  \'\'\'T\'\'\' is a computed \'\'\'\'\'m\'\'\'\'\' by \'\'\'\'\'r\'\'\'\'\' matrix of term vectors where \'\'\'\'\'r\'\'\'\'\' is the rank of \'\'\'A\'\'\'\xe2\x80\x94a measure of its unique dimensions \'\'\'\xe2\x89\xa4 min(\'\'m,n\'\')\'\'\'.  \'\'\'S\'\'\' is a computed \'\'\'\'\'r\'\'\'\'\' by \'\'\'\'\'r\'\'\'\'\' diagonal matrix of decreasing singular values, and \'\'\'D\'\'\' is a computed \'\'\'\'\'n\'\'\'\'\' by \'\'\'\'\'r\'\'\'\'\' matrix of document vectors.\n\nThe LSI modification to a standard SVD is to reduce the rank or truncate the singular value matrix \'\'\'S\'\'\' to size \'\'\'\'\'k\'\'\'\'\' \xc2\xab \'\'\'\'\'r\'\'\'\'\', typically on the order of a \'\'\'\'\'k\'\'\'\'\' in the range of 100 to 300 dimensions, effectively reducing the term and document vector matrix sizes to \'\'\'\'\'m\'\'\'\'\' by \'\'\'\'\'k\'\'\'\'\' and \'\'\'\'\'n\'\'\'\'\' by \'\'\'\'\'k\'\'\'\'\' respectively.  The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of \'\'\'A\'\'\'.  This reduced set of matrices is often denoted with a modified formula such as:\n\n:::::::\'\'\'A \xe2\x89\x88 A\'\'<sub>k\'\'</sub> = T\'\'<sub>k\'\'</sub> S\'\'<sub>k\'\'</sub> D\'\'<sub>k\'\'</sub><sup>T</sup>\'\'\'\n\nEfficient LSI algorithms only compute the first \'\'\'\'\'k\'\'\'\'\' singular values and term and document vectors as opposed to computing a full SVD and then truncating it.\n\nNote that this rank reduction is essentially the same as doing [[Principal Component Analysis]] (PCA) on the matrix \'\'\'A\'\'\', except that PCA subtracts off the means.  PCA loses the sparseness of the \'\'\'A\'\'\' matrix, which can make it infeasible for large lexicons.\n\n== Querying and augmenting LSI vector spaces ==\n\nThe computed \'\'\'T\'\'<sub>k\'\'</sub>\'\'\' and \'\'\'D\'\'<sub>k\'\'</sub>\'\'\' matrices define the term and document vector spaces, which with the computed singular values, \'\'\'S\'\'<sub>k\'\'</sub>\'\'\', embody the conceptual information derived from the document collection.  The similarity of terms or documents within these spaces is a factor of how close they are to each other in these spaces, typically computed as a function of the angle between the corresponding vectors.\n\nThe same steps are used to locate the vectors representing the text of queries and new documents within the document space of an existing LSI index.  By a simple transformation of the \'\'\'A = T S D<sup>T</sup>\'\'\' equation into the equivalent \'\'\'D = A<sup>T</sup> T S<sup>\xe2\x88\x921</sup>\'\'\' equation, a new vector, \'\'\'\'\'d\'\'\'\'\', for a query or for a new document can be created by computing a new column in \'\'\'A\'\'\' and then multiplying the new column by \'\'\'T S<sup>\xe2\x88\x921</sup>\'\'\'.  The new column in \'\'\'A\'\'\' is computed using the originally derived global term weights and applying the same local weighting function to the terms in the query or in the new document.\n\nA drawback to computing vectors in this way, when adding new searchable documents, is that terms that were not known during the SVD phase for the original index are ignored.  These terms will have no impact on the global weights and learned correlations derived from the original collection of text.  However, the computed vectors for the new text are still very relevant for similarity comparisons with all other document vectors.\n\nThe process of augmenting the document vector spaces for an LSI index with new documents in this manner is called \'\'folding in\'\'.  Although the folding-in process does not account for the new semantic content of the new text, adding a substantial number of documents in this way will still provide good results for queries as long as the terms and concepts they contain are well represented within the LSI index to which they are being added.  When the terms and concepts of a new set of documents need to be included in an LSI index, either the term-document matrix, and the SVD, must be recomputed or an incremental update method (such as the one described in <ref name="brand2006">{{cite journal | url=http://www.merl.com/reports/docs/TR2006-059.pdf |format=PDF| title=Fast Low-Rank Modifications of the Thin Singular Value Decomposition | author=Matthew Brand | journal=Linear Algebra and Its Applications | volume=415 | pages=20\xe2\x80\x9330 | year=2006 | doi=10.1016/j.laa.2005.07.021 }}</ref>) be used.\n\n== Additional uses of LSI ==\n\nIt is generally acknowledged that the ability to work with text on a semantic basis is essential to modern information retrieval systems.  As a result, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.\n\nLSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.<ref>Dumais, S., Latent Semantic Analysis, ARIST Review of Information Science and Technology, vol. 38, 2004, Chapter 4.</ref>   Below are some other ways in which LSI is being used:\n\n* Information discovery<ref>Best Practices Commentary on the Use of Search and Information Retrieval Methods in E-Discovery, the Sedona Conference, 2007, pp. 189\xe2\x80\x93223.</ref>  (eDiscovery, Government/Intelligence community, Publishing)\n* Automated document classification (eDiscovery, Government/Intelligence community, Publishing)<ref>Foltz, P. W. and Dumais, S. T. Personalized Information Delivery:  An analysis of information filtering methods, Communications of the ACM, 1992, 34(12), 51-60.</ref>\n* Text summarization<ref>Gong, Y., and Liu, X., Creating Generic Text Summaries, Proceedings, Sixth International Conference on Document Analysis and Recognition, 2001, pp. 903\xe2\x80\x93907.</ref>  (eDiscovery, Publishing)\n* Relationship discovery<ref>Bradford, R., Efficient Discovery of New Information in Large Text Databases, Proceedings, IEEE International Conference on Intelligence and Security Informatics, Atlanta, Georgia, LNCS Vol. 3495, Springer, 2005, pp. 374\xe2\x80\x93380.</ref>  (Government, Intelligence community, Social Networking)\n* Automatic generation of link charts of individuals and organizations<ref>Bradford, R., Application of Latent Semantic Indexing in Generating Graphs of Terrorist Networks, in: Proceedings, IEEE International Conference on Intelligence and Security Informatics, ISI 2006, San Diego, CA, USA, May 23\xe2\x80\x9324, 2006, Springer, LNCS vol. 3975, pp. 674\xe2\x80\x93675.</ref>  (Government, Intelligence community)\n* Matching technical papers and grants with reviewers<ref>Yarowsky, D., and Florian, R., Taking the Load off the Conference Chairs: Towards a Digital Paper-routing Assistant, Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in NLP and Very-Large Corpora, 1999, pp. 220\xe2\x80\x93230.</ref>  (Government)\n* Online customer support<ref>Caron, J., Applying LSA to Online Customer Support: A Trial Study, Unpublished Master\'s Thesis, May 2000.</ref>  (Customer Management)\n* Determining document authorship<ref>Soboroff, I., et al, Visualizing Document Authorship Using N-grams and Latent Semantic Indexing,   Workshop on New Paradigms in Information Visualization and Manipulation, 1997, pp. 43\xe2\x80\x9348.</ref>  (Education)\n* Automatic keyword annotation of images<ref>Monay, F., and Gatica-Perez, D., On Image Auto-annotation with Latent Space Models, Proceedings of the 11th ACM international conference on Multimedia, Berkeley, CA, 2003, pp. 275\xe2\x80\x93278.</ref>\n* Understanding software source code<ref>Maletic, J., and Marcus, A., Using Latent Semantic Analysis to Identify Similarities in Source Code to Support Program Understanding, Proceedings of 12th IEEE International Conference on Tools with Artificial Intelligence, Vancouver, British Columbia, November 13\xe2\x80\x9315, 2000, pp. 46\xe2\x80\x9353.</ref>  (Software Engineering)\n* Filtering [[Spam (electronic)|spam]]<ref>Gee, K., Using Latent Semantic Indexing to Filter Spam, in: Proceedings, 2003 ACM Symposium on Applied Computing, Melbourne, Florida, pp. 460\xe2\x80\x93464.</ref>  (System Administration)\n* Information visualization<ref>Landauer, T., Laham, D., and Derr, M., From Paragraph to Graph: Latent Semantic Analysis for Information Visualization, Proceedings of the National Academy of Science, 101, 2004, pp. 5214\xe2\x80\x935219.</ref>\n* [[Automated essay scoring|Essay scoring]]<ref>Foltz, Peter W., Laham, Darrell, and Landauer, Thomas K., Automated Essay Scoring: Applications to Educational Technology, Proceedings of EdMedia,  1999.</ref>  (Education)\n* [[Literature-based discovery]]<ref>Gordon, M., and Dumais, S., Using Latent Semantic Indexing for Literature Based Discovery, Journal of the American Society for Information Science, 49(8), 1998, pp. 674\xe2\x80\x93685.</ref>\n\nLSI is increasingly being used for electronic document discovery (eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is essential.  Concept-based searching using LSI has been applied to the eDiscovery process by leading providers as early as 2003.<ref>There Has to be a Better Way to Search, 2008, White Paper, Fios, Inc.</ref>\n\n== Challenges to LSI ==\n\nEarly challenges to LSI focused on scalability and performance.  LSI requires relatively high computational performance and memory in comparison to other information retrieval techniques.<ref>Karypis, G., Han, E., Fast Supervised Dimensionality Reduction Algorithm with Applications to Document Categorization and Retrieval, Proceedings of CIKM-00, 9th ACM Conference on Information and Knowledge Management.</ref>  However, with the implementation of modern high-speed processors and the availability of inexpensive memory, these considerations have been largely overcome.  Real-world applications involving more than 30 million documents that were fully processed through the matrix and SVD computations are not uncommon in some LSI applications. A fully scalable (unlimited number of documents, online training) implementation of LSI is contained in the open source [[gensim]] software package.<ref name="rehurek2011">{{cite journal | url=http://dx.doi.org/10.1007/978-3-642-20161-5_29 |format=PDF| title=Subspace Tracking for Latent Semantic Analysis | author=Radim \xc5\x98eh\xc5\xaf\xc5\x99ek | journal=Advances in Information Retrieval - 33rd European Conference on IR Research, ECIR 2011 | volume=6611 | pages=289\xe2\x80\x93300 | year=2011 | doi=10.1007/978-3-642-20161-5_29 }}</ref>\n\nAnother challenge to LSI has been the alleged difficulty in determining the optimal number of dimensions to use for performing the SVD.  As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts.  The actual number of dimensions that can be used is limited by the number of documents in the collection.  Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents).<ref>Bradford, R., An Empirical Study of Required Dimensionality for Large-scale Latent Semantic Indexing Applications, Proceedings of the 17th ACM Conference on Information and Knowledge Management, Napa Valley, California, USA, 2008, pp. 153\xe2\x80\x93162.</ref>   However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection.<ref>Landauer, Thomas K., and Dumais, Susan T., Latent Semantic Analysis, Scholarpedia, 3(11):4356, 2008.</ref>\n\nChecking the amount of variance in the data after computing the SVD can be used to determine the optimal number of dimensions to retain.  The variance contained in the data can be viewed by plotting the singular values (S) in a [[scree plot]].  Some LSI practitioners select the dimensionality associated with the knee of the curve as the cut-off point for the number of dimensions to retain.  Others argue that some quantity of the variance must be retained, and the amount of variance in the data should dictate the proper dimensionality to retain.  Seventy percent is often mentioned as the amount of variance in the data that should be used to select the optimal dimensionality for recomputing the SVD.<ref>Cangelosi, R., Goriely A., Component Retention In Principal Component Analysis With Application to Cdna Microarray Data, BMC Biology Direct 2(2) (2007).</ref><ref>Jolliffe, L. T., Principal Component Analysis, Springer-Verlag, New York, (1986).</ref><ref>Hu, X., Z. Cai, et al., LSA: First Dimension and Dimensional Weighting, 25th Annual Meeting of the Cognitive Science Society, Boston, MA.</ref>\n\n==See also==\n* [[Latent semantic analysis]]\n* [[Latent Semantic Structure Indexing]]\n* [[Principal component analysis]]\n* [[Correspondence analysis]]\n* [[Probabilistic latent semantic analysis]]\n\n{{Natural Language Processing}}\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n*{{cite book|authors=Berry, M. W., Browne M.|title=Understanding Search Engines: Mathematical Modeling and Text Retrieval|location=Philadelphia|publisher=Society for Industrial and Applied Mathematics|year=2005|isbn=978-0898715811|url=http://www.mblazquez.es/blog-ccdoc-recuperacion/documentos/book_understanding-search-engines.pdf}}\n*{{cite book|editors=Berry, M. W.|title=Survey of Text Mining: Clustering, Classification, and Retrieval|location=New York|publisher=Springer|year=2004|url=https://perso.uclouvain.be/vincent.blondel/publications/08-textmining.pdf|isbn=978-0387955636}}\n*{{cite book|authors=Landauer, T., et al.|title=Handbook of Latent Semantic Analysis|publisher=Lawrence Erlbaum Associates|year=2007|isbn= 978-0805854183|url=http://books.google.de/books/about/Handbook_of_latent_semantic_analysis.html?id=jgVWCuFXePEC&redir_esc=y}}\n*{{cite book|authors=Manning, C. D., Schutze H.|title=Foundations of Statistical Natural Language Processing|location=Cambridge, MA|publisher=The MIT Press|year=1999|url=http://nlp.stanford.edu/fsnlp/promo/contents.ps|isbn=9780262133609 }} [http://nlp.stanford.edu/fsnlp/ Companion webpage]\n\n==External links==\n* [http://www.cs.utk.edu/~lsi/ Michael Berry\xe2\x80\x99s site]\n* [http://radimrehurek.com/gensim Gensim] contains a scalable Python+[[NumPy]] implementation of LSI, even for datasets larger than the available RAM.\n* [http://scgroup.hpclab.ceid.upatras.gr/scgroup/Projects/TMG/ Text to Matrix Generator (TMG)]  MATLAB toolbox that can be used for various tasks in text mining (TM) specifically  i) indexing, ii) retrieval, iii) dimensionality reduction, iv) clustering, v) classification. Most of TMG is written in MATLAB and parts in Perl. It contains implementations of LSI, clustered LSI, NMF and other methods.\n* [http://www.youtube.com/watch?v=QGd06MTRMHs Stanford University Andrew Ng Video on LSI]\n\n{{DEFAULTSORT:Latent semantic indexing}}\n[[Category:Information retrieval]]\n[[Category:Semantic Web]]'
p66
sg6
S'Latent semantic indexing'
p67
ssI25
(dp68
g2
S'http://en.wikipedia.org/wiki/Precision and recall'
p69
sg4
S'[[File:Precisionrecall.svg|thumb|350px|Precision and recall]]\nIn [[pattern recognition]] and [[information retrieval]] with [[binary classification]], \'\'\'precision\'\'\' (also called [[positive predictive value]]) is the fraction of retrieved instances that are relevant, while \'\'\'recall\'\'\' (also known as [[Sensitivity and specificity|sensitivity]]) is the fraction of relevant instances that are retrieved. Both precision and recall are therefore based on an understanding and measure of [[relevance]]. Suppose a program for recognizing dogs in scenes from a video identifies 7 dogs in a scene containing 9 dogs and some cats. If 4 of the identifications are correct, but 3 are actually cats, the program\'s precision is 4/7 while its recall is 4/9.  When a search engine returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3.\n\nIn [[statistics]], if the [[null hypothesis]] is that all and only the relevant items are retrieved, absence of [[type I and type II errors]] corresponds respectively to maximum precision (no false positive) and maximum recall (no false negative).  The above pattern recognition example contained 7 &minus; 4 = 3 type I errors and 9 &minus; 4 = 5 type II errors.  Precision can be seen as a measure of exactness or \'\'quality\'\', whereas recall is a measure of completeness or \'\'quantity\'\'.\n\nIn simple terms, high \'\'\'precision\'\'\' means that an algorithm returned substantially more relevant results than irrelevant, while high \'\'\'recall\'\'\' means that an algorithm returned most of the relevant results.\n\n==Introduction==\nAs an example, in an [[information retrieval]] scenario, the instances are documents and the task is to return a set of relevant documents given a search term; or equivalently, to assign each document to one of two categories, "relevant" and "not relevant".  In this case, the "relevant" documents are simply those that belong to the "relevant" category.  Recall is defined as the \'\'number of relevant documents\'\' retrieved by a search \'\'divided by the total number of existing relevant documents\'\', while precision is defined as the \'\'number of relevant documents\'\' retrieved by a search \'\'divided by the total number of documents retrieved\'\' by that search.\n\nIn a [[classification (machine learning)|classification]] task, the precision for a class is the \'\'number of \'\'\'true positives\'\'\'\'\' (i.e. the \'\'number of items correctly labeled as belonging to the positive class\'\') \'\'divided by the total number of elements labeled as belonging to the positive class\'\' (i.e. the sum of true positives and \'\'\'[[Type I and type II errors|false positives]]\'\'\', which are items incorrectly labeled as belonging to the class).  Recall in this context is defined as the \'\'number of true positives\'\' \'\'divided by the total number of elements that actually belong to the positive class\'\' (i.e. the sum of true positives and \'\'\'[[Type I and type II errors|false negatives]]\'\'\', which are items which were not labeled as belonging to the positive class but should have been).\n\nIn information retrieval, a perfect precision score of 1.0 means that every result retrieved by a search was relevant (but says nothing about whether all relevant documents were retrieved) whereas a perfect recall score of 1.0 means that all relevant documents were retrieved by the search (but says nothing about how many irrelevant documents were also retrieved).\n\nIn a classification task, a precision score of 1.0 for a class C means that every item labeled as belonging to class C does indeed belong to class C (but says nothing about the number of items from class C that were not labeled correctly) whereas a recall of 1.0 means that every item from class C was labeled as belonging to class C (but says nothing about how many other items were incorrectly also labeled as belonging to class C).\n\nOften, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other. Brain surgery provides an obvious example of the tradeoff.  Consider a brain surgeon tasked with removing a cancerous tumor from a patient\xe2\x80\x99s brain. The surgeon needs to remove all of the tumor cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain she removes to ensure she has extracted all the cancer cells. This decision increases recall but reduces precision.  On the other hand, the surgeon may be more conservative in the brain she removes to ensure she extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome).  Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome).\n\nUsually, precision and recall scores are not discussed in isolation. Instead, either values for one measure are compared for a fixed level at the other measure (e.g. \'\'precision at a recall level of 0.75\'\') or both are combined into a single measure. Examples for measures that are a combination of precision and recall are the [[Precision and recall#F-measure|F-measure]] (the weighted [[harmonic mean]] of precision and recall), or the [[Matthews correlation coefficient]], which is a [[geometric mean]] of the chance-corrected variants: the [[regression coefficient]]s Informedness (DeltaP\') and Markedness (DeltaP).<ref name="Powers2007">{{cite journal |first=David M W |last=Powers |date=2007/2011 |title=Evaluation: From Precision, Recall and F-Factor  to ROC, Informedness, Markedness & Correlation |journal=Journal of Machine Learning Technologies |volume=2 |issue=1 |pages=37\xe2\x80\x9363 |url=http://www.bioinfo.in/uploadfiles/13031311552_1_1_JMLT.pdf}}</ref><ref>{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97\xe2\x80\x93119 |doi=10.1016/s0911-6044(03)00059-9}}</ref> [[Accuracy and precision#In binary classification|Accuracy]] is a weighted arithmetic mean of Precision and Inverse Precision (weighted by Bias) as well as a weighted arithmetic mean of Recall and Inverse Recall (weighted by Prevalence).<ref name="Powers2007"/> Inverse Precision and Recall are simply the Precision and Recall of the inverse problem where positive and negative labels are exchanged (for both real classes and prediction labels).  Recall and Inverse Recall, or equivalently true positive rate and false positive rate, are frequently plotted against each other as [[Receiver operating characteristic|ROC]] curves and provide a principled mechanism to explore operating point tradeoffs. Outside of Information Retrieval, the application of Recall, Precision and F-measure are argued to be flawed as they ignore the true negative cell of the contingency table, and they are easily manipulated by biasing the predictions.<ref name="Powers2007"/>  The first problem is \'solved\' by using [[Accuracy and precision#In binary classification|Accuracy]] and the second problem is \'solved\' by discounting the chance component and renormalizing to [[Cohen\'s kappa]], but this no longer affords the opportunity to explore tradeoffs graphically. However, Informedness and Markedness are Kappa-like renormalizations of Recall and Precision,<ref>{{cite conference |first=David M. W. |last=Powers |date=2012 |title=The Problem with Kappa |booktitle=Conference of the European Chapter of the Association for Computational Linguistics (EACL2012) Joint ROBUS-UNSUP Workshop}}</ref> and their geometric mean [[Matthews correlation coefficient]] thus acts like a debiased F-measure.\n\n== Definition (information retrieval context) ==\n\nIn [[information retrieval]] contexts, precision and recall are defined in terms of a set of \'\'\'retrieved documents\'\'\' (e.g. the list of documents produced by a [[web search engine]] for a query) and a set of \'\'\'relevant documents\'\'\' (e.g. the list of all documents on the internet that are relevant for a certain topic), cf. [[relevance]].\n\n===[[Positive predictive value | Precision]]===\n\nIn the field of [[information retrieval]], \'\'\'precision\'\'\' is the fraction of retrieved documents that are [[Relevance (information retrieval)|relevant]] to the find:\n\n:<math> \\text{precision}=\\frac{|\\{\\text{relevant documents}\\}\\cap\\{\\text{retrieved documents}\\}|}{|\\{\\text{retrieved documents}\\}|} </math>\n\nPrecision takes all retrieved documents into account, but it can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called \'\'\'precision at n\'\'\' or \'\'\'P@n\'\'\'.\n\nFor example for a text search on a set of documents precision is the number of correct results divided by the number of all returned results.\n\nPrecision is also used with [[recall (information retrieval)|recall]], the percent of \'\'all\'\' relevant documents that is returned by the search. The two measures are sometimes used together in the [[F1 Score]] (or f-measure) to provide a single measurement for a system.\n\nNote that the meaning and usage of "precision" in the field of Information Retrieval differs from the definition of [[accuracy and precision]] within other branches of science and technology.\n\n===Recall===\n\nRecall in information retrieval is the fraction of the documents that are relevant to the query that are successfully retrieved.\n\n:<math> \\text{recall}=\\frac{|\\{\\text{relevant documents}\\}\\cap\\{\\text{retrieved documents}\\}|}{|\\{\\text{relevant documents}\\}|} </math>\n\nFor example for text search on a set of documents recall is the number of correct results divided by the number of results that should have been returned\n\nIn binary classification, recall is called [[Sensitivity_and_specificity#Sensitivity|sensitivity]]. So it can be looked at as the probability that a relevant document is retrieved by the query.\n\nIt is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.\n\n== Definition (classification context) ==\n{| class="wikitable" align="right" width=35% style="font-size:98%; margin-left:0.5em; padding:0.25em; background:#f1f5fc;"\n|+ Terminology and derivations<br \n/>from a confusion matrix\n|- valign=top\n|\n; true positive (TP)\n:eqv. with hit\n; true negative (TN)\n:eqv. with correct rejection\n; false positive (FP)\n:eqv. with [[false alarm]], [[Type I error]]\n; false negative (FN)\n:eqv. with miss, [[Type II error]]\n--------------------------------------------------------\n; [[sensitivity (test)|sensitivity]] or true positive rate (TPR)\n:eqv. with [[hit rate]], [[Information retrieval#Recall|recall]]\n:<math>\\mathit{TPR} = \\mathit{TP} / P = \\mathit{TP} / (\\mathit{TP}+\\mathit{FN})</math>\n; [[Specificity (tests)|specificity]] (SPC) or True Negative Rate\n:<math>\\mathit{SPC} = \\mathit{TN} / N = \\mathit{TN} / (\\mathit{FP} + \\mathit{TN}) </math>\n; [[Information retrieval#Precision|precision]] or [[positive predictive value]] (PPV)\n:<math>\\mathit{PPV} = \\mathit{TP} / (\\mathit{TP} + \\mathit{FP})</math>\n; [[negative predictive value]] (NPV)\n:<math>\\mathit{NPV} = \\mathit{TN} / (\\mathit{TN} + \\mathit{FN})</math>\n; [[Information retrieval#Fall-out|fall-out]] or false positive rate (FPR)\n:<math>\\mathit{FPR} = \\mathit{FP} / N = \\mathit{FP} / (\\mathit{FP} + \\mathit{TN})</math>\n; [[false discovery rate]] (FDR)\n:<math>\\mathit{FDR} = \\mathit{FP} / (\\mathit{FP} + \\mathit{TP}) = 1 - \\mathit{PPV} </math>\n; [[false negative rate]] (FNR)\n:<math>\\mathit{FNR} = \\mathit{FN} / (\\mathit{FN} + \\mathit{TP}) = 1 - \\mathit{TPR} </math>\n------------------------------------------------\n; [[accuracy]] (ACC)\n:<math>\\mathit{ACC} = (\\mathit{TP} + \\mathit{TN}) / (P + N)</math>\n;[[F1 score]]\n: is the [[Harmonic mean#Harmonic mean of two numbers|harmonic mean]] of [[Information retrieval#Precision|precision]] and [[sensitivity (test)|sensitivity]]\n:<math>\\mathit{F1} = 2 \\mathit{TP} / (2 \\mathit{TP} + \\mathit{FP} + \\mathit{FN})</math>\n; [[Matthews correlation coefficient]] (MCC)\n:<math> \\frac{ TP \\times TN - FP \\times FN } {\\sqrt{ (TP+FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }\n</math>\n;\n<span style="font-size:90%;">\'\'Source: Fawcett (2006).\'\'<ref name=Fawcelt2006>{{cite journal|last=Fawcelt|first=Tom|title=An Introduction to ROC Analysis|journal=Pattern Recognition Letters|date=2006|volume=27|issue=8|pages=861 - 874|doi=10.1016/j.patrec.2005.10.010}}</ref></span>\n|}\n\nFor classification tasks, the terms \'\'\'true positives\'\'\', \'\'\'true negatives\'\'\', \'\'\'false positives\'\'\', and \'\'\'false negatives\'\'\' (see also [[Type I and type II errors]]) compare the results of the classifier under test with trusted external judgments.  The terms \'\'positive\'\' and \'\'negative\'\' refer to the classifier\'s prediction (sometimes known as the \'\'expectation\'\'), and the terms \'\'true\'\' and \'\'false\'\' refer to whether that prediction corresponds to the external judgment (sometimes known as the \'\'observation\'\'). \n\nLet us define an experiment from \'\'\'P\'\'\' positive instances and \'\'\'N\'\'\' negative instances for some condition. The four outcomes can be formulated in a 2\xc3\x972 \'\'[[contingency table]]\'\' or \'\'[[confusion matrix]]\'\', as follows:\n\n{{DiagnosticTesting_Diagram}}\n\n<!--\n{| border="0" align="center" style="text-align: center; background: #FFFFFF;"\n|+\n!\n! colspan="2" style="background: #ddffdd;"|actual class <br/> (observation)\n|-\n!\n|-----\n|+\n! rowspan="2" style="background: #ffdddd;"|predicted class <br/> (expectation)\n| \'\'\'tp\'\'\' <br> (true positive) <br/> Correct result\n| \'\'\'fp\'\'\' <br> (false positive) <br/> Unexpected result\n|-bgcolor="#EFEFEF"\n| \'\'\'fn\'\'\' <br> (false negative) <br/> Missing result\n| \'\'\'tn\'\'\' <br> (true negative) <br/> Correct absence of result\n|+\n|}\n\n-->\n\n\n\nPrecision and recall are then defined as:<ref name="OlsonDelen">Olson, David L.; and Delen, Dursun (2008); \'\'Advanced Data Mining Techniques\'\', Springer, 1st edition (February 1, 2008), page 138, ISBN 3-540-76916-1</ref>\n\n: <math>\\text{Precision}=\\frac{tp}{tp+fp} \\, </math>\n\n: <math>\\text{Recall}=\\frac{tp}{tp+fn} \\, </math>\n\nRecall in this context is also referred to as the true positive rate or [[Sensitivity and specificity|sensitivity]], and precision is also referred to as [[positive predictive value]] (PPV); other related measures used in classification include true negative rate and [[Accuracy_and_precision#In_binary_classification|accuracy]].<ref name="OlsonDelen" /> True negative rate is also called [[Specificity_(tests)#Specificity|specificity]].\n\n: <math>\\text{True negative rate}=\\frac{tn}{tn+fp} \\, </math>\n\n: <math>\\text{Accuracy}=\\frac{tp+tn}{tp+tn+fp+fn} \\, </math>\n\n== Probabilistic interpretation ==\n\nIt is possible to interpret precision and recall not as ratios but as probabilities:\n\n* \'\'\'Precision\'\'\' is the probability that a (randomly selected) retrieved document is relevant.\n\n* \'\'\'Recall\'\'\' is the probability that a (randomly selected) relevant document is retrieved in a search.\n\nNote that the random selection refers to a uniform distribution over the appropriate pool of documents; i.e. by \'\'\'randomly selected retrieved document\'\'\', we mean selecting a document from the set of retrieved documents in a random fashion. The random selection should be such that all documents in the set are equally likely to be selected. \n\nNote that, in a typical classification system, the probability that a retrieved document is relevant depends on the document. The above interpretation extends to that scenario also (needs explanation). \n\nAnother interpretation for precision and recall is as follows. Precision is the average probability of relevant retrieval. Recall is the average probability of complete retrieval. Here we average over multiple retrieval queries.\n\n== F-measure ==\n{{main|F1 score}}\nA measure that combines precision and recall is the [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score:\n\n: <math>F = 2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{ \\mathrm{precision} + \\mathrm{recall}}</math>\n\nThere are several reasons that the F-score can be criticized in particular circumstances due to its bias as an evaluation metric. <ref>{{cite journal|last=POWERS|first=D.M.W.|title=EVALUATION: FROM PRECISION, RECALL AND F-MEASURE TO ROC, INFORMEDNESS, MARKEDNESS & CORRELATION|journal=Journal of Machine Learning Technologies|date=February 27, 2011|volume=2|issue=1|pages=37-63|url=http://www.bioinfo.in/contents.php?id=51}}</ref> This is also known as the <math>F_1</math> measure, because recall and precision are evenly weighted.\n\nIt is a special case of the general <math>F_\\beta</math> measure (for non-negative real values of&nbsp;<math>\\beta</math>):\n\n:<math>F_\\beta = (1 + \\beta^2) \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall} }{ \\beta^2 \\cdot \\mathrm{precision} + \\mathrm{recall}}</math>\n\nTwo other commonly used <math>F</math> measures are the <math>F_2</math> measure, which weights recall higher than precision, and the <math>F_{0.5}</math> measure, which puts more emphasis on precision than recall.\n\nThe F-measure was derived by van Rijsbergen (1979) so that <math>F_\\beta</math> "measures the effectiveness of retrieval with respect to a user who attaches <math>\\beta</math> times as much importance to recall as precision".  It is based on van Rijsbergen\'s effectiveness measure <math>E = 1 - \\frac{1}{\\frac{\\alpha}{P} + \\frac{1-\\alpha}{R}}</math>.  Their relationship is <math>F_\\beta = 1 - E</math> where <math>\\alpha=\\frac{1}{1 + \\beta^2}</math>.\n\n==Limitations as goals==\nThere are other parameters and strategies for performance metric of information retrieval system, such as the area under the precision-recall curve (AUC).<ref>Zygmunt Zaj\xc4\x85c. What you wanted to know about AUC.  http://fastml.com/what-you-wanted-to-know-about-auc/</ref> \n\nFor [[web document]] retrieval, if the user\'s objectives are not clear, the  precision and recall can\'t be optimized. As summarized by Lopresti,<ref>Lopresti, Daniel (2001); [http://www.csc.liv.ac.uk/~wda2001/Panel_Presentations/Lopresti/Lopresti_files/v3_document.htm \'\'WDA 2001 panel\'\']</ref>\n:\'\'"[[Browsing]] is a comfortable and powerful paradigm (the [[Serendipity|serendipity effect]]).\'\'\n:* \'\'Search results don\'t have to be very good.\'\'\n:* \'\'Recall?    Not important (as long as you get at least some good hits).\'\'\n:* \'\'Precision? Not important (as long as at least some of the hits on the first page you return are good)."\'\'\n\n==See also==\n* [[Binary classification]]\n* [[Information retrieval]]\n* [[Receiver operating characteristic]]\n* [[Relevance]]\n* [[Sensitivity and specificity]]\n* [[Type I and type II errors]], where \'\'false positives\'\' and \'\'false negatives\'\' are defined\n* [[Uncertainty coefficient]], aka Proficiency\n\n== Sources ==\n<references>\n* Baeza-Yates, Ricardo; Ribeiro-Neto, Berthier (1999). \'\'Modern Information Retrieval\'\'. New York, NY: ACM Press, Addison-Wesley, Seiten 75 ff. ISBN 0-201-39829-X\n* Hj\xc3\xb8rland, Birger (2010); \'\'The foundation of the concept of relevance\'\', Journal of the American Society for Information Science and Technology, 61(2), 217-237\n* Makhoul, John; Kubala, Francis; Schwartz, Richard; and Weischedel, Ralph (1999); [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.4637 \'\'Performance measures for information extraction\'\'], in \'\'Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February 1999\'\'\n* van Rijsbergen, Cornelis Joost "Keith" (1979); \'\'Information Retrieval\'\', London, GB; Boston, MA: Butterworth, 2nd Edition, ISBN 0-408-70929-4\n</references>\n\n== External links ==\n* [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval \xe2\x80\x93 C. J. van Rijsbergen 1979]\n* [http://www.text-analytics101.com/2014/10/computing-precision-and-recall-for.html Computing Precision and Recall for a Multi-class Classification Problem]\n\n[[Category:Information retrieval]]\n[[Category:Information science]]\n[[Category:Bioinformatics]]\n[[Category:Summary statistics for contingency tables]]\n\n[[de:Beurteilung eines Klassifikators#Anwendung im Information Retrieval]]'
p70
sg6
S'Precision and recall'
p71
ssI154
(dp72
g2
S'http://en.wikipedia.org/wiki/AUTINDEX'
p73
sg4
S"{{multiple issues|\n{{COI|date=September 2014}}\n{{notability|Products|date=September 2014}}\n}}\n\n'''AUTINDEX''' is a commercial [[text mining]] software package based on sophisticated linguistics.<ref>Ripplinger, B\xc3\xa4rbel 2001: Das Indexierungssystem AUTINDEX, in GLDV Tagung, Giessen</ref><ref>Paul Schmidt, Mahmoud Gindiyeh & Gintare Grigonyte, 2009: Language Technology for Information Systems. In: Proceedings of KDIR - The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira, 6\xe2\x80\x938 October 2009, Portugal</ref><ref>Paul Schmidt & Mahmoud Gindiyeh, 2009: Language Technology for Multilingual Information and Document Management. In: Proceedings of ASLIB, London, 19\xe2\x80\x9320 November</ref>\n\n'''AUTINDEX''' resulting from research in [[information extraction]] <ref>Paul Schmidt, Thomas B\xc3\xa4hr & Dr.-Ing. Jens Biesterfeld &Thomas Risse & Kerstin Denecke & Claudiu Firan, 2008: LINSearch. Aufbereitung von Fachwissen f\xc3\xbcr die gezielte Informationsversorgung. In: Proceedings of Knowtech, Frankfurt</ref><ref>Ursula Deriu, J\xc3\xb6rn Lehmann & Paul Schmidt, 2009: \xe2\x80\x9aErstellung einer Technik-Ontologie auf der Basis ausgefeilter Sprachtechnologie\xe2\x80\x99. In: Proceedings Knowtech, Frankfurt</ref> is a product of the Institute of Applied Information Sciences (IAI) which is a non-profit institute that has been researching and developing [[language technology]] since its foundation in 1985. IAI is an institute affiliated to [[Saarland University]] in Saarbr\xc3\xbccken, Germany.\n\n'''AUTINDEX''' is the result of a number of research projects funded by the EU (Project BINDEX <ref>[//www.lrec-conf.org/proceedings/lrec2002/pdf/255.pdf]. Dieter Maas, Nuebel Rita, Catherine Pease, Paul Schmidt: Bilingual Indexing for Information Retrieval with AUTINDEX. LREC 2002.</ref>), by Deutsche Forschungsgemeinschaft and the German Ministry for Economy. Amongst the latter there are the projects LinSearch <ref>[//www.l3s.de/AR07/layout/L3S-AR2007_screen.pdf]. Project LinSearch. P. 32.</ref> and WISSMER,<ref>[//www.wissmer.info/index.php/de/]. Project Wissmer.</ref> see also the reference to IAI-Webite.<ref>[//www.iai-sb.de/forschung/content/view/67/89/]. Wissmer-Project on IAI-Site.</ref>\n\nThe basic functionality of AUTINDEX is the extraction of key words from a document to represent the semantics of the document.<ref>Paul Schmidt, Mahmoud Gindiyeh, Gintare Grigonyte: ''Language Technology for Information Systems.'' In: ''Proceedings of KDIR \xe2\x80\x93 The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira.'' 6.\xe2\x80\x938. Oktober 2009, Portugal. 2009, S. 259 - 262.</ref> Ideally the system is integrated with a [[thesaurus]] that defines the standardised terms to be used for key word assignment.<br> \nAUTINDEX is used in library applications (e.g. integrated in [[dandelon.com]]) as well as in high quality (expert) information systems <ref>[//www.wti-frankfurt.de]. WTI Information system.</ref> and in document management and content management environments. <br> \n \nTogether with AUTINDEX a number of additional software comes along such as an integration with [[Apache Solr]] / [[Lucene]] to provide a complete [[information retrieval]] environment, a classification and [[categorisation]] system on the basis of a [[machine learning]] <ref>Mahmoud Gindiyeh: Anwendung wahrscheinlichkeitstheoretischer Methoden in der linguistischen Informationsverarbeitung, Logos Verlag, Berlin, 2013.</ref> software that assigns domains to the document, and a system for searching with semantically similar terms that are collected in so called [[tag clouds]].<ref>[//www.wissmer.info]. Electro mobility information system.</ref>\n\n==See also==\n\n* [[Information retrieval]]\n* [[Linguistics]]\n* [[Knowledge Management]]\n* [[Natural Language Processing]]\n* [[Semantics]]\n\n== References ==\n{{reflist}}\n\n== Publications ==\n* Ripplinger, B\xc3\xa4rbel 2001: Das Indexierungssystem AUTINDEX, in GLDV Tagung, Giessen.\n* Paul Schmidt, Mahmoud Gindiyeh & Gintare Grigonyte, 2009: Language Technology for Information Systems. In: Proceedings of KDIR - The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira, 6\xe2\x80\x938 October 2009, Portugal.\n* Paul Schmidt & Mahmoud Gindiyeh, 2009: Language Technology for Multilingual Information and Document Management. In: Proceedings of ASLIB, London, 19\xe2\x80\x9320 November.\n* Paul Schmidt, Thomas B\xc3\xa4hr & Dr.-Ing. Jens Biesterfeld &Thomas Risse & Kerstin Denecke & Claudiu Firan, 2008: LINSearch. Aufbereitung von Fachwissen f\xc3\xbcr die gezielte Informationsversorgung. In: Proceedings of Knowtech, Frankfurt.\n* Paul Schmidt, Mahmoud Gindiyeh, Gintare Grigonyte: ''Language Technology for Information Systems.'' In: ''Proceedings of KDIR \xe2\x80\x93 The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira.'' 6.\xe2\x80\x938. Oktober 2009, Portugal. 2009, S. 259 - 262.\n* Paul Schmidt, Mahmoud Gindiyeh: ''Language Technology for Multilingual Information and Document Management.'' In: ''Proceedings of ASLIB.'' London, 19.\xe2\x80\x9320. November 2009.\n* R\xc3\xb6sener, Christoph, Ulrich Herb: ''Automatische Schlagwortvergabe aus der SWD f\xc3\xbcr Repositorien.'' Zusammen mit Ulrich Herb in ''Proceedings.'' Berufsverband Information Bibliothek, Bibliothekartage. 97. Deutscher Bibliothekartag, Mannheim, 2008.\n* Svenja Siedle: ''Suchst du noch oder wei\xc3\x9ft du schon? Inhaltserschlie\xc3\x9fung leicht gemacht mit automatischer Indexierung.'' In: ''tekom-Jahrestagung und tcworld conference 2013''\n* Michael Gerards, Adreas Gerards, Peter Weiland: ''Der Einsatz der automatischen Indexierungssoftware AUTINDEX im Zentrum f\xc3\xbcr Psychologische Information und Dokumentation (ZPID).'' 2006 ([http://zpid.de/download/PSYNDEXmaterial/autindex.pdf Online] bei zpid.de, PDF-Datei)\n* Mahmoud Gindiyeh: Anwendung wahrscheinlichkeitstheoretischer Methoden in der linguistischen Informationsverarbeitung. Logos Verlag, Berlin, 2013.\n\n== External links ==\n* http://www.iai-sb.de/ Institute for Applied Information Sciences\n\n[[Category:Natural language processing]]\n[[Category:Information retrieval]]"
p74
sg6
S'AUTINDEX'
p75
ssI28
(dp76
g2
S'http://en.wikipedia.org/wiki/Ordered weighted averaging aggregation operator'
p77
sg4
S'In applied mathematics \xe2\x80\x93 specifically in [[fuzzy logic]] \xe2\x80\x93 the \'\'\'ordered weighted averaging (OWA) operators\'\'\' provide a [[parameter]]ized class of mean type aggregation operators. They were introduced by [[Ronald R. Yager]]. Many notable mean operators such as the max, [[arithmetic average]], median and min, are members of this class. They have been widely used in [[computational intelligence]] because of their ability to model linguistically expressed aggregation instructions.\n\n== Definition ==\n\nFormally an \'\'\'OWA\'\'\' operator of dimension <math> \\ n </math> is a mapping <math> F: R_n \\rightarrow R </math> that has an associated collection of weights <math> \\  W = [w_1, \\ldots, w_n] </math> lying in the unit interval and summing to one and with \t\t\n\n:<math> F(a_1, \\ldots , a_n) =  \\sum_{j=1}^n  w_j b_j</math>\n\nwhere <math> b_j </math> is the \'\'j\'\'<sup>th</sup> largest of the <math> a_i </math>.\n\nBy choosing different \'\'W\'\' one can implement different aggregation operators. The OWA operator is a non-linear operator as a result of the process of determining the \'\'b\'\'<sub>\'\'j\'\'</sub>.\n\n== Properties ==\n\nThe OWA operator is a mean operator. It is [[Bounded operator|bounded]], [[monotonic]], [[symmetric operator|symmetric]], and [[idempotent]], as defined below.\n\n{|class="wikitable"\n|[[Bounded operator|Bounded]]\n|<math>   \\min(a_1, \\ldots, a_n) \\le F(a_1, \\ldots, a_n) \\le \\max(a_1, \\ldots, a_n) </math>\n|-\n|[[Monotonic]]\n|<math>   F(a_1, \\ldots, a_n) \\ge F(g_1, \\ldots, g_n) </math> if <math> a_i \\ge g_i </math> for <math>\\ i = 1,2,\\ldots,n </math>\n|-\n|[[symmetric operator|Symmetric]]\n|<math>   F(a_1, \\ldots, a_n)  = F(a_\\boldsymbol{\\pi(1)}, \\ldots, a_\\boldsymbol{\\pi(n)})</math> if <math>\\boldsymbol{\\pi} </math> is a permutation map\n|-\n|[[Idempotent]]\n|<math>  \\ F(a_1, \\ldots, a_n)  =  a </math> if all <math> \\ a_i = a </math>\n|}\n\n== Notable OWA operators ==\n:<math> \\ F(a_1, \\ldots, a_n) = \\max(a_1, \\ldots, a_n) </math> if <math> \\ w_1 = 1 </math> and <math> \\ w_j = 0 </math> for <math> j \\ne 1 </math>\n\n:<math> \\ F(a_1, \\ldots, a_n) = \\min(a_1, \\ldots, a_n) </math> if <math> \\ w_n = 1 </math> and <math> \\ w_j = 0 </math> for <math> j \\ne n </math>\n\n== Characterizing features ==\n\nTwo features have been used to characterize the OWA operators. The first is the attitudinal character(orness).\n\nThis is defined as\n:<math>A-C(W)= \\frac{1}{n-1} \\sum_{j=1}^n (n - j) w_j. </math>\n\nIt is known that <math> A-C(W) \\in [0, 1] </math>.\n\nIn addition \'\'A\'\'&nbsp;&minus;&nbsp;\'\'C\'\'(max) = 1, A&nbsp;&minus;&nbsp;C(ave) = A&nbsp;&minus;&nbsp;C(med) = 0.5 and A&nbsp;&minus;&nbsp;C(min) = 0. Thus the A&nbsp;&minus;&nbsp;C goes from 1 to 0 as we go from Max to Min aggregation. The attitudinal character characterizes the similarity of aggregation to OR operation(OR is defined as the Max).\n\nThe second feature is the dispersion. This defined as\n\n:<math>H(W) = -\\sum_{j=1}^n w_j \\ln (w_j).</math>\n\nAn alternative definition is <math>E(W) = \\sum_{j=1}^n w_j^2 .</math> The dispersion characterizes how uniformly the arguments are being used\n\n== A literature survey: OWA (1988-2014)==\nThe historical reconstruction of scientific development of the OWA field, the identification of the dominant direction of knowledge accumulation that emerged since the publication of the first OWA paper, and to discover the most active lines of research has recently been published, (see: http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full). The results suggest, as expected, that Yager\'s paper[1] (IEEE Trans. Systems Man Cybernet, 18(1), 183\xe2\x80\x93190, 1988) is the most influential paper and the starting point of all other research using OWA. Starting from his contribution, other lines of research developed and we describe them. Full list of papers published in OWA is also available at http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full) \n\n== Type-1 OWA aggregation operators ==\n\nThe above Yager\'s OWA operators are used to aggregate the crisp values. Can we aggregate fuzzy sets in the OWA mechanism ? The\n\'\'\'[[Type-1 OWA operators]]\'\'\' have been proposed for this purpose. So the \'\'\'[[type-1 OWA operators]]\'\'\' provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.\n\nThe \'\'\'[[Type-1 OWA operators|type-1 OWA operator]]\'\'\' is defined according to the alpha-cuts of fuzzy sets as follows:\n\nGiven the \'\'n\'\' linguistic weights <math>\\left\\{ {W^i} \\right\\}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\\;\\;1]</math>, then for each <math>\\alpha \\in [0,\\;1]</math>, an <math>\\alpha </math>-level type-1 OWA operator with <math>\\alpha </math>-level sets <math>\\left\\{ {W_\\alpha ^i } \\right\\}_{i = 1}^n </math> to aggregate the <math>\\alpha </math>-cuts of fuzzy sets <math>\\left\\{ {A^i} \\right\\}_{i =1}^n </math> is given as\n\n: <math>\n\\Phi_\\alpha \\left( {A_\\alpha ^1 , \\ldots ,A_\\alpha ^n } \\right) =\\left\\{ {\\frac{\\sum\\limits_{i = 1}^n {w_i a_{\\sigma (i)} } }{\\sum\\limits_{i = 1}^n {w_i } }\\left| {w_i \\in W_\\alpha ^i ,\\;a_i } \\right. \\in A_\\alpha ^i ,\\;i = 1, \\ldots ,n} \\right\\}</math>\n\nwhere <math>W_\\alpha ^i= \\{w| \\mu_{W_i }(w) \\geq \\alpha \\}, A_\\alpha ^i=\\{ x| \\mu _{A_i }(x)\\geq \\alpha \\}</math>, and <math>\\sigma :\\{\\;1, \\ldots ,n\\;\\} \\to \\{\\;1, \\ldots ,n\\;\\}</math> is a permutation function such that <math>a_{\\sigma (i)} \\ge a_{\\sigma (i + 1)} ,\\;\\forall \\;i = 1, \\ldots ,n - 1</math>, i.e., <math>a_{\\sigma (i)} </math> is the <math>i</math>th largest\nelement in the set <math>\\left\\{ {a_1 , \\ldots ,a_n } \\right\\}</math>.\n\nThe computation of the \'\'\'[[Type-1 OWA operators|type-1 OWA]]\'\'\' output is implemented by computing the left end-points and right end-points of the intervals <math>\\Phi _\\alpha \\left( {A_\\alpha ^1 , \\ldots ,A_\\alpha ^n } \\right)</math>:\n<math>\\Phi _\\alpha \\left( {A_\\alpha ^1 , \\ldots ,A_\\alpha ^n } \\right)_{-} </math> and <math>\n\\Phi _\\alpha \\left( {A_\\alpha ^1 , \\ldots ,A_\\alpha ^n } \\right)_ {+},</math>\nwhere <math>A_\\alpha ^i=[A_{\\alpha-}^i, A_{\\alpha+}^i], W_\\alpha ^i=[W_{\\alpha-}^i, W_{\\alpha+}^i]</math>. Then membership function of resulting aggregation fuzzy set is:\n\n:<math>\\mu _{G} (x) = \\mathop \\vee \\limits_{\\alpha :x \\in \\Phi _\\alpha \\left( {A_\\alpha ^1 , \\cdots\n,A_\\alpha ^n } \\right)_\\alpha } \\alpha </math>\n\nFor the left end-points, we need to solve the following programming problem:\n\n:<math> \\Phi _\\alpha \\left( {A_\\alpha ^1 , \\cdots ,A_\\alpha ^n } \\right)_{-} = \\mathop {\\min }\\limits_{\\begin{array}{l} W_{\\alpha - }^i \\le w_i \\le W_{\\alpha + }^i A_{\\alpha - }^i \\le a_i \\le A_{\\alpha + }^i  \\end{array}} \\sum\\limits_{i = 1}^n {w_i a_{\\sigma (i)} / \\sum\\limits_{i = 1}^n {w_i } } </math>\n\nwhile for the right end-points, we need to solve the following programming problem:\n\n:<math>\\Phi _\\alpha \\left( {A_\\alpha ^1 , \\cdots , A_\\alpha ^n } \\right)_{+} = \\mathop {\\max }\\limits_{\\begin{array}{l} W_{\\alpha - }^i \\le w_i \\le W_{\\alpha + }^i  A_{\\alpha - }^i \\le a_i \\le A_{\\alpha + }^i  \\end{array}} \\sum\\limits_{i = 1}^n {w_i a_{\\sigma (i)} / \\sum\\limits_{i =\n1}^n {w_i } } </math>\n\n[http://dx.doi.org/10.1109/TKDE.2010.191 This paper] has presented a fast method to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently.\n\n== References ==\n\n* Yager, R. R., "On ordered weighted averaging aggregation operators in multi-criteria decision making," IEEE Transactions on Systems, Man and Cybernetics 18, 183\xe2\x80\x93190, 1988.\n\n* Yager, R. R. and Kacprzyk, J., [http://www.amazon.com/dp/079239934X The Ordered Weighted Averaging Operators: Theory and Applications], Kluwer: Norwell, MA, 1997.\n\n* Liu, X., "The solution equivalence of minimax disparity and minimum variance problems for OWA operators," International Journal of Approximate Reasoning 45, 68\xe2\x80\x9381, 2007.\n\n* Emrouznejad (2009) SAS/OWA: ordered weighted averaging in SAS optimization, Soft Computing [http://www.springerlink.com/content/7277l73334r108x5/]\n\n* Emrouznejad, A. and M. Marra (2014), Ordered Weighted Averaging Operators 1988\xe2\x80\x932014: A citation-based literature survey, International Journal of Intelligent Systems, 29:994-1014 [http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full  & http://onlinelibrary.wiley.com/store/10.1002/int.21673/asset/supinfo/int21673-sup-0001-SupMat.docx?v=1&s=c0d8bdd220a31c876eb5885521cfa16d191f334d]. \n\n* Torra, V. and Narukawa, Y., Modeling Decisions: Information Fusion and Aggregation Operators, Springer: Berlin, 2007.\n\n* Majlender, P., "OWA operators with maximal R\xc3\xa9nyi entropy," Fuzzy Sets and Systems 155, 340\xe2\x80\x93360, 2005.\n\n* Szekely, G. J. and Buczolich, Z., " When is a weighted average of ordered sample elements a maximum likelihood estimator of the location parameter?" Advances in Applied Mathematics 10, 1989, 439\xe2\x80\x93456.\n\n* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, "Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers," Fuzzy Sets and Systems, Vol.159, No.24, pp.&nbsp;3281\xe2\x80\x933296, 2008 [http://dx.doi.org/10.1016/j.fss.2008.06.018]\n\n* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, "Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments," IEEE Transactions on Knowledge and Data Engineering, vol. 23, no.10, 2011, pp.&nbsp;1455\xe2\x80\x931468.[http://dx.doi.org/10.1109/TKDE.2010.191]\n\n* S.-M. Zhou, R. I. John, F. Chiclana and J. M. Garibaldi, "On aggregating uncertain information by type-2 OWA operators for soft decision making," International Journal of Intelligent Systems, vol. 25, no.6, pp.&nbsp;540\xe2\x80\x93558, 2010.[http://dx.doi.org/10.1002/int.20420]\n\n[[Category:Artificial intelligence]]\n[[Category:Logic in computer science]]\n[[Category:Fuzzy logic]]\n[[Category:Information retrieval]]'
p78
sg6
S'Ordered weighted averaging aggregation operator'
p79
ssI157
(dp80
g2
S'http://en.wikipedia.org/wiki/Contextual searching'
p81
sg4
S'\'\'\'Contextual search\'\'\' is a form of optimizing web-based search results based on context provided by the user and the computer being used to enter the query.<ref>Susan E. Feldman. \'\'The Answer Machine\'\', Synthesis Lectures on Information Concepts, Retrieval, and Services. [http://www.morganclaypool.com/doi/abs/10.2200/S00442ED1V01Y201208ICR023 http://www.morganclaypool.com/doi/abs/10.2200/S00442ED1V01Y201208ICR023]</ref> Contextual search services differ from current search engines based on traditional information retrieval that return lists of documents based on their [[Relevance (information retrieval)|relevance]] to the query. Rather, contextual search attempts to increase the [[Precision and recall|precision]] of results based on how valuable they are to individual users.<ref>Pitokow, James; Hinrich Sch\xc3\xbctze; Todd Cass; Rob Cooley; Don Turnbull; Andy Edmonds; Eytan Adar; Thomas Breuel (2002). "Personalized search". [http://www.cond.org/p50-pitkow.pdf http://www.cond.org/p50-pitkow.pdf] Communications of the ACM (CACM) 45 (9): 50\xe2\x80\x9355.</ref>\n\n== Basic Contextual Search ==\nThe basic form of contextual search is the process of scanning the full-text of a query in order to understand what the user needs. Web search engines scan HTML pages for content and return an index rating based on how relevant the content is to the entered query. HTML pages that have a higher occurrence of query keywords within their content are rated higher. Users have limited control over the context of their query based on the words they use to search with.<ref>Steve Lawrence. \'\'Context in Web Search\'\', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 25, 2000.</ref>  For example, users looking for the menu portion of a website can add \xe2\x80\x9cmenu\xe2\x80\x9d to the end of their query to provide the search engine with context of what they need. The next step in contextualizing search is for the search service itself to request information that narrows down the results, such as Google asking for a time range to search within.\n\n== Explicitly Supplied Context ==\nCertain search services, including many Meta search engines, request individual contextual information from users to increase the precision of returned documents. Inquirus 2 is a Meta search engine that acts as a mediator between the user query and other search engines. When searching on Inquirus 2, users enter a query and specify constraints such as the information need category, maximum number of hits, and display formats.<ref>Steve Lawrence. \'\'Context in Web Search\'\', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 27, 2000.</ref> For example a user looking for research papers can specify documents with \xe2\x80\x9creferences\xe2\x80\x9d or \xe2\x80\x9cabstracts\xe2\x80\x9d to be rated higher. If another user is searching for general information on the topic rather than research papers, they can specify the GenScore attribute to have a heavier weight.<ref>Steve Lawrence, C. Lee Giles. \'\'Inquirus, the NECI meta search engine\'\'[http://www7.scu.edu.au/1906/com1906.htm]</ref>\n\nExplicitly supplied context effectively increases the precision of results, however, these search services tend to suffer from poor user-experience. Learning the interface of programs like Inquirus can prove challenging for general users without knowledge of search metrics. Aspects of supplied context do appear on major search engines with better user-interaction such as Google and Bing. Google allows users to filter by type: Images, Maps, Shopping, News, Videos, Books, Flights, and Apps.<ref>[https://support.google.com/websearch/answer/142143?hl=en https://support.google.com/websearch/answer/142143?hl=en], Filter your search results</ref> Google has an extensive [https://support.google.com/websearch/answer/2466433?rd=1 list of search operators] that allow users to explicitly limit results to fit their needs such as restricting certain file types or removing certain words.<ref>[https://support.google.com/websearch/answer/2466433?rd=1 https://support.google.com/websearch/answer/2466433?rd=1], Search Operators</ref> Bing also uses a similar set of search operators to assist users in explicitly narrowing down the context of their queries. Bing allows users to search within a time range, by file type, by location, language, and more.<ref>[http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/ http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/], Bing Tricks</ref>\n\n== Automatically Inferred Context ==\nThere are other systems being developed that are working on automatically inferring the context of user queries based on the content of other documents they view or edit. [[Watson (computer)|IBM\'s Watson Project]] aims to create a cognitive technology that dynamically learns as it processes user queries. When presented with a query Watson creates a hypothesis that is evaluated against its present bank of knowledge based on previous questions. As related terms and relevant documents are matched against the query, Watson\'s hypothesis is modified to reflect the new information provided through unstructured data based on information it has obtained in previous situations.<ref>[http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html], How Watson Works - IBM</ref> Watson\'s ability to build off previous knowledge allows queries to be automatically filtered for similar contexts in order to supply precise results.\n\nMajor search services such as Google, Bing, and Yahoo also have a system of automatically inferring the context of particular user queries. Google tracks user\'s previous queries and selected results to further personalize results for those individuals. For example if a user consistently searches for articles related to animals, wild animals, or animal care a search for "jaguar" would rank an article on jaguar cats higher than links to Jaguar Cars.<ref>Eric J Glover, Steve Lawrence, Michael D. Gordon, William P. Birmingham, C. Lee Giles. \'\'Web Search - Your Way\'\', NEC Research Institution [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.41.7499&rep=rep1&type=pdf]</ref> Similar to Watson, search services strive to learn from users based on previous experiences to automatically provide context on current queries. Bing also provides automatic context for particular queries based on content of the query itself. A [http://www.bing.com/search?q=pizza&go=Submit&qs=n&form=GEOMA1&pq=pizza&sc=8-1&sp=-1&sk=&cvid=883269b61529466e810bc096e371ec19 search of "pizza"] returns an interactive list of restaurants and their ratings based on the approximate location of the user\'s computer. The Bing server automatically infers that when a user searches for a food item they are interested in documents within the context of purchasing that food item or finding restaurants that sell that particular item.\n\n=== Contextual Mobile Search ===\nThe drive to develop better contextualized search coincides with the increasing popularity of using mobile phones to complete searches. BIA/Kelsey research marketing firm projects that by 2015 mobile local search will "exceed local search by more than 27 billion queries".<ref>[http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp], Mobile Search to Surpass Desktop</ref> Mobile phones provide the opportunity to provide search services with a broader supply of contextual information, particularly for location services but also personalized searches based on the wealth of information stored locally on the phone including contacts information, geometric analysis such as speed and elevation, and installed apps.<ref>[http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/ http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/], Contextually Aware Mobile Devices</ref> Mobile start up company [http://everything.Me Everything.Me] is one company that is moving towards turning the smartphone into an all-in-one device the provides relevant information for specific users. Everything.Me pushes app updates and suggestion to a user\'s home-screen based on user movement, location, current time, past search queries, and entertainment preferences.<ref>[http://socialtimes.com/mobile-contextual-search-future_b149394 http://socialtimes.com/mobile-contextual-search-future_b149394], Contextual Search through Mobile and Everything.Me</ref> For example when a user opens their phone in the morning Everything.Me will present users with apps relevant to how that users interacts with their phone in the morning\xe2\x80\x94presenting weather apps, bus apps, and news apps.<ref>[http://everything.me/about/ Everything.Me]</ref> Later when that user goes to work Everything.Me will update the work related applications to be prioritized over other apps. Everything.Me anticipates a user\'s needs based on their current actions and past interactions on the web. This process of automatically obtaining context from mobile phones can help to increase the precision of user queries. For instance if a user searches for a place to eat while at work Everything.Me will take work into context and return restaurants that would be more appropriate for a lunch break at the office.<ref>[http://everything.me/ http://everything.me/], Video Information</ref>\n\n== References ==\n{{reflist}}\n\n{{Internet search}}\n\n{{DEFAULTSORT:Contextual Searching}}\n[[Category:Internet search engines]]\n[[Category:Semantic Web]]\n[[Category:Information retrieval]]\n[[Category:Internet terminology]]'
p82
sg6
S'Contextual searching'
p83
ssI31
(dp84
g2
S'http://en.wikipedia.org/wiki/Term Discrimination'
p85
sg4
S'\'\'\'Term Discrimination\'\'\' is a way to rank keywords in how useful they are for [[Information Retrieval]].\n\n== Overview ==\n\nThis is a method similar to [[tf-idf]] but it deals with finding keywords suitable for [[information retrieval]] and ones that are not.  Please refer to [[Vector Space Model]] first.\n\nThis method uses the concept of \'\'Vector Space Density\'\' that the less dense an [[occurrence matrix]] is, the better an information retrieval query will be.\n\nAn optimal index term is one that can distinguish two different documents from each other and relate two similar documents.  On the other hand, a sub-optimal index term can not distinguish two different document from two similar documents.  \n\nThe discrimination value is the difference in the occurrence matrix\'s vector-space density versus the same matrix\'s vector-space without the index term\'s density.\n\n Let:\n <math>A</math> be the occurrence matrix\n <math>A_k</math> be the occurrence matrix without the index term <math>k</math>\n and <math>Q(A)</math> be density of <math>A</math>.\n Then:\n The discrimination value of the index term <math>k</math> is: \n <math>DV_k = Q(A) - Q(A_k)</math>\n\n== How to compute ==\n\nGiven an [[occurrency matrix]]: <math>A</math> and one keyword: <math>k</math>\n* Find the global document [[centroid]]: <math>C</math> (this is just the average document vector)\n* Find the average [[euclidean distance]] from every document vector, <math>D_i</math> to <math>C</math>\n* Find the average euclidean distance from every document vector, <math>D_i</math> to <math>C</math> \'\'IGNORING\'\' <math>k</math>\n* The difference between the two values in the above step is the \'\'discrimination value\'\' for keyword <math>K</math>\n\nA higher value is better because including the keyword will result in better information retrieval.\n\n== Qualitative Observations ==\nKeywords that are \'\'[[Sparse matrix|sparse]]\'\' should be poor discriminators because they have poor \'\'[[Precision and recall|recall]],\'\'\nwhereas\nkeywords that are \'\'frequent\'\' should be poor discriminators because they have poor \'\'[[Precision and recall|precision]].\'\'\n\n== References ==\n* [[Gerard Salton|G. Salton]], A. Wong, and C. S. Yang (1975), "[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing]," \'\'Communications of the ACM\'\', vol. 18, nr. 11, pages 613\xe2\x80\x93620. \'\'(The article in which the vector space model was first presented)\'\'\n\n* Can, F., Ozkarahan, E. A (1987), "Computation of term/document discrimination values by use of the cover coefficient concept." \'\'Journal of the American Society for Information Science\'\', vol. 38, nr. 3, pages 171-183.\n\n[[Category:Information retrieval]]'
p86
sg6
S'Term Discrimination'
p87
ssI160
(dp88
g2
S'http://en.wikipedia.org/wiki/Comprehensive Model of Information Seeking'
p89
sg4
S'The \'\'\'Comprehensive Model of Information Seeking\'\'\', or CMIS, is a theoretical construct designed to predict how people will seek information.  It was first developed by J. David Johnson and has been utilized by a variety of disciplines including [[Library and Information Science]] and [[Health Communication]].\n\nThe CMIS has been empirically tested in health and organizational contexts<ref>Johnson, J. D., & Meischke, H. (1993). Cancer-related channel selection:  An extensionfor a sample of women who have had a mammogram. Women & Health, 20, 31-44.; Johnson, J. D., Donohue, W. A., Atkin, C. K., & Johnson, S. H. (1995). A comprehensive model of information seeking: Tests focusing on a technical organization. \nScience Communication, 16, 274-303.</ref> The CMIS has inherent strengths for studying how people react to health problems such as cancer.<ref name="auto">Johnson, J. D., Andrews, J. E. & Allard, S. (2001). A Model for Understanding and Affecting Genetics Information Seeking. Library and Information Science Research 23(4): 335-349.</ref> The CMIS specifies \'\'antecedents\'\' that explain why people become information seekers, \'\'information carrier characteristics\'\' that shape how people go about looking for information, and \'\'information seeking actions\'\' that reflect the nature of the search itself.\n\n==Design==\n\n[[File:Diagram of the Comprehensive Model of Information Seeking.jpg|thumb|right|The Comprehensive Model of Information Seeking]]\nThe CMIS has been quantitatively tested and performs well when it comes to health information seeking behaviors (HISB).<ref name="auto"/> There are three main schemas in the CMIS. These are:  Antecedents, information field, and information seeking actions.  The antecedents are those factors that determine how an information consumer will receive the information.  Those factors are:  Demographics, personal experience, salience, and beliefs.  These factors are fluid and can change during the health information seeking process.  The second schema is the information fields that consist of characteristics and utilities.  This schema is concerned with the channels and carriers of information.  A person\xe2\x80\x99s understanding is developed through the information field.  The third schema involves the transformational processes and measured by the consumer\xe2\x80\x99s understanding of the messages received through the information field.  The final schema involves information seeking actions.  This is what the consumer does as a result of the first two schemas through information seeking.  There are three major dimensions:  the scope, depth, and method of information seeking.<ref name="auto"/>\n\n==Antecdents==\nThe CMIS antecedents\xe2\x80\x94demographics, personal experience, salience, and beliefs\xe2\x80\x94are factors that determine an individual\'s natural predisposition to search for information from particular information carriers. Certain types of health information seeking can be triggered by an individual\'s degree of personal experience with disease.<ref>Johnson, J. D. (1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.</ref> In the CMIS framework, two personal relevance factors, salience and beliefs, are seen as the primary determinants in translating a perceived gap into an active search for information. Salience refers to the personal significance of health information to the individual, such as perceptions of risk to one\'s health, which are likely to result in information seeking action. However, people also may be motivated to gather information to determine the implications of health events for themselves and/or others related to their future activities, a factor directly related to the rapidly growing field of genetics. An individual\'s beliefs about the nature of a particular disease, its impacts, and level of control, all directly relate to self-efficacy, one of our key variables, and one that plays an important role in information seeking and people\'s more general pattern of actions related to health.<ref>Johnson, J. D.(1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.</ref>\n\n==Information Carrier Characteristics==\n\nThe information carrier characteristics are drawn from a model of Media Exposure and Appraisal (MEA) that has been tested on a variety of information carriers, including both sources and channels, and in a variety of cultural settings. Following the MEA, the CMIS focuses on editorial tone, communication potential, and utility. In the CMIS, characteristics are composed of editorial tone, which reflects an audience member\'s perception of credibility, while communication potential relates to issues of style and comprehensiveness. Utility relates the characteristics of a medium directly to the needs of an individual, and shares much with the uses and gratifications perspectives. For example, is the information contained in the medium relevant, topical, and important for the individual\'s purposes? In general, utility is very important for health information seeking.<ref name="auto"/>\n\n==Information Seeking Actions==\n\nThere are several types of information seeking actions that can result from the impetus provided by the factors identified by the CMIS. For example, search behavior can be characterized by its extent, or the number of activities carried out, which has two components: scope, the number of alternatives investigated; and, depth, the number of dimensions of an alternative investigated. There is also the method of the search, or channel, as another major dimension of the search.  For instance, an individual might choose the method of consulting a telephone information service, decide to have a narrow scope by only asking questions about smoking cessation clinics, but investigate every recommendation in detail, thus increasing the depth of the search.<ref name="auto"/>\n\n==Stages in the CMIS==\n\nA key concept from the CMIS is the notion of \xe2\x80\x9cstages,\xe2\x80\x9d or \xe2\x80\x9ccancer involvement\xe2\x80\x9d.  According to the CMIS, an individual may be at one of four stages regarding a cancer threat, and thereby have differing information needs and behaviors.\n\nThe first stage, \'\'Casual\'\', is characterized by a general lack of concern or interest. At this stage, individuals are not purposive in their search for cancer-related information; rather, their search is accidental and aimless, even apathetic.\n \nThe second stage is \'\'Purposive-Placid\'\'. This is characterized by the question, \xe2\x80\x9cWhat can I do to prevent cancer?\xe2\x80\x9d Individuals here might have some passing interest in cancer or genetic information, but are generally still not affected or directly concerned.\n\nThe third stage is \'\'Purposive-Clustered\'\'. Here, an individual will be in closer proximity to cancer. This is the point at which a person is motivated to look for practical information that will address the specific problem. For example, a first-degree relative of a recently diagnosed breast cancer patient may seek genetic screening or [[BRCA mutation|BRCA]] 1/2 testing. The person could clearly benefit from such information- seeking behavior since medical authorities acknowledge that early detection of cancer leads to earlier treatments and better treatment outcomes.\n\nThe fourth stage, \'\'Directed\'\', includes individuals who have been diagnosed as having cancer. Such individuals need knowledge for making informed decisions about treatment and management of the disease.<ref name="auto"/>\n\n== References ==\n{{Reflist}}\n\n\n\n[[Category:Communication]]\n[[Category:Information retrieval]]\n[[Category:Health sciences]]'
p90
sg6
S'Comprehensive Model of Information Seeking'
p91
ssI34
(dp92
g2
S'http://en.wikipedia.org/wiki/Information Retrieval Facility'
p93
sg4
S"{{Advert|date=May 2012}}\n\n[[Image:IRF logo 350x350.png|thumb|200px|right|IRF logo]]\n\nThe '''Information Retrieval Facility''' ('''IRF'''), founded 2006 and located in [[Vienna]], [[Austria]], was a research platform for networking and collaboration for professionals in the field of [[information retrieval]]. It ceased operations in 2012.\n\nThe IRF had members in the following categories:\n\n* Researchers in [[information retrieval]] (IR) or related scientific areas\n* Industrial/corporate information management professionals\n* Patent authorities and governmental institutions\n* Students of one of the above\n\n==The Scientific Board==\n'''Maristella Agosti''', Professor, [http://www.dei.unipd.it/wdyn/?IDsezione=1 Department of Information Engineering, University of Padova]\n\n'''Gerhard Budin''', Director of the [http://transvienna.univie.ac.at/forschung/professuren/dr-gerhard-budin/ Center of Translation Studies at the University of Vienna],\nDirector of the [http://www.oeaw.ac.at/icltt/ Department of Corpuslinguistics and Text Technology, Austrian Academy of Sciences]\n\n'''Jamie Callan''', Professor, [http://www.cs.cmu.edu/~callan/Bio.html Language Technologies Institute, CMU, Carnegie Mellon University]\n\n'''Yves Chiaramella''', Professor Emeritus, [http://www-clips.imag.fr/mrim/User/yves.chiaramella/ Department of Computer Science and Applied Mathematics, Joseph Fourier University]\n\n'''Kilnam Chon''', Professor, Computer Science Department, [http://cosmos.kaist.ac.kr/salab/professor/index02.html Korea Advanced Institute of Science and Technology (KAIST)]\n\n'''W. Bruce Croft''', Distinguished Professor, [http://ciir.cs.umass.edu/personnel/croft.html Department of Computer Science and Director Center for Intelligent IR University of Massachusetts Amherst]\n\n'''Hamish Cunningham''', Research Professor, [http://www.dcs.shef.ac.uk/~hamish/ Computer Science Department University Sheffield]\n\n'''Norbert Fuhr''', Chairman of the Scientific Board, Professor, [http://www.is.informatik.uni-duisburg.de/staff/fuhr.html Institute of Informatics and Interactive Systems University Duisburg-Essen]\n\n'''David Hawking''', Science Leader, Project Leader, [http://es.csiro.au/people/Dave/ CSIRO ICT Centre]\n\n'''Noriko Kando''', Professor, [http://www.nii.ac.jp/index.shtml.en Software Engineering Research, Software Research Division, National Institute of Informatics (NII)]\n\n'''Arcot Desai Narasimhalu''', Associate Dean, [http://www.sis.smu.edu.sg/faculty/infosys/arcotdesai.asp School of Information Systems Singapore Management University]\n\n'''John Tait''', Chief Scientific Officer of the IRF, [http://www.johntait.net/ Until July 2007 Professor of Intelligent Information Systems and Associate Dean of the School of Computing and Technology]\n\n'''Benjamin T'sou''', Director, [http://www.cityu.edu.hk/ Language Information Sciences Research Centre, City University of Hong Kong]\n\n'''[[C. J. van Rijsbergen|C.J. van Rijsbergen]]''',\n[http://www.dcs.gla.ac.uk/~keith/ Dept. Computer Science at the University of Glasgow]\n\n==Scientific Goals==\n\n* Modelling innovative and specialised information retrieval systems for global patent document collections.\n* Investigating and developing an adequate technical infrastructure that allows interactive experimentation with formal, mathematical retrieval concepts for very large-scale document collections.<\n* Studying the usability of multi modal user-interfaces to very large-scale information retrieval systems.\n* Integrating real users with actual information needs into the research process of modelling information retrieval systems to allow accurate performance evaluation.\n* Ability to create different views of patent data depending on the focus of the information need.\n* Defining standardised methods for benchmarking the information retrieval process in patent document collections.\n* Ability to handle text and non-text parts of a patent in a coherent manner.\n* Designing, experimenting and evaluating search engines able to retrieve structured and semi-structured documents in very large-scale patent collections.\n* Integrating the temporal dimension of patent documents in retrieval strategies.\n* Improving effectiveness and precision of patent retrieval, based on ontologies and natural-language understanding techniques.\n* Refining IR methods that allow unstructured querying by exploiting available structure within the patent documents.\n* Formal (mathematical) identification and specification of relevant business information needs in the field of intellectual property information.\n* Investigating efficient scaling mechanisms for information retrieval taking into account the characteristics of patent data.\n* Investigating and experimenting with computing architectures for very high-capacity information management.\n* Establishing an open [[eScience]] platform that enables a standardised and easy way of creating and performing IR experiments on a common research infrastructure.\n* Discovering and investigating novel use cases and business applications deriving from intellectual property information.\n* Enabling the formal information retrieval, natural language and semantic processing research to grow into the field of applied sciences in the global, industrial context.\n* Development and integration of different information access methods.\n* Research on effective methods for interactive information retrieval.\n\n==Semantic Supercomputing==\nCurrent technologies to extract concepts from unstructured documents are extremely computational intensive. To allow interactive experimentation with rich and huge text corpora, the IRF has built a high performance computing environment, into which the latest technological advances have been implemented:\n\n* multi-node clusters (currently 80 cores, up to 1024)\n* highest speed interconnect technology\n* single system image with large compound memory (currently 320 GB, up to 4 TB)\n* fully integrated configurable computing (currently 4 FPGA cores, up to 256)\n\nThe combination of these HPC features to accelerate text mining represents the IRF implementation of semantic supercomputing.\n\n==The World Patent Corpus==\nThe IRF aims to bring state-of-the-art information retrieval technology to the community of patent information professionals. We expect information retrieval (IR) technology to become the focus of information technology very soon. All industry sectors can profit from applying modern and future text mining processes to the special requirements of patent research. Although all ideas and concepts are universally applicable to all sorts of intellectual property information, patents require the most sophistication, and confront us with challenging technical and organisational problems. \nThe entire body of patent-related documents possibly constitutes the largest corpus of compound documents, making it a rewarding target for text mining scientists and end-users alike. What\xe2\x80\x99s more, patents have become a crucial issue, in particular for large global corporations and universities. The industrial users of patent data are among the most demanding and important information professionals. As a consequence, they could benefit the most from technology that relieves the burden of researching the large body of patent information.\n\n== Research Collections ==\nThe IRF provides a number of test data collections that have either been developed by the IRF, by one of its members or by third parties. These data collections can be used freely for scientific experimentations.\n\nThe MAtrixware REsearch Collection ([[MAREC]]) is the first standardised patent data corpus for research purposes. It consists of 19 million patent documents in different languages, normalised to a highly specific XML format. The collection has been developed by Matrixware for the IRF.\n\nThe ClueWeb09 collection is a 25 terabyte dataset of about 1 billion web pages crawled in January and February, 2009. It has been created by the Language Technologies Institute at [[Carnegie Mellon University]] to support research on information retrieval and related human language technologies.\n\n==External links==\n* [http://www.ir-facility.org/ Official site: ir-facility.org]\n* [http://youtube.com/watch?v=XpXtRu0XfeA YouTube: The future of information retrieval Part1] \n* [http://youtube.com/watch?v=dRaTeTaHBsI YouTube: The future of information retrieval Part2]\n\n==References==\n* [http://www.iwr.co.uk/information-world-review/analysis/2231880/patent-medicine-info-retrievers?page=2 Patent medicine for information retrievers, Information World Review]\n* [http://ecir2008.dcs.gla.ac.uk/industry.html The IRF and its Role in Professional Information Research, ECIR 2008]\n\n[[Category:Organizations established in 2006]]\n[[Category:Computer science organizations]]\n[[Category:Information retrieval]]\n[[Category:Education in Vienna]]"
p94
sg6
S'Information Retrieval Facility'
p95
ssI163
(dp96
g2
S'http://en.wikipedia.org/wiki/Explicit semantic analysis'
p97
sg4
S'In [[natural language processing]] and [[information retrieval]], \'\'\'explicit semantic analysis\'\'\' (\'\'\'ESA\'\'\') is a [[Vector space model|vectorial]] representation of text (individual words or entire documents) that uses a document corpus as a [[knowledge base]]. Specifically, in ESA, a word is represented as a column vector in the [[tf*idf|tf\xe2\x80\x93idf]] matrix of the text corpus and a document (string of words) is represented as the [[centroid]] of the vectors representing its words. Typically, the text corpus is [[Wikipedia]], though other corpora including the [[Open Directory Project]] have been used.<ref name="infosys">{{cite journal |authors=Ofer Egozi, Shaul Markovitch and Evgeniy Gabrilovich |year=2011 |title=Concept-Based Information Retrieval using Explicit Semantic Analysis |url=http://www.cs.technion.ac.il/~gabr/publications/papers/Egozi2011CBI.pdf|format=pdf|accessdate=January 3, 2015|journal=ACM Transactions on Information Systems |volume=29 |issue=2}}</ref>\n\nESA was designed by [[Evgeniy Gabrilovich]] and Shaul Markovitch as a means of improving [[document classification|text categorization]]<ref>{{cite conference |first1=Evgeniy |last1=Gabrilovich |first2=Shaul |last2=Markovitch |title=Overcoming the brittleness bottleneck using Wikipedia: enhancing text categorization with encyclopedic knowledge |conference=Proc. 21st National Conference on Artificial Intelligence (AAAI) |pages=1301\xe2\x80\x931306 |year=2006 |url=http://www.aaai.org/Papers/AAAI/2006/AAAI06-204.pdf}}</ref>\nand has been used by this pair of researchers to compute what they refer to as "[[Semantics|semantic]] relatedness" by means of [[cosine similarity]] between the aforementioned vectors, collectively interpreted as a space of "concepts explicitly defined and described by humans", where Wikipedia articles (or ODP entries, or otherwise titles of documents in the knowledge base corpus) are equated with concepts.\nThe name "explicit semantic analysis" contrasts with [[latent semantic analysis]] (LSA), because the use of a knowledge base makes it possible to assign human-readable labels to the concepts that make up the vector space.<ref>{{cite conference |first1=Evgeniy |last1=Gabrilovich |first2=Shaul |last2=Markovitch |title=Computing semantic relatedness using Wikipedia-based Explicit Semantic Analysis |conference=Proc. 20th Int\'l Joint Conf. on Artificial Intelligence (IJCAI) |pages=1606\xe2\x80\x931611 |year=2007 |url=http://www.cs.technion.ac.il/~gabr/papers/ijcai-2007-sim.pdf}}</ref><ref name="infosys"/>\n\nESA, as originally posited by Gabrilovich and Markovitch, operates under the assumption that the knowledge base contains topically [[Orthogonality|orthogonal]] concepts. However, it was later shown by Anderka and Stein that ESA also improves the performance of [[information retrieval]] systems when it is based not on Wikipedia, but on the [[Reuters]] corpus of newswire articles, which does not satisfy the orthogonality property; in their experiments, Anderka and Stein used newswire stories as "concepts".<ref>Maik Anderka and Benno Stein. [http://www.uni-weimar.de/medien/webis/publications/papers/stein_2009c.pdf The ESA retrieval model revisited]. Proceedings of the 32nd International ACM Conference on Research and Development in Information Retrieval (SIGIR), pp. 670-671, 2009.</ref>\nTo explain this observation, links have been shown between ESA and the [[generalized vector space model]].<ref>Thomas Gottron, Maik Anderka and Benno Stein. [http://www.uni-weimar.de/medien/webis/publications/papers/stein_2011o.pdf Insights into explicit semantic analysis]. Proceedings of the 20th ACM International Conference on Information and Knowledge Management (CIKM), pp. 1961-1964, 2011.</ref>\nGabrilovich and Markovitch replied to Anderka and Stein by pointing out that their experimental result was achieved using "a single application of ESA (text similarity)" and "just a single, extremely small and homogenous test collection of 50 news documents".<ref name="infosys" />\n\n\'\'\'Cross-language explicit semantic analysis\'\'\' (\'\'\'CL-ESA\'\'\') is a multilingual generalization of ESA.<ref>Martin Potthast, Benno Stein, and Maik Anderka. [http://www.uni-weimar.de/medien/webis/publications/papers/stein_2008b.pdf A Wikipedia-based multilingual retrieval model]. Proceedings of the 30th European Conference on IR Research (ECIR), pp. 522-530, 2008.</ref>\nCL-ESA exploits a document-aligned multilingual reference collection (e.g., again, Wikipedia) to represent a document as a language-independent concept vector. The relatedness of two documents in different languages is assessed by the cosine similarity between the corresponding vector representations.\n\n== See also ==\n* [[Topic model]]\n\n== External links ==\n* [http://www.cs.technion.ac.il/~gabr/resources/code/esa/esa.html Explicit semantic analysis] on Evgeniy Gabrilovich\'s homepage; has links to implementations\n\n== References ==\n{{reflist|2}}\n\n[[Category:Natural language processing]]\n[[Category:Vector space model]]'
p98
sg6
S'Explicit semantic analysis'
p99
ssI37
(dp100
g2
S'http://en.wikipedia.org/wiki/Concept Searching Limited'
p101
sg4
S'{{Infobox company |\n  name   = Concept Searching Limited |\n  logo = [[Image:conceptSearching.jpg]] |\n  company_slogan = "Retrieval Just Got Smarter" |\n  type   =  [[Privately held company|Private]] |\n  foundation     = 2002|\n  location       = [[UK]], [[USA]] |\n  area_served    = Global |\n  industry       = [[Information retrieval]] |\n  products       = conceptSearch<br/>conceptClassifier<br/>conceptClassifier for SharePoint<br/>conceptClassifier for SharePoint Online<br/>Taxonomy Manager<br/>Taxonomy Workflow |\n  homepage       = [http://www.conceptsearching.com/ www.conceptsearching.com]\n}}\n\n\'\'\'Concept Searching Limited\'\'\' is a [[software company]] which specializes in [[information retrieval]] software. It has products for [[Enterprise search]], Taxonomy Management and  [[Statistical classification]].\n\n==History==\nConcept Searching was founded in 2002 in the UK and now has offices in the USA and South Africa. In August 2003 the company introduced the idea of using [[Compound term processing]].<ref>[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&ETOC=RN Lateral thinking in information retrieval] \'\'Information Management and Technology.\'\' 2003. vol 36; part 4, pp 169-173</ref><ref>[http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf] Lateral Thinking in Information Retrieval</ref>\n\nCompound term processing allows statistical information retrieval applications to perform matching using multi-word concepts. This can improve the quality of search results and also allows unstructured information to be automatically classified with semantic metadata.<ref>[http://airforcemedicine.afms.mil/711hswom/InterSymp2008/AFMS%20-%20InterSymp%202008.html] US Air Force Medical Service presentation at InterSymp-2008</ref>\n\nThe company\'s products run on the Microsoft [[.NET Framework|.NET]] platform. The products integrate with Microsoft [[SharePoint]] and many other platforms.<ref>[http://pinpoint.microsoft.com/en-US/partners/Concept-Searching-Inc-4297066101] Microsoft Partner Profile</ref>\n\nConcept Searching has developed the \'\'\'Smart Content Framework\'\'\', which is a toolset that provides an enterprise framework to mitigate risk, automate processes, manage information, protect privacy, and address compliance issues. The Smart Content Framework is used by many large organizations including 23,000 users at the [[NASA]] Safety Center <ref>[http://www.aiim.org/About/News/CS-NASA-Safety] NASA Safety Center using Smart Content Framework</ref>\n\n== Awards ==\n* 100 Companies that Matter in Knowledge Management 2009/2010/2011/2012/2013/2014 <ref>{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-100-Companies-That-Matter-in-Knowledge-Management-94933.aspx |title=KMWorld Magazine}}</ref>\n* KMWorld Trend-Setting Products of 2009/2010/2011/2012/2013/2014 <ref>{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-Trend-Setting-Products-of-2014-98792.aspx |title=Trend-Setting Products}}</ref>\n\n==See also==\n* [[Compound term processing]]\n* [[Enterprise search]]\n* [[Full text search]]\n* [[Information retrieval]]\n* [[Concept Search]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.conceptsearching.com/ Company Website]\n\n[[Category:Information retrieval]]\n[[Category:Privately held companies of the United Kingdom]]'
p102
sg6
S'Concept Searching Limited'
p103
ssI166
(dp104
g2
S'http://en.wikipedia.org/wiki/Generalized vector space model'
p105
sg4
S'{{Confusing|date=January 2010}}\nThe \'\'\'Generalized vector space model\'\'\' is a generalization of the [[vector space model]] used in [[information retrieval]].  Many classifiers, especially those which are related to document or text classification, use the TFIDF basis of VSM.  However, this is where the similarity between the models ends - the generalized model uses the results of the TFIDF dictionary to generate similarity metrics based on distance or angle difference, rather than centroid based classification.  \'\'\'Wong et al.\'\'\'<ref name="wong">{{cite | title=Generalized vector spaces model in information retrieval | url=http://doi.acm.org/10.1145/253495.253506 | first=S. K. M. | last=Wong | coauthors=Wojciech Ziarko, Patrick C. N. Wong | publisher=[[Association for Computing Machinery|SIGIR ACM]] | date=1985-06-05}}</ref> presented an analysis of the problems that the pairwise orthogonality assumption of the [[vector space model]] (VSM) creates. From here they extended the VSM to the generalized vector space model (GVSM).\n\n==Definitions==\n\nGVSM introduces a term to term correlations, which deprecate the pairwise orthogonality assumption. More specifically, the factor considered a new space, where each term vector \'\'t<sub>i</sub>\'\' was expressed as a linear combination of \'\'2<sup>n</sup>\'\' vectors \'\'m<sub>r</sub>\'\' where \'\'r = 1...2<sup>n</sup>\'\'.\n\nFor a document \'\'d<sub>k</sub>\'\' and a query \'\'q\'\' the similarity function now becomes:\n\n:<math>sim(d_k,q) = \\frac{\\sum _{j=1}^n \\sum _{i=1}^n w_{i,k}*w_{j,q}*t_i \\cdot t_j }{\\sqrt{\\sum _{i=1}^n w_{i,k}^2}*\\sqrt{\\sum _{i=1}^n w_{i,q}^2}}</math>\n\nwhere \'\'t<sub>i</sub>\'\' and \'\'t<sub>j</sub>\'\' are now vectors of a \'\'2<sup>n</sup>\'\' dimensional space.\n\nTerm correlation <math>t_i \\cdot t_j</math> can be implemented in several ways. For an example, Wong et al. uses the term occurrence frequency matrix obtained from automatic indexing as input to their algorithm. The term occurrence  and the output is the term correlation between any pair of index terms.\n\n==Semantic information on GVSM==\n\nThere are at least two basic directions for embedding term to term relatedness, other than exact keyword matching, into a retrieval model:\n# compute semantic correlations between terms\n# compute frequency co-occurrence statistics from large corpora\n\nRecently Tsatsaronis<ref>{{cite | title=A Generalized Vector Space Model for Text Retrieval Based on Semantic Relatedness | url=http://www.aclweb.org/anthology/E/E09/E09-3009.pdf | last= Tsatsaronis | first=George | coauthors=Vicky Panagiotopoulou | publisher=[[Association for Computing Machinery|EACL ACM]] |date=2009-04-02}}</ref> focused on the first approach.\n\nThey measure semantic relatedness (\'\'SR\'\') using a thesaurus (\'\'O\'\') like [[WordNet]]. It considers the path length, captured by compactness (\'\'SCM\'\'), and the path depth, captured by semantic path elaboration (\'\'SPE\'\').\nThey estimate the <math>t_i \\cdot t_j</math> inner product by:\n\n<math>t_i \\cdot t_j = SR((t_i, t_j), (s_i, s_j), O)</math>\n\nwhere \'\'s<sub>i</sub>\'\' and \'\'s<sub>j</sub>\'\' are senses of terms \'\'t<sub>i</sub>\'\' and \'\'t<sub>j</sub>\'\' respectively, maximizing <math>SCM \\cdot SPE</math>.\n\n== References ==\n{{reflist}}\n\n[[Category:Vector space model]]'
p106
sg6
S'Generalized vector space model'
p107
ssI40
(dp108
g2
S'http://en.wikipedia.org/wiki/Text Retrieval Conference'
p109
sg4
S'{{Other uses of|TREC|TREC (disambiguation)}}\nThe \'\'\'Text REtrieval Conference (TREC)\'\'\' is an on-going series of [[workshop]]s focusing on a list of different [[information retrieval]] (IR) research areas, or \'\'tracks.\'\' It is co-sponsored by the [[National Institute of Standards and Technology]] (NIST) and the [[Intelligence Advanced Research Projects Activity]] (part of the office of the [[Director of National Intelligence]]), and began in 1992 as part of the [[DARPA TIPSTER Program|TIPSTER Text program]]. Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale \'\'evaluation\'\' of [[text retrieval]] methodologies and to increase the speed of lab-to-product [[technology transfer|transfer of technology]].\n\nEach track has a challenge wherein NIST provides participating groups with data sets and test problems. Depending on track, test problems might be questions, topics, or target extractable [[Features (pattern recognition)|features]]. Uniform scoring is performed so the systems can be fairly evaluated. After evaluation of the results, a workshop provides a place for participants to collect together thoughts and ideas and present current and future research work.\n\n== Tracks ==\n\n===Current Tracks===\n\'\'New tracks are added as new research needs are identified, this list is current for TREC 2014.\'\'\n* Contextual Suggestion Track - \'\'\'Goal:\'\'\' to investigate search techniques for complex information needs that are highly dependent on context and user interests.\n* Clinical Decision Support Track - \'\'\'Goal:\'\'\' to investigate techniques for linking medical cases to information relevant for patient care\n* Federated Web Search Track - \'\'\'Goal:\'\'\' to investigate techniques for the selection and combination of search results from a large number of real on-line web search services.\n* Knowledge Base Acceleration Track - \'\'\'Goal:\'\'\' to develop techniques to dramatically improve the efficiency of (human) knowledge base curators by having the system suggest modifications/extensions to the KB based on its monitoring of the data streams.\n* [[Microblog]] Track - \'\'\'Goal:\'\'\' to examine the nature of real-time information needs and their satisfaction in the context of microblogging environments such as Twitter. \n* Session Track - \'\'\'Goal:\'\'\' to develop methods for measuring multiple-query sessions where information needs drift or get more or less specific over the session.\n* Temporal Summarization Track - \'\'\'Goal:\'\'\' to develop systems that allow users to efficiently monitor the information associated with an event over time.\n* Web Track - \'\'\'Goal:\'\'\' to explore information seeking behaviors common in general web search.\n\n===\'\'Past tracks\'\'===\n* Chemical Track - \'\'\'Goal:\'\'\' to develop and evaluate technology for large scale search in [[chemistry]]-related documents, including academic papers and patents, to better meet the needs of professional searchers, and specifically [[patent search]]ers and chemists.\n* [[Crowdsourcing]] Track - \'\'\'Goal:\'\'\' to provide a collaborative venue for exploring [[crowdsourcing]] methods both for evaluating search and for performing search tasks. \n* [[TREC Genomics|Genomics Track]] - \'\'\'Goal:\'\'\' to study the retrieval of [[Genomics|genomic]] data, not just gene sequences but also supporting documentation such as research papers, lab reports, etc. Last ran on TREC 2007.\n* [[Enterprise search|Enterprise Track]] - \'\'\'Goal:\'\'\' to study search over the data of an organization to complete some task. Last ran on TREC 2008.\n* Entity Track - \'\'\'Goal:\'\'\' to perform entity-related search on Web data. These search tasks (such as finding entities and properties of entities) address common information needs that are not that well modeled as ad hoc document search.\n* [[Cross-language information retrieval|Cross-Language]] Track - \'\'\'Goal:\'\'\' to investigate the ability of retrieval systems to find documents topically regardless of source language.\n* [[Federated search|FedWeb]] Track - \'\'\'Goal:\'\'\' to select best resources to forward a query to, and merge the results so that most relevant are on the top.\n* Filtering Track - \'\'\'Goal:\'\'\' to binarily decide retrieval of new incoming documents given a stable [[information need]].\n* HARD Track - \'\'\'Goal:\'\'\' to achieve High Accuracy Retrieval from Documents by leveraging additional information about the searcher and/or the search context.\n* Interactive Track - \'\'\'Goal:\'\'\' to study user [[Human-computer interaction|interaction]] with text retrieval systems.\n* Legal Track - \'\'\'Goal:\'\'\' to develop search technology that meets the needs of lawyers to engage in effective [[discovery (law)|discovery]] in digital document collections.\n* Medical Records Track - \'\'\'Goal:\'\'\' to explore methods for searching unstructured information found in patient medical records. \n* Novelty Track - \'\'\'Goal:\'\'\' to investigate systems\' abilities to locate new (i.e., non-redundant) information.\n* [[Question answering|Question Answering]] Track - \'\'\'Goal:\'\'\' to achieve more [[information retrieval]] than just [[document retrieval]] by answering factoid, list and definition-style questions.\n* Robust Retrieval Track - \'\'\'Goal:\'\'\' to focus on individual topic effectiveness.\n* [[Relevance feedback|Relevance Feedback]] Track - \'\'\'Goal:\'\'\' to further deep evaluation of relevance feedback processes.\n* [[Spam (electronic)|Spam]] Track - \'\'\'Goal:\'\'\' to provide a standard evaluation of current and proposed [[spam filter]]ing approaches.\n* [[Terabyte]] Track - \'\'\'Goal:\'\'\' to investigate whether/how the [[information retrieval|IR]] community can scale traditional IR test-collection-based evaluation to significantly large collections.\n* [[Video search engine|Video]] Track - \'\'\'Goal:\'\'\' to research in automatic segmentation, [[index (search engine)|index]]ing, and content-based retrieval of [[digital video]].\n:In 2003, this track became its own independent evaluation named [[TRECVID]].\n\n===Related Events===\nIn 1997, a Japanese counterpart of TREC was launched (first workshop in 1999), called [http://research.nii.ac.jp/ntcir/ NTCIR] ([[National Institute of Informatics|NII]] Test Collection for IR Systems), and in 2000, a European counterpart was launched, called [http://www.clef-campaign.org/ CLEF] (Cross Language Evaluation Forum).\n\n== Conference Contributions ==\n<!-- contributions of conference to research/IR community -->\nNIST claims that within the first six years of the workshops, the effectiveness of retrieval systems approximately doubled.<ref>[http://trec.nist.gov/overview.html From TREC homepage: "... effectiveness approximately doubled in the first six years of TREC"]</ref> The conference was also the first to hold large-scale evaluations of non-English documents, speech, video and retrieval across languages. Additionally, the challenges have inspired a large body of [http://trec.nist.gov/pubs.html publications]. Technology first developed in TREC is now included in many of the world\'s commercial [[search engine]]s.  An independent report by RTII found that "about one-third of the improvement in web search engines from 1999 to 2009 is attributable to TREC. Those enhancements likely saved up to 3 billion hours of time using web search engines. ... Additionally, the report showed that for every $1 that NIST and its partners invested in TREC, at least $3.35 to $5.07 in benefits were accrued to U.S. information retrieval researchers in both the private sector and academia."\n<ref>{{cite web|url=http://rti.org/page.cfm?objectid=75E125DC-5056-B100-31A5A6BDE897DE6D |title=NIST Investment Significantly Improved Search Engines |publisher=Rti.org |date= |accessdate=2012-01-19}}</ref>\n<ref>http://www.nist.gov/director/planning/upload/report10-1.pdf</ref>\n\nWhile one study suggests that the state of the art for ad hoc search  has not advanced substantially in the past decade,<ref>Timothy G. Armstrong, Alistair Moffat, William Webber, Justin Zobel.  Improvements that don\'t add up: ad hoc retrieval results since 1998.  CIKM 2009.  ACM.</ref> it is referring just to search for topically relevant documents in small news and web collections of a few gigabytes.  There have been advances in other types of ad hoc search in the past decade.  For example, test collections were created for known-item web search which found improvements from the use of anchor text, title weighting and url length, which were not useful techniques on the older ad hoc test collections.  In 2009, a new billion-page web collection was introduced, and spam filtering was found to be a useful technique for ad hoc web search, unlike in past test collections.\n\nThe test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.  In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.\n\nTREC systems often provide a baseline for further research.  Examples include:\n* [[Hal Varian]], Chief Economist at [[Google]], says \'\'Better data makes for better science. The history of information retrieval illustrates this principle well," and describes TREC\'s contribution.<ref>[http://googleblog.blogspot.com/2008/03/why-data-matters.html Why Data Matters]</ref>\n* TREC\'s Legal track has influenced the e-Discovery community both in research and in evaluation of commercial vendors.<ref>[http://blogs.the451group.com/information_management/2009/01/29/standards-in-e-discovery-%E2%80%93-walking-the-walk/ The 451 Group: Standards in e-Discovery -- walking the walk]</ref>\n* The [[IBM]] researcher team building [[IBM Watson]] (aka [[DeepQA]]), which beat the world\'s best [[Jeopardy!]] players,<ref>[http://www-03.ibm.com/press/us/en/presskit/27297.wss IBM and Jeopardy! Relive History with Encore Presentation of Jeopardy!: The IBM Challenge]</ref> used data and systems from TREC\'s QA Track as baseline performance measurements.<ref>[http://www.aaai.org/AITopics/articles&columns/Ferrucci-Watson2010.pdf David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welt. \'\'\'Building Watson:  An Overview of the DeepQA Project\'\'\']</ref>\n\n== Participation ==\nThe conference is made up of a varied, international group of researchers and developers.<ref>{{cite web|url=https://wiki.ir-facility.org/index.php/Participants |title=Participants - IRF Wiki |publisher=Wiki.ir-facility.org |date=2009-12-01 |accessdate=2012-01-19}}</ref><ref>http://trec.nist.gov/pubs/trec17/papers/LEGAL.OVERVIEW08.pdf</ref><ref>{{cite web|url=http://trec.nist.gov/pubs/trec17/appendices/million.query.results.html |title=Text REtrieval Conference (TREC) TREC 2008 Million Query Track Results |publisher=Trec.nist.gov |date= |accessdate=2012-01-19}}</ref> In 2003, there were 93 groups from both academia and industry from 22 countries participating.\n\n==References==\n{{reflist}}\n\n== External links ==\n*[http://trec.nist.gov/ TREC website at NIST]\n*[http://www.nist.gov/itl/div894/894.02/related_projects/tipster/ TIPSTER]\n*[http://www.amazon.com/TREC-Experiment-Evaluation-Information-Electronic/dp/0262220733/ The TREC book (at Amazon)]\n\n[[Category:Information retrieval]]\n[[Category:Computational linguistics]]\n[[Category:Natural language processing]]\n[[Category:Computer science competitions]]'
p110
sg6
S'Text Retrieval Conference'
p111
ssI169
(dp112
g2
S'http://en.wikipedia.org/wiki/Ptx (Unix)'
p113
sg4
S"{{Unreferenced stub|auto=yes|date=December 2009}}\n{{Lowercase|title=ptx}}\n'''ptx''' is a [[Unix]] utility, named for the ''permuted index'' which can perform the function of the Keyword in Context ([[Key Word in Context|KWIC]]) search mode. There is a corresponding [[IBM mainframe]] utility which performs the same function. permuted indexes are often used in such places as bibliographic or medical databases, thesauruses, or web sites to aid in locating entries of interest.\n\n==See also==\n* [[Concordancer]]\n\n[[Category:Searching]]\n[[Category:Unix text processing utilities]]\n\n\n{{Unix-stub}}"
p114
sg6
S'Ptx (Unix)'
p115
ssI43
(dp116
g2
S'http://en.wikipedia.org/wiki/SimRank'
p117
sg4
S'\'\'\'SimRank\'\'\' is a general [[Semantic similarity|similarity measure]], based on a simple and intuitive [[Graph theory|graph-theoretic model]].\nSimRank is applicable in any [[Domain model|domain]] with object-to-object [[Relation (mathematics)|relationships]], that measures similarity of the structural context in which objects occur, based on their relationships with other objects.\nEffectively, SimRank is a measure that says "\'\'\'two objects are considered to be similar if they are referenced by similar objects\'\'\'."\n\n== Introduction ==\n\nMany [[Application software|applications]] require a measure of "similarity" between objects.\nOne obvious example is the "find-similar-document" query,\non traditional text corpora or the [[World Wide Web|World-Wide Web]].\nMore generally, a similarity measure can be used to [[Cluster analysis|cluster objects]], such as for [[collaborative filtering]] in a [[recommender system]], in which \xe2\x80\x9csimilar\xe2\x80\x9d users and items are grouped based on the users\xe2\x80\x99 preferences.\n\nVarious aspects of objects can be used to determine similarity, usually depending on the domain and the appropriate definition of similarity for that domain.\nIn a [[Text corpus|document corpus]], matching text may be used, and for collaborative filtering, similar users may be identified by common preferences.\nSimRank is a general approach that exploits the object-to-object relationships found in many domains of interest.\nOn the [[World Wide Web|Web]], for example, two pages are related if there are [[hyperlink]]s between them.\nA similar approach can be applied to scientific papers and their citations, or to any other document corpus with [[cross-reference]] information.\nIn the case of recommender systems, a user\xe2\x80\x99s preference for an item constitutes a relationship between the user and the item.\nSuch domains are naturally modeled as [[Graph (mathematics)|graphs]], with [[Vertex (graph theory)|nodes]] representing objects and [[Edge (graph theory)#Graph|edges]] representing relationships.\n\nThe intuition behind the SimRank algorithm is that, in many domains, \'\'\'similar objects are referenced by similar objects\'\'\'.\nMore precisely, objects <math>a</math> and <math>b</math> are considered to be similar if they are pointed from objects <math>c</math> and <math>d</math>, respectively, and <math>c</math> and <math>d</math> are themselves similar.\nThe [[Recursion (computer science)#Recursive programming|base case]] is that objects are maximally similar to themselves\n.<ref name=jeh_widom>G. Jeh and J. Widom. SimRank: A Measure of Structural-Context Similarity. In [[SIGKDD|KDD\'02]]: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 538-543. [[Association for Computing Machinery|ACM Press]], 2002. [http://www-cs-students.stanford.edu/~glenj/simrank.pdf]</ref>\n\nIt is important to note that SimRank is a general algorithm that determines only the similarity of structural context.\nSimRank applies to any domain where there are enough relevant relationships between objects to base at least some notion of similarity on relationships.\nObviously, similarity of other domain-specific aspects are important as well; these can \xe2\x80\x94 and should be combined with relational structural-context similarity for an overall similarity measure.\nFor example, for [[Web page]]s SimRank can be combined with traditional textual similarity; the same idea applies to scientific papers or other document corpora.\nFor recommendation systems, there may be built-in known similarities between items (e.g., both computers, both clothing, etc.), as well as similarities between users (e.g., same gender, same spending level).\nAgain, these similarities can be combined with the similarity scores that are computed based on preference patterns, in order to produce an overall similarity measure.\n\n== Basic SimRank equation ==\n\nFor a node <math>v</math> in a directed graph, we denote by <math>I(v)</math> and <math>O(v)</math> the set of in-neighbors and out-neighbors of <math>v</math>, respectively.\nIndividual in-neighbors are denoted as <math>I_i(v)</math>, for <math>1 \\le i \\le \\left|I(v)\\right|</math>, and individual\nout-neighbors are denoted as <math>O_i(v)</math>, for <math>1 \\le i \\le \\left|O(v)\\right|</math>.\n\nLet us denote the similarity between objects <math>a</math> and <math>b</math> by <math>s(a, b) \\in [0, 1]</math>. \nFollowing the earlier motivation, a recursive equation is written for <math>s(a, b)</math>.\nIf <math>a = b</math> then <math>s(a, b)</math> is defined to be <math>1</math>.\nOtherwise,\n:<math>s(a, b) = \\frac{C}{\\left|I(a)\\right| \\left|I(b)\\right|}\n \\sum_{i=1}^{\\left|I(a)\\right|}\\sum_{j=1}^{\\left|I(b)\\right|}\n s(I_i(a), I_j(b))</math>\nwhere <math>C</math> is a constant between <math>0</math> and <math>1</math>.\nA slight technicality here is that either <math>a</math> or <math>b</math> may not have any in-neighbors.\nSince there is no way to infer any similarity between <math>a</math> and <math>b</math> in this case, similarity is set to <math>s(a, b) = 0</math>, so the summation in the above equation is defined to be <math>0</math> when <math>I(a) = \\emptyset</math> or <math>I(b) = \\emptyset</math>.\n\n== Matrix representation of SimRank ==\n\nLet <math>\\mathbf{S}</math> be the similarity matrix whose entry <math>[\\mathbf{S}]_{a,b}</math> denotes the similarity score <math>s(a,b)</math>, and <math>\\mathbf{A}</math> be the column normalized adjacency matrix whose entry <math>[\\mathbf{A}]_{a,b}=\\tfrac{1}{|\\mathcal{I}(b)|}</math> if there is an edge from <math>a</math> to <math>b</math>, and 0 otherwise. Then, in matrix notations, SimRank can be formulated as\n\n:<math>\n   {{\\mathbf{S}}}= \\max\\{C\\cdot (\\mathbf{A}^{T} \\cdot {{\\mathbf{S}}}\\cdot {{\\mathbf{A}}} ) , {{\\mathbf{I}}}\\},</math>\n\nwhere <math>\\mathbf{I}</math> is an identity matrix.\n\n== Computing SimRank ==\n\nA solution to the SimRank equations for a graph <math>G</math> can be reached by [[Iterative method|iteration]] to a [[Fixed point (mathematics)|fixed-point]].\nLet <math>n</math> be the number of nodes in <math>G</math>.\nFor each iteration <math>k</math>, we can keep <math>n^2</math> entries <math>s_k(*, *)</math>, where <math>s_k(a, b)</math> gives the score between <math>a</math> and <math>b</math> on iteration <math>k</math>.\nWe successively compute <math>s_{k+1}(*, *)</math> based on <math>s_k(*, *)</math>.\nWe start with <math>s_0(*, *)</math> where each <math>s_0(a, b)</math> is a lower bound on the actual SimRank score <math>s(a, b)</math>:\n:<math> s_0(a, b) =\n \\begin{cases}\n  1 \\mbox{  } , \\mbox{    } \\mbox{if } a = b  \\mbox{  } , \\\\\n  0 \\mbox{  } , \\mbox{    } \\mbox{if } a \\neq b \\mbox{  } .\n \\end{cases}</math>\n\nTo compute <math>s_{k+1}(a, b)</math> from <math>s_k(*, *)</math>, we use the basic SimRank equation to get:\n:<math>s_{k + 1}(a, b) = \n \\frac{C}{\\left|I(a)\\right| \\left|I(b)\\right|}\n \\sum_{i=1}^{\\left|I(a)\\right|}\\sum_{j=1}^{\\left|I(b)\\right|}\n  s_k(I_i(a), I_j(b))</math>\nfor <math>a \\ne b</math>, and <math>s_{k+1}(a, b) = 1</math> for <math>a = b</math>.\nThat is, on each iteration <math>k + 1</math>, we update the similarity of <math>(a, b)</math> using the similarity scores of the neighbours of <math>(a, b)</math> from the previous iteration <math>k</math> according to the basic SimRank equation.\nThe values <math>s_k(*, *)</math> are [[Monotonic function|nondecreasing]] as <math>k</math> increases.\nIt was shown in <ref name="jeh_widom"/> that the values [[Limit of a sequence|converge]] to [[Limit of a sequence|limits]] satisfying the basic SimRank equation, the SimRank scores <math>s(*, *)</math>, i.e., for all <math>a, b \\in V</math>, <math>\\lim_{k \\to \\infty} s_k(a, b) = s(a, b)</math>.\n\nThe original SimRank proposal suggested choosing the decay factor <math>C = 0.8</math> and a fixed number <math>K = 5</math> of iterations to perform.\nHowever, the recent research <ref name="lizorkin">D. Lizorkin, P. Velikhov, M. Grinev and D. Turdakov. Accuracy Estimate and Optimization Techniques for\nSimRank Computation. In [[Very large database|VLDB \'08]]: Proceedings of the 34th International Conference on Very Large Data Bases, pages 422--433. [http://modis.ispras.ru/Lizorkin/Publications/simrank_accuracy.pdf]</ref> showed that the given values for <math>C</math> and <math>K</math> generally imply relatively low [[Accuracy and precision|accuracy]] of iteratively computed SimRank scores.\nFor guaranteeing more accurate computation results, the latter paper suggests either using a smaller decay factor (in particular, <math>C = 0.6</math>) or taking more iterations.\n\n== Partial Sums Memoization ==\n\nThe recent work of Lizorkin et al.<ref name="lizorkin"/> proposed three optimization techniques for speeding up the computation of SimRank:\n\n(1) Essential nodes selection may eliminate the computation of a fraction of node pairs with a-priori zero scores.\n\n(2) Partial sums memoization can effectively reduce repeated calculations of the similarity among different node pairs by caching part of similarity summations for later reuse.\n\n(3) A threshold setting on the similarity enables a further reduction in the number of node pairs to be computed. \n\nIn particular, the second observation of partial sums memoization plays a paramount role in greatly speeding up the computation of SimRank from <math>O(Kd^2n^2)</math> to <math>O(Kdn^2)</math>, where <math>K</math> is the number of iterations, <math>d</math> is average degree of a graph, and <math>n</math> is the number of nodes in a graph. The central idea of partial sums memoization consists of two steps:\n\nFirst, the partial sums over <math>{\\mathcal I}(a)</math> are memoized as\n:<math>\nPartial_{{\\mathcal I}(a)}^{s_{k}}(j)=\\sum_{i\\in{\\mathcal I}(a)}s_{k}(i,j), \\qquad (\\forall j \\in {\\mathcal I}(b))\n</math>\nand then <math>s_{k+1} (a,b)</math> is iteratively computed from <math>Partial_{{\\mathcal I}(a)}^{s_{k}}(j)</math> as\n:<math>\ns_{k+1}( a,b )=\\tfrac{C}{| \\mathsf{\\mathcal{I}}( a ) | | \\mathsf{\\mathcal{I}}( b ) |}\\sum_{j \\in \\mathsf{\\mathcal{I}}( b ) } Partial_{{\\mathcal I}(a)}^{s_{k}}(j).\n</math>\nConsequently, the results of <math>Partial_{{\\mathcal I}(a)}^{s_{k}}(j)</math>, <math>\\forall j \\in {\\mathcal I}(b)</math>,\ncan be reused later when we compute the similarities <math>s_{k+1}(a,*)</math> for a given vertex <math>a</math> as the first argument.\n\n== Further research on SimRank ==\n\n* Fogaras and Racz <ref name="fogaras_racz">D. Fogaras and B. Racz. Scaling link-based similarity search. In [[World Wide Web Conference|WWW \'05]]: Proceedings of the 14th international conference on World Wide Web, pages 641--650, New York, NY, USA, 2005. [[Association for Computing Machinery|ACM]]. [http://www2005.org/docs/p641.pdf]</ref> suggested speeding up SimRank computation through [[Probability theory|probabilistic]] computation using the [[Monte Carlo method]].\n\n* Antonellis et al.<ref name="simrank_plusplus">I. Antonellis, H. Garcia-Molina and C.-C. Chang. Simrank++: Query Rewriting through Link Analysis of the Click Graph. In [[Very large database|VLDB \'08]]: Proceedings of the 34th International Conference on Very Large Data Bases, pages 408--421. [http://dbpubs.stanford.edu/pub/showDoc.Fulltext?lang=en&doc=2008-17&format=pdf&compression=&name=2008-17.pdf]</ref> extended SimRank equations to take into consideration (i) evidence factor for [[Graph (mathematics)#Properties of graphs|incident nodes]] and (ii) link weights.\n\n* Lizorkin et al.<ref name="lizorkin"/> proposed several [[Optimization (computer science)|optimization]] techniques for speeding up SimRank iterative computation.\n\n* Yu et al.<ref name="yu_icde13">W. Yu, X. Lin, W. Zhang. Towards Efficient SimRank Computation on Large Networks. In [[International Conference on Data Engineering|ICDE \'13]]: Proceedings of the 29th IEEE International Conference on Data Engineering, pages 601--612. [http://www.cse.unsw.edu.au/~weirenyu/pubs/icde13.pdf]</ref> further improved SimRank computation via a fine-grained [[memoization]] method to share small common parts among different partial sums.\n\n== See also ==\n\n* [[PageRank]]\n\n== Citations ==\n{{reflist|colwidth=30em}}\n\n[[Category:Information retrieval]]'
p118
sg6
S'SimRank'
p119
ssI172
(dp120
g2
S'http://en.wikipedia.org/wiki/Controlled vocabulary'
p121
sg4
S'{{refimprove|date=June 2012}}\n\n\'\'\'Controlled vocabularies\'\'\' provide a way to organize knowledge for subsequent retrieval.  They are used in [[subject indexing]] schemes, [[subject heading]]s, [[thesauri]], [[Taxonomy (general)|taxonomies]] and other forms of [[knowledge organization system]]s. Controlled vocabulary schemes mandate the use of predefined, authorised terms that have been preselected by the designer of the vocabulary, in contrast to natural language vocabularies, where there is no restriction on the vocabulary.\n\n== In library and information science ==\n\nIn [[library and information science]] controlled vocabulary is a carefully selected list of [[word (linguistics)|word]]s and [[phrase]]s, which are used to [[Tag (metadata)|tag]] units of information (document or work) so that they may be more easily retrieved by a search.{{ref|warner}}{{ref|fast}} Controlled vocabularies solve the problems of [[homographs]], [[synonyms]] and [[polyseme]]s by a [[bijection]] between concepts and authorized terms. In short, controlled vocabularies reduce ambiguity inherent in normal human languages where the same concept can be given different names and ensure consistency.\n\nFor example, in the [[Library of Congress Subject Headings]] (a subject heading system that uses a controlled vocabulary), authorized terms -- subject headings in this case -- have to be chosen to handle choices between variant spellings of the same concept (American versus British), choice among scientific and popular terms (Cockroaches versus \'\'Periplaneta americana\'\'), and choices between synonyms (automobile versus cars), among other difficult issues.\n\nChoices of authorized terms are based on the principles of \'\'user warrant\'\' (what terms users are likely to use), \'\'literary warrant\'\' (what terms are generally used in the literature and documents), and \'\'structural warrant\'\' (terms chosen by considering the structure, scope of the controlled vocabulary).\n\nControlled vocabularies also typically handle the problem of [[homographs]], with qualifiers. For example, the term "pool" has to be qualified to refer to either swimming pool, or the game pool to ensure that each authorized term or heading refers to only one concept.\n\nThere are two main kinds of controlled vocabulary tools used in libraries: subject headings and thesauri. While the differences between the two are diminishing, there are still some minor differences.\n\nHistorically subject headings were designed to describe books in library catalogs by catalogers while thesauri were used by indexers to apply index terms to documents and articles. Subject headings tend to be broader in scope describing whole books, while thesauri tend to be more specialized covering very specific disciplines. Also because of the card catalog system, subject headings tend to have terms that are in indirect order (though with the rise of automated systems this is being removed), while thesaurus terms are always in direct order. Subject headings also tend to use more pre-coordination of terms such that the designer of the controlled vocabulary will combine various concepts together to form one authorized subject heading. (e.g., children and terrorism) while thesauri tend to use singular direct terms. Lastly thesauri list not only equivalent terms but also narrower, broader terms and related terms among various authorized and non-authorized terms, while historically most subject headings did not.\n\nFor example, the [[Library of Congress Subject Heading]] itself did not have much syndetic structure until 1943, and it was not until 1985 when it began to adopt the thesauri type term "Broader term" and "Narrow term".\n\nThe [[terminology|terms]] are chosen and organized by trained professionals (including librarians and information scientists) who possess expertise in the subject area. Controlled vocabulary terms can accurately describe what a given document is actually about, even if the terms themselves do not occur within the document\'s text. Well known subject heading systems include the [[Library of Congress Subject Headings|Library of Congress system]], [[Medical Subject Headings|MeSH]], and [[Sears Subject Headings|Sears]]. Well known thesauri include the [[Art and Architecture Thesaurus]] and the [[Education Resources Information Center|ERIC]] Thesaurus.\n\nChoosing authorized terms to be used is a tricky business, besides the areas already considered above, the designer has to consider the specificity of the term chosen, whether to use direct entry, inter consistency and stability of the language. Lastly the amount of pre-co-ordinate (in which case the degree of enumeration versus synthesis becomes an issue) and post co-ordinate in the system is another important issue.\n\nControlled vocabulary elements (terms/phrases) employed as [[Tag (metadata)|tags]], to aid in the content identification process of documents, or other information system entities (e.g. DBMS, Web Services) qualifies as [[metadata]].\n\n== Indexing languages ==\n\nThere are three main types of indexing languages.\n\n* Controlled indexing language - Only approved terms can be used by the indexer to describe the document\n* [[Natural language]] indexing language - Any term from the document in question can be used to describe the document.\n* Free indexing language  - Any term (not only from the document) can be used to describe the document.\n\nWhen indexing a document, the indexer also has to choose the level of indexing exhaustivity, the level of detail in which the document is described. For example using low indexing exhaustivity, minor aspects of the work will not be described with index terms. In general the higher the indexing exhaustivity, the more terms indexed for each documen\n\nIn recent years [[free text search]] as a means of access to documents has become popular. This involves using natural language indexing with an indexing exhaustively set to maximum (every word in the text is \'\'indexed\'\'). Many studies have been done to compare the efficiency and effectiveness of free text searches against documents that have been indexed by experts using a few well chosen controlled vocabulary descriptors.\n\nControlled vocabularies are often claimed to improve the accuracy of free text searching, such as to reduce [[Relevance (Information Retrieval)|irrelevant]] items in the retrieval list. These irrelevant items ([[false positives]]) are often caused by the inherent ambiguity of [[natural language]]. Take the English word \'\'football\'\' for example. \'\'Football\'\' is the name given to a number of different [[team sport]]s. Worldwide the most popular of these team sports is [[Football (soccer)|Association football]], which also happens to be called \'\'[[soccer]]\'\' in several countries. The [[English language]] [[football (word)|word football]] is also applied to [[Rugby football]] ([[Rugby union]] and [[rugby league]]), [[American football]], [[Australian rules football]], [[Gaelic football]], and [[Canadian football]]. A search for \'\'football\'\' therefore will retrieve documents that are about several completely different sports. Controlled vocabulary solves this problem by [[Tag (metadata)|tagging]] the documents in such a way that the ambiguities are eliminated.\n\nCompared to free text searching, the use of a controlled vocabulary can dramatically increase the performance of an information retrieval system, if performance is measured by precision (the percentage of documents in the retrieval list that are actually [[relevance|relevant]] to the search topic).\n\nIn some cases controlled vocabulary can enhance recall as well, because unlike natural language schemes, once the correct authorized term is searched, you don\'t need to worry about searching for other terms that might be synonyms of that term.\n\nHowever, a controlled vocabulary search may also lead to unsatisfactory [[Recall (information retrieval)|recall]], in that it will fail to retrieve some documents that are actually relevant to the search question.\n\nThis is particularly problematic when the search question involves terms that are sufficiently tangential to the subject area such that the indexer might have decided to tag it using a different term (but the searcher might consider the same). Essentially, this can be avoided only by an experienced user of controlled vocabulary whose understanding of the vocabulary coincides with the way it is used by the indexer.\n\nAnother possibility is that the article is just not tagged by the indexer because indexing exhaustivity is low. For example an article might mention football as a secondary focus, and the indexer might decide not to tag it with "football" because it is not important enough compared to the main focus. But it turns out that for the searcher that article is relevant and hence recall fails. A free text search would automatically pick up that article regardless.\n\nOn the other hand free text searches have high exhaustivity (you search on every word) so it has potential for high recall (assuming you solve the problems of synonyms by entering every combination) but will have much lower precision.\n\nControlled vocabularies are also quickly out-dated and in fast developing fields of knowledge, the authorized terms available might not be available if they are not updated regularly. Even in the best case scenario, controlled language is often not as specific as using the words of the text itself. Indexers trying to choose the appropriate index terms might misinterpret the author, while a free text search is in no danger of doing so, because it uses the author\'s own words.\n\nThe use of controlled vocabularies can be costly compared to free text searches because human experts  or expensive automated systems are necessary to index each entry.  Furthermore, the user has to be familiar with the controlled vocabulary scheme to make best use of the system. But as already mentioned, the control of synonyms, homographs can help increase precision.\n\nNumerous methodologies have been developed to assist in the creation of controlled vocabularies, including [[faceted classification]], which enables a given data record or document to be described in multiple ways.\n\n==Applications==\nControlled vocabularies, such as the [[Library of Congress Subject Headings]],  are an essential component of [[bibliography]], the study and classification of books. They were initially developed in [[library and information science]]. In the 1950s, government agencies  began to develop controlled vocabularies for the burgeoning journal literature in specialized fields; an example is the [[Medical Subject Headings]] (MeSH) developed by the [[United States National Library of Medicine|U.S. National Library of Medicine]]. Subsequently, for-profit firms (called Abstracting and indexing services) emerged to index the fast-growing literature in every field of knowledge. In the 1960s, an online bibliographic database industry developed based on dialup [[X.25]] networking. These services were seldom made available to the public because they were difficult to use; specialist librarians called search intermediaries handled the searching job. In the 1980s, the first [[full text]] databases appeared; these databases contain the full text of the index articles as well as the bibliographic information. Online bibliographic databases have migrated to the Internet and are now publicly available; however, most are proprietary and can be expensive to use. Students enrolled in colleges and universities may be able to access some of these services without charge; some of these services may be accessible without charge at a public library.\n\nIn large organizations, controlled vocabularies may be introduced to improve [[technical communication]]. The use of controlled vocabulary ensures that everyone is using the same word to mean the same thing.  This consistency of terms is one of the most important concepts in [[technical writing]] and [[knowledge management]], where effort is expended to use the same word throughout a [[document]] or [[organization]] instead of slightly different ones to refer to the same thing.\n\nWeb searching could be dramatically improved by the development of a controlled vocabulary for describing Web pages; the use of such a vocabulary could culminate in a [[Semantic Web]], in which the content of Web pages is described using a machine-readable [[metadata]] scheme. One of the first proposals for such a scheme is the [[Dublin Core]] Initiative. An example of a controlled vocabulary which is usable for [[Web indexing|indexing web pages]] is [[Polythematic Structured Subject Heading System|PSH]].\n\nIt is unlikely that a single metadata scheme will ever succeed in describing the content of the entire Web.{{ref|doctorow}} To create a Semantic Web, it may be necessary to draw from two or more metadata systems to describe a Web page\'s contents. The [[eXchangeable Faceted Metadata Language]] (XFML) is designed to enable controlled vocabulary creators to publish and share metadata systems. XFML is designed on [[faceted classification]] principles.{{ref|pilgrim}}\n\n==See also==\n*[[Controlled natural language]]\n*[[IMS VDEX|IMS Vocabulary Definition Exchange]]\n*[[Nomenclature]]\n*[[Ontology (computer science)]]\n*[[Terminology]]\n*[[Thesaurus]]\n*[[Universal Data Element Framework]]\n*[[Vocabulary-based transformation]]\n\n==References==\n#{{note|warner}} Amy Warner, [http://www.ischool.utexas.edu/~i385e/readings/Warner-aTaxonomyPrimer.html A taxonomy primer].\n#{{note|fast}} Karl Fast, Fred Leise and Mike Steckel, [http://boxesandarrows.com/what-is-a-controlled-vocabulary/]\n#{{note|doctorow}} Cory Doctorow, [http://www.well.com/~doctorow/metacrap.htm Metacrap].\n#{{note|pilgrim}} Mark Pilgrim, [http://petervandijck.com/xfml/ eXchangeable Faceted Metadata Language].\n#[http://www.imresources.fit.qut.edu.au/vocab/ Controlled Vocabularies] {{Dead link|date=February 2011}} Links to examples of thesauri and classification schemes.\n#[http://www.fao.org/aims/kos_list_type.htm Controlled Vocabularies] {{Dead link|date=February 2011}} Links to examples of thesauri and classification schemes used in the domain of Agriculture, Fisheries, Forestry etc.\n\n==External links==\n* [http://www.controlledvocabulary.com/ controlledvocabulary.com] \xe2\x80\x94 explains how controlled vocabularies are useful in describing images and information for classifying content in electronic databases.\n* [http://www.photo-keywords.com/ photo-keywords.com/] \xe2\x80\x94 useful guides to creating and editing your own controlled vocabulary suitable for image cataloging.\n* [http://www.niso.org/standards/resources/Z39-19.html ANSI/NISO Z39.19 - 2005 Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies]\n\n{{Lexicography}}\n\n[[Category:Searching]]\n[[Category:Library cataloging and classification]]\n[[Category:Knowledge representation]]\n[[Category:Technical communication]]\n[[Category:Semantic Web]]\n[[Category:Ontology (information science)]]\n[[Category:Controlled vocabularies]]\n[[Category:Library science]]\n[[Category:Information science]]'
p122
sg6
S'Controlled vocabulary'
p123
ssI46
(dp124
g2
S'http://en.wikipedia.org/wiki/Category:Sound production technology'
p125
sg4
S'{{Commons category|Sound production technology}}\n[[Category:Audio electronics|Production]]\n[[Category:Information retrieval]]\n[[Category:Sound production|Technology]]'
p126
sg6
S'Category:Sound production technology'
p127
ssI175
(dp128
g2
S'http://en.wikipedia.org/wiki/Category:Internet search'
p129
sg4
S'{{Cat main|Internet search}}\n\n[[Category:Web services]]\n[[Category:Searching]]\n[[Category:World Wide Web|Search]] <!-- searching is a web function. Note that "Internet search" redirects to "Web page search" (or something like that)--->'
p130
sg6
S'Category:Internet search'
p131
ssI49
(dp132
g2
S'http://en.wikipedia.org/wiki/Subject indexing'
p133
sg4
S'\'\'\'Subject indexing\'\'\' is the act of describing or [[Document classification|classifying]] a [[document]] by [[keyword (search)|index terms]] or other symbols in order to indicate what the document is \'\'\'[[Aboutness|about]],\'\'\' to summarize its [[content (media and publishing)|content]] or to increase its [[findability]].  In other words, it is about identifying and describing the \'\'\'[[Subject (documents)|subject]]\'\'\' of documents.  Indexes are constructed, separately, on three distinct levels:  terms in a document such as a book;  objects in a collection such as a library;  and documents (such as books and articles) within a field of knowledge.\n\nSubject indexing is used in [[information retrieval]] especially to create [[bibliographic database]]s to retrieve documents on a particular subject. Examples of academic indexing services are [[Zentralblatt MATH]], [[Chemical Abstracts]] and [[PubMed]]. The index terms were mostly assigned by experts but author keywords are also common.\n\nThe process of indexing begins with any analysis of the subject of the document. The indexer must then identify terms which appropriately identify the subject either by extracting words directly from the document or assigning words from a [[controlled vocabulary]].<ref name="Lancaster2003a">F. W. Lancaster (2003): "Indexing and abstracting in theory and practise". Third edition. London, Facet ISBN 1-85604-482-3. page 6</ref> The terms in the index are then presented in a systematic order.\n\nIndexers must decide how many terms to include and how specific the terms should be. Together this gives a depth of indexing.\n\n== Subject analysis ==\nThe first step in indexing is to decide on the subject matter of the document. In manual indexing, the indexer would consider the subject matter in terms of answer to a set of questions such as "Does the document deal with a specific product, condition or phenomenon?".<ref name="Chowdhury2004">G.G. Chowdhury (2004): "Introduction to modern information retrieval". Third Edition. London, Facet. ISBN 1-85604-480-7. page 71</ref> As the analysis is influenced by the knowledge and experience of the indexer, it follows that two indexers may analyse the content differently and so come up with different index terms. This will impact on the success of retrieval.\n\n=== Automatic vs. manual subject analysis ===\nAutomatic indexing follows set processes of analysing frequencies of word patterns and comparing results to other documents in order to assign to subject categories. This requires no understanding of the material being indexed therefore leads to more uniform indexing but this is at the expense of the true meaning being interpreted. A computer program will not understand the meaning of statements and may therefore fail to assign some relevant terms or assign incorrectly. Human indexers focus their attention on certain parts of the document such as the title, abstract, summary and conclusions, as analysing the full text in depth is costly and time consuming <ref name="Lancaster2003b">F. W. Lancaster (2003): "Indexing and abstracting in theory and practise". Third edition. London, Facet ISBN 1-85604-482-3. page 24</ref> An automated system takes away the time limit and allows the entire document to be analysed, but also has the option to be directed to particular parts of the document.\n\n== Term selection ==\nThe second stage of indexing involves the translation of the subject analysis into a set of [[keyword (search)|index terms]]. This can involve extracting from the document or assigning from a [[controlled vocabulary]]. With the ability to conduct a [[full text search]] widely available, many people have come to rely on their own expertise in conducting information searches and [[full text search]] has become very popular.  Subject indexing and its experts, professional indexers, [[catalogers]], and [[librarians]], remains crucial to information organization and retrieval.  These experts understand [[controlled vocabularies]] and are able to find information that cannot be located by [[full text search]].  The cost of expert analysis to create subject indexing is not easily compared to the cost of hardware, software and labor to manufacture a comparable set of full-text, fully searchable materials.  With new web applications that allow every user to annotate documents, [[social tagging]] has gained popularity especially in the Web.<ref name="Voss2007">\n{{cite conference\n  | first= Jakob | last = Voss\n  | title = Tagging, Folksonomy & Co - Renaissance of Manual Indexing?\n  | booktitle = Proceedings of the International Symposium of Information Science\n  | pages = 234\xe2\x80\x93254\n  | year = 2007\n  | arxiv = cs/0701072\n}}</ref>\n\nOne application of indexing, the [[Index (publishing)|book index]], remains relatively unchanged despite the information revolution.\n\n=== Extraction/Derived indexing ===\nExtraction indexing involves taking words directly from the document. It uses [[natural language]] and lends itself well to automated techniques where word frequencies are calculated and those with a frequency over a pre-determined threshold are used as index terms. A stop-list containing common words such as the, and would be referred to and such [[stop words]] would be excluded as index terms. Automated extraction indexing may lead to loss of meaning of terms by indexing single words as opposed to phrases. Although it is possible to extract commonly occurring phrases, it becomes more difficult if key concepts are inconsistently worded in phrases.\nAutomated extraction indexing also has the problem that even with use of a stop-list to remove common words such as \xe2\x80\x9cthe,\xe2\x80\x9d some frequent words may not be useful for allowing discrimination between documents. For example, the term glucose is likely to occur frequently in any document related to diabetes. Therefore use of this term would likely return most or all the documents in the database. Post-co-ordinated indexing where terms are combined at the time of searching would reduce this effect but the onus would be on the searcher to link appropriate terms as opposed to the information professional. In addition terms that occur infrequently may be highly significant for example a new drug may be mentioned infrequently but the novelty of the subject makes any reference significant. One method for allowing rarer terms to be included and common words to be excluded by automated techniques  would be a relative frequency approach where frequency of a word in a document is compared to frequency in the database as a whole. Therefore a term that occurs more often in a document than might be expected based on the rest of the database could then be used as an index term, and terms that occur equally frequently throughout will be excluded. Another problem with automated extraction is that it does not recognise when a concept is discussed but is not identified in the text by an indexable keyword.<ref name="Lamb2008">J. Lamb (2008): \'\'[http://www.indexers.org.uk/index.php?id=463 Human or computer produced indexes?]\'\' [online] Sheffield, Society of Indexers. Accessed 15 January 2009.</ref>\n\n=== Assignment indexing ===\nAn alternative is assignment indexing where index terms are taken from a controlled vocabulary. This has the advantage of controlling for [[synonym]]s as the preferred term is indexed and synonyms or related terms direct the user to the preferred term. This means the user can find articles regardless of the specific term used by the author and saves the user from having to know and check all possible synonyms.<ref name="Tenopir">C. Tenopir (1999): "Human or automated, indexing is important". \'\'Library Journal\'\' \'\'\'124\'\'\'(18) pages 34-38.</ref> It also removes any confusion caused by [[homograph]]s by inclusion of a qualifying term. A third advantage is that it allows the linking of related terms whether they are linked by hierarchy or association, e.g. an index entry for an oral medication may list other oral medications as related terms on the same level of the hierarchy but would also link to broader terms such as treatment. Assignment indexing is used in manual indexing to improve inter-indexer consistency as different indexers will have a controlled set of terms to choose from. Controlled vocabularies do not completely remove inconsistencies as two indexers may still interpret the subject differently.<ref name="Chowdhury2004" />\n\n== Index presentation ==\nThe final phase of indexing is to present the entries in a systematic order. This may involve linking entries. In a pre-coordinated index the indexer determines the order in which terms are linked in an entry by considering how a user may formulate their search. In a post-coordinated index, the entries are presented singly and the user can link the entries through searches, most commonly carried out by computer software. Post-coordination results in a loss of precision in comparison to pre-coordination <ref name="Bodoff1998">D. Bodoff and A. Kambil, (1998): "Partial coordination. I. The best of pre-coordination and post-coordination." \'\'Journal of the American Society for Information Science\'\', \'\'\'49\'\'\'(14), 1254-1269.</ref>\n\n== Depth of Indexing ==\nIndexers must make decisions about what entries should be included and how many entries an index should incorporate. The depth of indexing describes the thoroughness of the indexing process with reference to exhaustivity and specificity <ref name="Cleveland2001">D.B. Cleveland and A.D. Cleveland (2001): "Introduction to indexing and abstracting". 3rd Ed. Englewood, libraries Unlimited, Inc. ISBN 1-56308-641-7. page 105</ref>\n\n=== Exhaustivity ===\nAn exhaustive index is one which lists all possible index terms. Greater exhaustivity gives a higher [[Recall (information retrieval)|recall]], or more likelihood of all the relevant articles being retrieved, however, this occurs at the expense of [[Precision (information retrieval)|precision]]. This means that the user may retrieve a larger number of irrelevant documents or documents which only deal with the subject in little depth. In a manual system a greater level of exhaustivity brings with it a greater cost as more man hours are required. The additional time taken in an automated system would be much less significant. At the other end of the scale, in a selective index only the most important aspects are covered.<ref name="Weinberg1999">B.H. Weinberg (1990): "Exhaustivity of indexes: Books, journals, and electronic full texts; Summary of a workshop presented at the 1999 ASI Annual Conference". \'\'Key Words\'\', \'\'\'7\'\'\'(5), pages 1+.</ref> Recall is reduced in a selective index as if an indexer does not include enough terms, a highly relevant article may be overlooked. Therefore indexers should strive for a balance and consider what the document may be used. They may also have to consider the implications of time and expense.\n\n=== Specificity ===\nThe specificity describes how closely the index terms match the topics they represent <ref name="Anderson1997">J.D. Anderson (1997): \'\'[http://www.niso.org/publications/tr/ Guidelines for indexes and related information retrieval devices]\'\' [online]. Bethesda, Maryland, Niso Press. 10 December 2008.</ref> An index is said to be specific if the indexer uses parallel descriptors to the concept of the document and reflects the concepts precisely.<ref name="Cleveland2001b">D.B. Cleveland and A.D. Cleveland (2001): "Introduction to indexing and abstracting". 3rd Ed. Englewood, libraries Unlimited, Inc. ISBN 1-56308-641-7. page 106</ref> Specificity tends to increase with exhaustivity as the more terms you include, the narrower those terms will be.\n\n==Indexing theory==\n[[Birger Hj\xc3\xb8rland|Hj\xc3\xb8rland]] (2011)<ref>Hj\xc3\xb8rland, Birger (2011). The Importance of Theories of Knowledge: Indexing and Information retrieval as an example. \'\'Journal of the American Society for Information Science and Technology\'\', 62(1,), 72-77.</ref> found that theories of indexing is at the deepest level connected to different theories of knowledge:\n\n\'\'\'Rationalist theories of indexing\'\'\' (such as Ranganathan\'s theory) suggest that subjects are constructed logically from a fundamental set of categories. The basic method of subject analysis is then "analytic-synthetic", to isolate a set of basic categories (=analysis) and then to construct the subject of any given document by combining those categories according to some rules (=synthesis). \'\'\'Empiricist theories of indexing\'\'\' are based on selecting similar documents based on their properties, in particular by applying numerical statistical techniques.  \'\'\'Historicist and hermeneutical theories of indexing\'\'\' suggest that the subject of a given document is relative to a given discourse or domain, why the indexing should reflect the need of a particular discourse or domain. According to hermeneutics is a document always written and interpreted from particular horizon. The same is the case with systems of knowledge organization and with all users searching such systems. Any question put to such a system is put from a particular horizon. All those horizons may be more or less in consensus or in conflict. To index a document is to try to contribute to the retrieval of \xe2\x80\x9crelevant\xe2\x80\x9d documents by knowing about those different horizons. \'\'\'Pragmatic and critical theories of indexing\'\'\' (such as Hj\xc3\xb8rland, 1997)<ref>Hj\xc3\xb8rland, B. (1997). Information Seeking and Subject Representation. An Activity-theoretical approach to Information Science. Westport & London: Greenwood Press.</ref> is in agreement with the historicist point of view that subjects are relative to specific discourses but emphasizes that subject analysis should support given goals and values and should consider the consequences of indexing one way or another. These theories believe that indexing cannot be neutral and that it is a wrong goal to try to index in a neutral way. Indexing is an act (and computer based indexing is acting according to the programmers intentions). Acts serve human goals. Libraries and information services also serve human goals, why their indexing should be done in a way that supports these goals as much as possible. At a first glance this looks strange because the goals of libraries and information services is to identify any document or piece of information. Nonetheless is any specific way of indexing always supporting some kind of uses at the expense of other. The documents to be indexed intend to serve some specific purposes in a community. Basically the indexing should intend serving the same purposes. Primary and secondary documents and information services are parts of the same overall social system. In such a system different theories, epistemologies, worldviews etc. may be at play and users need to be able to orient themselves and to navigate among those different views. This calls for a mapping of the different epistemologies in the field and classification of the single document into such a map. Excellent examples of such different paradigms and their consequences for indexing and classification systems are provided in the domain of art by \xc3\x98rom (2003)<ref>\xc3\x98rom, Anders (2003). Knowledge Organization in the domain of Art Studies - History, Transition and Conceptual Changes. Knowledge Organization. 30(3/4), 128-143.</ref> and in music by Abrahamsen (2003).<ref>Abrahamsen, Knut T. (2003). Indexing of Musical Genres. An Epistemological Perspective. Knowledge Organization, 30(3/4), 144-169. \n</ref>\n\nThe core of indexing is, as stated by Rowley & Farrow<ref name=rowley2000>Rowley, J. E. & Farrow, J. (2000). Organizing Knowledge: An Introduction to Managing Access to Information. 3rd. Alderstot: Gower Publishing Company</ref> to evaluate a papers contribution to knowledge and index it accordingly. Or, with the words of Hj\xc3\xb8rland (1992,<ref>Hj\xc3\xb8rland, Birger (1992). The Concept of "Subject" in Information Science. Journal of Documentation. 48(2), 172-200. http://iva.dk/bh/Core%20Concepts%20in%20LIS/1992JDOC%5FSubject.PDF</ref> 1997) to index its informative potentials.\n\n"In order to achieve good consistent indexing, the indexer must have a thorough appreciation of the structure of the subject  and the nature of the contribution that the document is making to the advancement of knowledge." (Rowley & Farrow, 2000,<ref name=rowley2000/> p.&nbsp;99).\n\n== See also ==\n* [[Indexing and abstracting service]]\n* [[Document classification]]\n* [[Metadata]]\n* [[Overcategorization]]\n* [[Thomas of Ireland]], a medieval pioneer in subject indexing\n\n== References ==\n<references/>\n*{{cite book|author=Fugman, Robert|year=1993|title=Subject analysis and indexing. Theoretical foundation and practical advice|place=Frankfurt/Main|publisher=Index Verlag}}\n*{{cite journal|author=Frohmann, B.|year=1990|title=Rules of Indexing: A Critique of [[Mentalism]] in Information Retrieval Theory|journal=Journal of Documentation|volume=46|issue=2|pages=81\xe2\x80\x93101|doi=10.1108/eb026855}}\n\n[[Category:Library science]]\n[[Category:Information science]]\n[[Category:Information retrieval]]'
p134
sg6
S'Subject indexing'
p135
ssI178
(dp136
g2
S'http://en.wikipedia.org/wiki/Search-oriented architecture'
p137
sg4
S"{{unreferenced|date=October 2007}}\nThe use of [[search engine technology]] is the main integration component in an [[information system]]. In a traditional business environment the [[architectural layer]] usually occupied by a [[relational database management system]] (RDBMS) is supplemented or replaced with a search engine or the indexing technology used to build search engines. Queries for information which would usually be performed using [[Structured Query Language]] (SQL) are replaced by keyword or fielded (or field-enabled) searches for structured, [[Semi-structured model|semi-structured]], or unstructured data.\n\nIn a typical [[Multitier architecture|multi-tier]] or [[Multitier architecture|N tier]] architecture information is maintained in a data tier where it can be stored and retrieved from a database or file system. The data tier is queried by the logic or business tier when information is needed using a data retrieval language like SQL.\n\nIn a '''search-oriented architecture''' the data tier may be replaced or placed behind another tier which contains a search engine and search engine index which is queried instead of the database management system. Queries from the business tier are made in the search engine query language instead of SQL. The search engine itself crawls the relational database management system in addition to other traditional data sources such as web pages or traditional file systems and consolidates the results when queried.\n\nThe benefit of adding a search layer to the architecture stack is rapid response time large dynamic datasets made possible by search indexing technology such as an [[inverted index]]. \n\n== Contrast with ==\n* [[Service-oriented architecture]] (SOA)\n* [[Service-Oriented Modeling]]\n\n== See also ==\n* [[Hibernate search]]\n \n[[Category:Software architecture]]\n[[Category:Data search engines]]\n[[Category:Searching]]"
p138
sg6
S'Search-oriented architecture'
p139
ssI52
(dp140
g2
S'http://en.wikipedia.org/wiki/Collaborative search engine'
p141
sg4
S'{{Recommender systems}}\n\'\'\'Collaborative search engines\'\'\' (CSE) are [[Web search engine]]s and [[enterprise search]]es within company intranets that let users combine their efforts in [[information retrieval]] (IR) activities, share information resources collaboratively using [[knowledge tags]], and allow experts to guide less experienced people through their searches. Collaboration partners do so by providing query terms, collective tagging, adding comments or opinions, rating search results, and links clicked of former (successful) IR activities to users having the same or a related [[information need]].\n\n== Models of collaboration ==\n\nCollaborative search engines can be classified along several dimensions: intent (explicit and implicit) and synchronization\n<ref name=Golo2007>{{citation\n | title = Collaborative Exploratory Search\n | year = 2007\n | author = Golovchinsky Gene, Pickens Jeremy\n | journal = Proceedings of HCIR 2007 workshop\n | pages = \n | volume = \n | issue = \n | doi = \n | isbn = \n | url = http://projects.csail.mit.edu/hcir/web/hcir07.pdf\n}}</ref> and depth of mediation \n,<ref name=Pickens2008>{{citation\n | title = Collaborative Exploratory Search\n | year = 2008\n | author = Pickens Jeremy, Golovchinsky Gene, Shah Chirag, Qvarfordt Pernilla, Back Maribeth\n | booktitle = SIGIR \'08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval\n | pages = 315\xe2\x80\x93322\n | volume = \n | issue = \n | doi = 10.1145/1390334.1390389\n | isbn = \n 9781605581644| url = http://portal.acm.org/citation.cfm?id=1390389\n| chapter = Algorithmic mediation for collaborative exploratory search\n }}</ref> task vs. trait,<ref name=Morris2008>{{citation\n | contribution = Understanding Groups\xe2\x80\x99 Properties as a Means of Improving Collaborative Search Systems\n | year = 2008\n | author = Morris Meredith, Teevan Jaime\n | title = 1st International Workshop on Collaborative Information Retrieval, held in conjunction with [[Joint Confrence on Digital Libraries|JCDL]] 2008\n | pages = \n | volume = \n | issue = \n | doi = \n | isbn = \n | contribution-url = http://workshops.fxpal.com/jcdl2008/submissions/tmpDF.pdf\n}}</ref> and division of labor and sharing of knowledge.<ref name=Foley2008>{{citation\n | title = Division of Labour and Sharing of Knowledge for Synchronous Collaborative Information Retrieval\n | year = 2008\n | author = Foley Colum\n | booktitle = PhD Thesis, Dublin City University\n | pages = \n | volume = \n | issue = \n | doi = \n | isbn = \n | url = http://www.computing.dcu.ie/~cfoley/cfoley-PhD_thesis.pdf\n}}</ref>\n\n=== Explicit vs. implicit collaboration ===\n\nImplicit collaboration characterizes [[Collaborative filtering]] and [[recommendation systems]] in which the system infers similar information needs. I-Spy,<ref name=Smith2003>{{citation\n | title = Collaborative Web Search\n | year = 2003\n | author = Barry Smyth, Evelyn Balfe, Peter Briggs, Maurice Coyle, Jill Freyne\n | journal = IJCAI\n | pages = 1417\xe2\x80\x931419\n | volume = \n | issue = \n | doi = \n | isbn = \n | url = \n}}</ref> [[Jumper 2.0]], [[Seeks]], the Community Search Assistant,<ref name=Glance2001>{{citation\n | title = Community search assistant\n | year = 2001\n | author = Natalie S. Glance\n | journal = Workshop on AI for Web Search AAAI\'02\n | pages = \n | volume = \n | issue = \n | doi = \n | isbn = \n | url = \n}}</ref> the CSE of Burghardt et al.,<ref name=BurghardtWI2008>{{citation\n | title = Discovering the Scope of Privacy Needs in Collaborative Search\n | year = 2008\n | author = Thorben Burghardt, Erik Buchmann, Klemens B\xc3\xb6hm\n | journal = Web Intelligence (WI)\n | pages = \n 910| volume = \n | issue = \n | doi = 10.1109/WIIAT.2008.165\n | isbn = \n 978-0-7695-3496-1}}</ref> and the works of Longo et al.\n<ref name=Longo2009a>{{citation\n | title = Toward Social Search - From Explicit to Implicit Collaboration\n               to Predict Users\' Interests\n | year = 2009\n | author = Longo Luca, Barrett Stephen, Dondio Pierpaolo\n | journal = WEBIST 2009 - Proceedings of the Fifth International Conference\n               on Web Information Systems and Technologies, Lisbon, Portugal,\n               March 23\xe2\x80\x9326, 2009\n | pages = 693\xe2\x80\x93696\n | volume = 1\n | issue = \n | doi = \n | isbn = 978-989-8111-81-4\n | url = \n}}</ref> \n<ref name=Longo2010>{{citation\n | title = Enhancing Social Search: A Computational Collective Intelligence Model of Behavioural Traits, Trust and Time\n | year = 2010\n | author = Longo Luca, Barrett Stephen, Dondio Pierpaolo\n | journal = Transaction Computational Collective Intelligence II\n | pages = 46\xe2\x80\x9369\n | volume = 2\n | issue = \n | doi = 10.1007/978-3-642-17155-0_3\n | isbn = \n 978-3-642-17154-3| url = http://www.springerlink.com/content/e12233858017h042/\n| series = Lecture Notes in Computer Science\n }}</ref> \n<ref name=Longo2009b>{{citation\n | title = Information Foraging Theory as a Form of Collective Intelligence                for Social Search\n | year = 2009\n | author = Longo Luca, Barrett Stephen, Dondio Pierpaolo\n | journal = Computational Collective Intelligence. Semantic Web, Social\n               Networks and Multiagent Systems, First International Conference,\n               ICCCI 2009, Wroclaw, Poland, October 5\xe2\x80\x937, 2009. Proceedings\n | pages = 63\xe2\x80\x9374\n | volume = 1\n | issue = \n | doi = \n | isbn = 978-3-642-04440-3\n | url = http://dl.acm.org/citation.cfm?id=1692026\n}}</ref> \nall represent examples of implicit collaboration. Systems that fall under this category identify similar users, queries and links clicked automatically, and recommend related queries and links to the searchers.\n\nExplicit collaboration means that users share an agreed-upon information need and work together toward that goal. For example, in a chat-like application, query terms and links clicked are automatically exchanged. The most prominent example of this class is SearchTogether<ref name=Morris2007>{{citation\n | title = SearchTogether: An Interface for Collaborative Web Search\n | year = 2007\n | author = Meredith Ringel Morris, Eric Horvitz\n | journal = UIST\n| url = http://portal.acm.org/citation.cfm?id=1294211.1294215\n}}</ref> published in 2007. SearchTogether offers an interface that combines search results from standard search engines and a chat to exchange queries and links. Reddy et al.<ref name=Redy2008>{{citation\n | title = The Role of Communication in Collaborative Information Searching\n | year = 2008\n | author = Madhu C. Reddy, Bernhard J. Jansen, Rashmi Krishnappa\n | journal = ASTIS\n}}</ref> (2008) follow a similar approach and compares two implementations of their CSE called MUSE and MUST. Reddy et al. focuses on the role of communication required for efficient CSEs. Representatives for the class of implicit collaboration are I-Spy,<ref name="Smith2003"/> the Community Search Assistant,<ref name="Glance2001"/> and the CSE of Burghardt et al.<ref name="BurghardtWI2008" /> Cerciamo <ref name=Pickens2008 /> supports explicit collaboration by allowing one person to concentrate on finding promising groups of documents, while having the other person make in-depth judgments of relevance on documents found by the first person.\n\nHowever, in Papagelis et al.<ref name=Papagelis2007>{{citation| title = Searchius: A Collaborative Search Engine| year = 2007| author = Athanasios Papagelis, Christos Zaroliagis| journal = ENC \'07: Proceedings of the Eighth Mexican International Conference on Current Trends in Computer Science| pages = 88\xe2\x80\x9398| doi = 10.1109/ENC.2007.34| url = http://portal.acm.org/citation.cfm?id=1302894| isbn = 0-7695-2899-6}}</ref> terms are used differently: they combine explicitly shared links and implicitly collected browsing histories of users to a hybrid CSE.\n\n=== Community of practice  ===\n\nRecent work in collaborative filtering and information retrieval has shown that sharing of search experiences among users having similar interests, typically called a [[community of practice]] or [[community of interest]], reduces the effort put in by a given user in retrieving the exact information of interest.<ref name=Rohini&Ambati>{{citation\n | title = A Collaborative Filtering based Re-ranking Strategy for Search in Digital Libraries\n | year = 2002\n | author = Rohini U, Vamshi Ambati\n | journal = ICADL2005: the 8th International Conference on Asian Digital Libraries\n | pages = \n | volume = \n | issue = \n | doi = \n | isbn = \n | url = http://www.aaai.org/Papers/Workshops/2006/WS-06-10/WS06-10-004.pdf }}</ref>\n\nCollaborative search deployed within a community of practice deploys novel techniques for exploiting context during search by indexing and ranking search results based on the learned preferences of a community of users.<ref name=Coyle2008>{{citation\n | title = Social Aspects of a Collaborative, Community-Based Search Network\n | editor4-first = Eelco\n | editor3-first = Pearl\n | editor2-first = Judy\n | editor1-first = Wolfgang\n | year = 2008\n | editor1-last = Nejdl\n | author = Maurice Coyle and Barry Smyth\n | journal = Adaptive Hypermedia and Adaptive Web-Based Systems\n | pages =  103\xe2\x80\x93112  \n | volume = 5149/2008\n | issue = \n \n | series = Volume| doi = 10.1007/978-3-540-70987-9\n | isbn = 978-3-540-70984-8\n | url = http://portal.acm.org/citation.cfm?id=1485050\n | editor2-last = Kay\n | editor4-last = Herder\n | editor3-last = Pu}}</ref> The users benefit by sharing information, experiences and awareness to personalize result-lists to reflect the preferences of the community as a whole. The community representing a group of users who share common interests, similar professions.  The best known example is the open-source project Jumper 2.0.<ref name=Jumper2010>{{citation\n | title = Jumper Networks Releases Jumper 2.0.1.5 Platform with New Community Search Features\n | year = 2010\n | author = Jumper Networks Inc\n | journal = Press release\n | pages = \n | volume =\n | issue = \n | doi =\n | isbn =\n | url = http://www.trilexnet.com/labs/jumper}}</ref>\n\n=== Depth of mediation ===\n\nThis refers to the degree that the CSE mediates search.<ref name=Pickens2008 /> SearchTogether<ref name=Morris2007 /> is an example of UI-level mediation: users exchange query results and judgments of relevance, but the system does not distinguish among users when they run queries. Cerchiamo<ref name=Pickens2008 /> and recommendation systems such as I-Spy<ref name=Smith2003 /> keep track of each person\'s search activity independently, and use that information to affect their search results. These are examples of deeper algorithmic mediation.\n\n=== Task vs. trait ===\n\nThis model classifies people\'s membership in groups based on the task at hand vs. long-term interests; these may be correlated with explicit and implicit collaboration.<ref name=Morris2008 />\n\n== Privacy-aware collaborative search engines ==\n\nSearch terms and links clicked that are shared among users reveal their interests, habits, social\nrelations and intentions.<ref name=EUArticle29>{{citation\n | title = Article 29 EU Data Protection Working Party\n | year = 2008\n | author = Data Protection Working Party\n | journal = EU\n | pages = \n | volume = \n | issue = \n | doi = \n | isbn = \n | url = \n}}</ref> In other words, CSEs put the privacy of the users at risk. Studies have shown that CSEs increase efficiency. \n<ref name="Morris2007"/><ref name=Smith2005>{{citation\n | title = A Live-User Evaluation of Collaborative Web Search\n | year = 2005\n | author = Barry Smyth, Evelyn Balfe, Oisin Boydell, Keith Bradley, Peter Briggs, Maurice Coyle, Jill Freyne\n | journal = IJCAI\n | pages = \n | volume = \n | issue = \n | doi = \n | isbn = \n | url = \n}}</ref>\n<ref name=Smith2006>{{citation\n | title = Anonymous personalization in collaborative web search\n | year = 2005\n | author = Smyth,, Barry and Balfe,, Evelyn\n | journal = Inf. Retr.\n | pages = 165\xe2\x80\x93190\n | volume = 9\n | issue = 2| doi = 10.1007/s10791-006-7148-z| isbn = \n | url = \n}}</ref>\n<ref name=Jung2004>{{citation\n | title = Applying Collaborative Filtering for Efficient Document Search\n | year = 2004\n | author = Seikyung Jung, Juntae Kim, Herlocker, J.L.\n | journal = Inf. Retr.\n | pages = 640\xe2\x80\x93643\n | volume = \n | issue = \n | doi = \n | isbn = \n | url = \n}}</ref> Unfortunately, by the lack of privacy enhancing technologies, a privacy aware user who wants to benefit from a CSE has to disclose his entire search log. (Note, even when explicitly sharing queries and links clicked, the whole (former) log is disclosed to any user that joins a search session).  Thus, sophisticated mechanisms that allow on a more fine grained level which information is disclosed to whom are desirable.\n\nAs CSEs are a new technology just entering the market, identifying user privacy preferences and integrating [[Privacy enhancing technologies]] (PETs) into collaborative search are in conflict. On one hand, PETs have to meet user preferences, on the other hand one cannot identify these preferences without using a CSE, i.e., implementing PETs into CSEs. Today, the only work addressing this problem comes from Burghardt et al.<ref name=BurghardtCC2008>{{citation\n | title = Collaborative Search And User Privacy: How Can They Be Reconciled?\n | year = 2008\n | author = Thorben Burghardt, Erik Buchmann, Klemens B\xc3\xb6hm, Chris Clifton\n | journal = CollaborateCom\n | pages = \n | volume = \n | issue = \n | doi = \n | isbn = \n | url = http://dbis.ipd.uni-karlsruhe.de/1184.php\n}}</ref> They implemented a CSE with experts from the information system domain and derived the scope of possible privacy preferences in a user study with these experts. Results show that users define preferences referring to (i) their current context (e.g., being at work), (ii) the query content (e.g., users exclude topics from sharing), (iii) time constraints (e.g., do not publish the query X hours after the query has been issued, do not store longer than X days, do not share between working time), and that users intensively use the option to (iv) distinguish between different social groups when sharing information. Further, users require (v) anonymization and (vi) define reciprocal constraints, i.e., they refer to the behavior of other users, e.g., if a user would have shared the same query in turn.\n\n== References ==\n{{reflist|2}}\n{{Internet search}}\n\n[[Category:Information retrieval]]'
p142
sg6
S'Collaborative search engine'
p143
ssI181
(dp144
g2
S'http://en.wikipedia.org/wiki/Reverse telephone directory'
p145
sg4
S'A \'\'\'reverse telephone directory\'\'\' (also known as a \'\'\'gray pages\'\'\' directory, criss-cross directory or \'\'\'reverse phone lookup\'\'\') is a collection of telephone numbers and associated customer details. However, unlike a standard [[telephone directory]], where the user uses customer\'s details (such as name and address) in order to retrieve the telephone number of that person or business, a reverse telephone directory allows users to search by a telephone service number in order to retrieve the customer details for that service.\n\nReverse telephone directories are used by law enforcement and other emergency services in order to determine the origin of any request for assistance, however these systems include both publicly accessible (listed) and private (unlisted) services. As such, these directories are restricted to internal use only.\n\nPublicly accessible reverse telephone directories may be provided as part of the standard directory services from the telecommunications carrier in some countries. In other countries these directories are often created by [[phreaking|phone phreaker]]s by collecting the information available via the publicly accessible directories and then providing a search function which allows users to search by the telephone service details.\n\n==History==\nPrinted reverse phone directories have been produced by the telephone companies (in the United States) for decades, and were distributed to the phone companies, law enforcement, and [[public library|public libraries]].<ref>{{cite news | url=http://news.google.com/newspapers?nid=1454&dat=19720102&id=87osAAAAIBAJ&sjid=vgkEAAAAIBAJ&pg=3122,379459 | title=Clinton Directory Issued | date=Jan 2, 1972 | accessdate=9 February 2014 | location=Page 16}}</ref> In the early 1990s, businesses started offering reverse telephone lookups for fees, and by the early 2000s advertising-based reverse directories were available online, prompting occasional alarms about privacy concerns.\n\n==Australia==\nIn 2001, a legal case \'\'[[Telstra|Telstra Corporation Ltd]] v Desktop Marketing Systems Pty Ltd\'\' was heard in the Australian Federal Court.<ref>{{cite web|url=http://www.austlii.edu.au/au/cases/cth/federal_ct/2001/612.html|title=Telstra Corporation Limited v Desktop Marketing Systems Pty Ltd (2001) FCA 612 (25 May 2001)|author=[[Federal Court of Australia]]|publisher=Australasian Legal Information Institute|accessdate=2008-01-03}}</ref><ref name=austliiPP>{{cite web|url=http://www.austlii.edu.au/au/journals/PLPR/2001/25.html|title=Private parts - PLPR 25; (2001) 8 PLPR 24|publisher=Australasian Legal Information Institute|accessdate=2008-01-03}}</ref> gave Telstra, the predominant carrier within Australia and the maintainer of the publicly accessible [[White Pages]] (residential) and [[Yellow Pages]] (commercial) directories, [[copyright]] over the content of these directories.\n\nIn February 2010 a Federal Court of Australia case \'\'[[Telstra|Telstra Corporation Ltd]] v Phone Directories Company Pty Ltd\'\' determined that Telstra does not hold copyright in the White Pages or the Yellow Pages.<ref>{{cite news|url=http://www.smh.com.au/business/copyright-to-enter-a-new-dimension-20101215-18y9o.html|title=Copyright to enter a new dimension|newspaper=[[The Sydney Morning Herald]]| first=Malcolm|last=Maiden|date=16 December 2010|accessdate=20 December 2012}}</ref>\n\nAs it currently{{when|date=October 2014}} stands there is no legal way to ensure a particular number is not listed in the directories currently available.\n\n==United States==\n\nIn United States, landline phone subscribers can pay a small fee to exclude their number from the directory. This service is usually called "Your Listing Not Published" and the cost ranges between $0.80 and $1.50 for residential customers.\n\nAs [[cellular phones]] become more popular, there has been debate about releasing cell phone numbers into public [[4-1-1|411]] and reverse number directories. (S. 1963, the "Wireless 411 Privacy Act" 9/2004). However, opposition led by leading consumer-protection organization [[Consumers Union]] presented several privacy concerns in their congressional [http://www.consumersunion.org/pub/wireless%20411%20senate%20testimony%20final.pdf testimony]. Right now,{{when|date=October 2014}} cell phone numbers are not available in any public 411 or reverse-number directories. However, several information companies provide reverse cell phone lookups that are obtained from utility resources, and are available online. Because there is no central database of cell phone numbers, reverse phone directories that claim to be free cannot return information on those numbers.<ref>{{cite web | url=http://www.ncbi.nlm.nih.gov/pubmed/15652722 | title=Evaluating the utility and accuracy of a reverse telephone directory to identify the location of survey respondents. | publisher=Ncbi.nlm.nih.gov | work=2005 Feb | accessdate=9 February 2014 | author=Schootman M, Jeffe D, Kinman E, Higgs G, Jackson-Thompson J.}}</ref>\n\nIn recent years{{when|date=October 2014}} community web based services offer a reverse telephone directory of known telemarketers, debt collectors, fund raisers, and other solicitors which contact consumers by telephone.  Users of these services can perform a search of the telephone number which showed up on their caller ID and read through user comments to find the identity of the calling company or individual.\n\n==United Kingdom==\nIn the United Kingdom proper, reverse directory information is not publicly available.<ref>{{cite web | url=http://ico.org.uk/for_organisations/privacy_and_electronic_communications/the_guide/directories_of_subscribers | title=Directories of subscribers | publisher=Information Commissioner\'s Office | accessdate=9 February 2014}}</ref> However, in the [[Channel Islands]] it is provided in the printed telephone directories. \n\nAlthough the information is, of necessity, available to emergency services, for other agencies it is treated as \'communication data\' in the [[RIPA]] regime and subject to the same controls as requests for lists of and content of calls.\n\n==References==\n{{reflist}}\n==External links==\n<!-- Do not delete these comments. -->\n<!-- Do not put commercial links into this list. Doing so can get you blocked with no further warning. --> \n*[http://web.archive.org/web/20010721175437/http://blackpages.2600.org.au/ Wayback Machine (21 July 2001) archive of http://blackpages.2600.org.au]\n*[http://www.austlii.edu.au/au/cases/cth/federal_ct/2001/612.html Federal Court of Australia Case 612 (25 May 2001): Telstra Corporation Limited v Desktop Marketing Systems Pty Ltd]\n\n\n[[Category:Telephone numbers]]\n[[Category:Directories]]\n[[Category:Searching]]'
p146
sg6
S'Reverse telephone directory'
p147
ssI55
(dp148
g2
S'http://en.wikipedia.org/wiki/Compound term processing'
p149
sg4
S'{{copy edit|for=Use of references (both inline, and ref tags)|date=February 2015}}\n\n\'\'\'Compound term processing\'\'\' refers to a category of techniques used in [[information retrieval]] applications that perform matching on the basis of [[compound term]]s. Compound terms are built by combining two or more simple terms; for example, "triple" is a single word term, but "triple heart bypass" is a compound term.\n\nIn August 2003, [[Concept Searching Limited]] introduced the idea of using statistical Compound Term Processing <ref>{{cite journal|url=http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf|title=Lateral Thinking in Information Retrieval|journal=INFORMATION MANAGEMENT AND TECHNOLOGY|volume=36 PART 4}} British Library Direct catalogue entry can be found here:[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&ETOC=RN]</ref>\n\nCLAMOUR<ref>[http://www.statistics.gov.uk/methods_quality/clamour/coordination/wp03.asp] National Statistics CLAMOUR project</ref><ref>[http://www.statistics.gov.uk/methods_quality/clamour/downloads/Clamour_march2002_final_reportAO.pdf] CLAMOUR Final Report</ref> is a European collaborative project which aims to find a better way to classify when collecting and disseminating industrial information & statistics. In contrast to the techniques discussed by Concept Searching Limited, CLAMOUR appears to use a linguistic approach, rather than one based on statistical modelling.\n\nCompound Term Processing allows information retrieval applications, such as search engines, to perform their matching on the basis of multi-word concepts, rather than on single words in isolation which can be highly ambiguous.\n\nMost [[search engine]]s simply look for documents containing the words entered by the user into the search box . These are known as [[keyword search]] engines. [[Boolean search]] engines add a degree of sophistication by allowing the user to specify additional requirements. For example, "Tiger NEAR Woods AND (golf OR golfing) NOT Volkswagen" uses the operators "NEAR", "AND", "OR" and "NOT" to specify that these words must follow certain requirements. [[Phrase search]] is simpler to use, but requires that the exact phrase specified appear in the results.\n\nTechniques for probabilistic weighting of single word terms dates back to at least 1976 in the landmark publication by [[Stephen Robertson (computer scientist)|Stephen E. Robertson]] and [[Karen Sp\xc3\xa4rck Jones]] entitled "Relevance weighting of search terms", originally published in the \'\'Journal of the American Society for Information Science\'\'.<ref>{{cite doi | 10.1002/asi.4630270302}}</ref>  Robertson stated that the assumption of word independence is not justified and exists simply as a matter of mathematical convenience. His objection to term independence is not a new idea, dating back to at least 1964 when H. H. Williams expressed that "[t]he assumption of independence of words in a document is usually made as a matter of mathematical convenience".<ref>WILLIAMS, J.H., \'Results of classifying documents with multiple discriminant functions\', In : Statistical Association Methods for Mechanized Documentation, National Bureau of Standards, Washington, 217-224 (1965).</ref>\n\nCompound term processing is a new approach to an old problem: how can one improve the relevance of search results while maintaining ease of use? By forming compound terms and placing these terms in a search engine\'s index, searches can be performed with a higher degree of accuracy, as the ambiguity inherent in single words is no longer a problem. Using this technique, a search for \'\'survival rates following a triple heart bypass in elderly people\'\' will locate documents about this topic even if this precise phrase is not contained in any document. This can be performed by a [[concept search]], which itself uses compound term processing. This will extract the key concepts automatically (in this case "survival rates", "triple heart bypass" and "elderly people") and use these concepts to select the most relevant documents.\n\nIn 2004, Anna Lynn Patterson filed a number of patents on "phrase-based searching in an information retrieval system"<ref>[http://appft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&d=PG01&p=1&u=%2Fnetahtml%2FPTO%2Fsrchnum.html&r=1&f=G&l=50&s1=%2220060031195%22.PGNR.&OS=DN/20060031195&RS=DN/20060031195] US Patent: 20060031195</ref> to which Google subsequently acquired the rights. A full discussion of the patents can be found at [http://www.webmasterwoman.com/search-engines/phrase-based-indexing.html Webmaster Woman]{{dead link|date=February 2015}}.\n\nStatistical compound term processing is a method more adaptive than the process described by Patterson in her patent applications. Her process is targeted at searching the World Wide Web where an extensive statistical knowledge of common searches can be used to identify candidate phrases. Statistical compound term processing is more suited to [[enterprise search]] applications where such [[A priori and a posteriori|a priori]] knowledge is not available.\n\nStatistical compound term processing is also more adaptive than the linguistic approach taken by the CLAMOUR project, which must take into consideration the syntactic properties of the terms (i.e. part of speech, gender, number, etc.) and their combinations. CLAMOUR is highly language-dependent, whereas the statistical approach is language-independent.\n\n==See also==\n* [[Enterprise search]]\n* [[Information retrieval]]\n\n== References ==\n{{Reflist}}\n\n==  External links ==\n*[http://www.conceptsearching.com/ Concept Searching Limited]\n*[http://www.webmasterwoman.com/search-engines/phrase-based-indexing.html Webmaster Woman]\n\n{{Natural Language Processing}}\n\n{{DEFAULTSORT:Compound Term Processing}}\n[[Category:Information retrieval]]'
p150
sg6
S'Compound term processing'
p151
ssI184
(dp152
g2
S'http://en.wikipedia.org/wiki/Unified Information Access'
p153
sg4
S'{{Multiple issues|\n{{confusing|date=November 2010}}\n{{cleanup|date=November 2010}}\n}}\n\n\'\'\'Unified Information Access Platforms\'\'\' are [[computing platforms]] that integrate large volumes of [[unstructured information|unstructured]], semi-structured, and structured information into a unified environment for processing, analysis and decision-making. These platforms are highly scalable, hybrid architectures that combine elements of database and search technologies in order to make information access dynamic and ad hoc, while offering the reporting and visualization features commonly found in business intelligence applications. While the vision for such integrated platforms has been around for years, only since 20XX have products been released into the market. Companies like [[Applied Relevance]], [[Attivio]], [[BA-Insight]], [[Cambridge Semantics]], [[Endeca Technologies Inc.|Endeca]], [[Exalead]], [[HP Autonomy]], [[PolySpot]], [[MarkLogic]], [[PerfectSearch]], [[Palantir Technologies|Palantir]], [[TopQuadrant]], [[Sinequa]] and [http://www.virtualworks.com VirtualWorks] have recognized the need for this approach.\n\nUnified access applications:\n*Create [[Hybrid computer|hybrid]] data structures that combine structured data and data operators with [[Text (literary theory)|text]] and semi-structured operations and analytics. They combine semantic understanding, fuzzy matching, sorting, joins, and various operations such as [[range searching]] within a single architecture, rather than federating a query to multiple sources in multiple forms.\n*Leverage these hybrid structures to provide real-time access through ad hoc queries to multiple sources of information, including information across a spectrum of [[File format|format]]s (e.g. rich media) through a single [[Interface (computer science)|interface]].\n*Handle sparse matrices of unpredictable content.\n*Optimize interactions for consumption and decisions, [[Process (computing)|processing]] queries faster than traditional database and/or BI applications and implementing visual consumption metaphors.\n*Scale to [[terabyte]]s.\n*Provide reporting tools that are BI-like, or integrate easily with BI applications and reporting tools.\n\n== References ==\n* Worldwide Search and Discovery 2009 Vendor Shares and Update on Market Trends, IDC #223926, July, 2010 by Susan Feldman and Hadley Reynolds.\n* Building the Intelligent Enterprise: The Case for Unified Access and Analytics\nSusan Feldman, Jul 2009 - Doc # 219467   \n<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->\n{{Reflist}}\n\n== External links ==\n* Video: [http://www.attivio.com/poweringbusiness/videos/63-attivio/869-unified-information-access-in-4-minutes.html "Unified Information Access in 4 Minutes"]\n* http://www.computerworld.com/s/article/9180280/Five_Advantages_of_Unified_Information_Access_UIA\n* http://www.eweek.com/c/a/Enterprise-Applications/How-to-Use-Unified-Information-Access-to-Get-Most-Value-from-Your-Data/\n\n[[Category:Searching]]'
p154
sg6
S'Unified Information Access'
p155
ssI58
(dp156
g2
S'http://en.wikipedia.org/wiki/Web query classification'
p157
sg4
S'{{Cleanup|date=March 2011}}\n\'\'\' \nA Web query topic classification/categorization is a problem in [[information science]]. The task is to assign a [[Web search query]] to one or more predefined [[Categorization|categories]], based on its topics. The importance of query classification is underscored by many services provided by Web search. A direct application is to provide better search result pages for users with interests of different categories. For example, the users issuing a Web query \xe2\x80\x9c\'\'apple\'\'\xe2\x80\x9d might expect to see Web pages related to the fruit apple, or they may prefer to see products or news related to the computer company. Online advertisement services can rely on the query classification results to promote different products more accurately. Search result pages can be grouped according to the categories predicted by a query classification algorithm. However, the computation of query classification is non-trivial. Different from the [[document classification]] tasks, queries submitted by Web search users are usually short and ambiguous; also the meanings of the queries are evolving over time. Therefore, query topic classification is much more difficult than traditional document classification tasks.\n\n== KDDCUP 2005 ==\n\nKDDCUP 2005 competition<ref>[http://www.sigkdd.org/kdd2005/kddcup.html KDDCUP 2005 dataset]</ref> highlighted the interests in query classification. The objective of this competition is to classify 800,000 real user queries into 67 target categories. Each query can belong to more than one target category. As an example of a QC task, given the query \xe2\x80\x9c\'\'apple\'\'\xe2\x80\x9d, it should be classified into ranked categories: \xe2\x80\x9c\'\'Computers \\ Hardware\'\'; \'\'Living \\ Food & Cooking\'\'\xe2\x80\x9d.\n\n{| class="wikitable"\n|-\n! Query\n! Categories\n|-\n| apple\n| Computers \\ Hardware<br />Living \\ Food & Cooking\n|-\n| FIFA 2006\n| Sports \\ Soccer<br />Sports \\ Schedules & Tickets<br />Entertainment \\ Games & Toys\n|-\n| cheesecake recipes\n| Living \\ Food & Cooking<br />Information \\ Arts & Humanities\n|-\n| friendships poem\n| Information \\ Arts & Humanities<br />Living \\ Dating & Relationships\n|}\n\n[[Image:Web query length.gif]]\n[[Image:Web query meaning.gif]]\n\n== Difficulties ==\n\nWeb query topic classification is to automatically assign a query to some predefined categories. Different from the traditional document classification tasks, there are several major difficulties which hinder the progress of Web query understanding:\n\n=== How to derive an appropriate feature representation for Web queries? ===\n\nMany queries are short and query terms are noisy. As an example, in the KDDCUP 2005 dataset, queries containing 3 words are most frequent (22%). Furthermore, 79% queries have no more than 4 words. A user query often has multiple meanings. For example, "\'\'apple\'\'" can mean a kind of fruit or a computer company. "\'\'Java\'\'" can mean a programming language or an island in Indonesia. In the KDDCUP 2005 dataset, most of the queries contain more than one meaning. Therefore, only using the keywords of the query to set up a [[vector space model]] for classification is not appropriate.\n\n* Query-enrichment based methods<ref>Shen et al.  [http://www.sigkdd.org/sites/default/files/issues/7-2-2005-12/KDDCUP2005Report_Shen.pdf "Q2C@UST: Our Winning Solution to Query Classification"]. \'\'ACM SIGKDD Exploration, December 2005, Volume 7, Issue 2\'\'.</ref><ref>Shen et al. [http://portal.acm.org/ft_gateway.cfm?id=1165776 "Query Enrichment for Web-query Classification"]. \'\'ACM TOIS, Vol. 24, No. 3, July 2006\'\'.</ref> start by enriching user queries to a collection of text documents through [[search engines]]. Thus, each query is represented by a pseudo-document which consists of the snippets of top ranked result pages retrieved by search engine. Subsequently, the text documents are classified into the target categories using synonym based classifier or statistical classifiers, such as [[Naive Bayes]] (NB) and [[Support Vector Machines]] (SVMs).\n\nHow about disadvantages and advantages??\ngive the answers:\n\n=== How to adapt the changes of the queries and categories over time? ===\n\nThe meanings of queries may also evolve over time. Therefore, the old labeled training queries may be out-of-data and useless soon. How to make the classifier adaptive over time becomes a big issue. For example, the word "\'\'Barcelona\'\'" has a new meaning of the new micro-processor of AMD, while it refers to a city or football club before 2007. The distribution of the meanings of this term is therefore a function of time on the Web.\n\n* Intermediate taxonomy based method<ref>Shen et al.  [http://portal.acm.org/ft_gateway.cfm?id=1148196 "Building bridges for web query classification"]. \'\'ACM SIGIR, 2006\'\'.</ref> first builds a bridging classifier on an intermediate taxonomy, such as [[Open Directory Project]] (ODP), in an offline mode. This classifier is then used in an online mode to map user queries to the target categories via the intermediate taxonomy. The advantage of this approach is that the bridging classifier needs to be trained only once and is adaptive for each new set of target categories and incoming queries.\n\n=== How to use the unlabeled query logs to help with query classification? ===\n\nSince the manually labeled training data for query classification is expensive, how to use a very large web search engine query log as a source of unlabeled data to aid in automatic query classification becomes a hot issue. These logs record the Web users\' behavior when they search for information via a search engine. Over the years, query logs have become a rich resource which contains Web users\' knowledge about the World Wide Web.\n\n* Query clustering method<ref>Wen et al. [http://portal.acm.org/ft_gateway.cfm?id=503108 "Query Clustering Using User Logs"], \'\'ACM TOIS, Volume 20, Issue 1, January 2002\'\'.</ref> tries to associate related queries by clustering \xe2\x80\x9csession data\xe2\x80\x9d, which contain multiple queries and click-through information from a single user interaction. They take into account terms from result documents that a set of queries has in common. The use of query keywords together with session data is shown to be the most effective method of performing query clustering.\n\n* Selectional preference based method<ref>Beitzel et al. [http://portal.acm.org/ft_gateway.cfm?id=1229183 "Automatic Classification of Web Queries Using Very Large Unlabeled Query Logs"], \'\'ACM TOIS, Volume 25, Issue 2, April 2007\'\'.</ref> tries to exploit some [[association rules]] between the query terms to help with the query classification. Given the training data, they exploit several classification approaches including exact-match using labeled data, N-Gram match using labeled data and classifiers based on perception. They emphasize on an approach adapted from computational linguistics named selectional preferences. If x and y form a pair (x; y) and y belongs to category c, then all other pairs (x; z) headed by x belong to c. They use unlabeled query log data to mine these rules and validate the effectiveness of their approaches on some labeled queries.\n\n== Applications ==\n\n* \'\'\'[[metasearch|Metasearch engines]]\'\'\' send a user\'s query to multiple search engines and blend the top results from each into one overall list. The search engine can organize the large number of Web pages in the search results, according to the potential categories of the issued query, for the convenience of Web users\' navigation.\n* \'\'\'[[Vertical search]]\'\'\', compared to general search, focuses on specific domains and addresses the particular information needs of niche audiences and professions. Once the search engine can predict the category of information a Web user is looking for, it can select a certain vertical search engine automatically, without forcing the user to access the vertical search engine explicitly.\n* \'\'\'[[Online advertising]]\'\'\'<ref>[http://www.kdd2007.com/workshops.html#adkdd Data Mining and Audience Intelligence for Advertising (ADKDD\'07)], KDD workshop 2007</ref><ref>[http://research.yahoo.com/workshops/troa-2008/ Targeting and Ranking for Online Advertising (TROA\'08)], WWW workshop 2008</ref> aims at providing interesting advertisements to Web users during their search activities. The search engine can provide relevant advertising to Web users according to their interests, so that the Web users can save time and effort in research while the advertisers can reduce their advertising costs.\nAll these services rely on the understanding Web users\' search intents through their Web queries.\n\n== See also ==\n\n* [[Document classification]]\n* [[Web search query]]\n* [[Information retrieval]]\n* [[Query expansion]]\n* [[Naive Bayes classifier]]\n* [[Support vector machines]]\n* [[Meta search]]\n* [[Vertical search]]\n* [[Online advertising]]\n\n== References ==\n\n{{reflist}}\n\n== Further reading ==\n* Shen.  [http://lbxml.ust.hk/th/th_search.pl?smode=VIEWBYCALLNUM&skeywords=CSED%202007%20Shen "Learning-based Web Query Understanding"]. \'\'Phd Thesis\'\', \'\'HKUST\'\', June 2007.\n{{Internet search}}\n\n{{DEFAULTSORT:Web Query Classification}}\n[[Category:Information retrieval]]\n[[Category:Internet search]]'
p158
sg6
S'Web query classification'
p159
ssI187
(dp160
g2
S'http://en.wikipedia.org/wiki/Web indexing'
p161
sg4
S'{{no footnotes|date=December 2014}}\n\'\'\'Web indexing\'\'\' (or \'\'\'Internet indexing\'\'\') refers to various methods for indexing the contents of a [[website]] or of the [[Internet]] as a whole. Individual websites or [[intranet]]s may use a [[back-of-the-book index]], while [[search engines]] usually use keywords and [[Metadata (computing)|metadata]] to provide a more useful vocabulary for Internet or onsite searching. With the increase in the number of [[periodical]]s that have articles online, web indexing is also becoming important for periodical websites.\n\nBack-of-the-book-style web indexes may be called "web site A-Z indexes". The implication with "A-Z" is that there is an alphabetical browse view or interface. This interface differs from that of a browse through layers of hierarchical categories (also known as a [[Taxonomy (general)|taxonomy]]) which are not necessarily alphabetical, but are also found on some web sites. Although an A-Z index could be used to index multiple sites, rather than the multiple pages of a single site, this is unusual.\n\nMetadata web indexing involves assigning keywords or phrases to web pages or web sites within a [[metadata tag]] (or "meta-tag") field, so that the web page or web site can be retrieved with a search engine that is customized to search the keywords field. This may or may not involve using keywords restricted to a controlled vocabulary list. This method is commonly used by [[search engine indexing]].\n\n==See also==\n* [[Information architecture]]\n* [[Search engine indexing]]\n* [[Search engine optimization]]\n* [[Site map]]\n* [[Web navigation]]\n* [[Web search engine]]\n\n==References==\n{{reflist}}\n\n==External links==\n<!--========================({{No More Links}})============================\n    | PLEASE BE CAUTIOUS IN ADDING MORE LINKS TO THIS ARTICLE. WIKIPEDIA  |\n    | IS NOT A COLLECTION OF LINKS NOR SHOULD IT BE USED FOR ADVERTISING. |\n    |                                                                     |\n    |           Excessive or inappropriate links WILL BE DELETED.         |\n    | See [[Wikipedia:External links]] & [[Wikipedia:Spam]] for details.  |\n    |                                                                     |\n    | If there are already plentiful links, please propose additions or   |\n    | replacements on this article\'s discussion page, or submit your link |\n    | to the relevant category at the Open Directory Project (dmoz.org)   |\n    | and link back to that category using the {{dmoz}} template.         |\n    =======================({{No More Links}})=============================-->\n*TheAlphaWeb [http://www.eprodoffice.com/shhh/abcdefghijklmnopqrstuvwxyz.htm \'\'An example of an Internet A-Z\'\']\n*Glenda Browne and Jonathan Jermey, [http://www.webindexing.biz/ \'\'Website indexing: enhancing access to information within websites, 2nd Edition\'\'], ISBN 1-875145-56-7\n*James Lamb, [http://www.jalamb.com/publications.html \'\'Website Indexes: visitors to content in two clicks, or website indexing with XRefHT32 freeware\'\'], ISBN 978-1-4116-7937-5\n*[http://www.infotoday.com/books/books/BeyondBookIndex.shtml \'\'Beyond Book Indexing: How to Get Started in Web Indexing, Embedded Indexing, and Other Computer-Based Media\'\'], edited by Marilyn Rowland and Diane Brenner, American Society of Indexers, Info Today, Inc, NJ, 2000, ISBN 1-57387-081-1\n* {{Cite web\n  |url=http://www.boxesandarrows.com/view/improving_usability_with_a_website_index\n  |title=Improving Usability with a Website Index\n  |archiveurl=http://www.webcitation.org/5vJwZDVkj\n  |archivedate=2010-12-28\n  |accessdate=2010-12-28\n  |first=Fred\n  |last=Leise\n  |date=2002-07-15\n}}\n* [http://www.web-indexing.org/article-brown.htm Why Create an Index?]\n* [http://www.theeasybee.com/directory/web-content-extraction Open Social Web 3.0 Directory] \xe2\x80\x93 Compare and review web indexing programs\n{{Internet search}}\n[[Category:Searching]]\n[[Category:Indexing]]\n\n\n{{internet-stub}}'
p162
sg6
S'Web indexing'
p163
ssI61
(dp164
g2
S'http://en.wikipedia.org/wiki/Matthews correlation coefficient'
p165
sg4
S'The \'\'\'Matthews correlation coefficient\'\'\' is used in [[machine learning]] as a measure of the quality of binary (two-class) [[Binary classification|classifications]]. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between &minus;1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and &minus;1 indicates total disagreement between prediction and observation. The statistic is also known as the [[phi coefficient]]. MCC is related to the [[Pearson\'s chi-square test|chi-square statistic]] for a 2\xc3\x972 [[contingency table]]\n\n: <math>|\\text{MCC}| = \\sqrt{\\frac{\\chi^2}{n}}</math>\n\nwhere \'\'n\'\' is the total number of observations.\n\nWhile there is no perfect way of describing the [[confusion matrix]] of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures{{Citation needed|reason=Source needed for being \'best\'|date=December 2014}}. Other measures, such as the proportion of correct predictions (also termed [[accuracy]]), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification.\n\nThe MCC can be calculated directly from the [[confusion matrix]] using the formula:\n\n: <math>\n\\text{MCC} = \\frac{ TP \\times TN - FP \\times FN } {\\sqrt{ (TP + FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }\n</math>\n\nIn this equation, \'\'TP\'\' is the number of [[true positive]]s, \'\'TN\'\' the number of [[true negative]]s, \'\'FP\'\' the number of [[false positive]]s and \'\'FN\'\' the number of [[false negative]]s. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.\n\nThe measure was introduced in 1975 by Matthews.<ref>{{cite journal|last=Matthews|first=B. W.|title=Comparison of the predicted and observed secondary structure of T4 phage lysozyme|journal=Biochimica et Biophysica Acta (BBA) - Protein Structure|date=1975|volume=405|issue=2|pages=442-451|doi=10.1016/0005-2795(75)90109-9}}</ref> The original formula equal to above was:\n: <math>\n\\text{N} = TN + TP + FN + FP\n</math>\n: <math>\n\\text{S} = \\frac{ TP + FN } { N }\n</math>\n: <math>\n\\text{P} = \\frac{ TP + FP } { N }\n</math>\n: <math>\n\\text{MCC} = \\frac{ TP / N - S \\times P } {\\sqrt{ P S  ( 1 - S)  ( 1 - P ) } }\n</math>\n\nAs a [[Correlation and dependence|correlation coefficient]], the Matthews correlation coefficient is the [[geometric mean]] of the [[regression coefficient]]s of the problem and its [[Dual (mathematics)|dual]]. The component regression coefficients of the Matthews correlation coefficient are [[markedness]] (deltap) and informedness (deltap\').<ref name="Perruchet2004">{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97\xe2\x80\x93119 |doi=10.1016/s0911-6044(03)00059-9}}</ref><ref name="Powers2007">{{cite journal |first=David M W |last=Powers |date=2007/2011 |title=Evaluation: From Precision, Recall and F-Measure  to ROC, Informedness, Markedness & Correlation |journal=Journal of Machine Learning Technologies |volume=2 |issue=1 |pages=37\xe2\x80\x9363 |url=http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf}}</ref>\n\n== Confusion Matrix ==\n{{main|Confusion matrix}}\n\n{| class="wikitable" align="right" width=35% style="font-size:98%; margin-left:0.5em; padding:0.25em; background:#f1f5fc;"\n|+ Terminology and derivations<br \n/>from a confusion matrix\n|- valign=top\n|\n; true positive (TP)\n:eqv. with hit\n; true negative (TN)\n:eqv. with correct rejection\n; false positive (FP)\n:eqv. with [[false alarm]], [[Type I error]]\n; false negative (FN)\n:eqv. with miss, [[Type II error]]\n----\n; [[sensitivity (test)|sensitivity]] or true positive rate (TPR)\n:eqv. with [[hit rate]], [[Information retrieval#Recall|recall]]\n:<math>\\mathit{TPR} = \\mathit{TP} / P = \\mathit{TP} / (\\mathit{TP}+\\mathit{FN})</math>\n; [[Specificity (tests)|specificity]] (SPC) or True Negative Rate\n:<math>\\mathit{SPC} = \\mathit{TN} / N = \\mathit{TN} / (\\mathit{FP} + \\mathit{TN}) </math>\n; [[Information retrieval#Precision|precision]] or [[positive predictive value]] (PPV)\n:<math>\\mathit{PPV} = \\mathit{TP} / (\\mathit{TP} + \\mathit{FP})</math>\n; [[negative predictive value]] (NPV)\n:<math>\\mathit{NPV} = \\mathit{TN} / (\\mathit{TN} + \\mathit{FN})</math>\n; [[Information retrieval#Fall-out|fall-out]] or false positive rate (FPR)\n:<math>\\mathit{FPR} = \\mathit{FP} / N = \\mathit{FP} / (\\mathit{FP} + \\mathit{TN})</math>\n; [[false discovery rate]] (FDR)\n:<math>\\mathit{FDR} = \\mathit{FP} / (\\mathit{FP} + \\mathit{TP}) = 1 - \\mathit{PPV} </math>\n; Miss Rate or [[Type I and type II errors#False positive and false negative rates|False Negative Rate]] (FNR)\n:<math>\\mathit{FNR} = \\mathit{FN} / (\\mathit{FN} + \\mathit{TP}) </math>\n----\n; [[accuracy]] (ACC)\n:<math>\\mathit{ACC} = (\\mathit{TP} + \\mathit{TN}) / (P + N)</math>\n;[[F1 score]]\n: is the [[Harmonic mean#Harmonic mean of two numbers|harmonic mean]] of [[Information retrieval#Precision|precision]] and [[sensitivity (test)|sensitivity]]\n:<math>\\mathit{F1} = 2 \\mathit{TP} / (2 \\mathit{TP} + \\mathit{FP} + \\mathit{FN})</math>\n; Matthews correlation coefficient (MCC)\n:<math> \\frac{ TP \\times TN - FP \\times FN } {\\sqrt{ (TP+FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }\n</math>\n\n;Informedness\n:<math>TPR + SPC - 1</math>\n;Markedness\n:<math>PPV + NPV - 1</math>\n;\n<span style="font-size:90%;">\'\'Source: Fawcett (2006).\'\'<ref name=Fawcelt2006>{{cite journal|last=Fawcelt|first=Tom|title=An Introduction to ROC Analysis|journal=Pattern Recognition Letters|date=2006|volume=27|issue=8|pages=861 - 874|doi=10.1016/j.patrec.2005.10.010}}</ref></span>\n|}\n\nLet us define an experiment from \'\'\'P\'\'\' positive instances and \'\'\'N\'\'\' negative instances for some condition. The four outcomes can be formulated in a 2\xc3\x972 \'\'[[contingency table]]\'\' or \'\'[[confusion matrix]]\'\', as follows:\n\n{{DiagnosticTesting_Diagram}}\n\n== See also ==\n* [[Phi coefficient]]\n* [[F1 score]]\n* [[Cram\xc3\xa9r\'s V (statistics)|Cram\xc3\xa9r\'s V]], a similar measure of association between nominal variables.\n* [[Cohen\'s kappa]]\xe2\x99\x99\n\n== References ==\n\n{{Reflist}}\n\n=== General References ===\n* [[Pierre Baldi|Baldi, P.]]; Brunak, S.; Chauvin, Y.; Andersen, C. A. F.; Nielsen, H. Assessing the accuracy of prediction algorithms for classification: an overview" \'\'Bioinformatics\'\' 2000, 16, 412&ndash;424. [http://bioinformatics.oxfordjournals.org/cgi/content/abstract/16/5/412]\n* Matthews, B.W., Comparison of the predicted and observed secondary structure of T4 phage lysozyme" \'\'Biochim. Biophys. Acta\'\' 1975, 405, 442&ndash;451\n* Carugo, O., Detailed estimation of bioinformatics prediction reliability through the Fragmented Prediction Performance Plots. BMC Bioinformatics 2007. [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2148069/]\n\n{{DEFAULTSORT:Matthews Correlation Coefficient}}\n[[Category:Machine learning]]\n[[Category:Information retrieval]]\n[[Category:Statistical classification]]\n[[Category:Computational chemistry]]\n[[Category:Cheminformatics]]\n[[Category:Bioinformatics]]\n[[Category:Statistical ratios]]\n[[Category:Summary statistics for contingency tables]]'
p166
sg6
S'Matthews correlation coefficient'
p167
ssI190
(dp168
g2
S'http://en.wikipedia.org/wiki/Bayesian search theory'
p169
sg4
S'\'\'\'Bayesian search theory\'\'\' is the application of [[Bayesian statistics]] to the search for lost objects. It has been used several times to find lost sea vessels, for example the [[USS Scorpion (SSN-589)|USS \'\'Scorpion\'\']]. It also played a key role in the recovery of the flight recorders in the [[Air France Flight 447]] disaster of 2009. \n\n==Procedure==\n\nThe usual procedure is as follows:\n\n# Formulate as many reasonable hypotheses as possible about what may have happened to the object. \n# For each hypothesis, construct a [[probability density function]] for the location of the object.\n# Construct a function giving the probability of actually finding an object in location&nbsp;X when searching there if it really is in location&nbsp;X. In an ocean search, this is usually a function of water depth \xe2\x80\x94 in shallow water chances of finding an object are good if the search is in the right place. In deep water chances are reduced.\n# Combine the above information coherently to produce an overall probability density map. (Usually this simply means multiplying the two functions together.) This gives the probability of finding the object by looking in location&nbsp;X, for all possible locations&nbsp;X. (This can be visualized as a [[contour map]] of probability.)\n# Construct a search path which starts at the point of highest probability and \'scans\' over high probability areas, then intermediate probabilities, and finally low probability areas.\n# Revise all the probabilities continuously during the search. For example, if the hypotheses for location&nbsp;X imply the likely disintegration of the object and the search at location&nbsp;X has yielded no fragments, then the probability that the object is somewhere around there is greatly reduced (though not usually to zero) while the probabilities of its being at other locations is correspondingly increased. The revision process is done by applying [[Bayes\' theorem]].\n\nIn other words, first search where it most probably will be found, then search where finding it is less probable, then search where the probability is even less (but still possible due to limitations on fuel, range, water currents, etc.), until insufficient hope of locating the object at acceptable cost remains.\n\nThe advantages of the Bayesian method are that all information available is used coherently (i.e., in a "leak-proof" manner) and the method automatically produces estimates of the cost for a given success probability. That is, even before the start of searching, one can say, hypothetically, "there is a 65% chance of finding it in a 5-day search. That probability will rise to 90% after a 10-day search and 97% after 15&nbsp;days" or a similar statement. Thus the economic viability of the search can be estimated before committing resources to a search.\n\nApart from the [[USS Scorpion (SSN-589)|USS \'\'Scorpion\'\']], other vessels located by Bayesian search theory include the [[MV Derbyshire|MV&nbsp;\'\'Derbyshire\'\']], the largest British vessel ever lost at sea, and the [[SS Central America|SS&nbsp;\'\'Central America\'\']]. It also proved successful in the search for a lost [[hydrogen bomb]] following the [[1966 Palomares B-52 crash]] in Spain, and the recovery in the Atlantic Ocean of the crashed [[Air France Flight 447]].\n\nBayesian search theory is incorporated into the CASP (Computer Assisted Search Program) mission planning software used by the [[United States Coast Guard]] for [[search and rescue]]. This program was later adapted for inland search by adding terrain and ground cover factors for use by the [[United States Air Force]] and [[Civil Air Patrol]].\n\n==Mathematics==\n\nSuppose a grid square has a probability \'\'p\'\' of containing the wreck and that the probability of successfully detecting the wreck if it is there is \'\'q\'\'. If the square is searched and no wreck is found, then, by Bayes\' theorem, the revised probability of the wreck being in the square is given by\n\n: <math>  p\' = \\frac{p(1-q)}{(1-p)+p(1-q)} = p \\frac{1-q}{1-pq} < p.</math>\nFor each other grid square, if its prior probability is \'\'r\'\', its posterior probability is given by\n\n: <math> r\' = r \\frac{1}{1- pq} > r. </math>\n\n\n==Optimal Distribution of Search Effort==\n\nThe classical book on this subject, based on probabilistic information, by [[Lawrence D. Stone]], won the 1975 [[Frederick W. Lanchester Prize]] by the [[Operations Research Society of America]].\n\n\n\n<!-- Material on USS Scorpion, moved from Bayesian inference\n\nIn May 1968, the [[U.S. Navy]]\'s [[nuclear submarine]] [[USS Scorpion (SSN-589)|USS \'\'Scorpion\'\' (SSN-589)]] failed to arrive as expected at her home port of [[Norfolk, Virginia]]. The command officers of the U.S. Navy were nearly convinced that the vessel had been lost off the [[East Coast of the United States|Eastern Seaboard]],  but an extensive search there failed to discover the remains of the \'\'Scorpion\'\'.\n\nThen, a Navy deep-water expert, [[John Craven USN|John P. Craven]], suggested that the USS \'\'Scorpion\'\' had sunk elsewhere. Craven organised a search southwest of the [[Azores]] based on a controversial approximate triangulation by [[hydrophone]]s. He was allocated only a single ship, the [[USNS Mizar (AGOR-11)|\'\'Mizar\'\']], and he took advice from a firm of consultant mathematicians in order to maximise his resources. A Bayesian search methodology was adopted. Experienced submarine commanders were interviewed to construct hypotheses about what could have caused the loss of the \'\'Scorpion\'\'.\n\nThe sea area was divided up into grid squares and a probability assigned to each square, under each of the hypotheses, to give a number of probability grids, one for each hypothesis. These were then added together to produce an overall probability grid. The probability attached to each square was then the probability that the wreck was in that square. A second grid was constructed with probabilities that represented the probability of successfully finding the wreck if that square were to be searched and the wreck were to be actually there. This was a known function of water depth. The result of combining this grid with the previous grid is a grid which gives the probability of finding the wreck in each grid square of the sea if it were to be searched.\n\n\n==Optimal Distribution of Search Effort==\n\nThe classical book on this subject by [[Lawrence D. Stone]] won the 1975 [[Lancaster Prize]] by the American Operations Research Society.\n\n-->\n\n==See also==\n* [[Bayesian inference]]\n* [[Search games]]\n\n== References ==\n* [[Stone, Lawrence D.]], \'\'The Theory of Optimal Search\'\', published by the [[Operations Research Society of America]], 1975\n* [[Stone, Lawrence D.]], In search of Air France Flight 447. Institute of Operations Research and the Management Sciences, 2011\n* Iida, Koji., \'\' Studies on the Optimal Search Plan\'\', Vol.&nbsp;70, Lecture Notes in Statistics, Springer-Verlag, 1992.\n* De Groot, Morris H., \'\'Optimal Statistical Decisions\'\', Wiley Classics Library, 2004.\n* Richardson, Henry R; and Stone, Lawrence D. Operations Analysis during the underwater search for \'\'Scorpion\'\'. \'\'Naval Research Logistics Quarterly\'\', June&nbsp;1971, Vol.&nbsp;18, Number&nbsp;2. Office of Naval Research.\n* Stone, Lawrence D. Search for the SS \'\'Central America\'\': Mathematical Treasure Hunting. Technical Report, Metron Inc. Reston, Virginia.\n* [[Bernard Koopman|Koopman, B.O.]] \'\'Search and Screening\'\', Operations Research Evaluation Group Report 56, Center for Naval Analyses, Alexandria, Virginia. 1946.\n* Richardson, Henry R; and Discenza, J.H. The United States Coast Guard computer-assisted search planning system (CASP). \'\'Naval Research Logistics Quarterly\'\'. Vol.&nbsp;27 number&nbsp;4. pp.&nbsp;659\xe2\x80\x93680. 1980.\n* [[Ross, Sheldon M.]], \'\'An Introduction to Stochastic Dynamic Programming\'\', Academic Press. 1983.\n\n[[Category:Bayesian statistics|Search theory]]\n[[Category:Searching]]\n[[Category:Search algorithms]]\n[[Category:Operations research]]'
p170
sg6
S'Bayesian search theory'
p171
ssI64
(dp172
g2
S'http://en.wikipedia.org/wiki/Gain (information retrieval)'
p173
sg4
S"{{other uses2|Gain}}\n{{Unreferenced|date=August 2009}}\nThe '''gain''', also called '''improvement over random''' {{cn|date=March 2013}} can be specified for a [[classifier (mathematics)|classifier]] and is an important measure {{dubious|date=March 2013}} to describe the performance of it.\n\n== Definition ==\nIn the following a random classifier is defined such that it randomly predicts the same amount of either class.\n\nThe gain is defined as described in the following:\n\n=== Gain in Precision ===\n\nThe random [[positive predictive value|precision]] of a classifier is defined as\n\n<math>\nr = \\frac{TP+FN}{TP+TN+FP+FN} = \\frac{\\textit{Positives}}{N}\n</math>\n\nwhere TP, TN, FP and FN are the numbers of true positives, true negatives, false positives and false negatives respectively, positives is the number of positive instances in the target dataset and N is the size of the dataset.\n\nThe random precision defines the lowest baseline of a classifier.\n\nAnd '''Gain''' is defined as \n\n<math>\nG = \\frac{\\textit{precision}}{r}\n</math>\n\nwhich gives a factor by which a classifier is better when compared to its random counterpart. A Gain of 1 would indicate a classifier that is not better than random. The larger the gain, the better.\n\n=== Gain in Overall Accuracy ===\n\nThe [[accuracy]] of a classifier in general is defined as\n\n<math>\nAcc = \\frac{TP+TN}{TP+TN+FP+FN} = \\frac{\\textit{Corrects}}{N}\n</math>\n\nHere, the random accuracy of a classifier can be defined as\n\n<math>\nr = \\left ( \\frac{\\textit{Positives}}{N} \\right ) ^2+ \\left ( \\frac{\\textit{Negatives}}{N} \\right ) ^2=f(\\textit{Positives})^2 + f(\\textit{Negatives})^2\n</math>\n\nf(Positives) and f(Negatives) is the fraction of positive and negative classes in the dataset.\n\nAnd again '''gain''' is\n\n<math>\nG = \\frac{\\textit{Acc}}{r}\n</math>\n\nThis time the gain is measured not only with respect to the prediction of a so-called positive class, but with respect to the overall classifier ability to distinguish the two equally important classes.\n\n== Application ==\nIn [[Bioinformatics]] as an example, the gain is measured for methods that predict residue contacts in proteins.\n\n== See also ==\n* [[Accuracy and precision]]\n* [[Binary classification]]\n* [[Brier score]]\n* [[Confusion matrix]]\n* [[Detection theory]]\n* [[F-score]]\n* [[Information retrieval]]\n* [[Matthews correlation coefficient]]\n* [[Receiver operating characteristic]] or ROC curve\n* [[Selectivity (electronic)|Selectivity]]\n* [[Sensitivity and specificity]]\n* [[Sensitivity index]]\n* [[Statistical significance]]\n* [[Youden's J statistic]]\n\n{{DEFAULTSORT:Gain (Information Retrieval)}}\n[[Category:Logic]]\n[[Category:Information retrieval]]"
p174
sg6
S'Gain (information retrieval)'
p175
ssI193
(dp176
g2
S'http://en.wikipedia.org/wiki/Category:Search engine software'
p177
sg4
S'[[Category:Searching]]\n[[Category:Data search engines]]\n[[Category:Utility software by type]]\n[[Category:Marketing software]]\n[[Category:Web software]]'
p178
sg6
S'Category:Search engine software'
p179
ssI67
(dp180
g2
S'http://en.wikipedia.org/wiki/Full text search'
p181
sg4
S'{{Multiple issues|\n{{refimprove|date=August 2012}}\n{{cleanup|date=September 2009}}\n}}\n\nIn [[text retrieval]], \'\'\'full-text search\'\'\' refers to techniques for searching a single [[computer]]-stored [[document]] or a collection in a [[full text database]]. Full-text search is distinguished from searches based on [[metadata]] or on parts of the original texts represented in databases (such as titles, abstracts, selected sections, or bibliographical references).\n\nIn a full-text search, a [[search engine]] examines all of the words in every stored document as it tries to match search criteria (text specified by a user). Full-text-searching techniques became common in online [[bibliographic databases]] in the 1990s.{{Verify source|date=October 2008}} Many websites and application programs (such as [[word processing]] software) provide full-text-search capabilities. Some web search engines, such as [[AltaVista]], employ full-text-search techniques, while others index only a portion of the web pages examined by their indexing systems.<ref>In practice, it may be difficult to determine how a given search engine works. The [[search algorithms]] actually employed by web-search services are seldom fully disclosed out of fear that web entrepreneurs will use [[search engine optimization]] techniques to improve their prominence in retrieval lists.</ref>\n\n==Indexing==\nWhen dealing with a small number of documents, it is possible for the full-text-search engine to directly scan the contents of the documents with each [[Information retrieval|query]], a strategy called "serial scanning." This is what some tools, such as [[grep]], do when searching.\n\nHowever, when the number of documents to search is potentially large, or the quantity of search queries to perform is substantial, the problem of full-text search is often divided into two tasks: indexing and searching. The indexing stage will scan the text of all the documents and build a list of search terms (often called an [[Search index|index]], but more correctly named a [[concordance (publishing)|concordance]]). In the search stage, when performing a specific query, only the index is referenced, rather than the text of the original documents.<ref name="Capabilities of Full Text Search System ">[http://www.lucidimagination.com/full-text-search Capabilities of Full Text Search System] {{Dead link |date=October 2012}}</ref>\n\nThe indexer will make an entry in the index for each term or word found in a document, and possibly note its relative position within the document. Usually the indexer will ignore [[stop words]] (such as "the" and "and") that are both common and insufficiently meaningful to be useful in searching. Some indexers also employ language-specific [[stemming]] on the words being indexed. For example, the words "drives", "drove", and "driven" will be recorded in the index under the single concept word "drive."\n\n==The precision vs. recall tradeoff==\n[[Image:Full-text-search-results.png|150px|thumb|right|This diagram represents a low-precision, low-recall search as described in the text.]]\nRecall measures the quantity of relevant results returned by a search, while precision is the measure of the quality of the results returned. Recall is the ratio of relevant results returned divided by all relevant results. Precision is the number of relevant results returned divided by the total number of results returned.\n\nThe diagram at right represents a low-precision, low-recall search. In the diagram the red and green dots represent the total population of potential search results for a given search. Red dots represent irrelevant results, and green dots represent relevant results. Relevancy is indicated by the proximity of search results to the center of the inner circle. Of all possible results shown, those that were actually returned by the search are shown on a light-blue background. In the example only one relevant result of three possible relevant results was returned, so the recall is a very low ratio of 1/3 or 33%. The precision for the example is a very low 1/4 or 25%, since only one of the four results returned was relevant.<ref name="isbn1430215941">{{cite book|last=Coles|first=Michael|year=2008|title=Pro Full-Text Search in SQL Server 2008|edition=Version 1|publisher=[[Apress|Apress Publishing Company]]|isbn=1-4302-1594-1}}</ref>\n\nDue to the ambiguities of [[natural language]], full text search systems typically includes options like [[stop words]] to increase precision and [[stemming]] to increase recall. [[Controlled vocabulary|Controlled-vocabulary]] searching also helps alleviate low-precision issues by [[tag (metadata)|tagging]] documents in such a way that ambiguities are eliminated. The trade-off between precision and recall is simple: an increase in precision can lower overall recall while an increase in recall lowers precision.<ref name="YuwonoLee">{{Cite conference | first = Yuwono | last = B. |author2=Lee, D.L. | title = Search and ranking algorithms for locating resources on the World Wide Web | pages = 164 | publisher = 12th International Conference on Data Engineering (ICDE\'96) | year = 1996}}</ref>\n\n{{See also|Precision and recall}}\n\n==False-positive problem==\n\nFree text searching is likely to retrieve many documents that are not [[relevance|relevant]] to the \'\'intended\'\' search question. Such documents are called \'\'false positives\'\' (see [[Type I and type II errors#Type I error|Type I error]]). The retrieval of irrelevant documents is often caused by the inherent ambiguity of [[natural language]]. In the sample diagram at right, false positives are represented by the irrelevant results (red dots) that were returned by the search (on a light-blue background).\n\nClustering techniques based on [[Bayesian inference|Bayesian]] algorithms can help reduce false positives. For a search term of "football", clustering can be used to categorize the document/data universe into "American football", "corporate football", etc. Depending on the occurrences of words relevant to the categories, search terms a search result can be placed in one or more of the categories. This technique is being extensively deployed in the e-discovery domain.{{clarify|date=January 2012}}\n\n==Performance improvements==\n\nThe deficiencies of free text searching have been addressed in two ways: By providing users with tools that enable them to express their search questions more precisely, and by developing new search algorithms that improve retrieval precision.\n\n===Improved querying tools===\n\n*[[Index term|Keyword]]s. Document creators (or trained indexers) are asked to supply a list of words that describe the subject of the text, including synonyms of words that describe this subject. Keywords improve recall, particularly if the keyword list includes a search word that is not in the document text.\n* [[Field-restricted search]]. Some search engines enable users to limit free text searches to a particular [[field (computer science)|field]] within a stored [[Record (computer science)|data record]], such as "Title" or "Author."\n* [[Boolean query|Boolean queries]]. Searches that use [[Boolean logic|Boolean]] operators (for example, "encyclopedia" AND "online" NOT "Encarta") can dramatically increase the precision of a free text search. The AND operator says, in effect, "Do not retrieve any document unless it contains both of these terms." The NOT operator says, in effect, "Do not retrieve any document that contains this word." If the retrieval list retrieves too few documents, the OR operator can be used to increase [[recall (information retrieval)|recall]]; consider, for example, "encyclopedia" AND "online" OR "Internet" NOT "Encarta". This search will retrieve documents about online encyclopedias that use the term "Internet" instead of "online." This increase in precision is very commonly counter-productive since it usually comes with a dramatic loss of recall.<ref>Studies have repeatedly shown that most users do not understand the negative impacts of boolean queries.[http://eprints.cs.vt.edu/archive/00000112/]</ref>\n* [[Phrase search]]. A phrase search matches only those documents that contain a specified phrase, such as "Wikipedia, the free encyclopedia."\n* [[Concept search]]. A search that is based on multi-word concepts, for example [[Compound term processing]]. This type of search is becoming popular in many e-Discovery solutions.\n* [[Concordance search]]. A concordance search produces an alphabetical list of all principal words that occur in a [[Plain text|text]] with their immediate context.\n* [[Proximity search (text)|Proximity search]]. A phrase search matches only those documents that contain two or more words that are separated by a specified number of words; a search for "Wikipedia" WITHIN2 "free" would retrieve only those documents in which the words "Wikipedia" and "free" occur within two words of each other.\n* [[Regular expression]]. A regular expression employs a complex but powerful querying [[syntax]] that can be used to specify retrieval conditions with precision.\n* [[Fuzzy search]] will search for document that match the given terms and some variation around them (using for instance [[edit distance]] to threshold the multiple variation)\n* [[Wildcard character|Wildcard search]]. A search that substitutes one or more characters in a search query for a wildcard character such as an [[asterisk]]. For example using the asterisk in a search query "s*n" will find "sin", "son", "sun", etc. in a text.\n\n===Improved search algorithms===\nThe [[PageRank]] algorithm developed by [[Google]] gives more prominence to documents to which other [[Web page]]s have linked.<ref>{{Cite patent | inventor-last = Page | inventor-first = Lawrence | publication-date = 1/9/1998 | issue-date = 9/4/2001 | title = Method for node ranking in a linked database | country-code = US | description = A method assigns importance ranks to nodes in a linked database, such as any database of documents containing citations, the world wide web or any other hypermedia database. The rank assigned to a document is calculated from the ranks of documents citing it. In addition, the rank of a document is... | patent-number = 6285999 | postscript = <!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref> See [[Search engine]] for additional examples.\n\n==Software==\n\nThe following is a partial list of available software products whose predominant purpose is to perform full text indexing and searching. Some of these are accompanied with detailed descriptions of their theory of operation or internal algorithms, which can provide additional insight into how full text search may be accomplished.\n\n=== Free and open source software ===\n<!--\n\nPlease do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.\n\n-->\n* [[Apache Solr]]\n* [[BaseX]]\n* [[DataparkSearch]]\n* [[ElasticSearch]]\n* [[Ht-//Dig|ht://Dig]]\n* [[KinoSearch]]\n* [[Lemur Project|Lemur/Indri]]\n* [[Lucene]]\n* [[mnoGoSearch]]\n* [[Searchdaimon]]\n* [[Sphinx (search engine)|Sphinx]]\n* [[Swish-e]]\n* [[Xapian]]\n\n=== Proprietary software ===\n<!--\n\nPlease do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.\n\n-->\n* [[Attivio]]\n* [[Autonomy Corporation]]\n* [[Bar Ilan Responsa Project]]\n* [[Brainware]]\n* [[BRS/Search]] \n* [[Clusterpoint|Clusterpoint Server]]\n* [[Concept Searching Limited]]\n* [[Dieselpoint]]\n* [[dtSearch]]\n* [[Endeca]]\n* [[Exalead]]\n* [[Fast Search & Transfer]]\n* [[Inktomi (company)|Inktomi]]\n* [[Dan Wagner#Locayta|Locayta]](rebranded to [[ATTRAQT]] in 2014)\n* [[Lookeen]]\n* [[Lucid Imagination]]\n* [[MarkLogic]]\n* [[Swiftype]]\n* [[Thunderstone Software LLC.]]\n* [[Viv\xc3\xadsimo]]\n\n==Notes==\n{{Reflist}}\n\n==See also==\n*[[Pattern matching]] and [[string matching]]\n*[[Compound term processing]]\n*[[Controlled vocabulary]]\n*[[Enterprise search]]\n*[[Information Extraction]]\n*[[Information retrieval]]\n*[[Faceted search]]\n*[[Full text database]]\n*[[List of enterprise search vendors]]\n*[[Search engine]]\n*[[WebCrawler]], first FTS engine\n*[[Search engine indexing]] - how search engines generate indices to support full text searching\n*[[SQL Server Full Text Search|SQL Server Full Text Search (implementation of)]]\n\n{{DEFAULTSORT:Full Text Search}}\n[[Category:Searching]]\n[[Category:Text editor features]]\n[[Category:Information retrieval]]'
p182
sg6
S'Full text search'
p183
ssI196
(dp184
g2
S'http://en.wikipedia.org/wiki/Category:Concordances (publishing)'
p185
sg4
S'{{Cat main|Concordance (publishing)}}\n[[Category:Biblical studies]]\n[[Category:Indexing]]\n[[Category:Linguistics]]\n[[Category:Reference works]]\n[[Category:Searching]]\n[[Category:Hypertext]]'
p186
sg6
S'Category:Concordances (publishing)'
p187
ssI70
(dp188
g2
S'http://en.wikipedia.org/wiki/DtSearch'
p189
sg4
S'{{Lowercase}}\n\n{{Infobox company |\n  name   = dtSearch Corp. |\n  slogan = "The Smart Choice for Text Retrieval since 1991" |\n  type   =  Private company |\n  foundation     = 1991 |\n  location       = [[Bethesda, Maryland|Bethesda]], [[Maryland]] |\n  key_people     = David Thede, President |\n  industry       = [[Software]] |\n\n  homepage       = [http://www.dtsearch.com/ www.dtsearch.com]\n}}\n\n\'\'\'dtSearch Corp.\'\'\' is a [[software company]] which specializes in [[text retrieval]] software. It was founded in 1991, and is headquartered in [[Bethesda, Maryland|Bethesda]], [[Maryland]]. Its current range of software includes products for enterprise [[desktop search]], Intranet/Internet [[spidering]] and search, and [[search engines]] for developers ([[Software development kit|SDK]]) to integrate into other software applications.\n\n==History==\ndtSearch Corp was founded by David Thede<ref>[http://www.lets-talk-computers.com/guests/dtsearch/6.2/index.htm; Lets talk computers - Interview May 31, 2003]</ref><ref>[https://www.google.com/patents/US6782380 Method and system for indexing and searching contents of extensible mark-up language(XML) documents US 6782380 B1]</ref><ref>[https://www.google.com/patents/US7464098 Method for rapidly searching elements or attributes or for rapidly filtering fragments in binary representations of structured, for example, XML-based documents US 7464098 B2]</ref> the company started research and development in text retrieval in 1988  and incorporated in Virginia in 1991 as D T Software. Marketing of dtSearch 1.0 a DOS Text Retrieval software product began in the first quarter of 1991. Initially it was distributed as [[Association of Shareware Professionals]]-approved [[shareware]]. The product was featured in an article entitled "Text Retrieval Software" in an early edition of \'\'[[PC Magazine]]\'\'<ref>"Text Retrieval Software". (July 1992). [[PC Magazine]] (UK ed)</ref> as a shareware alternative to the commercial products reviewed; these included [[ISYS Search Software|ISYS]], [[ZyLAB Technologies|ZyIndex]], Strix, [[AskSAM]], [[ideaList]], Assassin PC, [[Folio Corporation|Folio Views]] and Lotus SmartText.\n\nIn the first few years after its initial release, dtSearch was an end-user application only. Then, in 1994, [[Symantec]] approached dtSearch about including its search technology into one of the first applications for 32-bit Windows; the dtSearch end-user application was developed into a [[Dynamic-link library]] (DLL) which Symantec embedded in Norton Navigator, which was released alongside Microsoft\xe2\x80\x99s initial release of its 32-bit Windows operating system, [[Windows 95]].<ref>[http://www.processor.com/editorial/article.asp?article=articles%2Fp3012%2F11p12%2F11p12.asp dtSearch Performs Incredible Feats. Processor Mag. March 21, 2008]</ref>\n\nIn 2007 the company was listed in the [[EContent]] 100 list, a list of companies that matter most in the digital content industry.<ref>[http://www.econtentmag.com/Articles/ArticleReader.aspx?ArticleID=40160&PageNum=22007 EContent 100 list]</ref>\n\n==Products==\nThe current (v 7.7) product range is [[Unicode]]-based and has an index that can handle over 1 [[terabyte|TB]] of data per index.\n\n*dtSearch Desktop with Spider -  Windows client Desktop search software (32 and 64 bit indexers)\n*dtSearch Network with Spider -  as dtSearch Desktop but licensed for Network use (32 and 64 bit indexers)\n*dtSearch Web with Spider -  browser based search-only client for Intranet/Internet usage based on Microsoft IIS (32 and 64 bit indexers)\n*dtSearch Engine with Spider - SDK with C++, .NET, COM, Java, Delphi APIs (32-bit and 64-bit versions)\n*dtSearch Engine for Linux - SDK with C++ and Java APIs\n*dtSearch Publish <ref>[http://www.law.com/jsp/lawtechnologynews/PubArticleLTN.jsp?id=1202463957873&slreturn=1&hbxlogin=1 dtSearch Publish for EDD Production Law Technology News July 29, 2010]</ref> - a search front-end for CD\\DVD publishing (32 and 64 bit indexers)\n\n==See also==\n* [[Enterprise search]]\n* [[List of enterprise search vendors]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.dtsearch.com/ Company Website]\n*[http://www.searchtools.com/tools/dtsearch.html Product description on SearchTools.com ]\n*[http://www.windowsitpro.com/article/desktop-management/dtsearch-7-desktop-with-spider.aspx The index is mightier than the sword - Windows IT Pro. August 27, 2008]\n*[http://www.infoworld.com/t/platforms/desktop-search-gets-down-business-610 Desktop search gets down to business - InfoWorld. September 01, 2005]\n*[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC150357/ Integrating Query of Relational and Textual Data in Clinical Databases - J Am Med Inform Assoc. 2003 Jan\xe2\x80\x93Feb]\n*[http://radiographics.rsna.org/content/29/5/1233.full.pdf Informatics in Radiology. Render: An Online Searchable Radiology Study Repository - RadioGraphics 2009; 29:1233\xe2\x80\x931246] \n*[http://jms.ndmctsgh.edu.tw/fdarticlee%5C2606199.pdf Use Of Intelligent Computer Search for the Patterns of Abnormal Lymphatic Uptake by F-18 FDG PET in Primary Lung Cancers - J Med Sci 2006;26(6):199-204]\n\n{{DEFAULTSORT:Dtsearch Corp.}}\n[[Category:Desktop search engines]]\n[[Category:Information retrieval]]\n[[Category:Software companies based in Maryland]]'
p190
sg6
S'DtSearch'
p191
ssI199
(dp192
g2
S'http://en.wikipedia.org/wiki/Hamming distance'
p193
sg4
S'{| align="right"\n|-\n| [[Image:Hamming distance 3 bit binary.svg|thumb|140px|3-bit binary [[cube]] for finding Hamming distance]]\n| [[Image:Hamming distance 3 bit binary example.svg|thumb|140px|Two example distances: 100\xe2\x86\x92011 has distance 3 (red path); 010\xe2\x86\x92111 has distance 2 (blue path)]]\n|-\n|colspan=2 | [[Image:Hamming distance 4 bit binary.svg|thumb|280px|4-bit binary [[tesseract]] for finding Hamming distance]]\n|-\n|colspan=2 | [[Image:Hamming distance 4 bit binary example.svg|thumb|280px|Two example distances: 0100\xe2\x86\x921001 has distance 3 (red path); 0110\xe2\x86\x921110 has distance 1 (blue path)]]\n|}\n\nIn [[information theory]], the \'\'\'Hamming distance\'\'\' between two [[String (computer science)|string]]s of equal length is the number of positions at which the corresponding symbols are different. In another way, it measures the minimum number of \'\'substitutions\'\' required to change one string into the other, or the minimum number of \'\'errors\'\' that could have transformed one string into the other.\n\n==Examples==\nThe Hamming distance between:\n* "\'\'\'</span>ka<span style="color:#0082ff">rol</span>in</span>\'\'\'" and "\'\'\'</span>ka<span style="color:red;">thr</span>in</span>\'\'\'" is 3.\n* "\'\'\'</span>k<span style="color:#0082ff">a</span>r<span style="color:#0082ff">ol</span>in</span>\'\'\'" and "\'\'\'</span>k<span style="color:red;">e</span>r<span style="color:red;">st</span>in</span>\'\'\'" is 3.\n* \'\'\'10<span style="color:#0082ff">1</span>1<span style="color:#0082ff">1</span>01\'\'\' and \'\'\'10<span style="color:red;">0</span>1<span style="color:red;">0</span>01\'\'\' is 2.\n* \'\'\'2<span style="color:#0082ff">17</span>3<span style="color:#0082ff">8</span>96\'\'\' and \'\'\'2<span style="color:red;">23</span>3<span style="color:red;">7</span>96\'\'\' is 3.\n\n==Special properties==\nFor a fixed length \'\'n\'\', the Hamming distance is a [[Metric (mathematics)|metric]] on the vector space of the words of length n, as it fulfills the conditions of non-negativity, identity of indiscernibles and symmetry, and it can be shown by [[complete induction]] that it satisfies the [[triangle inequality]] as well. The Hamming distance between two words \'\'a\'\' and \'\'b\'\' can also be seen as the [[Hamming weight]] of \'\'a\'\'&minus;\'\'b\'\' for an appropriate choice of the &minus; operator.\n\nFor \'\'\'binary strings\'\'\' \'\'a\'\' and \'\'b\'\' the Hamming distance is equal to the number of ones ([[Hamming weight|population count]]) in \'\'a\'\' [[Exclusive or|XOR]] \'\'b\'\'. The metric space of length-\'\'n\'\' binary strings, with the Hamming distance, is known as the \'\'Hamming cube\'\'; it is equivalent as a metric space to the set of distances between vertices in a [[hypercube graph]]. One can also view a binary string of length \'\'n\'\' as a vector in <math>R^n</math> by treating each symbol in the string as a real coordinate; with this embedding, the strings form the vertices of an \'\'n\'\'-dimensional [[hypercube]], and the Hamming distance of the strings is equivalent to the [[Manhattan distance]] between the vertices.\n\n==History and applications==\n\nThe Hamming distance is named after [[Richard Hamming]], who introduced it in his fundamental paper on [[Hamming code]]s \'\'Error detecting and error correcting codes\'\' in 1950.<ref>{{harvtxt|Hamming|1950}}.</ref> It is used in [[telecommunication]] to count the number of flipped bits in a fixed-length binary word as an estimate of error, and therefore is sometimes called the \'\'\'signal distance\'\'\'. Hamming weight analysis of bits is used in several disciplines including [[information theory]], [[coding theory]], and [[cryptography]]. However, for comparing strings of different lengths, or strings where not just substitutions but also insertions or deletions have to be expected, a more sophisticated metric like the [[Levenshtein distance]] is more appropriate.\nFor \'\'q\'\'-ary strings over an [[alphabet]] of size \'\'q\'\'&nbsp;\xe2\x89\xa5&nbsp;2 the Hamming distance is applied in case of orthogonal [[modulation]], while the [[Lee distance]] is used for phase modulation. If \'\'q\'\'&nbsp;=&nbsp;2 or \'\'q\'\'&nbsp;=&nbsp;3 both distances coincide.\n\nThe Hamming distance is also used in [[systematics]] as a measure of genetic distance.<ref name="pmid18351799">{{harvtxt|Pilcher|Wong|Pillai|2008}}.</ref>\n\nOn a grid such as a chessboard, the Hamming distance is the minimum number of moves it would take a [[Rook_(chess)|rook]] to move from one cell to the other.\n\n== Algorithm example ==\nThe [[Python (programming language)|Python]] function <code>hamming_distance()</code> computes the Hamming distance between\ntwo strings (or other [[Iterator|iterable]] objects) of equal length, by creating a sequence of Boolean values indicating mismatches and matches between corresponding positions in the two inputs, and then summing the sequence with False and True values being interpreted as zero and one.\n{{-}}\n\n<syntaxhighlight lang="python">\ndef hamming_distance(s1, s2):\n    """Return the Hamming distance between equal-length sequences"""\n    if len(s1) != len(s2):\n        raise ValueError("Undefined for sequences of unequal length")\n    return sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2))\n</syntaxhighlight>\n\nThe following [[C (programming language)|C]] function will compute the Hamming distance of two integers (considered as binary values, that is, as sequences of bits). The running time of this procedure is proportional to the Hamming distance rather than to the number of bits in the inputs. It computes the [[bitwise operation|bitwise]] [[exclusive or]] of the two inputs, and then finds the [[Hamming weight]] of the result (the number of nonzero bits) using an algorithm of {{harvtxt|Wegner|1960}} that repeatedly finds and clears the lowest-order nonzero bit.\n\n<syntaxhighlight lang="c">\nint hamming_distance(unsigned x, unsigned y)\n{\n    int       dist;\n    unsigned  val;\n\n    dist = 0;\n    val = x ^ y;    // XOR\n\n    // Count the number of bits set\n    while (val != 0)\n    {\n        // A bit is set, so increment the count and clear the bit\n        dist++;\n        val &= val - 1;\n    }\n\n    // Return the number of differing bits\n    return dist;\n}\n</syntaxhighlight>\n\n==See also==\n{{Portal|Mathematics}}\n* [[Closest string]]\n* [[Damerau\xe2\x80\x93Levenshtein distance]]\n* [[Euclidean distance]]\n* [[Mahalanobis distance]]\n* [[Jaccard index]]\n* [[String metric]]\n* [[S\xc3\xb8rensen similarity index]]\n* [[Word ladder]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n*{{FS1037C}}\n*{{citation\n | last = Hamming | first = Richard W. | author-link = Richard W. Hamming\n | mr = 0035935\n | issue = 2\n | journal = [[Bell System Technical Journal]]\n | pages = 147\xe2\x80\x93160\n | title = Error detecting and error correcting codes\n | url = http://wayback.archive.org/web/20060525060427/http://www.caip.rutgers.edu/~bushnell/dsdwebsite/hamming.pdf\n | volume = 29\n | year = 1950\n | doi=10.1002/j.1538-7305.1950.tb00463.x}}.\n*{{citation\n | last1 = Pilcher | first1 = C. D.\n | last2 = Wong | first2 = J. K.\n | last3 = Pillai | first3 = S. K.\n | date = March 2008\n | doi = 10.1371/journal.pmed.0050069\n | issue = 3\n | journal = PLoS Med.\n | page = e69\n | pmid = 18351799\n | title = Inferring HIV transmission dynamics from phylogenetic sequence relationships\n | volume = 5\n | pmc = 2267810}}.\n*{{citation\n | last = Wegner | first = Peter | author-link = Peter Wegner\n | doi = 10.1145/367236.367286\n | issue = 5\n | journal = [[Communications of the ACM]]\n | page = 322\n | title = A technique for counting ones in a binary computer\n | volume = 3\n | year = 1960}}.\n\n[[Category:String similarity measures]]\n[[Category:Coding theory]]\n[[Category:Articles with example Python code]]\n[[Category:Articles with example C++ code]]\n[[Category:Metric geometry]]\n[[Category:Cubes]]'
p194
sg6
S'Hamming distance'
p195
ssI73
(dp196
g2
S'http://en.wikipedia.org/wiki/Cognitive models of information retrieval'
p197
sg4
S'{{Orphan|date=September 2012}}\n\n\'\'\'Cognitive models of information retrieval\'\'\' rest on the mix of areas such as [[cognitive science]], [[human-computer interaction]], [[information retrieval]], and  [[library science]]. They describe the relationship between a person\'s cognitive model of the information sought and the organization of this information in an information system.  These models attempt to understand how a person is searching for information so that the database and the search of this database can be designed in such a way as to best serve the user. [[Information retrieval]] may incorporate multiple tasks and cognitive problems, particularly because different people may have different methods for attempting to find this information and expect the information to be in different forms.  Cognitive models of information retrieval may be attempts at something as apparently prosaic as improving search results or may be something more complex, such as attempting to create a database which can be queried with natural language search.\n\n==Berrypicking==\nOne way of understanding how users search for information has been described by [[Marcia Bates]]<ref>[[Marcia Bates]] (1989). "The Design of Browsing and Berrypicking Techniques for the Online Search Interface." http://www.gseis.ucla.edu/faculty/bates/berrypicking.html</ref> at the [[University of California at Los Angeles]]. Bates argues that "berrypicking" better reflects how users search for information than previous models of information retrieval.  This may be because previous models were strictly linear and did not incorporate cognitive questions.  For instance, one typical model is of a simple linear match between a query and a document.  However, Bates points out that there are simple modifications that can be made to this process.  For instance, Salton has argued that user feedback may help improve the search results.<ref>[[Gerard Salton]] (1968). \'\'Automatic Information and Retrieval\'\' (Computer Science). Dubuque, Iowa: Mcgraw-Hill Inc.</ref>\n\nBates argues that searches are evolving and occur bit by bit.  That is to say, a person constantly changes his or her search terms in response to the results returned from the information retrieval system.  Thus, a simple linear model does not capture the nature of information retrieval because the very act of searching causes feedback which causes the user to modify his or her [[cognitive model]] of the information being searched for.  In addition, information retrieval can be bit by bit.  Bates gives a number of examples.  For instance, a user may look through footnotes and follow these sources.  Or, a user may scan through recent journal articles on the topic.  In each case, the user\'s question may change and thus the search evolves.\n\n==Exploratory Search==\nResearchers in the areas of [[human-computer interaction]] and [[cognitive science]] focus on how people explore for information when interacting with the WWW. This kind of search, sometimes called [[exploratory search]], focuses on how people iteratively refine their search activities and update their internal representations of the search problems.<ref>Qu, Yan & Furnas, George. "Model-driven formative evaluation of exploratory search: A study under a sensemaking framework"</ref> Existing search engines were designed based on traditional library science theories related to retrieval basic facts and simple information through an interface. However, exploratory information retrieval often involves ill-defined search goals and evolving criteria for evaluation of relevance. The interactions between humans and the information system will therefore involve more cognitive activity, and systems that support exploratory search will therefore need to take into account the cognitive complexities involved during the dynamic information retrieval process.\n\n==Natural language searching==\n\nAnother way in which cognitive models of information may help in information retrieval is with natural language searching.  For instance, How Stuff Works imagines a world in which, rather than searching for local movies, reading the reviews, then searching for local Mexican restaurants, and reading their reviews, you will simply type ""I want to see a funny movie and then eat at a good Mexican restaurant. What are my options?" into your browser, and you will receive a useful and relevant response.<ref>Strickland, J. (n.d.). HowStuffWorks "How Web 3.0 Will Work". Howstuffworks "Computer". Retrieved November 4, 2009, from http://computer.howstuffworks.com/web-30.htm</ref>  Although such a thing is not possible today, it represents a holy grail for researchers into cognitive models of information retrieval.  The goal is to somehow program information retrieval programs to respond to natural language searches.  This would require a fuller understanding of how people structure queries.\n\n==Notes==\n{{Reflist}}\n\n[[Category:Information retrieval]]\n[[Category:Cognitive modeling]]'
p198
sg6
S'Cognitive models of information retrieval'
p199
ssI202
(dp200
g2
S'http://en.wikipedia.org/wiki/Tversky index'
p201
sg4
S"The '''Tversky index''', named after [[Amos Tversky]],<ref>{{cite journal |last=Tversky |first=Amos |title=Features of Similarity |journal=Psychological Reviews |volume=84 |number=4 |year=1977 |pages=327\xe2\x80\x93352 |url=http://www.cogsci.ucsd.edu/~coulson/203/tversky-features.pdf}}</ref> is an asymmetric [[similarity measure]] on [[set theory|sets]] that compares a variant to a prototype. The Tversky index can be seen as a generalization of [[Dice's coefficient]] and [[Tanimoto coefficient]].\n\nFor sets ''X'' and ''Y'' the Tversky index is a number between 0 and 1 given by \n\n<math>S(X, Y) = \\frac{| X \\cap Y |}{| X \\cap Y | + \\alpha | X - Y | + \\beta | Y - X |} </math>,\n\nHere, <math>X - Y</math> denotes the  [[Complement (set theory)| relative complement ]] of Y in X.\n\nFurther, <math>\\alpha, \\beta \\ge 0 </math> are parameters of the Tversky index.  Setting <math>\\alpha = \\beta = 1 </math> produces the Tanimoto coefficient; setting <math>\\alpha = \\beta = 0.5 </math> produces Dice's coefficient. \n\nIf we consider ''X'' to be the prototype and ''Y'' to be the variant, then <math>\\alpha</math> corresponds to the weight of the prototype and <math>\\beta</math> corresponds to the weight of the variant. Tversky measures with <math>\\alpha + \\beta = 1</math> are of special interest.<ref>http://www.daylight.com/dayhtml/doc/theory/theory.finger.html</ref>\n\nBecause of the inherent asymmetry, the Tversky index does not meet the criteria for a similarity metric. However, if symmetry is needed a variant of the original formulation has been proposed using '''max''' and '''min''' functions <ref>Jimenez, S., Becerra, C., Gelbukh, A. [http://aclweb.org/anthology/S/S13/S13-1028.pdf SOFTCARDINALITY-CORE: Improving Text Overlap with Distributional Measures for Semantic Textual Similarity]. Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, p.194-201, June 7\xe2\x80\x938, 2013, Atlanta, Georgia, USA.</ref>\n.\n\n<math>S(X,Y)=\\frac{| X \\cap Y |}{| X \\cap Y |+\\beta\\left(\\alpha a+(1-\\alpha)b\\right)}</math>,\n\n<math>a=\\min\\left(|X-Y|,|Y-X|\\right) </math>,\n\n<math>b=\\max\\left(|X-Y|,|Y-X|\\right) </math>,\n\nThis formulation also re-arranges parameters <math>\\alpha </math> and <math>\\beta </math>. Thus, <math> \\alpha </math> controls the balance between <math> |X - Y| </math> and <math> |Y - X| </math> in the denominator. Similarly, <math>\\beta</math> controls the effect of the symmetric difference <math> |X\\,\\triangle\\,Y\\,| </math> versus <math> | X \\cap Y | </math> in the denominator.\n\n==Notes==\n{{reflist}}\n\n[[Category:Index numbers]]\n[[Category:String similarity measures]]\n[[Category:Measure theory]]"
p202
sg6
S'Tversky index'
p203
ssI76
(dp204
g2
S'http://en.wikipedia.org/wiki/Rocchio algorithm'
p205
sg4
S'The \'\'\'Rocchio algorithm\'\'\' is based on a method of [[relevance feedback]] found in [[information retrieval]] systems which stemmed from the [[SMART Information Retrieval System]] around the year 1970. Like many other retrieval systems, the Rocchio feedback approach was developed using the [[Vector Space Model]].  The [[algorithm]] is based on the assumption that most users have a general conception of which documents should be denoted as  [[Relevance (information retrieval)|relevant]] or non-relevant.<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Sch\xc3\xbctze: \'\'An Introduction to Information Retrieval\'\', page 181. Cambridge University Press, 2009.</ref>  Therefore, the user\'s search query is revised to include an arbitrary percentage of  relevant and non-relevant documents as a means of increasing the [[search engine]]\'s [[Information_retrieval#Recall|recall]], and possibly the precision as well.  The number of  relevant and non-relevant documents allowed to enter a [[Information retrieval|query]] is dictated by the weights of the a, b, c variables listed below in the [[Rocchio_Classification#Algorithm|Algorithm section]].<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Sch\xc3\xbctze: \'\'An Introduction to Information Retrieval\'\', page 292. Cambridge University Press, 2009.</ref>\n\n==Algorithm==\nThe [[Formula (mathematical logic)|formula]] and variable definitions for Rocchio relevance feedback is as follows:<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Sch\xc3\xbctze: \'\'An Introduction to Information Retrieval\'\', page 182. Cambridge University Press, 2009.</ref>\n\n<math> \\overrightarrow{Q_m} = \\bigl(a \\cdot \\overrightarrow{Q_o}\\bigr) + \\biggl(b \\cdot {\\tfrac{1}{|D_r|}} \\cdot \\sum_{\\overrightarrow{D_j} \\in D_r} \\overrightarrow{D_j}\\biggr)\n- \\biggl(c \\cdot {\\tfrac{1}{|D_{nr}|}} \\cdot \\sum_{\\overrightarrow{D_k} \\in D_{nr}} \\overrightarrow{D_k}\\biggr) </math>\n\n{| class="wikitable"\n|-\n! Variable\n! Value\n|-\n| <math> \\overrightarrow{Q_m} </math>\n| Modified Query Vector\n|-\n| <math> \\overrightarrow{Q_o} </math>\n| Original Query Vector\n|-\n| <math> \\overrightarrow{D_j} </math>\n| Related Document Vector\n|-\n| <math> \\overrightarrow{D_k} </math>\n| Non-Related Document Vector\n|-\n| <math> a </math>\n| Original Query Weight\n|-\n| <math> b </math>\n| Related Documents Weight\n|-\n| <math> c </math>\n| Non-Related Documents Weight\n|-\n| <math> D_r </math>\n| Set of Related Documents\n|-\n| <math> D_{nr} </math>\n| Set of Non-Related Documents\n|}\n[[Image:Rocchioclassgraph.jpg|thumb|right|250px|Rocchio Classification]]\n\nAs demonstrated in the Rocchio formula, the associated weights (\'\'\'a\'\'\', \'\'\'b\'\'\', \'\'\'c\'\'\') are responsible for shaping the modified [[vector space|vector]] in a direction closer, or farther away, from the original query, related documents, and non-related documents.  In particular, the values for \'\'\'b\'\'\' and \'\'\'c\'\'\' should be incremented or decremented proportionally to the set of documents classified by the user.  If the user decides that the modified query should not contain terms from either the original query, related documents, or non-related documents, then the corresponding weight (\'\'\'a\'\'\', \'\'\'b\'\'\', \'\'\'c\'\'\') value for the category should be set to 0.\n\nIn the later part of the algorithm, the variables \'\'\'Dr\'\'\', and \'\'\'Dnr\'\'\' are presented to be sets of [[Tuple|vectors]] containing the coordinates of related documents and non-related documents.  Though \'\'\'Dr\'\'\' and \'\'\'Dnr\'\'\' are not  vectors themselves, <math> \\overrightarrow{Dj} </math> and <math> \\overrightarrow{Dk} </math> are the vectors used to iterate through the two sets and form vector [[summation]]s. These summations will be multiplied against the [[Multiplicative inverse]] of their respective document set (\'\'\'Dr\'\'\', \'\'\'Dnr\'\'\') to complete the addition or subtraction of related or non-related documents.\n\nIn order to visualize the changes taking place on the modified vector, please refer to the image below.<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Sch\xc3\xbctze: \'\'An Introduction to Information Retrieval\'\', page 293. Cambridge University Press, 2009.</ref> As the weights are increased or decreased for a particular category of documents, the coordinates for the modified vector begin to move either closer, or farther away, from the [[centroid]] of the document collection. Thus if the weight is increased for related documents, then the modified vectors [[coordinate]]s will reflect being closer to the centroid of related documents.\n\n==Time complexity==\nThe [[time complexity]] for training and testing the algorithm are listed below and followed by the definition of each [[variable (mathematics)|variable]]. Note that when in testing phase, the time complexity can be reduced to that of calculating the [[euclidean distance]] between a class [[centroid]] and the respective document.  As shown by: <math>\\Theta(\\vert\\mathbb{C}\\vert M_{a})</math>.\n\nTraining = <math>\\Theta(\\vert\\mathbb{D}\\vert L_{ave}+\\vert\\mathbb{C}\\vert\\vert V\\vert)</math> <br>\nTesting = <math>\\Theta( L_{a}+\\vert\\mathbb{C}\\vert M_{a})= \\Theta(\\vert\\mathbb{C}\\vert M_{a})</math> <ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Sch\xc3\xbctze: \'\'An Introduction to Information Retrieval\'\', page 296. Cambridge University Press, 2009.</ref>\n\n{| class="wikitable"\n|-\n! Variable\n! Value\n|-\n| <math> \\mathbb{D} </math>\n| Labeled Document Set\n|-\n| <math> L_{ave} </math>\n| Average Tokens Per Document\n|-\n| <math> \\mathbb{C} </math>\n| Class Set\n|-\n| <math> V </math>\n| Vocabulary/Term Set\n|-\n| <math> L_{a} </math>\n| Number of Tokens in Document\n|-\n| <math> M_{a} </math>\n| Number of Types in Document\n|}\n\n==Usage==\nThough there are benefits to ranking documents as not-relevant, a [[relevant]] document ranking will result in more precise documents being made available to the user. Therefore, traditional values for the algorithm\'s weights (\'\'\'a\'\'\', \'\'\'b\'\'\', \'\'\'c\'\'\') in Rocchio Classification are typically around \'\'\'a = 1\'\'\', \'\'\'b = 0.8\'\'\', and \'\'\' c = 0.1\'\'\'. Modern [[information retrieval]] systems have moved towards eliminating the non-related documents by setting \'\'\'c = 0\'\'\' and thus only accounting for related documents. Although not all [[Information retrieval|retrieval systems]] have eliminated the need for non-related documents, most have limited the effects on modified query by only accounting for strongest non-related documents in the \'\'\'Dnr\'\'\' set.\n\n==Limitations==\nThe Rocchio algorithm often fails to classify multimodal classes and relationships. For instance, the country of [[Burma]] was renamed to [[Myanmar]] in 1989. Therefore the two queries of "Burma" and "Myanmar" will appear much farther apart in the [[vector space model]], though they both contain similar origins.<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Sch\xc3\xbctze: \'\'An Introduction to Information Retrieval\'\', page 296. Cambridge University Press, 2009.</ref>\n\n== See also ==\n*  [[Nearest centroid classifier]], aka Rocchio classifier\n\n==References==\n{{reflist}}\n* [http://nlp.stanford.edu/IR-book/pdf/09expand.pdf Relevance Feedback and Query Expansion]\n* [http://nlp.stanford.edu/IR-book/pdf/14vcat.pdf Vector Space Classification]\n* [http://cs.nyu.edu/courses/fall07/G22.2580-001/lec7.html Data Classification]\n\n[[Category:Information retrieval]]'
p206
sg6
S'Rocchio algorithm'
p207
ssI205
(dp208
g2
S'http://en.wikipedia.org/wiki/Euclidean distance'
p209
sg4
S'In [[mathematics]], the \'\'\'Euclidean distance\'\'\' or \'\'\'Euclidean metric\'\'\' is the "ordinary" [[distance]] between two points in [[Euclidean space]]. With this distance, Euclidean space becomes a [[metric space]]. The associated [[Norm (mathematics)|norm]] is called the \'\'\'[[Norm (mathematics)#Euclidean norm|Euclidean norm]].\'\'\' Older literature refers to the metric as \'\'\'Pythagorean metric\'\'\'.\n\n==Definition==\nThe \'\'\'Euclidean distance\'\'\' between points \'\'\'p\'\'\' and \'\'\'q\'\'\' is the length of the [[line segment]] connecting them (<math>\\overline{\\mathbf{p}\\mathbf{q}}</math>).\n\nIn [[Cartesian coordinates]], if \'\'\'p\'\'\'&nbsp;=&nbsp;(\'\'p\'\'<sub>1</sub>,&nbsp;\'\'p\'\'<sub>2</sub>,...,&nbsp;\'\'p\'\'<sub>\'\'n\'\'</sub>) and \'\'\'q\'\'\'&nbsp;=&nbsp;(\'\'q\'\'<sub>1</sub>,&nbsp;\'\'q\'\'<sub>2</sub>,...,&nbsp;\'\'q\'\'<sub>\'\'n\'\'</sub>) are two points in [[Euclidean space|Euclidean \'\'n\'\'-space]], then the distance (d) from \'\'\'p\'\'\' to \'\'\'q\'\'\', or from \'\'\'q\'\'\' to \'\'\'p\'\'\' is given by the [[Pythagorean theorem|Pythagorean formula]]:\n\n{{NumBlk|:|<math>\\begin{align}\\mathrm{d}(\\mathbf{p},\\mathbf{q}) = \\mathrm{d}(\\mathbf{q},\\mathbf{p}) & = \\sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \\cdots + (q_n-p_n)^2} \\\\[8pt]\n& = \\sqrt{\\sum_{i=1}^n (q_i-p_i)^2}.\\end{align}</math>|{{EquationRef|1}}}}\n\nThe position of a point in a Euclidean \'\'n\'\'-space is a [[Euclidean vector]]. So, \'\'\'p\'\'\' and \'\'\'q\'\'\' are Euclidean vectors, starting from the origin of the space, and their tips indicate two points. The \'\'\'[[Euclidean norm]]\'\'\', or \'\'\'Euclidean length\'\'\', or \'\'\'magnitude\'\'\' of a vector measures the length of the vector:\n:<math>\\|\\mathbf{p}\\| = \\sqrt{p_1^2+p_2^2+\\cdots +p_n^2} = \\sqrt{\\mathbf{p}\\cdot\\mathbf{p}}</math>\nwhere the last equation involves the [[dot product]].\n\nA vector can be described as a directed line segment from the [[Origin (mathematics)|origin]] of the Euclidean space (vector tail), to a point in that space (vector tip). If we consider that its length is actually the distance from its tail to its tip, it becomes clear that the Euclidean norm of a vector is just a special case of Euclidean distance: the Euclidean distance between its tail and its tip.\n\nThe distance between points \'\'\'p\'\'\' and \'\'\'q\'\'\' may have a direction (e.g. from \'\'\'p\'\'\' to \'\'\'q\'\'\'), so it may be represented by another vector, given by\n\n:<math>\\mathbf{q} - \\mathbf{p} = (q_1-p_1, q_2-p_2, \\cdots, q_n-p_n)</math>\n\nIn a three-dimensional space (\'\'n\'\'=3), this is an arrow from \'\'\'p\'\'\' to \'\'\'q\'\'\', which can be also regarded as the position of \'\'\'q\'\'\' relative to \'\'\'p\'\'\'. It may be also called a [[displacement (vector)|displacement]] vector if \'\'\'p\'\'\' and \'\'\'q\'\'\' represent two positions of the same point at two successive instants of time.\n\nThe Euclidean distance between \'\'\'p\'\'\' and \'\'\'q\'\'\' is just the Euclidean length of this distance (or displacement) vector:\n{{NumBlk|:|<math>\\|\\mathbf{q} - \\mathbf{p}\\| = \\sqrt{(\\mathbf{q}-\\mathbf{p})\\cdot(\\mathbf{q}-\\mathbf{p})}.</math>|{{EquationRef|2}}}}\n\nwhich is equivalent to equation 1, and also to:\n\n:<math>\\|\\mathbf{q} - \\mathbf{p}\\| = \\sqrt{\\|\\mathbf{p}\\|^2 + \\|\\mathbf{q}\\|^2 - 2\\mathbf{p}\\cdot\\mathbf{q}}.</math>\n\n===One dimension===\nIn one dimension, the distance between two points on the [[real line]] is the [[absolute value]] of their numerical difference.  Thus if \'\'x\'\' and \'\'y\'\' are two points on the real line, then the distance between them is given by:\n:<math>\\sqrt{(x-y)^2} = |x-y|.</math>\n\nIn one dimension, there is a single homogeneous, translation-invariant [[Metric (mathematics)|metric]] (in other words, a distance that is induced by a [[Norm (mathematics)|norm]]), up to a scale factor of length, which is the Euclidean distance. In higher dimensions there are other possible norms.\n\n===Two dimensions===\nIn the [[Euclidean plane]], if \'\'\'p\'\'\'&nbsp;=&nbsp;(\'\'p\'\'<sub>1</sub>,&nbsp;\'\'p\'\'<sub>2</sub>) and \'\'\'q\'\'\'&nbsp;=&nbsp;(\'\'q\'\'<sub>1</sub>,&nbsp;\'\'q\'\'<sub>2</sub>) then the distance is given by\n\n:<math>\\mathrm{d}(\\mathbf{p},\\mathbf{q})=\\sqrt{(p_1-q_1)^2 + (p_2-q_2)^2}.</math>\n\nThis is equivalent to the [[Pythagorean theorem]].\n\nAlternatively, it follows from ({{EquationRef|2}}) that if the [[polar coordinates]] of the point \'\'\'p\'\'\' are (\'\'r\'\'<sub>1</sub>,&nbsp;\xce\xb8<sub>1</sub>) and those of \'\'\'q\'\'\' are (\'\'r\'\'<sub>2</sub>,&nbsp;\xce\xb8<sub>2</sub>), then the distance between the points is\n\n:<math>\\sqrt{r_1^2 + r_2^2 - 2 r_1 r_2 \\cos(\\theta_1 - \\theta_2)}.</math>\n\n===Three dimensions===\nIn three-dimensional Euclidean space, the distance  is\n\n:<math>d(p, q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2+(p_3 - q_3)^2}.</math>\n\n===\'\'n\'\' dimensions <!-- This is a lower-case italicized "n" for a reason. -->===\nIn general, for an \'\'n\'\'-dimensional space, the distance is\n\n:<math>d(p, q) = \\sqrt{(p_1- q_1)^2 + (p_2 - q_2)^2+\\cdots+(p_i - q_i)^2+\\cdots+(p_n - q_n)^2}.</math>\n\n===Squared Euclidean distance===\nThe standard Euclidean distance can be squared in order to place progressively greater weight on objects that are farther apart. In this case, the equation becomes\n\n:<math>d^2(p, q) = (p_1 - q_1)^2 + (p_2 - q_2)^2+\\cdots+(p_i - q_i)^2+\\cdots+(p_n - q_n)^2.</math>\n\nSquared Euclidean Distance is not a metric as it does not satisfy the [[triangle inequality]], however it is frequently used in optimization problems in which distances only have to be compared.\n\nIt is also referred to as [[rational trigonometry#Quadrance|quadrance]] within the field of [[rational trigonometry]].\n\n==See also==\n*[[Chebyshev distance]] measures distance assuming only the most significant dimension is relevant.\n*[[Euclidean distance matrix]]\n*[[Hamming distance]] identifies the difference bit by bit of two strings\n*[[Mahalanobis distance]] normalizes based on a covariance matrix to make the distance metric scale-invariant.\n*[[Manhattan distance]] measures distance following only axis-aligned directions.\n*[[Metric (mathematics)|Metric]]\n*[[Minkowski distance]] is a generalization that unifies Euclidean distance, Manhattan distance, and Chebyshev distance.\n*[[Pythagorean addition]]\n\n==References==\n* {{cite book |first=Elena |last=Deza |first2=Michel Marie |last2=Deza |year=2009 |title=Encyclopedia of Distances |page=94 |publisher=Springer }}\n* {{cite web |url=http://www.statsoft.com/textbook/cluster-analysis/ |title=Cluster analysis |date=March 2, 2011 }}\n\n{{DEFAULTSORT:Euclidean Distance}}\n[[Category:Metric geometry]]\n[[Category:Length]]\n[[Category:String similarity measures]]'
p210
sg6
S'Euclidean distance'
p211
ssI79
(dp212
g2
S'http://en.wikipedia.org/wiki/Query expansion'
p213
sg4
S"'''Query expansion''' ('''QE''') is the process of reformulating a seed query to improve retrieval performance in [[information retrieval]] operations.<ref>{{cite journal\n | last = Vectomova | first = Olga |author2=Wang, Ying  | year = 2006\n | title = A study of the effect of term proximity on query expansion | journal = [[Journal of Information Science]]\n | volume = 32 | issue = 4 | pages = 324&ndash;333\n | doi = 10.1177/0165551506065787 | id =  | url = http://jis.sagepub.com/cgi/content/abstract/32/4/324\n | format = Abstract | accessdate = 2006-12-09\n }}</ref>\nIn the context of web [[search engine]]s, query expansion involves evaluating a user's input (what words were typed into the search query area, and sometimes other types of [[data]]) and expanding the search query to match additional documents.  Query expansion involves techniques such as:\n\n* Finding [[synonym]]s of words, and searching for the synonyms as well\n* Finding all the various [[Morphology (linguistics)|morphological]] forms of words by [[stemming]] each word in the [[search query]]\n* Fixing [[Typographical error|spelling errors]] and automatically searching for the corrected form or suggesting it in the results\n* Re-weighting the terms in the original query\n\nQuery expansion is a methodology studied in the field of [[computer science]], particularly within the realm of [[natural language processing]] and [[information retrieval]].\n\n== Precision and recall tradeoffs ==\n\nSearch engines invoke query expansion to increase the quality of user search results.  It is assumed that users do not always formulate search queries using the best terms. Best in this case may be because the database does not contain the user entered terms.  \n\nBy [[stemming]] a user-entered term, more documents are matched, as the alternate word forms for a user entered term are matched as well, increasing the total [[recall (information retrieval)|recall]]. This comes at the expense of reducing the [[precision (information retrieval)|precision]].  By expanding a search query to search for the synonyms of a user entered term, the recall is also increased at the expense of precision.  This is due to the nature of the equation of how precision is calculated, in that a larger recall implicitly causes a decrease in precision, given that factors of recall are part of the denominator. It is also inferred that a larger recall negatively impacts overall search result quality, given that many users do not want more results to comb through, regardless of the precision.\n\nThe goal of query expansion in this regard is by increasing recall, precision can potentially increase (rather than decrease as mathematically equated), by including in the result set pages which are more relevant (of higher quality), or at least equally relevant. Pages which would not be included in the result set, which have the potential to be more relevant to the user's desired query, are included, and without query expansion would not have, regardless of relevance.  At the same time, many of the current commercial search engines use word frequency ([[Tf-idf]]) to assist in ranking.  By ranking the occurrences of both the user entered words and synonyms and alternate morphological forms, documents with a higher density (high frequency and close proximity) tend to migrate higher up in the search results, leading to a higher quality of the search results near the top of the results, despite the larger recall.\n\nThis tradeoff is one of the defining problems in query expansion, regarding whether it is worthwhile to perform given the questionable effects on precision and recall. Critics{{Who|date=March 2009}} state one of the problems is that the dictionaries and [[thesauri]], and the stemming algorithm, are driven by human bias and while this is implicitly handled by the query expansion algorithm, this explicitly affects the results in a non-automated manner (similar to how statisticians can 'lie' with statistics){{Citation needed|date=July 2013}}. Other critics{{Who|date=March 2009}} point out potential for corporate influence on the dictionaries, promoting advertising of online web pages in the case of [[web search engine]]s. {{Citation needed|date=December 2007}}\n\n==See also==\n\n* [[Search engine]]\n* [[Search engine indexing]]\n* [[Information retrieval]]\n* [[Document retrieval]]\n* [[Linguistics]]\n* [[Natural language processing]]\n* [[Stemming]]\n* [[Morphology (linguistics)]]\n\n== Software libraries ==\n*[http://qtanalyzer.codeplex.com/ QueryTermAnalyzer] open-source, C#. Machine learning based query term weight and synonym analyzer for query expansion.\n*[http://lucene-qe.sourceforge.net/ LucQE] - open-source, Java.  Provides a framework along with several implementations that allow to perform query expansion with the use of Apache [[Lucene]].\n*[[Xapian]] is an open-source search library which includes support for query expansion\n\n== References ==\n\n* D. Abberley, D. Kirby, S. Renals, and T. Robinson, The THISL broadcast news  retrieval system. In ''Proc. ESCA ETRW Workshop Accessing Information in Spoken Audio'', (Cambridge), pp.&nbsp;14\xe2\x80\x9319, 1999. Section on [http://homepages.inf.ed.ac.uk/srenals/pubs/1999/esca99-thisl/node6.html Query Expansion] - Concise, mathematical overview.\n* R. Navigli, P. Velardi. [http://www.dcs.shef.ac.uk/~fabio/ATEM03/navigli-ecml03-atem.pdf An Analysis of Ontology-based Query Expansion Strategies]. ''Proc. of Workshop on Adaptive Text Extraction and Mining (ATEM 2003)'', in the ''14th European Conference on Machine Learning (ECML 2003)'', Cavtat-Dubrovnik, Croatia, September 22-26th, 2003, pp.&nbsp;42\xe2\x80\x9349 - An analysis of query expansion methods relying on WordNet as the reference ontology.\n* Y. Qiu and H.P. Frei. [http://citeseer.ist.psu.edu/qiu93concept.html Concept Based Query Expansion]. In ''Proceedings of SIGIR-93, 16th ACM International Conference on Research and Development in Information Retrieval'', Pittsburgh, SIGIR Forum, ACM Press, June 1993 - Academic document on a specific method of query expansion\n* Efthimis N. Efthimiadis. [http://faculty.washington.edu/efthimis/pubs/Pubs/qe-arist/QE-arist.html Query Expansion]. In: Martha E. Williams (ed.), ''Annual Review of Information Systems and Technology (ARIST)'', v31, pp 121\xe2\x80\x93187, 1996 - An introduction for less-technical viewers.\n\n=== Notes ===\n{{reflist}}\n\n{{DEFAULTSORT:Query Expansion}}\n[[Category:Information retrieval]]\n[[Category:Searching]]"
p214
sg6
S'Query expansion'
p215
ssI208
(dp216
g2
S'http://en.wikipedia.org/wiki/Jaro\xe2\x80\x93Winkler distance'
p217
sg4
S'{{About|the measure|other uses|Jaro (disambiguation){{!}}Jaro}}\n\n{{Original research|date=May 2013}}\n{{Notability|date=May 2013}}\n\nIn [[computer science]] and [[statistics]], the \'\'\'Jaro\xe2\x80\x93Winkler distance\'\'\' (Winkler, 1990) is a measure of similarity between two [[String (computer science)|strings]].  It is a variant of the \'\'\'Jaro distance\'\'\' metric (Jaro, 1989, 1995), a type of string [[edit distance]], and was developed in the area of [[record linkage]] (duplicate detection) (Winkler, 1990). The higher the Jaro\xe2\x80\x93Winkler distance for two strings is, the more similar the strings are.  The Jaro\xe2\x80\x93Winkler distance metric is designed and best suited for short strings such as person names.  The score is normalized such that 0 equates to no similarity and 1 is an exact match.\n\n== Definition ==\n\nThe Jaro distance <math>d_j</math> of two given strings <math>s_1</math> and <math>s_2</math> is\n\n: <math>d_j = \\left\\{\n\n\\begin{array}{l l}\n  0 & \\text{if }m = 0\\\\\n  \\frac{1}{3}\\left(\\frac{m}{|s_1|} + \\frac{m}{|s_2|} + \\frac{m-t}{m}\\right) & \\text{otherwise} \\end{array} \\right.</math>\n\nWhere:\n\n* <math>m</math> is the number of \'\'matching characters\'\' (see below);\n* <math>t</math> is half the number of \'\'transpositions\'\' (see below).\n\nTwo characters from <math>s_1</math> and <math>s_2</math> respectively, are considered \'\'matching\'\' only if they are the same and not farther than <math>\\left\\lfloor\\frac{\\max(|s_1|,|s_2|)}{2}\\right\\rfloor-1</math>.\n\nEach character of <math>s_1</math> is compared with all its matching\ncharacters in <math>s_2</math>. The number of matching (but different sequence order) characters\ndivided by 2 defines the number of \'\'transpositions\'\'.\nFor example, in comparing CRATE with TRACE, only \'R\'   \'A\'   \'E\'  are the matching characters, i.e. m=3. Although \'C\', \'T\' appear in both strings, they are farther than 1, i.e., floor(5/2)-1=1. Therefore, t=0 . In DwAyNE versus DuANE the matching letters are already in the same order D-A-N-E, so no transpositions are needed.\n\nJaro\xe2\x80\x93Winkler distance uses a [[prefix]] scale <math>p</math> which gives more favourable ratings to strings that match from the beginning for a set prefix length <math>\\ell</math>.  Given two strings <math>s_1</math> and <math>s_2</math>, their Jaro\xe2\x80\x93Winkler distance <math>d_w</math> is:\n\n: <math>d_w = d_j + (\\ell p (1 - d_j))</math>\n\nwhere:\n\n* <math>d_j</math> is the Jaro distance for strings <math>s_1</math> and <math>s_2</math>\n* <math>\\ell</math> is the length of common prefix at the start of the string up to a maximum of 4 characters\n* <math>p</math> is a constant [[scaling factor]] for how much the score is adjusted upwards for having common prefixes.  <math>p</math> should not exceed 0.25, otherwise the distance can become larger than 1.  The standard value for this constant in Winkler\'s work is <math>p = 0.1</math>\n\nAlthough often referred to as a \'\'distance metric\'\', the Jaro\xe2\x80\x93Winkler distance is actually not a [[metric (mathematics)|metric]] in the mathematical sense of that term because it does not obey the [[triangle inequality]] [http://richardminerich.com/tag/jaro-winkler/].\n\nIn some implementations of Jaro-Winkler, the prefix bonus <math>\\ell p (1 - d_j)</math> is only added when the compared strings have a Jaro distance above a set "boost threshold" <math>b_t</math>. The boost threshold in Winkler\'s implementation was 0.7.\n\n: <math>d_w = \\left\\{\n\n\\begin{array}{l l}\n  d_j & \\text{if }d_j < b_t\\\\\n  d_j + (\\ell p (1 - d_j)) & \\text{otherwise} \\end{array} \\right.</math>\n\n== Example ==\n\n\'\'Note that Winkler\'s "reference" C code differs in at least two ways from published accounts of the Jaro\xe2\x80\x93Winkler metric. First is his use of a typo table (adjwt) and also some optional additional tolerance for long strings.\'\'\n\nGiven the strings <math>s_1</math> \'\'MARTHA\'\' and <math>s_2 </math> \'\'MARHTA\'\' we find:\n\n* <math>m = 6</math>\n* <math>|s_1| = 6</math>\n* <math>|s_2| = 6</math>\n* There are mismatched characters T/H and H/T leading to <math>t = \\frac{2}{2} = 1</math>\n\nWe find a Jaro score of:\n\n<math>d_j = \\frac{1}{3}\\left(\\frac{6}{6} + \\frac{6}{6} + \\frac{6-1}{6}\\right) = 0.944</math>\n\nTo find the Jaro\xe2\x80\x93Winkler score using the standard weight <math>p = 0.1</math>, we continue to find:\n\n* <math>\\ell = 3</math>\n\nThus:\n\n: <math>d_w = 0.944 + (3 * 0.1 (1 - 0.944)) = 0.961</math>\n\nGiven the strings <math>s_1</math> \'\'DWAYNE\'\' and <math>s_2</math> \'\'DUANE\'\' we find:\n\n* <math>m = 4</math>\n* <math>|s_1| = 6</math>\n* <math>|s_2| = 5</math>\n* <math>t = 0</math>\n\nWe find a Jaro score of:\n\n: <math>d_j = \\frac{1}{3}\\left(\\frac{4}{6} + \\frac{4}{5} + \\frac{4-0}{4}\\right) = 0.822</math>\n\nTo find the Jaro\xe2\x80\x93Winkler score using the standard weight <math>p = 0.1</math>, we continue to find:\n\n* <math>\\ell = 1</math>\n\nThus:\n\n: <math>d_w = 0.822 + (1 * 0.1 (1 - 0.822)) = 0.84</math>\n\nGiven the strings <math>s_1</math> \'\'DIXON\'\' and <math>s_2</math> \'\'DICKSONX\'\' we find:\n\n{{elucidate|date=March 2013}}\n\n{| class="wikitable"\n|-\n|\n| D\n| I\n| X\n| O\n| N\n|-\n| D\n| <span style="background: #ffcc33">1\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">0\n| 0\n|-\n| I\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">1\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">0\n|-\n| C\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">0\n|-\n| K\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">0\n|-\n| S\n| 0\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">0\n|-\n| O\n| 0\n| 0\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">1\n| <span style="background: #ffcc33">0\n|-\n| N\n| 0\n| 0\n| 0\n| <span style="background: #ffcc33">0\n| <span style="background: #ffcc33">1\n|-\n| X\n| 0\n| 0\n| 0\n| 0\n| <span style="background: #ffcc33">0\n|}\n\n* <math>m = 4</math>  Note that the two \'\'X\'\'s are not considered matches because they are outside the match window of 3.\n\n* <math>|s_1| = 5</math>\n* <math>|s_2| = 8</math>\n* <math>t = 0</math>\n\nWe find a Jaro score of:\n\n: <math>d_j = \\frac{1}{3}\\left(\\frac{4}{5} + \\frac{4}{8} + \\frac{4-0}{4}\\right) = 0.767</math>\n\nTo find the Jaro\xe2\x80\x93Winkler score using the standard weight <math>p = 0.1</math>, we continue to find:\n\n* <math>\\ell = 2</math>\n\nThus:\n\n: <math>d_w = 0.767 + (2 * 0.1 (1 - 0.767)) = 0.814</math>\n\n== See also ==\n\n* [[Levenshtein distance]]\n* [[Record linkage]]\n* [[Census]]\n\n== References ==\n\n* {{cite journal | last=Cohen |first=W. W. |last2=Ravikumar |first2=P. |last3=Fienberg |first3=S. E. |year=2003 |title=A comparison of string distance metrics for name-matching tasks |journal=KDD Workshop on Data Cleaning and Object Consolidation |volume=3 |pages=73-8 |url=https://www.cs.cmu.edu/afs/cs/Web/People/wcohen/postscript/kdd-2003-match-ws.pdf}}\n* {{cite journal | author = [[Matthew A. Jaro|Jaro, M. A.]] | title = Advances in record linkage methodology as applied to the 1985 census of Tampa Florida | journal = Journal of the American Statistical Association | year = 1989 | volume = 84 | issue = 406 |pages=414\xe2\x80\x9320| url = | doi = 10.1080/01621459.1989.10478785 }}\n* {{cite journal |author=Jaro, M. A. |title=Probabilistic linkage of large public health data file  |journal= Statistics in Medicine |year=1995 |volume=14 |issue=5\xe2\x80\x937 |pages=491\xe2\x80\x938  |pmid=7792443 |doi=10.1002/sim.4780140510}}\n* {{cite journal\n\n  | author = [[William E. Winkler|Winkler, W. E.]]\n  | title = String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage\n  | journal = Proceedings of the Section on Survey Research Methods\n  | publisher = American Statistical Association\n  | pages = 354\xe2\x80\x93359\n  | year = 1990\n  | url = http://www.amstat.org/sections/srms/Proceedings/papers/1990_056.pdf }}\n\n* {{cite journal | author = [[William E. Winkler|Winkler, W. E.]] | title = Overview of Record Linkage and Current Research Directions | journal = Research Report Series, RRS | year = 2006 | volume = | issue = | url = http://www.census.gov/srd/papers/pdf/rrs2006-02.pdf}}\n\n== External links ==\n\n* [http://web.archive.org/web/20100227020019/http://www.census.gov/geo/msb/stand/strcmp.c strcmp.c - Original C Implementation by the author of the algorithm]\n\n{{DEFAULTSORT:Jaro-Winkler distance}}\n\n[[Category:String similarity measures]]'
p218
sg6
S'Jaro\xe2\x80\x93Winkler distance'
p219
ssI82
(dp220
g2
S'http://en.wikipedia.org/wiki/Uncertain inference'
p221
sg4
S"'''Uncertain inference''' was first described by [[C. J. van Rijsbergen]]<ref>{{cite | author=C. J. van Rijsbergen | title=A non-classical logic for information retrieval | publisher=The Computer Journal | pages=481\xe2\x80\x93485 | year=1986}}</ref> as a way to formally define a query and document relationship in [[Information retrieval]]. This formalization is a [[logical consequence|logical implication]] with an attached measure of uncertainty.\n\n==Definitions==\nRijsbergen proposes that the measure of [[uncertainty]] of a document ''d'' to a query ''q'' be the probability of its logical implication, i.e.:\n\n<math>P(d \\to q)</math>\n\nA user's query can be interpreted as a set of assertions about the desired document. It is the system's task to [[inference|infer]], given a particular document, if the query assertions are true. If they are, the document is retrieved.\nIn many cases the contents of documents are not sufficient to assert the queries. A [[knowledge base]] of facts and rules is needed, but some of them may be uncertain because there may be a probability associated to using them for inference. Therefore, we can also refer to this as ''plausible inference''. The [[plausibility]] of an inference <math>d \\to q</math> is a function of the plausibility of each query assertion. Rather than retrieving a document that exactly matches the query we should rank the documents based on their plausibility in regards to that query.\nSince ''d'' and ''q'' are both generated by users, they are error prone; thus <math>d \\to q</math> is uncertain. This will affect the plausibility of a given query.\n\nBy doing this it accomplishes two things:\n* Separate the processes of revising probabilities from the logic\n* Separate the treatment of relevance from the treatment of requests\n\n[[Multimedia]] documents, like images or videos, have different inference properties for each datatype. They are also different from text document properties. The framework of plausible inference allows us to measure and combine the probabilities coming from these different properties.\n\nUncertain inference generalizes the notions of [[autoepistemic logic]], where truth values are either known or unknown, and when known, they are true or false.\n\n==Example==\nIf we have a query of the form:\n\n<math>q = A \\wedge B \\wedge C</math>\n\nwhere A, B and C are query assertions, then for a document D we want the probability:\n\n<math>P (D \\to (A \\wedge B \\wedge C))</math>\n\nIf we transform this into the [[conditional probability]] <math>P ((A \\wedge B \\wedge C) | D)</math> and if the query assertions are independent we can calculate the overall probability of the implication as the product of the individual assertions probabilities.\n\n==Further work==\nCroft and Krovetz<ref>{{cite | title=Interactive retrieval office documents | url=http://doi.acm.org/10.1145/45410.45435 | author=W. B. Croft | coauthors=R. Krovetz | year=1988 }}</ref> applied uncertain inference to an information retrieval system for office documents they called ''OFFICER''. In office documents the independence assumption is valid since the query will focus on their individual attributes. Besides analysing the content of documents one can also query about the author, size, topic or collection for example. They devised methods to compare document and query attributes, infer their plausibility and combine it into an overall rating for each document. Besides that uncertainty of document and query contents also had to be addressed.\n\n[[Probabilistic logic network]]s is a system for performing uncertain inference; crisp true/false truth values are replaced not only by a probability, but also by a confidence level, indicating the certitude of the probability.\n\n[[Markov logic network]]s allow uncertain inference to be performed; uncertainties are computed using the [[maximum entropy principle]], in analogy to the way that [[Markov chain]]s describe the uncertainty of [[finite state machine]]s.\n\n== See also ==\n* [[Fuzzy logic]]\n* [[Probabilistic logic]]\n* [[Plausible reasoning]]\n* [[Imprecise probability]]\n\n==References==\n{{reflist}}\n\n[[Category:Fuzzy logic]]\n[[Category:Information retrieval]]\n[[Category:Inference]]"
p222
sg6
S'Uncertain inference'
p223
ssI211
(dp224
g2
S'http://en.wikipedia.org/wiki/Levenshtein distance'
p225
sg4
S'{{refimprove|date=February 2010}}\n\nIn [[information theory]] and [[computer science]], the \'\'\'Levenshtein distance\'\'\' is a [[string metric]] for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (i.e. insertions, deletions or substitutions) required to change one word into the other. It is named after [[Vladimir Levenshtein]], who considered this distance in 1965.<ref>{{cite journal |author=\xd0\x92\xd0\xbb\xd0\xb0\xd0\xb4\xd0\xb8\xcc\x81\xd0\xbc\xd0\xb8\xd1\x80 \xd0\x98. \xd0\x9b\xd0\xb5\xd0\xb2\xd0\xb5\xd0\xbd\xd1\x88\xd1\x82\xd0\xb5\xd0\xb9\xd0\xbd |script-title=ru:\xd0\x94\xd0\xb2\xd0\xbe\xd0\xb8\xd1\x87\xd0\xbd\xd1\x8b\xd0\xb5 \xd0\xba\xd0\xbe\xd0\xb4\xd1\x8b \xd1\x81 \xd0\xb8\xd1\x81\xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\xd0\xbc \xd0\xb2\xd1\x8b\xd0\xbf\xd0\xb0\xd0\xb4\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb9, \xd0\xb2\xd1\x81\xd1\x82\xd0\xb0\xd0\xb2\xd0\xbe\xd0\xba \xd0\xb8 \xd0\xb7\xd0\xb0\xd0\xbc\xd0\xb5\xd1\x89\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb9 \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd0\xbe\xd0\xb2 |language=Russian |trans_title=Binary codes capable of correcting deletions, insertions, and reversals |journal=\xd0\x94\xd0\xbe\xd0\xba\xd0\xbb\xd0\xb0\xd0\xb4\xd1\x8b \xd0\x90\xd0\xba\xd0\xb0\xd0\xb4\xd0\xb5\xd0\xbc\xd0\xb8\xd0\xb9 \xd0\x9d\xd0\xb0\xd1\x83\xd0\xba \xd0\xa1CCP |volume=163 |issue=4 |pages=845\xe2\x80\x938 |year=1965}} Appeared in English as: {{cite journal |author=Levenshtein, Vladimir I. |title=Binary codes capable of correcting deletions, insertions, and reversals |journal=Soviet Physics Doklady |volume=10 |number=8 |pages=707\xe2\x80\x93710 |date=February 1966  |url=<!--http://profs.sci.univr.it/~liptak/ALBioinfo/files/levenshtein66.pdf right to publish copy of journal unclear: see http://www.sherpa.ac.uk/romeo/search.php?issn=1028-3358&type=issn&la=en/&fIDnum=%7C&mode=simple ; in any event, liptak does not appear to be the author or the translator -->}}</ref>\n\nLevenshtein distance may also be referred to as \'\'\'edit distance\'\'\', although that may also denote a larger [[Edit distance|family of distance metrics]].<ref name="navarro">{{Cite doi/10.1145.2F375360.375365}}</ref>{{rp|32}} It is closely related to [[Sequence alignment#Pairwise alignment|pairwise string alignments]].\n\n== Definition ==\nMathematically, the Levenshtein distance between two strings <math>a, b</math> is given by <math>\\operatorname{lev}_{a,b}(|a|,|b|)</math> where\n\n:<math>\\qquad\\operatorname{lev}_{a,b}(i,j) = \\begin{cases}\n  \\max(i,j) & \\text{ if} \\min(i,j)=0, \\\\\n  \\min \\begin{cases}\n          \\operatorname{lev}_{a,b}(i-1,j) + 1 \\\\\n          \\operatorname{lev}_{a,b}(i,j-1) + 1 \\\\\n          \\operatorname{lev}_{a,b}(i-1,j-1) + 1_{(a_i \\neq b_j)}\n       \\end{cases} & \\text{ otherwise.}\n\\end{cases}</math>\nwhere  <math>1_{(a_i \\neq b_j)}</math> is the [[indicator function]] equal to 0 when  <math>a_i = b_j</math> and equal to 1 otherwise.\n\nNote that the first element in the minimum corresponds to deletion (from <math>a</math> to <math>b</math>), the second to insertion and the third to match or mismatch, depending on whether the respective symbols are the same.\n\n=== Example ===\nFor example, the Levenshtein distance between "kitten" and "sitting" is 3, since the following three edits change one into the other, and there is no way to do it with fewer than three edits:\n\n# \'\'\'k\'\'\'itten \xe2\x86\x92 \'\'\'s\'\'\'itten (substitution of "s" for "k")\n# sitt\'\'\'e\'\'\'n \xe2\x86\x92 sitt\'\'\'i\'\'\'n (substitution of "i" for "e")\n# sittin \xe2\x86\x92 sittin\'\'\'g\'\'\' (insertion of "g" at the end).\n\n===Upper and lower bounds===\nThe Levenshtein distance has several simple upper and lower bounds. These include:\n* It is always at least the difference of the sizes of the two strings.\n* It is at most the length of the longer string.\n* It is zero if and only if the strings are equal.\n* If the strings are the same size, the [[Hamming distance]] is an upper bound on the Levenshtein distance.\n* The Levenshtein distance between two strings is no greater than the sum of their Levenshtein distances from a third string ([[triangle inequality]]).\n\n==Applications==\nIn [[approximate string matching]], the objective is to find matches for short strings in many longer texts, in situations where a small number of differences is to be expected. The short strings could come from a dictionary, for instance. Here, one of the strings is typically short, while the other is arbitrarily long. This has a wide range of applications, for instance, [[spell checker]]s, correction systems for [[optical character recognition]], and software to assist natural language translation based on [[translation memory]].\n\nThe Levenshtein distance can also be computed between two longer strings, but the cost to compute it, which is roughly proportional to the product of the two string lengths, makes this impractical.  Thus, when used to aid in [[fuzzy string searching]] in applications such as [[record linkage]], the compared strings are usually short to help improve speed of comparisons.\n\n==Relationship with other edit distance metrics==\n{{main|Edit distance}}\nThere are other popular measures of [[edit distance]], which are calculated using a different set of allowable edit operations. For instance,\n* the [[Damerau\xe2\x80\x93Levenshtein distance]] allows insertion, deletion, substitution, and the [[Transposition (mathematics)|transposition]] of two adjacent characters;\n* the [[longest common subsequence problem|longest common subsequence]] metric allows only insertion and deletion, not substitution;\n* the [[Hamming distance]] allows only substitution, hence, it only applies to strings of the same length.\n\n[[Edit distance]] is usually defined as a parameterizable metric calculated with a specific set of allowed edit operations, and each operation is assigned a cost (possibly infinite).  This is further generalized by DNA [[sequence alignment]] algorithms such as the [[Smith\xe2\x80\x93Waterman algorithm]], which make an operation\'s cost depend on where it is applied.\n\n==Computing Levenshtein distance==\n\n===Recursive===\nThis is a straightforward, but inefficient, recursive [[pseudocode]] implementation of a <code>LevenshteinDistance</code> function that takes two strings, \'\'s\'\' and \'\'t\'\', together with their lengths, and returns the Levenshtein distance between them:\n\n<!--\n  Please do not add an additional implementation in your language of choice.\n  Many of those have been added to and deleted from this article in the past.\n  See the talk page archive for relevant discussion\n-->\n<source lang="C">\n// len_s and len_t are the number of characters in string s and t respectively\nint LevenshteinDistance(string s, int len_s, string t, int len_t)\n{\n  /* base case: empty strings */\n  if (len_s == 0) return len_t;\n  if (len_t == 0) return len_s;\n\n  /* test if last characters of the strings match */\n  if (s[len_s-1] == t[len_t-1])\n      cost = 0;\n  else\n      cost = 1;\n\n  /* return minimum of delete char from s, delete char from t, and delete char from both */\n  return minimum(LevenshteinDistance(s, len_s - 1, t, len_t    ) + 1,\n                 LevenshteinDistance(s, len_s    , t, len_t - 1) + 1,\n                 LevenshteinDistance(s, len_s - 1, t, len_t - 1) + cost);\n}\n</source>\n\nUnfortunately, this straightforward recursive implementation is very inefficient because it recomputes the Levenshtein distance of the same substrings many times.\n\nA more efficient method would never repeat the same distance calculation. For example, the Levenshtein distance of all possible prefixes might be stored in an array <code>d[][]</code> where <code>d[i][j]</code> is the distance between the first <code>i</code> characters of string <code>s</code> and the first <code>j</code> characters of string <code>t</code>. The table is easy to construct one row at a time starting with row 0. When the entire table has been built, the desired distance is <code>d[len_s][len_t]</code>. While this technique is significantly faster, it will consume <code>len_s * len_t</code> more memory than the straightforward recursive implementation.\n\n===Iterative with full matrix===\n{{main|Wagner\xe2\x80\x93Fischer algorithm}}\n::{{small|Note: This section uses 1-based strings instead of 0-based strings}}\nComputing the Levenshtein distance is based on the observation that if we reserve a [[Matrix (mathematics)|matrix]] to hold the Levenshtein distances between all [[prefix (computer science)|prefix]]es of the first string and all prefixes of the second, then we can compute the values in the matrix in a [[dynamic programming]] fashion, and thus find the distance between the two full strings as the last value computed.\n\nThis algorithm, an example of bottom-up [[dynamic programming]], is discussed, with variants, in the 1974 article \'\'The [[String-to-string correction problem]]\'\' by Robert A. Wagner and Michael J. Fischer.<ref>{{citation |first=Robert A. |last=Wagner |first2=Michael J. |last2=Fischer |author2-link=Michael J. Fischer |title=The String-to-String Correction Problem |journal=Journal of the ACM |volume=21 |issue=1 |year=1974 |pages=168\xe2\x80\x93173 |doi= 10.1145/321796.321811}}</ref>\n\nThis is a straightforward pseudocode implementation for a function \'\'LevenshteinDistance\'\' that takes two strings, \'\'s\'\' of length \'\'m\'\', and \'\'t\'\' of length \'\'n\'\', and returns the Levenshtein distance between them:\n\n<!--\n  Please do not add an additional implementation in your language of choice.\n  Many of those have been added to and deleted from this article in the past.\n  See the talk page archive for relevant discussion\n-->\n<!-- choose random language for highlights -->\n<source lang="C">\nint LevenshteinDistance(char s[1..m], char t[1..n])\n{\n  // for all i and j, d[i,j] will hold the Levenshtein distance between\n  // the first i characters of s and the first j characters of t;\n  // note that d has (m+1)*(n+1) values\n  declare int d[0..m, 0..n]\n \n  clear all elements in d // set each element to zero\n \n  // source prefixes can be transformed into empty string by\n  // dropping all characters\n  for i from 1 to m\n    {\n      d[i, 0] := i\n    }\n \n  // target prefixes can be reached from empty source prefix\n  // by inserting every character\n  for j from 1 to n\n    {\n      d[0, j] := j\n    }\n \n  for j from 1 to n\n    {\n      for i from 1 to m\n        {\n          if s[i] = t[j] then\n            d[i, j] := d[i-1, j-1]       // no operation required\n          else\n            d[i, j] := minimum\n                    (\n                      d[i-1, j] + 1,  // a deletion\n                      d[i, j-1] + 1,  // an insertion\n                      d[i-1, j-1] + 1 // a substitution\n                    )\n        }\n    }\n \n  return d[m, n]\n}\n</source>\n\nNote that this implementation does not fit the [[#Definition|definition]] precisely: it always prefers matches, even if insertions or deletions provided a better score. This is equivalent; it can be shown that for every optimal alignment (which induces the Levenshtein distance) there is another optimal alignment that prefers matches in the sense of this implementation.<ref>[http://cs.stackexchange.com/a/2997 Micro-optimisation for edit distance computation: is it valid?]</ref>\n\nTwo examples of the resulting matrix (hovering over a number reveals the operation performed to get that number):\n<center>\n{{col-begin|width=auto}}\n{{col-break}}\n{|class="wikitable"\n|-\n| \n| \n!k\n!i\n!t\n!t\n!e\n!n\n|-\n| ||0 ||1 ||2 ||3 ||4 ||5 ||6\n|-\n!s\n|1 ||{{H:title|substitution of \'k\' for \'s\'|1}} ||2 ||3 ||4 ||5 ||6\n|-\n!i\n|2 ||2 ||{{H:title|\'i\' equals \'i\'|1}} ||2 ||3 ||4 ||5\n|-\n!t\n|3 ||3 ||2 ||{{H:title|\'t\' equals \'t\'|1}} ||2 ||3 ||4\n|-\n!t\n|4 ||4 ||3 ||2 ||{{H:title|\'t\' equals \'t\'|1}} ||2 ||3\n|-\n!i\n|5 ||5 ||4 ||3 ||2 ||{{H:title|substitution of \'e\' for \'i\'|2}} ||3\n|-\n!n\n|6 ||6 ||5 ||4 ||3 ||3 ||{{H:title|\'n\' equals \'n\'|2}}\n|-\n!g\n|7 ||7 ||6 ||5 ||4 ||4 ||{{H:title|insert \'g\'|3}}\n|}\n{{col-break|gap=1em}}\n{|class="wikitable"\n|\n|\n!S\n!a\n!t\n!u\n!r\n!d\n!a\n!y\n|-\n| \n|0 ||1 ||2 ||3 ||4 ||5 ||6 ||7 ||8\n|-\n!S\n|1 ||{{H:title|\'S\' equals \'S\'|0}} ||{{H:title|delete \'a\'|1}} ||{{H:title|delete \'t\'|2}} ||3 ||4 ||5 ||6 ||7\n|-\n!u\n|2 ||1 ||1 ||2 ||{{H:title|\'u\' equals \'u\'|2}} ||3 ||4 ||5 ||6\n|-\n!n\n|3 ||2 ||2 ||2 ||3 ||{{H:title|substitution of \'r\' for \'n\'|3}} ||4 ||5 ||6\n|-\n!d\n|4 ||3 ||3 ||3 ||3 ||4 ||{{H:title|\'d\' equals \'d\'|3}} ||4 ||5\n|-\n!a\n|5 ||4 ||3 ||4 ||4 ||4 ||4 ||{{H:title|\'a\' equals \'a\'|3}} ||4\n|-\n!y\n|6 ||5 ||4 ||4 ||5 ||5 ||5 ||4 ||{{H:title|\'y\' equals \'y\'|3}}\n|}\n{{col-end}}\n</center>\n\nThe [[invariant (mathematics)|invariant]] maintained throughout the algorithm is that we can transform the initial segment <code>s[1..i]</code> into <code>t[1..j]</code> using a minimum of <code>d[i,j]</code> operations. At the end, the bottom-right element of the array contains the answer.\n\n===Iterative with two matrix rows===\nIt turns out that only two rows of the table are needed for the construction if one does not want to reconstruct the edited input strings (the previous row and the current row being calculated).\n\nThe Levenshtein distance may be calculated iteratively using the following algorithm:<ref>{{Citation |title=Fast, memory efficient Levenshtein algorithm |first=Sten |last=Hjelmqvist |date=26 Mar 2012 |url=http://www.codeproject.com/Articles/13525/Fast-memory-efficient-Levenshtein-algorithm}}</ref>\n<syntaxhighlight lang="CSharp">\nint LevenshteinDistance(string s, string t)\n{\n    // degenerate cases\n    if (s == t) return 0;\n    if (s.Length == 0) return t.Length;\n    if (t.Length == 0) return s.Length;\n\n    // create two work vectors of integer distances\n    int[] v0 = new int[t.Length + 1];\n    int[] v1 = new int[t.Length + 1];\n\n    // initialize v0 (the previous row of distances)\n    // this row is A[0][i]: edit distance for an empty s\n    // the distance is just the number of characters to delete from t\n    for (int i = 0; i < v0.Length; i++)\n        v0[i] = i;\n\n    for (int i = 0; i < s.Length; i++)\n    {\n        // calculate v1 (current row distances) from the previous row v0\n\n        // first element of v1 is A[i+1][0]\n        //   edit distance is delete (i+1) chars from s to match empty t\n        v1[0] = i + 1;\n\n        // use formula to fill in the rest of the row\n        for (int j = 0; j < t.Length; j++)\n        {\n            var cost = (s[i] == t[j]) ? 0 : 1;\n            v1[j + 1] = Minimum(v1[j] + 1, v0[j + 1] + 1, v0[j] + cost);\n        }\n\n        // copy v1 (current row) to v0 (previous row) for next iteration\n        for (int j = 0; j < v0.Length; j++)\n            v0[j] = v1[j];\n    }\n\n    return v1[t.Length];\n}\n</syntaxhighlight>\n\n==See also==\n{{colbegin||25em}}\n*[[agrep]]\n*[[Approximate string matching]]\n*[[Bitap algorithm]]\n*[[Damerau\xe2\x80\x93Levenshtein distance]]\n*[[diff]]\n*[[MinHash]]\n*[[Dynamic time warping]]\n*[[Euclidean distance]]\n*[[Fuzzy string searching]]\n*[[Hamming weight]]\n*[[Hirschberg\'s algorithm]]\n*[[Homology (biology)#Sequence homology|Homology of sequences in genetics]]\n*[[Hunt\xe2\x80\x93McIlroy algorithm]]\n*[[Jaccard index]]\n*[[Jaro\xe2\x80\x93Winkler distance]]\n*[[Levenshtein automaton]]\n*[[Locality-sensitive hashing]]\n*[[Longest common subsequence problem]]\n*[[Lucene]] (an open source search engine that implements edit distance)\n*[[Manhattan distance]]\n*[[Metric space]]\n*[[Most frequent k characters]]\n*[[Needleman\xe2\x80\x93Wunsch algorithm]]\n*[[Optimal matching]] algorithm\n*[[Sequence alignment]]\n*[[Similarity space]] on [[Numerical taxonomy]]\n*[[Smith\xe2\x80\x93Waterman algorithm]]\n*[[S\xc3\xb8rensen similarity index]]\n*[[String distance metric]]\n*[[Wagner-Fischer algorithm]]\n{{colend}}\n\n==References==\n{{reflist|30em}}\n\n==External links==\n{{Wikibooks| R_Programming|Text_Processing#Edit_distance|Levenshtein distance in R}}\n{{Wikibooks| Algorithm implementation|Strings/Levenshtein distance|Levenshtein distance}}\n*[http://www.postgresql.org/docs/current/static/fuzzystrmatch.html Levenshtein in PostgreSQL]\n*{{citation |contribution=Levenshtein distance |title=Dictionary of Algorithms and Data Structures [online] |editor-first=Paul E. |editor-last=Black |publisher=U.S. National Institute of Standards and Technology |date=14 August 2008 |accessdate=3 April 2013 |url=http://www.nist.gov/dads/HTML/Levenshtein.html }}\n\n{{DEFAULTSORT:Levenshtein Distance}}\n[[Category:String similarity measures]]\n[[Category:Dynamic programming]]\n[[Category:Articles with example pseudocode]]\n[[Category:Quantitative linguistics]]'
p226
sg6
S'Levenshtein distance'
p227
ssI85
(dp228
g2
S'http://en.wikipedia.org/wiki/Database search engine'
p229
sg4
S"There are several categories of search engine software:  Web search or full-text search (example: [[Lucene]]), database or structured data search (example: [[Dieselpoint]]), and mixed or [[enterprise search]] (example: [[Google Search Appliance]]).  The largest web search engines such as [[Google]] and [[Yahoo!]] utilize tens or hundreds of thousands of computers to process billions of web pages and return results for thousands of searches per second. High volume of queries and text processing requires the software to run in highly distributed environment with high degree of redundancy. Modern search engines have the following main components:\n\nSearching for text-based content in [[databases]] or other [[structured data]] formats ([[XML]], [[Comma-separated values|CSV]], etc.) presents some special challenges and opportunities which a number of specialized search engines resolve.  Databases are slow when solving complex queries (with multiple logical or [[string matching]] arguments.  Databases allow logical queries which full-text search doesn't (use of multi-field boolean logic for instance).  There is no crawling necessary for a database since the data is already structured but it is often necessary to index the data in a more compact form designed to allow for faster search.\n\nDatabase search engines were initially (and still usually are) included with major database software products.  As such, they are usually called indexing engines.  However, these indexing engines are relatively limited in their ability to customize indexing formats (compounding, normalization, transformation, [[transliteration]], etc.)   Usually they do not provide sophisticated data matching technology ([[string matching]], [[boolean logic]], algorithmic methods, search scripting, etc.).\n\nIn more advanced Database search systems relational databases are indexed by compounding multiple tables into a single table containing only the fields that need to be queried (or displayed in search results).  The actual data matching engines can include any functions from basic string matching, normalization, transformation,  Database search technology is heavily used by government database services, e-commerce companies, web advertising platforms, telecommunications service providers, etc.\n\n==See also==\n\n*[[Search engine]]\n*[[Web crawler]]\n*[[Search engine indexing]]\n*[[Enterprise search]]\n\n==External links==\n* [http://www.searchtools.com/info/database-search.html Searching for Text Information in Databases]\n\n{{DEFAULTSORT:Search Engine Technology}}\n[[Category:Internet search engines]]\n[[Category:Information retrieval]]"
p230
sg6
S'Database search engine'
p231
ssI214
(dp232
g2
S'http://en.wikipedia.org/wiki/Most frequent k characters'
p233
sg4
S'{{multiple issues|\n{{Third-party|date=March 2014}}\n{{Notability|date=March 2014}}\n}}\n\nIn [[information theory]], \'\'\'MostFreqKDistance\'\'\' is a [[string metric]] technique for quickly estimating how [[Similarity measure|similar]] two [[Order theory|ordered sets]] or [[String (computer science)|strings]] are. The scheme was invented by {{harvs|first=Sadi Evren|last=SEKER|authorlink=Sadi Evren SEKER|year=2014|txt}},<ref name="mfkc"/> and initially used in [[text mining]] applications like [[author recognition]].<ref name="mfkc">{{citation\n | last1 = SEKER | first1 = Sadi E. | author1-link = Sadi Evren SEKER\n | last2 = Altun | first2 = Oguz\n | last3 = Ayan | first3 = Ugur\n | last4 = Mert | first4 = Cihan\n | contribution = A Novel String Distance Function based on Most Frequent K Characters\n | volume = 4\n | issue = 2\n | pages = 177\xe2\x80\x93183\n | publisher = [[International Association of Computer Science and Information Technology Press (IACSIT Press)]]\n | title = [[International Journal of Machine Learning and Computing (IJMLC)]]\n | contribution-url = http://arxiv.org/abs/1401.6596\n | year = 2014}}</ref>\nMethod is originally based on a hashing function MaxFreqKChars <ref name="hashfunc">{{citation\n | last1 = Seker | first1 = Sadi E. | author1-link = Sadi Evren SEKER\n | last2 = Mert | first2 = Cihan\n | contribution = A Novel Feature Hashing For Text Mining\n | url = http://journal.ibsu.edu.ge/index.php/jtst/article/view/428\n | pages = 37\xe2\x80\x9341\n | publisher = [[International Black Sea University]]\n | title = Journal of Technical Science and Technologies\n | ISSN = 2298-0032\n | volume = 2\n | issue = 1\n | year = 2013}}</ref> classical [[author recognition]] problem and idea first came out while studying on [[data stream mining]].<ref name="author">{{citation\n | last1 = Seker | first1 = Sadi E. | author1-link = Sadi Evren SEKER\n | last2 = Al-Naami | first2 = Khaled\n | last3 = Khan | first3 = Latifur\n | contribution = Author attribution on streaming data\n | doi = 10.1109/IRI.2013.6642511\n | url = http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6642511\n | pages = 497\xe2\x80\x93503\n | publisher = [[IEEE]]\n | title = Information Reuse and Integration (IRI), 2013 IEEE 14th International Conference on, San Francisco, USA, Aug 14-16, 2013\n | year = 2013}}</ref> Algorithm is suitable for coding in most of the programming languages like [[Java (programming language)|Java]], [[Tcl]], [[Python (programming language)|Python]] or [[J (programming language)|J]]. <ref>\n{{Citation |title=Rosetta Code: Most frequent k chars distance , code sources for Python, Java, Tcl and J |accessdate=16 Oct 2014 |url=http://rosettacode.org/wiki/Most_frequent_k_chars_distance}}\n</ref>\n\n\n==Definition==\nMethod has two steps.\n* [[Hash function|Hash]] input strings str1 and str2 separately using MostFreqKHashing and output hstr1 and hstr2 respectively\n* Calculate string distance (or string similarity coefficient) of two hash outputs, hstr1 and hstr2 and output an integer value\n\n===Most frequent K hashing===\nThe first step of algorithm is calculating the hashing based on the most frequent k characters. The hashing algorithm has below steps:\n<syntaxhighlight lang="Java">\nString function MostFreqKHashing (String inputString, int K)\n    def string outputString\n    for each distinct character\n        count occurrence of each character\n    for i := 0 to K\n        char c = next most freq ith character  (if two chars have same frequency than get the first occurrence in inputString)\n        int count = number of occurrence of the character\n        append to outputString, c and count\n    end for\n    return outputString\n</syntaxhighlight>\n\nAbove function, simply gets an input string and an integer K value and outputs the most frequent K characters from the input string. The only condition during the creation of output string is adding the first occurring character first, if the frequencies of two characters are equal. Similar to the most of [[hashing function]]s, \'\'Most Frequent K Hashing\'\' is also a [[one way function]].\n\n===Most frequent K distance===\nThe second step of algorithm works on two outputs from two different input strings and outputs the similarity coefficient (or distance metric).\n<syntaxhighlight lang="Java">\nint function MostFreqKSimilarity (String inputStr1, String inputStr2, int limit)\n    def int similarity\n    for each c = next character from inputStr1\n        lookup c in inputStr2\n        if c is null\n             continue\n             similarity += frequency of c in inputStr1\n    return limit-similarity\n</syntaxhighlight>\nAbove function, simply gets two input strings, previously outputted from the <code>MostFreqKHashing</code> function. From the most frequent k hashing function, the characters and their frequencies are returned. So, the similarity function calculates the similarity based on characters and their frequencies by checking if the same character appears on both strings. The limit is usually taken to be 10 and in the end the function returns the result of the subtraction of the sum of similarities from limit.\n\nIn some implementations, the distance metric is required instead of similarity coefficient. In order to convert the output of above similarity coefficient to distance metric, the output can be subtracted from any constant value (like the maximum possible output value). For the case, it is also possible to implement a [[wrapper function]] over above two functions.\n\n===String distance wrapper function===\nIn order to calculate the distance between two strings, below function can be implemented\n<syntaxhighlight lang="Java">\nint function MostFreqKSDF (String inputStr1, String inputStr2, int K, int maxDistance)\n    return maxDistance - MostFreqKSimilarity(MostFreqKHashing(inputStr1, K), MostFreqKHashing(inputStr2, K))\n</syntaxhighlight>\n\nAny call to above string distance function will supply two input strings and a maximum distance value. The function will calculate the similarity and subtract that value from the maximum possible distance. It can be considered as a simple [[additive inverse]] of similarity.\n\n==Examples==\nLet\'s consider maximum 2 frequent hashing over two strings \xe2\x80\x98research\xe2\x80\x99 and \xe2\x80\x98seeking\xe2\x80\x99.\nMostFreqKHashing(\'research\', 2) = r2e2\nbecause we have 2 \'r\' and 2 \'e\' characters with the highest frequency and we return in the order they appear in the string.\nMostFreqKHashing(\'seeking\', 2) = e2s1\nAgain we have character \'e\' with highest frequency and rest of the characters have same frequency of 1, so we return the first character of equal frequencies, which is \'s\'.\nFinally we make the comparison:\nMostFreqKSimilarity(\'r2e2\', \'e2s1\') = 2\nWe simply compared the outputs and only character occurring in both input is character \'e\' and the occurrence in both input is 2.\nInstead running the sample step by step as above, we can simply run by using the string distance wrapper function as below:\nMostFreqKSDF(\'research\', \'seeking\', 2) = 2\n\nBelow table holds some sample runs between example inputs for K=2:\n{|class="wikitable"\n|-\n! Inputs\n! Hash Outputs\n! SDF Output (max from 10)\n|-\n|\'night\'\n\'nacht\'\n|n1i1\nn1a1\n|9\n|-\n|\'my\'\n\'a\'\n|m1y1\na1NULL0\n|10\n|-\n|\xe2\x80\x98research\xe2\x80\x99\n\xe2\x80\x98research\xe2\x80\x99\t\n|r2e2\nr2e2\t\n|6\n|-\n|\xe2\x80\x98aaaaabbbb\xe2\x80\x99\n\xe2\x80\x98ababababa\xe2\x80\x99\t\n|a5b4\na5b4\t\n|1\n|-\n|\xe2\x80\x98significant\xe2\x80\x99\n\xe2\x80\x98capabilities\xe2\x80\x99\t\n|i3n2\ni3a2\t\n|7\n|}\n\nMethod is also suitable for bioinformatics to compare the genetic strings like in [[FASTA format]].\n\nStr1 = LCLYTHIGRNIYYGSYLYSETWNTGIMLLLITMATAFMGYVLPWGQMSFWGATVITNLFSAIPYIGTNLV\n\nStr2 = EWIWGGFSVDKATLNRFFAFHFILPFTMVALAGVHLTFLHETGSNNPLGLTSDSDKIPFHPYYTIKDFLG\n\nMostFreqKHashing(str1, 2) = L9T8\n\nMostFreqKHashing(str2, 2) = F9L8\n\nMostFreqKSDF(str1, str2, 2, 100) = 83\n\n==Algorithm complexity and comparison==\nThe motivation behind algorithm is calculating the similarity between two input strings. So, the hashing function should be able to reduce the size of input and at the same time keep the characteristics of the input. Other hashing algorithms like [[MD5]] or [[SHA-1]], the output is completely unrelated with the input and those hashing algorithms are not suitable for string similarity check.\n\nOn the other hand string similarity functions like [[Levenshtein distance]] have the algorithm complexity problem.\n\nAlso algorithms like [[Hamming distance]], [[Jaccard coefficient]] or [[Tanimoto coefficient]] have relatively low algorithm complexity but the success rate in [[text mining]] studies are also low.\n\n===Time complexity===\nThe calculation of time complexity of \'most frequent k char string similarity\' is quite simple. In order to get the maximum frequent K characters from a string, the first step is sorting the string in a lexiconical manner. After this sort, the input with highest occurrence can be achieved with a simple pass in linear time complexity. Since major classical sorting algorithms are working in O(nlogn) complexity like [[merge sort]] or [[quick sort]], we can sort the first string in O(nlogn) and second string on O(mlogm) times. The total complexity would be O(nlog n ) + O (m log m) which is O(n log n) as the upper bound [[worst case analysis]].\n\n===Comparison===\nBelow table compares the complexity of algorithms:\n{|class="wikitable"\n|-\n! Algorithm\n! Time Complexity\n|-\n| [[Levenshtein distance]]\n| O(nm) = O(n^2)\n|-\n| [[Jaccard index]]\n| O(n+m) = O(n)\n|-\n| MostFreqKSDF\n| O(nlogn+mlogm) = O(n log n)\n|}\n\nFor the above table, n is the length of first string and m is the length of second string.\n\n==Success on text mining==\nThe success of string similarity algorithms are compared on a study. The study is based on IMDB62 dataset which is holding 1000 comment entries in [[Internet Movie Database]] from each 62 people. The data set is challenged for three string similarity functions and the success rates are as below:\n\n{|class="wikitable"\n|-\n! Algorithm\n! Running Time\n! Error (RMSE)\n! Error (RAE)\n|-\n|[[Levenshtein distance]]\n|3647286.54 sec\n|29\n|0.47\n|-\n|[[Jaccard index]]\n|228647.22 sec\n|45\n|0.68\n|-\n|MostFreqKSDF\n|2712323.51 sec\n|32\n|0.49\n|}\n\nThe running times for [[author recognition]] are in seconds and the error rates are [[root mean square error]] (RMSE) and [[relative absolute error]] (RAE).\n\nAbove table shows, the \'most frequent k similarity\' is better than [[Levenshtein distance]] by time and [[Jaccard index]] by success rate.\n\nFor the time performance and the success rates, the bitwise similarity functions like [[Dice\'s coefficient|S\xc3\xb8rensen\xe2\x80\x93Dice index]], [[Tversky index]] or [[Hamming Distance]] are all in the same category with similar success rates and running times. There are obviously slight differences but the idea behind bitwise operation, looses the string operations like deletion or addition. For example a single bit addition to the front of one of the input strings would yield a catastrophic result on the similarity for bitwise operators while Levenshtein distance is successfully catching.\n\nUnfortunately, [[big data]] studies requires a faster algorithm with still acceptable success. Here the \'max frequent k characters\' is an easy and simple algorithm (as in [[Occams razor]]), which is straight forward to implement.\n\n==See also==\n[http://rosettacode.org/wiki/Most_frequent_k_chars_distance RosettaCode,Code reposistory of Most Frequent K Chars Distance Algorithm in Java, Python, TCL or J languages] (Retrieved Oct. 16 2014)\n<div class= style="-moz-column-count:2; column-count:2;">\n* [[agrep]]\n* [[Approximate string matching]]\n* [[Bitap algorithm]]\n* [[Damerau\xe2\x80\x93Levenshtein distance]]\n* [[diff]]\n* [[MinHash]]\n* [[Dynamic time warping]]\n* [[Euclidean distance]]\n* [[Fuzzy string searching]]\n* [[Hamming weight]]\n* [[Hirschberg\'s algorithm]]\n* [[Sequence homology|Homology of sequences in genetics]]\n* [[Hunt\xe2\x80\x93McIlroy algorithm]]\n* [[Jaccard index]]\n* [[Jaro\xe2\x80\x93Winkler distance]]\n* [[Levenshtein distance]]\n* [[Longest common subsequence problem]]\n* [[Lucene]] (an open source search engine that implements edit distance)\n* [[Manhattan distance]]\n* [[Metric space]]\n* [[Needleman\xe2\x80\x93Wunsch algorithm]]\n* [[Optimal matching]] algorithm\n* [[Sequence alignment]]\n* Similarity space on [[Numerical taxonomy]]\n* [[Smith\xe2\x80\x93Waterman algorithm]]\n* [[S\xc3\xb8rensen similarity index]]\n* [[String distance metric]]\n* [[String similarity function]]\n* [[Wagner-Fischer algorithm]]\n* [[Locality-sensitive hashing]]\n</div>\n\n==References==\n{{reflist}}\n\n[[Category:String similarity measures]]\n[[Category:Dynamic programming]]\n[[Category:Articles with example pseudocode]]\n[[Category:Quantitative linguistics]]\n[[Category:Hash functions]]\n[[Category:Hashing]]'
p234
sg6
S'Most frequent k characters'
p235
ssI88
(dp236
g2
S'http://en.wikipedia.org/wiki/Music information retrieval'
p237
sg4
S'{{multiple issues|\n{{technical|date=October 2012}}\n{{Expert-subject|date=July 2010}}\n{{Expert-subject|Science|date=July 2010}}\n}}\n\n\n\'\'\'Music information retrieval\'\'\' (\'\'\'MIR\'\'\') is the interdisciplinary science of retrieving [[information]] from [[music]]. MIR is a small but growing field of research with many real-world applications. Those involved in MIR may have a background in [[musicology]], [[psychology]], academic music study, [[signal processing]], [[machine learning]] or some combination of these.\n\n== Applications ==\nMIR is being used by businesses and academics to categorize, manipulate and even create music.\n\n=== Recommender systems ===\nSeveral [[recommender systems]] for music already exist, but surprisingly few are based upon MIR techniques, instead making use of similarity between users or laborious data compilation. [[Pandora]], for example, uses experts to tag the music with particular qualities such as "female singer" or "strong bassline". Many other systems find users whose listening history is similar and suggests unheard music to the users from their respective collections. MIR techniques for similarity in music are now beginning to form part of such systems.\n\n=== Track separation and instrument recognition ===\nTrack separation is about extracting the original tracks as recorded, which could have more than one instrument played per track. Instrument recognition is about identifying the instruments involved and/or separating the music into one track per instrument. Various programs have been developed that can separate music into its component tracks without access to the master copy. In this way e.g. karaoke tracks can be created from normal music tracks, though the process is not yet perfect owing to vocals occupying some of the same frequency space as the other instruments.\n\n===Automatic music transcription===\nAutomatic music transcription is the process of converting an audio recording into symbolic notation, such as a score or a [[MIDI_file#File_formats|MIDI file]].<ref>A. Klapuri and M. Davy, editors. Signal Processing Methods for Music Transcription. Springer-Verlag, New York, 2006.</ref> This process involves several subtasks, which include multi-pitch detection, [[Onset_detection#Onset_detection|onset detection]], duration estimation, instrument identification, and the extraction of rhythmic information. This task becomes more difficult with greater numbers of instruments and a greater [[Polyphony and monophony in instruments|polyphony level]].\n\n===Automatic categorization===\nMusical genre categorization is a common task for MIR and is the usual task for the yearly Music Information Retrieval Evaluation eXchange(MIREX).<ref>http://www.music-ir.org/mirex/wiki/MIREX_HOME - Music Information Retrieval Evaluation eXchange.</ref> Machine learning techniques such as [[Support Vector Machines]] tend to perform well, despite the somewhat subjective nature of the classification. Other potential classifications include identifying the artist, the place of origin or the mood of the piece. Where the output is expected to be a number rather than a class, [[regression analysis]] is required.\n\n===Music generation===\nThe automatic generation of music is a goal held by many MIR researchers. Attempts have been made with limited success in terms of human appreciation of the results.\n\n==Methods used==\n\n===Data source===\n[[Sheet music|Scores]] give a clear and logical description of music from which to work, but access to sheet music, whether digital or otherwise, is often impractical. [[MIDI]] music has also been used for similar reasons, but some data is lost in the conversion to MIDI from any other format, unless the music was written with the MIDI standards in mind, which is rare. Digital audio formats such as [[WAV]], [[mp3]], and [[ogg]] are used when the audio itself is part of the analysis. Lossy formats such as mp3 and ogg work well with the human ear but may be missing crucial data for study. Additionally some encodings create artifacts which could be misleading to any automatic analyser. Despite this the ubiquity of the mp3 has meant much research in the field involves these as the source material. Increasingly, metadata mined from the web is incorporated in MIR for a more rounded understanding of the music within its cultural context, and this recently includes analysis of [[social tagging|social tags]] for music.\n\n===Feature representation===\nAnalysis can often require some summarising,<ref>Eidenberger, Horst (2011). \xe2\x80\x9cFundamental Media Understanding\xe2\x80\x9d, atpress. ISBN 978-3-8423-7917-6.</ref> and for music (as with many other forms of data) this is achieved by feature extraction, especially when the audio content itself is analysed and machine learning is to be applied. The purpose is to reduce the sheer quantity of data down to a manageable set of values so that learning can be performed within a reasonable time-frame. One common feature extracted is the [[Mel-frequency cepstral coefficient|Mel-Frequency Cepstral Coefficient]] (MFCC) which is a measure of the [[timbre]] of a piece of music. Other features may be employed to represent the [[Tonality#Computational_methods_to_determine_the_key|key]], chords, harmonies, melody, main pitch, beats per minute or rhythm in the piece.\n\n===Statistics and machine learning===\n*Computational methods for classification, clustering, and modelling \xe2\x80\x94 musical feature extraction for mono- and [[polyphonic]] music, similarity and [[pattern matching]], retrieval\n* Formal methods and databases \xe2\x80\x94 applications of automated [[music identification]] and recognition, such as [[score following]], automatic accompaniment, routing and filtering for music and music queries, query languages, standards and other metadata or protocols for music information handling and [[information retrieval|retrieval]], [[multi-agent system]]s, distributed search)\n*Software for music information retrieval \xe2\x80\x94 [[Semantic Web]] and musical digital objects, intelligent agents, collaborative software, web-based search and [[semantic retrieval]], [[query by humming]], [[acoustic fingerprinting]]\n* Music analysis and knowledge representation \xe2\x80\x94 automatic summarization, citing, excerpting, downgrading, transformation, formal models of music, digital scores and representations, music indexing and [[metadata]].\n\n==Other issues==\n*Human-computer interaction and interfaces \xe2\x80\x94 multi-modal interfaces, [[user interface]]s and [[usability]], mobile applications, user behavior\n* Music perception, cognition, affect, and emotions \xe2\x80\x94 music [[similarity metrics]], syntactical parameters, semantic parameters, musical forms, structures, styles ands, music annotation methodologies\n* Music archives, libraries, and digital collections \xe2\x80\x94 music [[digital library|digital libraries]], public access to musical archives, benchmarks and research databases\n* [[Intellectual property]] rights and music \xe2\x80\x94 national and international [[copyright]] issues, [[digital rights management]], identification and traceability\n* Sociology and Economy of music \xe2\x80\x94 music industry and use of MIR in the production, distribution, consumption chain, user profiling, validation, user needs and expectations, evaluation of music IR systems, building test collections, experimental design and metrics\n\n== See also ==\n* [[Audio mining]]\n* [[Artificial intelligence]]\n* [[Digital rights management]]\n* [[Digital signal processing]]\n* [[Ethnomusicology]]\n* [[Multimedia Information Retrieval]]\n* [[Music notation]]\n* [[Musicology]]\n* [[Parsons code]]\n* [[Sound and music computing]]\n* [[Music OCR]]\n\n== References ==\n{{Reflist}}\n* Michael Fingerhut (2004). [http://mediatheque.ircam.fr/articles/textes/Fingerhut04b "Music Information Retrieval, or how to search for (and maybe find) music and do away with incipits"], \'\'IAML-IASA Congress\'\', Oslo (Norway), August 8\xe2\x80\x9313, 2004.\n\n==External links==\n* [http://www.ismir.net/ International Society for Music Information Retrieval]\n* [http://music-ir.org/ Music Information Retrieval research]\n* [http://www.music-ir.org/jdownie_papers/downie_mir_arist37.pdf J. Stephen Downie: Music information retrieval]\n* [http://dx.doi.org/10.1561/1500000042 M. Schedl, E. G\xc3\xb3mez and J. Urbano: Music Information Retrieval: Recent Developments and Applications]\n* [http://www.nowpublishers.com/product.aspx?product=INR&doi=1500000002 Nicola Orio: Music Retrieval: A Tutorial and Review]\n* [https://ccrma.stanford.edu/wiki/MIR_workshop_2011 Intelligent Audio Systems: Foundations and Applications of Music Information Retrieval, introductory course at Stanford University\'s Center for Computer Research in Music and Acoustics]\n* [http://biblio.ugent.be/record/470088 Micheline Lesaffre: Music Information Retrieval: Conceptual Framework, Annotation and User behavior.]\n* [http://the.echonest.com/ The Echo Nest: a company specialising in MIR research and applications.]\n* [http://www.imagine-research.com/ Imagine Research : develops platform and software for MIR applications ]\n* [http://www.AudioContentAnalysis.org/ AudioContentAnalysis.org: MIR resources and matlab code ]\n\n==Example MIR applications==\n* [http://www.musipedia.org/ Musipedia \xe2\x80\x94 A melody search engine that offers several modes of searching, including whistling, tapping, piano keyboard, and Parsons code.]\n* [http://www.listengame.org/ The Listen Game \xe2\x80\x94 UCSD Computer Audition Lab MIR music ranking game]\n* [http://www.peachnote.com/ Peachnote \xe2\x80\x94 A melody search engine and n-gram viewer that searches through digitized music scores]\n\n[[Category:Information retrieval]]\n[[Category:Music software]]'
p238
sg6
S'Music information retrieval'
p239
ssI217
(dp240
g2
S'http://en.wikipedia.org/wiki/Category:Legal citators'
p241
sg4
S'{{Cat main|Citator}}\n\n[[Category:Citation indices]]\n[[Category:Legal research]]\n[[Category:Legal citation]]'
p242
sg6
S'Category:Legal citators'
p243
ssI91
(dp244
g2
S'http://en.wikipedia.org/wiki/Fuzzy retrieval'
p245
sg4
S'\'\'\'Fuzzy retrieval\'\'\' techniques are based on the [[Extended Boolean model]] and the [[Fuzzy set]] theory. There are two classical fuzzy retrieval models: Mixed Min and Max (MMM) and the Paice model. Both models do not provide a way of evaluating query weights, however this is considered by the [[Extended Boolean model|P-norms]] algorithm.\n\n==Mixed Min and Max model (MMM)==\n\nIn fuzzy-set theory, an element has a varying degree of membership, say \'\'d<sub>A</sub>\'\', to a given set \'\'A\'\' instead of the traditional membership choice (is an element/is not an element).<br />\nIn MMM<ref>{{citation | last=Fox | first=E. A. | coauthors=S. Sharat | year=1986 | title=A Comparison of Two Methods for Soft Boolean Interpretation in Information Retrieval | publisher=Technical Report TR-86-1, Virginia Tech, Department of Computer Science}}</ref> each index term has a fuzzy set associated with it. A document\'s weight with respect to an index term \'\'A\'\' is considered to be the degree of membership of the document in the fuzzy set associated with \'\'A\'\'. The degree of membership for union and intersection are defined as follows in Fuzzy set theory:<br/>\n:<math>d_{A\\cap B}= min(d_A, d_B)</math>\n:<math>d_{A\\cup B}= max(d_A,d_B)</math>\n\nAccording to this, documents that should be retrieved for a query of the form \'\'A or B\'\', should be in the fuzzy set associated with the union of the two sets \'\'A\'\' and \'\'B\'\'. Similarly, the documents that should be retrieved for a query of the form \'\'A and B\'\', should be in the fuzzy set associated with the intersection of the two sets. Hence, it is possible to define the similarity of a document to the \'\'or\'\' query to be \'\'max(d<sub>A</sub>, d<sub>B</sub>)\'\' and the similarity of the document to the \'\'and\'\' query to be \'\'min(d<sub>A</sub>, d<sub>B</sub>)\'\'. The MMM model tries to soften the Boolean operators by considering the query-document similarity to be a linear combination of the \'\'min\'\' and \'\'max\'\' document weights.\n\nGiven a document \'\'D\'\' with index-term weights \'\'d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>\'\' for terms \'\'A<sub>1</sub>, A<sub>2</sub>, ..., A<sub>n</sub>\'\', and the queries:\n\n\'\'Q<sub>or</sub> = (A<sub>1</sub> or A<sub>2</sub> or ... or A<sub>n</sub>)\'\'<br />\n\'\'Q<sub>and</sub> = (A<sub>1</sub> and A<sub>2</sub> and ... and A<sub>n</sub>)\'\'\n\nthe query-document similarity in the MMM model is computed as follows:\n\n\'\'SlM(Q<sub>or</sub>, D) = C<sub>or1</sub> * max(d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>) + C<sub>or2</sub> * min(d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>)\'\'<br />\n\'\'SlM(Q<sub>and</sub>, D) = C<sub>and1</sub> * min(d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>) + C<sub>and2</sub> * max(d<sub>A1</sub>, d<sub>A2</sub> ..., d<sub>An</sub>)\'\'\n\nwhere \'\'C<sub>or1</sub>, C<sub>or2</sub>\'\' are "softness" coefficients for the \'\'or\'\' operator, and \'\'C<sub>and1</sub>, C<sub>and2</sub>\'\' are softness coefficients for the \'\'and\'\' operator. Since we would like to give the maximum of the document weights more importance while considering an \'\'or\'\' query and the minimum more importance while considering an \'\'and\'\' query, generally we have \'\'C<sub>or1</sub> > C<sub>or2</sub> and C<sub>and1</sub> > C<sub>and2</sub>\'\'. For simplicity it is generally assumed that \'\'C<sub>or1</sub> = 1 - C<sub>or2</sub>\'\' and \'\'C<sub>and1</sub> = 1 - C<sub>and2</sub>\'\'.\n\nLee and Fox<ref name="leefox">{{citation | last=Lee | first=W. C. | coauthors=E. A. Fox | year=1988 | title=Experimental Comparison of Schemes for Interpreting Boolean Queries}}</ref> experiments indicate that the best performance usually occurs with \'\'C<sub>and1</sub>\'\' in the range [0.5, 0.8] and with \'\'C<sub>or1</sub>\'\' > 0.2. In general, the computational cost of MMM is low, and retrieval effectiveness is much better than with the [[Standard Boolean model]].\n\n==Paice model==\n\nThe Paice model<ref>{{citation | last=Paice | first=C. P. | year=1984 | title=Soft Evaluation of Boolean Search Queries in Information Retrieval Systems | publisher=Information Technology, Res. Dev. Applications, 3(1), 33-42 }}</ref> is a general extension to the MMM model. In comparison to the MMM model that considers only the minimum and maximum weights for the index terms, the Paice model incorporates all of the term weights when calculating the similarity:\n\n:<math>S(D,Q) = \\sum_{i=1}^n\\frac{r^{i-1}*w_{di}}{\\sum_{j=1}^n r^{j-1}}</math>\n\nwhere \'\'r\'\' is a constant coefficient and \'\'w<sub>di</sub>\'\' is arranged in ascending order for \'\'and\'\' queries and descending order for \'\'or\'\' queries. When n = 2 the Paice model shows the same behavior as the MMM model.\n\nThe experiments of Lee and Fox<ref name="leefox"/> have shown that setting the \'\'r\'\' to 1.0 for \'\'and\'\' queries and 0.7 for \'\'or\'\' queries gives good retrieval effectiveness. The computational cost for this model is higher than that for the MMM model. This is because the MMM model only requires the determination of \'\'min\'\' or \'\'max\'\' of a set of term weights each time an \'\'and\'\' or \'\'or\'\' clause is considered, which can be done in \'\'O(n)\'\'. The Paice model requires the term weights to be sorted in ascending or descending order, depending on whether an \'\'and\'\' clause or an \'\'or\'\' clause is being considered. This requires at least an \'\'0(n log n)\'\' sorting algorithm. A good deal of floating point calculation is needed too.\n\n==Improvements over the Standard Boolean model==\nLee and Fox<ref name="leefox"/> compared the Standard Boolean model with MMM and Paice models with three test collections, CISI, CACM and INSPEC. These are the reported results for average mean precision improvement:\n{| class="wikitable"\n|-\n!\n! CISI\n! CACM\n! INSPEC\n|-\n! MMM\n| 68%\n| 109%\n| 195%\n|-\n! Paice\n| 77%\n| 104%\n| 206%\n|}\n\nThese are very good improvements over the Standard model. MMM is very close to Paice and P-norm results which indicates that it can be a very good technique, and is the most efficient of the three.\n\n==Recent work==\n\nRecently \'\'\'Kang \'\'et al.\'\'\'.<ref>{{citation | title=Fuzzy Information Retrieval Indexed by Concept Identification | url=http://www.springerlink.com/content/ac96v4qf4f8adatp/ | last=Kang | first=Bo-Yeong | coauthors=Dae-Won Kim, Hae-Jung Kim | publisher=Springer Berlin / Heidelberg | year=2005}}</ref> have devised a fuzzy retrieval system indexed by concept identification.\n\nIf we look at documents on a pure [[Tf-idf]] approach, even eliminating stop words, there will be words more relevant to the topic of the document than others and they will have the same weight because they have the same term frequency. If we take into account the user intent on a query we can better weight the terms of a document. Each term can be identified as a concept in a certain lexical chain that translates the importance of that concept for that document.<br />\nThey report improvements over Paice and P-norm on the average precision and recall for the Top-5 retrieved documents.\n\nZadrozny<ref>{{citation | title=Fuzzy information retrieval model revisited | doi=10.1016/j.fss.2009.02.012 | first=S\xc5\x82awomir | last=Zadrozny | coauthors=Nowacka, Katarzyna | year=2009 | publisher=Elsevier North-Holland, Inc.}}</ref> revisited the fuzzy information retrieval model. He further extends the fuzzy extended Boolean model by:\n* assuming linguistic terms as importance weights of keywords also in documents\n* taking into account the uncertainty concerning the representation of documents and queries\n* interpreting the linguistic terms in the representation of documents and queries as well as their matching in terms of the Zadeh\xe2\x80\x99s fuzzy logic (calculus of linguistic statements)\n* addressing some pragmatic aspects of the proposed model, notably the techniques of indexing documents and queries\n\nThe proposed model makes it possible to grasp both imprecision and uncertainty concerning the textual information representation and retrieval.\n\n==See also==\n*[[Information retrieval]]\n\n==Further reading==\n* {{citation | title=Information Retrieval: Algorithms and Data structures; Extended Boolean model | last=Fox | first=E. | coauthors=S. Betrabet , M. Koushik , W. Lee | year=1992 | publisher=Prentice-Hall, Inc. | url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes}}\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Fuzzy Retrieval}}\n[[Category:Information retrieval]]'
p246
sg6
S'Fuzzy retrieval'
p247
ssI220
(dp248
g2
S'http://en.wikipedia.org/wiki/Scopus'
p249
sg4
S'{{Use dmy dates|date=August 2013}}\n{{other uses}}\n{{infobox bibliographic database\n| title = Scopus\n| image = [[File:Scopus_type_logo.jpg]]\n| caption = Scopus logo\n| producer = [[Elsevier]]\n| country = \n| history = \n| languages = English\n| providers = \n| cost = Subscription\n| disciplines= \n| depth = \n| formats = \n| temporal = 1995-present\n| geospatial = Worldwide\n| number = 55 million\n| updates = \n| p_title = \n| p_dates = \n| ISSN = \n| web = http://www.scopus.com\n| titles = \n}}\n\'\'\'Scopus\'\'\' is a [[bibliographic database]] containing [[Abstract (summary)|abstracts]] and [[citation]]s for [[academic journal]] [[Article (publishing)|articles]]. It covers nearly 22,000 titles from over 5,000 publishers, of which 20,000 are [[peer review|peer-reviewed]] journals in the scientific, technical, medical, and social sciences (including arts and humanities).<ref>{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview |title=Scopus Content Overview |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}</ref> It is owned by [[Elsevier]] and is available online by [[subscription business model|subscription]]. Searches in Scopus also incorporate searches of patent databases.<ref>{{cite journal |doi=10.1001/jama.2009.1307 |title=Comparisons of Citations in Web of Science, Scopus, and Google Scholar for Articles Published in General Medical Journals |year=2009 |last1=Kulkarni |first1=A. V. |last2=Aziz |first2=B. |last3=Shams |first3=I. |last4=Busse |first4=J. W. |journal=[[JAMA (journal)|JAMA]] |volume=302 |issue=10 |pages=1092\xe2\x80\x936 |pmid=19738094}}</ref>\n\nSince Elsevier is the owner of Scopus and is also one of the main international publishers of scientific journals, an independent and international Scopus Content Selection and Advisory Board was established to prevent a potential conflict of interest in the choice of journals to be included in the database and to maintain an open and transparent content coverage policy, regardless of publisher.<ref>{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview#content-policy-and-selection |title=Scopus Content Overview: Content Policy and Selection |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}</ref> The board consists of scientists and subject librarians.\n\nA 2008 study  compared [[PubMed]], Scopus, [[Web of Science]], and [[Google Scholar]] and concluded: <blockquote>"PubMed and Google Scholar are accessed for free [...] Scopus offers about 20% more coverage than Web of Science, whereas Google Scholar offers results of inconsistent accuracy. PubMed remains an optimal tool in biomedical electronic research. Scopus covers a wider journal range [...] but it is currently limited to recent articles (published after 1995) compared with Web of Science. Google Scholar, as for the Web in general, can help in the retrieval of even the most obscure information but its use is marred by inadequate, less often updated, citation information."<ref>{{Cite journal |pmid=17884971 |year=2008 |last1=Falagas |first1=ME |last2=Pitsouni |first2=EI |last3=Malietzis |first3=GA |last4=Pappas |first4=G |title=Comparison of PubMed, Scopus, Web of Science, and Google Scholar: Strengths and weaknesses |volume=22 |issue=2 |pages=338\xe2\x80\x9342 |doi=10.1096/fj.07-9492LSF |journal=[[FASEB Journal]]}}</ref></blockquote>\n\nEvaluating ease of use and coverage of Scopus and the Web of Science (WOS), a 2006 study concluded that "Scopus is easy to navigate, even for the novice user. [...] The ability to search both forward and backward from a particular citation would be very helpful to the researcher. The multidisciplinary aspect allows the researcher to easily search outside of his discipline" and "One advantage of WOS over Scopus is the depth of coverage, with the full WOS database going back to 1945 and Scopus going back to 1966. However, Scopus and WOS complement each other as neither resource is all inclusive. [...]".<ref>{{Cite journal |pmid=16522216 |year=2006 |last1=Burnham |first1=JF |title=Scopus database: A review |volume=3 |pages=1 |doi=10.1186/1742-5581-3-1 |pmc=1420322 |journal=Biomedical Digital Libraries}}</ref>\n\nScopus also offers author profiles which cover affiliations, number of publications and their [[bibliographic]] data, [[references]], and details on the number of citations each published document has received. It has [[alerts|alerting]] features that allows registered users to track changes to a profile and a facility to calculate authors\' [[h-index]].\n\nScopus can be integrated with [[ORCID]].<ref name="Scopus">{{cite web |url=http://orcid.scopusfeedback.com |title=Scopus2Orcid |publisher=Scopus |accessdate=7 May 2014}}</ref>\n\n== References ==\n{{reflist|30em}}\n\n== External links ==\n* {{Official website|http://www.scopus.com/}}\n* [http://www.elsevier.com/online-tools/scopus Scopus information]\n\n\n{{Reed Elsevier}}\n\n[[Category:Bibliographic databases]]\n[[Category:Elsevier]]\n[[Category:Citation indices]]\n[[Category:Library cataloging and classification]]\n[[Category:Scholarly search services]]'
p250
sg6
S'Scopus'
p251
ssI94
(dp252
g2
S'http://en.wikipedia.org/wiki/Collaborative filtering'
p253
sg4
S'{{external links|date=November 2013}}\n{{Use dmy dates|date=June 2013}}\n{{Recommender systems}}\n[[File:Collaborative filtering.gif|600px|thumb|\n\nThis image shows an example of predicting of the user\'s rating using collaborative filtering. At first, people rate different items (like videos, images, games). After that, the system is making predictions about user\'s rating for an item, which the user hasn\'t rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user. For instance, in our case the system has made a prediction, that the active user won\'t like the video.\n\n]]\n\n\'\'\'Collaborative filtering\'\'\' (\'\'\'CF\'\'\') is a technique used by some [[recommender system]]s.<ref name="handbook">Francesco Ricci and Lior Rokach and Bracha Shapira, [http://www.inf.unibz.it/~ricci/papers/intro-rec-sys-handbook.pdf Introduction to Recommender Systems Handbook], Recommender Systems Handbook, Springer, 2011, pp. 1-35</ref> Collaborative filtering has two senses, a narrow one and a more general one.<ref name=recommender>{{cite web|title=Beyond Recommender Systems: Helping People Help Each Other|url=http://www.grouplens.org/papers/pdf/rec-sys-overview.pdf|publisher=Addison-Wesley|accessdate=16 January 2012|page=6|year=2001|last1=Terveen|first1=Loren|last2=Hill|first2=Will}}</ref>  In general, collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc.<ref name="recommender" />  Applications of collaborative filtering typically involve very large data sets.   Collaborative filtering methods have been applied to many different kinds of data including: sensing and monitoring data, such as in mineral exploration, environmental sensing over large areas or multiple sensors; financial data, such as financial service institutions that integrate many financial sources; or in electronic commerce and web applications  where the focus is on user data, etc.  The remainder of this discussion focuses on collaborative filtering for user data, although some of the methods and approaches may apply to the other major applications as well.\n\nIn the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or [[taste (sociology)|taste]] information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person \'\'A\'\' has the same opinion as a person \'\'B\'\' on an issue, A is more likely to have B\'s opinion on a different issue \'\'x\'\' than to have the opinion on x of a person chosen randomly. For example, a collaborative filtering recommendation system for [[television]] tastes could make predictions about which television show a user should like given a partial list of that user\'s tastes (likes or dislikes).<ref>[http://www.redbeemedia.com/insights/integrated-approach-tv-vod-recommendations An integrated approach to TV & VOD Recommendations]</ref> Note that these predictions are specific to the user, but use information gleaned from many users. This differs from the simpler approach of giving an [[average]] (non-specific) score for each item of interest, for example based on its number of [[vote]]s.\n\n==Introduction==\nThe growth of the Internet has made it much more difficult to effectively extract useful information from all the available online information. The overwhelming amount of data necessitates  mechanisms for efficient information filtering. One of the techniques used for dealing with this problem is called collaborative filtering.\n\nThe motivation for collaborative filtering comes from the idea that people often get the best recommendations from someone with similar tastes to themselves. Collaborative filtering explores techniques for matching people with similar interests and making recommendations on this basis.\n\nCollaborative filtering algorithms often require (1) users\xe2\x80\x99 active participation, (2) an easy way  to represent users\xe2\x80\x99 interests to the system, and (3) algorithms that are able to match people with similar interests.\n\nTypically, the workflow of a collaborative filtering system is:\n# A user expresses his or her preferences by rating items (e.g. books, movies or CDs) of the system. These ratings can be viewed as an approximate representation of the user\'s interest in the corresponding domain.\n# The system matches this user\xe2\x80\x99s ratings against other users\xe2\x80\x99  and finds the people with most \xe2\x80\x9csimilar\xe2\x80\x9d tastes.\n# With similar users, the system recommends items that the similar users have rated highly but not yet being rated by this user (presumably the absence of rating is often considered as the unfamiliarity of an item)\nA key problem of collaborative filtering is how to combine and weight the preferences of user neighbors. Sometimes, users can immediately rate the recommended items. As a result, the system gains an increasingly accurate representation of user preferences over time.\n\n==Methodology==\n\n[[File:Collaborative Filtering in Recommender Systems.jpg|thumb|Collaborative Filtering in Recommender Systems]]\n\nCollaborative filtering systems have many forms, but many common systems can be reduced to two steps:\n# Look for users who share the same rating patterns with the active user (the user whom the prediction is for).\n# Use the ratings from those like-minded users found in step 1 to calculate a prediction for the active user\nThis falls under the category of user-based collaborative filtering. A specific application of this is the user-based [[K-nearest neighbor algorithm|Nearest Neighbor algorithm]].\n\nAlternatively, [[item-item collaborative filtering|item-based collaborative filtering]] (users who bought x also bought y), proceeds in an item-centric manner:\n# Build an item-item matrix determining relationships between pairs of items\n# Infer the tastes of the current user by examining the matrix and matching that user\'s data\nSee, for example, the [[Slope One]] item-based collaborative filtering family.\n\nAnother form of collaborative filtering can be based on implicit observations of normal user behavior (as opposed to the artificial behavior imposed by a rating task). These systems observe what a user has done together with what all users have done (what music they have listened to, what items they have bought) and use that data to predict the user\'s behavior in the future, or to predict how a user might like to behave given the chance.  These predictions then have to be filtered through [[business logic]] to determine how they might affect the actions of a business system.  For example, it is not useful to offer to sell somebody a particular album of music if they already have demonstrated that they own that music.\n\nRelying on a scoring or rating system which is averaged across all users ignores specific demands of a user, and is particularly poor in tasks where there is large variation in interest (as in the recommendation of music). However, there are other methods to combat information explosion, such as [[WWW|web]] search and [[data clustering]].\n\n==Types==\n\n===Memory-based===\nThis mechanism uses user rating data to compute similarity between users or items. This is used for making recommendations. This was the earlier mechanism and is used in many commercial systems. It is easy to implement and is effective. Typical examples of this mechanism are neighbourhood based CF and item-based/user-based top-N recommendations.[3] For example, in user based approaches, the value of ratings user \'u\' gives to item \'i\' is calculated as an aggregation of some similar users rating to the item:\n:<math>r_{u,i} = \\operatorname{aggr}_{u^\\prime \\in U} r_{u^\\prime, i}</math>\n\nwhere \'U\' denotes the set of top \'N\' users that are most similar to user \'u\' who rated item \'i\'. Some examples of the aggregation function includes:\n:<math>r_{u,i} = \\frac{1}{N}\\sum\\limits_{u^\\prime \\in U}r_{u^\\prime, i}</math>\n:<math>r_{u,i} = k\\sum\\limits_{u^\\prime \\in U}\\operatorname{simil}(u,u^\\prime)r_{u^\\prime, i}</math>\n:<math>r_{u,i} = \\bar{r_u} +  k\\sum\\limits_{u^\\prime \\in U}\\operatorname{simil}(u,u^\\prime)(r_{u^\\prime, i}-\\bar{r_{u^\\prime}} )</math>\n\nwhere k is a normalizing factor defined as <math>k =1/\\sum_{u^\\prime \\in U}|\\operatorname{simil}(u,u^\\prime)| </math>. and <math>\\bar{r_u}</math> is the average rating of user u for all the items rated by that user.\n\nThe neighborhood-based algorithm calculates the similarity between two users or items, produces a prediction for the user taking the weighted average of all the ratings. Similarity computation between items or users is an important part of this approach. Multiple mechanisms such as [[Pearson product-moment correlation coefficient|Pearson correlation]] and [[Cosine similarity|vector cosine]] based similarity are used for this.\n\nThe Pearson correlation similarity of two users x, y is defined as \n:<math> \\operatorname{simil}(x,y) = \\frac{\\sum\\limits_{i \\in I_{xy}}(r_{x,i}-\\bar{r_x})(r_{y,i}-\\bar{r_y})}{\\sqrt{\\sum\\limits_{i \\in I_{xy}}(r_{x,i}-\\bar{r_x})^2\\sum\\limits_{i \\in I_{xy}}(r_{y,i}-\\bar{r_y})^2}} </math>\n\nwhere I<sub>xy</sub> is the set of items rated by both user x and user y.\n\nThe cosine-based approach defines the cosine-similarity between two users x and y as:<ref name="Breese1999">John S. Breese, David Heckerman, and Carl Kadie, [http://uai.sis.pitt.edu/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=231&proceeding_id=14 Empirical Analysis of Predictive Algorithms for Collaborative Filtering], 1998</ref>\n:<math>\\operatorname{simil}(x,y) = \\cos(\\vec x,\\vec y) = \\frac{\\vec x \\cdot \\vec y}{||\\vec x|| \\times ||\\vec y||} = \\frac{\\sum\\limits_{i \\in I_{xy}}r_{x,i}r_{y,i}}{\\sqrt{\\sum\\limits_{i \\in I_{x}}r_{x,i}^2}\\sqrt{\\sum\\limits_{i \\in I_{y}}r_{y,i}^2}}</math>\n\nThe user based top-N recommendation algorithm identifies the k most similar users to an active user using similarity based vector model. After the k most similar users are found, their corresponding user-item matrices are aggregated to identify the set of items to be recommended. A popular method to find the similar users is the [[Locality-sensitive hashing]], which implements the [[Nearest neighbor search|nearest neighbor mechanism]] in linear time.\n\nThe advantages with this approach include:  the explainability of the results, which is an important aspect of recommendation systems; it is easy to create and use; new data can be added easily and incrementally; it need not consider the content of the items being recommended; and the mechanism scales well with co-rated items.\n\nThere are several disadvantages with this approach.  Its performance decreases when data gets sparse, which is frequent with web related items. This prevents the scalability of this approach and has problems with large datasets. Although it can efficiently handle new users because it relies on a data structure, adding new items becomes more complicated since that representation usually relies on a specific vector space. That would require to include the new item and re-insert all the elements in the structure.\n\n===Model-based===\nModels are developed using [[data mining]], [[machine learning]] algorithms to find patterns based on training data. These are used to make predictions for real data. There are many model-based CF algorithms. These include [[Bayesian networks]], [[Cluster Analysis|clustering models]], [[Latent Semantic Indexing|latent semantic models]] such as [[singular value decomposition]], [[probabilistic latent semantic analysis]], Multiple Multiplicative Factor, [[Latent Dirichlet allocation]] and [[markov decision process]] based models.<ref name="Suetal2009">Xiaoyuan Su, Taghi M. Khoshgoftaar, [http://www.hindawi.com/journals/aai/2009/421425/ A survey of collaborative filtering techniques], Advances in Artificial Intelligence archive, 2009.</ref>\n\nThis approach has a more holistic goal to uncover latent factors that explain observed ratings.<ref>[http://research.yahoo.com/pub/2435 Factor in the Neighbors: Scalable and Accurate Collaborative Filtering]</ref> Most of the models are based on creating a classification or clustering technique to identify the user based on the test set. The number of the parameters can be reduced based on types of [[Principal Component Analysis|principal component analysis]].\n\nThere are several advantages with this paradigm. It handles the sparsity better than memory based ones. This helps with scalability with large data sets. It improves the prediction performance. It gives an intuitive rationale for the recommendations.\n\nThe disadvantages with this approach are in the expensive model building. One needs to have a tradeoff between prediction performance and scalability. One can lose useful information due to reduction models. A number of models have difficulty explaining the predictions.\n\n===Hybrid===\nA number of applications combines the memory-based and the model-based CF algorithms. These overcome the limitations of native CF approaches. It improves the prediction performance. Importantly, it overcomes the CF problems such as sparsity and loss of information. However, they have increased complexity and are expensive to implement.<ref>Kernel Mapping Recommender System Algorithms, www.sciencedirect.com/science/article/pii/S0020025512002587\n</ref> Usually most of the commercial recommender systems are hybrid, for example, Google news recommender system.<ref>[http://dl.acm.org/citation.cfm?id=1242610 Google News Personalization: Scalable Online Collaborative Filtering]</ref>\n\n==Application on social web==\nUnlike the traditional model of mainstream media, in which there are few editors who set guidelines, collaboratively filtered social media can have a very large number of editors, and content improves as the number of participants increases. Services like [[Reddit]], [[YouTube]], and [[Last.fm]] are typical example of collaborative filtering based media.<ref>[http://www.readwriteweb.com/archives/collaborative_filtering_social_web.php Collaborative Filtering: Lifeblood of The Social Web]</ref>\n\nOne scenario of collaborative filtering application is to recommend interesting or popular information as judged by the community. As a typical example, stories appear in the front page of [[Digg]] as they are "voted up" (rated positively) by the community. As the community becomes larger and more diverse, the promoted stories can better reflect the average interest of the community members.\n\nAnother aspect of collaborative filtering systems is the ability to generate more personalized recommendations by analyzing information from the past activity of a specific user, or the history of other users deemed to be of similar taste to a given user. These resources are used as user profiling and helps the site recommend content on a user-by-user basis. The more a given user makes use of the system, the better the recommendations become, as the system gains data to improve its model of that user.\n\n===Problems===\nA collaborative filtering system does not necessarily succeed in automatically matching content to one\'s preferences. Unless the platform achieves unusually good diversity and independence of opinions, one point of view will always dominate another in a particular community. As in the personalized recommendation scenario, the introduction of new users or new items can cause the [[cold start]] problem, as there will be insufficient data on these new entries for the collaborative filtering to work accurately. In order to make appropriate recommendations for a new user, the system must first learn the user\'s preferences by analysing past voting or rating activities. The collaborative filtering system requires a substantial number of users to rate a new item before that item can be recommended.\n\n==Challenges of collaborative filtering==\n\n===Data sparsity===\nIn practice, many commercial recommender systems are based on large datasets. As a result, the user-item matrix used for collaborative filtering could be extremely large and sparse, which brings about the challenges in the performances of the recommendation.\n\nOne typical problem caused by the data sparsity is the [[cold start]] problem. As collaborative filtering methods recommend items based on users\xe2\x80\x99 past preferences,  new users will need to rate sufficient number of items to enable the system to capture their preferences accurately and thus provides reliable recommendations.\n\nSimilarly,  new items also have the same problem. When new items are added to system, they need to be rated by substantial number of users before they could be recommended to users who have similar tastes with the ones rated them. The new item problem does not limit the [[Recommender system#Content-based filtering|content-based recommendation]], because the recommendation of an item is based on its discrete set of descriptive qualities rather than its ratings.\n\n===Scalability===\nAs the numbers of users and items grow, traditional CF algorithms will suffer serious scalability problems{{Citation needed|date=April 2013}}. For example, with tens of millions of customers <math>O(M)</math> and millions of items <math>O(N)</math>, a CF algorithm with the complexity of <math>n</math> is already too large. As well, many systems need to react immediately to online requirements and make recommendations for all users regardless of their purchases and ratings history, which demands a higher scalability of a CF system. Large web companies such as Twitter use clusters of machines to scale recommendations for their millions of users, with most computations happening in very large memory machines.<ref name="twitterwtf">Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong Wang, and Reza Bosagh Zadeh [http://dl.acm.org/citation.cfm?id=2488433 WTF: The who-to-follow system at Twitter], Proceedings of the 22nd international conference on World Wide Web</ref>\n\n===Synonyms===\n[[Synonyms]] refers to the tendency of a number of the same or very similar items to have different names or entries. Most recommender systems are unable to discover this latent association and thus treat these products differently.\n\nFor example, the seemingly different items \xe2\x80\x9cchildren movie\xe2\x80\x9d and \xe2\x80\x9cchildren film\xe2\x80\x9d are actually referring to the same item. Indeed, the degree of variability in descriptive term usage is greater than commonly suspected.{{citation needed|date=September 2013}} The prevalence of synonyms decreases the recommendation performance of CF systems. Topic Modeling (like the Latent Dirichlet Allocation technique) could solve this by grouping different words belonging to the same topic.{{citation needed|date=September 2013}}\n\n===Grey sheep===\nGrey sheep refers to the users whose opinions do not consistently agree or disagree with any group of people and thus do not benefit from collaborative filtering. [[Black sheep]] are the opposite group whose idiosyncratic tastes make recommendations nearly impossible. Although this is a failure of the recommender system, non-electronic recommenders also have great problems in these cases, so black sheep is an acceptable failure.\n\n===Shilling attacks===\nIn a recommendation system where everyone can give the ratings, people may give lots of positive ratings  for their own items and negative ratings for their competitors. It is often necessary for the collaborative filtering systems to introduce precautions to discourage such kind of manipulations.\n\n===Diversity and the Long Tail===\nCollaborative filters are expected to increase diversity because they help us discover new products. Some algorithms, however, may unintentionally do the opposite. Because collaborative filters recommend products based on past sales or ratings, they cannot usually recommend products with limited historical data. This can create a rich-get-richer effect for popular products, akin to [[positive feedback]]. This bias toward popularity can prevent what are otherwise better consumer-product matches. A [[Wharton School of the University of Pennsylvania|Wharton]] study details this phenomenon along with several ideas that may promote diversity and the "[[long tail]]."<ref>{{cite journal| last1= Fleder | first1= Daniel | first2= Kartik |last2= Hosanagar | title=Blockbuster Culture\'s Next Rise or Fall: The Impact of Recommender Systems on Sales Diversity|journal=Management Science |date=May 2009|url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=955984}}</ref>\n\n==Innovations==\n{{Prose|date=May 2012}}\n* New algorithms have been developed for CF as a result of the [[Netflix prize]].\n* Cross-System Collaborative Filtering where user profiles across multiple [[recommender systems]] are combined in a privacy preserving manner.\n* Robust Collaborative Filtering, where recommendation is stable towards efforts of manipulation. This research area is still active and not completely solved.<ref>{{cite web|url=http://dl.acm.org/citation.cfm?id=1297240 |title=Robust collaborative filtering |doi=10.1145/1297231.1297240 |publisher=Portal.acm.org |date=19 October 2007 |accessdate=2012-05-15}}</ref>\n\n==See also==\n* [[Attention Profiling Mark-up Language|Attention Profiling Mark-up Language (APML)]]\n* [[Cold start]]\n* [[Collaborative model]]\n* [[Collaborative search engine]]\n* [[Collective intelligence]]\n* [[Customer engagement]]\n* [[Delegative Democracy]], the same principle applied to voting rather than filtering\n* [[Enterprise bookmarking]]\n* [[Firefly (website)]], a defunct website which was based on collaborative filtering\n* [[Long tail]]\n* [[Preference elicitation]]\n* [[Recommendation system]]\n* [[Relevance (information retrieval)]]\n* [[Reputation system]]\n* [[Robust collaborative filtering]]\n* [[Similarity search]]\n* [[Slope One]]\n* [[Social translucence]]\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n*[http://www.grouplens.org/papers/pdf/rec-sys-overview.pdf \'\'Beyond Recommender Systems: Helping People Help Each Other\'\'], page 12, 2001\n*[http://www.prem-melville.com/publications/recommender-systems-eml2010.pdf Recommender Systems.] Prem Melville and Vikas Sindhwani. In Encyclopedia of Machine Learning, Claude Sammut and Geoffrey Webb (Eds), Springer, 2010.\n*[http://arxiv.org/abs/1203.4487 Recommender Systems in industrial contexts - PHD thesis (2012) including a comprehensive overview of many collaborative recommender systems]\n*[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1423975  Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions]. Adomavicius, G. and Tuzhilin, A. IEEE Transactions on Knowledge and Data Engineering 06.2005\n*[http://ectrl.itc.it/home/laboratory/meeting/download/p5-l_herlocker.pdf Evaluating collaborative filtering recommender systems]{{dead link|date=May 2012}} ([http://www.doi.org/ DOI]: [http://dx.doi.org/10.1145/963770.963772 10.1145/963770.963772])\n*[http://www.grouplens.org/publications.html GroupLens research papers].\n*[http://www.cs.utexas.edu/users/ml/papers/cbcf-aaai-02.pdf Content-Boosted Collaborative Filtering for Improved Recommendations.] Prem Melville, Raymond J. Mooney, and Ramadass Nagarajan. Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), pp.&nbsp;187\xe2\x80\x93192, Edmonton, Canada, July 2002.\n*[http://agents.media.mit.edu/projects.html A collection of past and present "information filtering" projects (including collaborative filtering) at MIT Media Lab]\n*[http://www.ieor.berkeley.edu/~goldberg/pubs/eigentaste.pdf Eigentaste: A Constant Time Collaborative Filtering Algorithm. Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Information Retrieval, 4(2), 133-151. July 2001.]\n*[http://downloads.hindawi.com/journals/aai/2009/421425.pdf A Survey of Collaborative Filtering Techniques] Su, Xiaoyuan and Khoshgortaar, Taghi. M\n*[http://dl.acm.org/citation.cfm?id=1242610 Google News Personalization: Scalable Online Collaborative Filtering] Abhinandan Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. International World Wide Web Conference, Proceedings of the 16th international conference on World Wide Web\n*[http://research.yahoo.com/pub/2435 Factor in the Neighbors: Scalable and Accurate Collaborative Filtering] Yehuda Koren, Transactions on Knowledge Discovery from Data (TKDD) (2009)\n*[http://webpages.uncc.edu/~asaric/ISMIS09.pdf Rating Prediction Using Collaborative Filtering]\n*[http://www.cis.upenn.edu/~ungar/CF/ Recommender Systems]\n*[http://www2.sims.berkeley.edu/resources/collab/ Berkeley Collaborative Filtering]\n\n{{DEFAULTSORT:Collaborative Filtering}}\n[[Category:Collaboration]]\n[[Category:Collaborative software| Collaborative filtering]]\n[[Category:Collective intelligence]]\n[[Category:Information retrieval]]\n[[Category:Recommender systems]]\n[[Category:Social information processing]]\n[[Category:Behavioral and social facets of systemic risk]]'
p254
sg6
S'Collaborative filtering'
p255
ssI223
(dp256
g2
S'http://en.wikipedia.org/wiki/SPIN bibliographic database'
p257
sg4
S"{{Infobox Bibliographic Database\n|title =SPIN  (Searchable Physics Information Notices)  \n|image = \n|caption = \n|producer =[[American Institute of Physics]] (AIP) \n|country =USA, Russia, Ukraine\n|history = \n|languages =English, [[Russian language|Russian]], [[Ukrainian language|Ukrainian]] \n|providers =[[Dialog (online database)|Dialog]], [[American Institute of Physics|AIP website]], [[SPIE|SPIE Digital Library]] \n|cost = \n|disciplines =Physics, Astronomy, Mathematics, Geophysics, Geosciences, Nuclear Science, Science & Technology \n|depth =Word, Phrase, Abstract, Author and Author affiliations, Descriptor, Errata (coden, or date, or volume) Identifier, Title, Astronomical objects, CODEN, Conference (location, or title, or year), Journal name, and more...   \n|formats =Journal Articles, Book Reviews, Conferences, Meetings, Patents, Symposia\n|temporal =1975 to the present  \n|geospatial =International \n|number =over 1.5 million \n|updates =Weekly \n|p_title =No print counterparts \n|p_dates = \n|ISSN =\n|web =https://scitation.aip.org/jhtml/scitation/coverage.jsp \n|titles =  \n}}\n\n'''SPIN''' (Searchable Physics Information Notices) '''bibliographic database''' is an indexing and abstracting service produced by the [[American Institute of Physics]] (AIP). The content focus of SPIN is described as the most significant areas of [[physics]] [[research]]. This type of [[scientific literature|literature coverage]] spans the major [[scientific journal|physical science journals]] and magazines. Major [[conference proceedings]] that are reported by the American Institute of Physics, member societies, as well as affiliated organizations are also included as part of this database. References, or citations, provide access to more than 1.5 million articles as of 2010. ''SPIN''  has no print counterpart.<ref name=DialogSpin/><ref name=AIP-SPIN/>\n\n==Journals==\nDelivery of timely indexing and abstracting is for, what are deemed to be, the significant or important [[physics]] and [[astronomy]] journals from the [[United States]], [[Russia]], and the [[Ukraine]]. Citations for journal articles are derived from original publications of the ''AIP'', which includes published translated works. At the same time, citations are included from member societies, and selectively chosen American journals. Citations become typically available online on the same date as the corresponding journal article.<ref name=DialogSpin/><ref name=AIP-SPIN> {{Cite web\n  | title =What is the SPIN database? \n  | work =Information about SPIN \n  | publisher =[[American Institute of Physics]] \n  | date =July 2010 \n  | url =http://scitation.aip.org/servlet/HelpSystem?KEY=SCI&TYPE=HELP/FAQ#ques3 \n  | format = \n  | accessdate =2010-07-12}}</ref>\n\n==Sources==\nOverall, the source citations are derived from material published by the AIP and member societies,  which are English-speaking, Russian, and Ukrainian journals and conference proceedings. Certain American physics-related articles are also sources of citations. About 60 journals have cover to cover indexing, and about 100 journals, overall, are indexed.<ref name=DialogSpin/><ref name=pub-coverage>{{Cite web\n  | title =SPIN Publication Coverage \n  | work =Complete list of publications covered and coverage years. \n  | publisher =American Institute of Physics \n  | date =July 2010 \n  | url =http://scitation.aip.org/jhtml/scitation/spincodens.jsp \n  | format = \n  | accessdate =2010-07-12}}</ref>  \n\n==Scope==\nSubject coverage encompasses the following: <ref name=DialogSpin>  {{Cite web\n  | title =Indexes and Databases \n  | work =SPIN: Searchable Physics Information Notices\n  | publisher =Raymond H. Fogler Library, The University of Maine\n  | date =October 2010 \n  | url =http://www.library.umaine.edu/indexesdb/dbdetails.asp?field=Name&search=SPIN:+Searchable+Physics+Information+Notices \n  | format = \n  | accessdate =2010-07-12}}</ref>\n\n*[[Applied physics]], [[Electromagnetic spectrum|Electromagnetic]] technology, [[Microelectronics]] \n*[[Atomic physics]] and [[Molecular physics]] \n*[[Biological physics]] and [[Medical physics]] \n*[[Classical physics]] and [[Quantum physics]] \n*[[Condensed matter physics]] \n*[[Elementary particle physics]] \n*[[Physics|General physics]], [[Optics]], [[Acoustics]], and [[Fluid dynamics]] \n*[[Geophysics]], [[Astronomy]], [[Astrophysics]] \n*[[Materials science]] \n*[[Nuclear physics]] \n*[[Plasma physics]] \n*[[Physical chemistry]]\n\n==See also==\n*[[List of academic databases and search engines]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.aip.org/press_release/spin.html AIP'S SPIN Database Reaches One Million Records].  American Institute of Physics. March 1, 2002.\n*[http://scholarlykitchen.sspnet.org/2009/06/17/physics-papers-and-the-arxiv/ Can everything published in physics can be found in the [[arXiv]]?]. The Scholarly Kitchen. [[Society for Scholarly Publishing]]. June, 2010.\n*[http://www.pub4stm.org/ AIP partnerships] (society publishing). July 2010.\n\n\n[[Category:Bibliographic databases]]\n[[Category:Bibliographic indexes]]\n[[Category:Citation indices]]\n[[Category:Scientific databases]]"
p258
sg6
S'SPIN bibliographic database'
p259
ssI97
(dp260
g2
S'http://en.wikipedia.org/wiki/Search suggest drop-down list'
p261
sg4
S"{{Refimprove|date=June 2009}}\n\nA '''search [[Suggestion|suggest]] [[drop-down list]]''' is a [[Query language|query]] feature used in [[computing]]. A quick system to show the searcher [[Computer shortcut|shortcut]]s, while the query is typed. Before the query has been typed, a drop-down list with the suggested complete search queries, is given as options to select and access. The suggested queries then enable the searcher to complete the required search quickly.\n\nIt is a form of [[Autocomplete|autocompletion]] while typing into a query [[text box]], before a detailed search result is entered. Lists can be based on popular searches or other options. The [[Computer science|computing science]] of [[syntax]] and [[algorithm]]s are used to form search results from data or a [[database]], with search suggested drop-down lists being a common industry standard for an instant search.\n\nSearch suggested lists are used by [[internet browsers]], [[website]]s and [[search engine]]s, local [[operating system]]s and [[database]]s.\n\n[[Content management system]]s and frequent searches can assist [[Software engineering|software engineers]] in [[Optimization (computer science)|optimizing]] more refined queries with methods of parameters and subroutines. Suggestions can be results for the current query or related queries by words, time and dates, categories and [[Tag (metadata)|tags]]. The suggestion list may be reordered by other options, as [[Enumeration|enumerative]], [[Hierarchical organization|hierarchical]] or [[Faceted classification|faceted]].\n\n==See also ==\n*[[Autocomplete]]\n*[[Search engine (computing)]]\n*[[Search box]]\n*[[Search algorithm]]\n*[[Censorship by Google#Search suggestions]]\n\n\n{{DEFAULTSORT:Search Suggest Drop-Down List}}\n[[Category:Data search engines]]\n[[Category:Information retrieval]]\n[[Category:Search algorithms]]"
p262
sg6
S'Search suggest drop-down list'
p263
ssI226
(dp264
g2
S'http://en.wikipedia.org/wiki/SCImago Journal Rank'
p265
sg4
S'\'\'\'SCImago Journal Rank\'\'\' (SJR indicator) is a measure of scientific influence of [[academic journal|scholarly journal]]s that accounts for both the number of [[citation]]s received by a journal and the importance or prestige of the journals where such citations come from. The SJR indicator is a variant of the [[centrality|eigenvector centrality measure]] used in network theory. Such measures establish the importance of a node in a network based on the principle that connections to high-scoring nodes contribute more to the score of the node. The SJR indicator, which is inspired by the [[PageRank]] algorithm, has been developed to be used in extremely large and heterogeneous journal citation networks. It is a size-independent indicator and its values order journals by their "average prestige per article" and can be used for journal comparisons in science evaluation processes.\n\nThe \'\'SJR indicator\'\' is a free journal metric which uses an algorithm similar to [[PageRank]] and provides an alternative to the [[impact factor]] (IF), which is based on data from the [[Science Citation Index]].<ref>{{cite journal | url = http://www.nature.com/news/2008/080102/full/451006a.html | title= Free journal-ranking tool enters citation market | journal = [[Nature (journal)|Nature]] | date= 2 January 2008 | volume= 451 | issue= 6 | doi= 10.1038/451006a | author = Declan Butler | pmid= 18172465 | pages= 6 |accessdate=14 May 2010}}</ref><ref>{{cite journal | url = http://www.fasebj.org/cgi/content/short/22/8/2623 | title = Comparison of SCImago journal rank indicator with journal impact factor | author= Matthew E. Falagas et al | doi = 10.1096/fj.08-107938 | journal = [[The FASEB Journal]] | year = 2008 | issue = 22 | pages = 2623\xe2\x80\x932628 | pmid = 18408168 | volume = 22 }}</ref> Average citations per document in a 2-year period, abbreviated as Cites per Doc. (2y), is another index that measures the scientific impact of an average article published in the journal. It is computed using the same formula that journal [[impact factor]] ([[Thomson Reuters]]).\n\n== Rationale ==\nIf scientific impact is considered related to the number of endorsements, in the form of citations, a journal receives, then prestige can be understood as a combination of the number of endorsements and the prestige or importance of the journals issuing them. The \'\'SJR indicator\'\' assigns different values to citations depending on the importance of the journals where they come from. This way, citations coming from highly important journals will be more valuable and hence will provide more prestige to the journals receiving them. The calculation of the \'\'SJR indicator\'\' is very similar to the \'\'[[Eigenfactor]] score\'\', with the former being based on the [[Scopus]] database and the latter on the ISI [[Web of Science]] database.<ref>{{cite web | title=SCImago Journal & Country Rank (SJR) as an alternative to Thomson Reuters\'s Impact Factor and EigenFactor | url=http://www.scimagojr.com/news.php?id=41 | date=21 Aug 2008 | accessdate=20 September 2012}}</ref>\n\n== Computation ==\nThe SJR indicator computation is carried out using an iterative [[algorithm]] that distributes prestige values among the journals until a steady-state solution is reached. The SJR algorithm begins by setting an identical amount of prestige to each journal, then using an iterative procedure, this prestige is redistributed in a process where journals transfer their achieved prestige to each other through citations. The process ends up when the difference between journal prestige values in consecutive iterations do not reach a minimum threshold value any more. The process is developed in two phases, (a) the computation of \'\'Prestige SJR\'\' (\'\'PSJR\'\') for each journal: a size-dependent measure that reflects the whole journal prestige, and (b) the normalization of this measure to achieve a size-independent measure of prestige, the \'\'SJR indicator\'\'.\n\n== See also ==\n* [[Journal Citation Reports]]\n* [[Citation index]]\n* [[Eigenfactor]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n* {{official website|http://www.scimagojr.com/}}\n* [http://blogs.openaccesscentral.com/blogs/bmcblog/entry/scimago_a_new_source_of SCImago \xe2\x80\x93 a new source of journal metrics offering a wealth of free data on open access journals]\n* [http://www.earlham.edu/~peters/fos/2008/01/more-on-scimago-journal-rank-v-impact.html More on SCImago Journal Rank v. Impact Factors]\n\n{{DEFAULTSORT:Scimago journal rank}}\n[[Category:Citation indices]]\n[[Category:Academic publishing]]'
p266
sg6
S'SCImago Journal Rank'
p267
ssI100
(dp268
g2
S'http://en.wikipedia.org/wiki/Legal information retrieval'
p269
sg4
S'\'\'\'Legal information retrieval\'\'\' is the science of [[information retrieval]] applied to legal text, including [[legislation]], [[case law]], and scholarly works.<ref>Maxwell, K.T., and Schafer, B. 2009, p. 1</ref> Accurate legal information retrieval is important to provide access to the law to laymen and legal professionals. Its importance has increased because of the vast and quickly increasing amount of legal documents available through electronic means.<ref name=Jackson>Jackson et al., p. 60</ref> Legal information retrieval is a part of the growing field of [[legal informatics]].  \n\n== Overview ==\n\nIn a legal setting, it is frequently important to retrieve all information related to a specific query. However, commonly used [[boolean search]] methods (exact matches of specified terms) on full text legal documents have been shown to have an average [[recall rate]] as low as 20 percent,<ref name="Blair, D.C. 1985, p.293">Blair, D.C., and Maron, M.E., 1985, p.293</ref> meaning that only 1 in 5 relevant documents are actually retrieved. In that case, researchers believed that they had retrieved over 75% of relevant documents.<ref name="Blair, D.C. 1985, p.293"/> This may result in failing to retrieve important or [[precedential]] cases. In some jurisdictions this may be especially problematic, as legal professionals are [[legal ethics|ethically]] obligated to be reasonably informed as to relevant legal documents.<ref>American Bar Association, Model Rules of Professional Conduct Rule 1.1, http://www.abanet.org/cpr/mrpc/rule_1_1.html</ref> \n\nLegal Information Retrieval attempts to increase the effectiveness of legal searches by increasing the number of relevant documents (providing a high [[recall rate]]) and reducing the number of irrelevant documents (a high [[precision rate]]). This is a difficult task, as the legal field is prone to [[jargon]],<ref>Peters, W. et al. 2007, p. 118</ref> [[polysemes]]<ref>Peters, W. et al. 2007, p. 130</ref> (words that have different meanings when used in a legal context), and constant change. \n\nTechniques used to achieve these goals generally fall into three categories: [[boolean search|boolean]] retrieval, manual classification of legal text, and [[natural language processing]] of legal text.\n\n== Problems ==\n\nApplication of standard [[information retrieval]] techniques to legal text can be more difficult than application in other subjects. One key problem is that the law rarely has an inherent [[Taxonomy (general)|taxonomy]].<ref name=LOIS1>Peters, W. et al. 2007, p. 120</ref> Instead, the law is generally filled with open-ended terms, which may change over time.<ref name=LOIS1 /> This can be especially true in [[common law]] countries, where each decided case can subtly change the meaning of a certain word or phrase.<ref>Saravanan, M. et al.  2009, p. 101</ref>\n\nLegal information systems must also be programmed to deal with law-specific words and phrases. Though this is less problematic in the context of words which exist solely in law, legal texts also frequently use polysemes, words may have different meanings when used in a legal or common-speech manner, potentially both within the same document. The legal meanings may be dependent on the area of law in which it is applied. For example, in the context of European Union legislation, the term "worker" has four different meanings:<ref name="Peters, W. et al. 2007, p. 131">Peters, W. et al. 2007, p. 131</ref> \n\n#Any worker as defined in Article 3(a) of [[Directive 89/391/EEC]] who habitually uses display screen equipment as a significant part of his normal work.\n#Any person employed by an employer, including trainees and apprentices but excluding domestic servants;\n#Any person carrying out an occupation on board a vessel, including trainees and apprentices, but excluding port pilots and shore personnel carrying out work on board a vessel at the quayside;\n#Any person who, in the Member State concerned, is protected as an employee under national employment law and in accordance with national practice;\n\nIn addition, it also has the common meaning: \n<ol start="5">\n<li>A person who works at a specific occupation.<ref name="Peters, W. et al. 2007, p. 131"/> </li>\n</ol>\n\nThough the terms may be similar, correct information retrieval must differentiate between the intended use and irrelevant uses in order to return the correct results. \n\nEven if a system overcomes the language problems inherent in law, it must still determine the relevancy of each result. In the context of judicial decisions, this requires determining the precedential value of the case.<ref name=MaxwellA >Maxwell, K.T., and Schafer, B. 2008, p. 8</ref> Case decisions from senior or [[superior court]]s may be more relevant than those from [[lower court]]s, even where the lower court\'s decision contains more discussion of the relevant facts.<ref name=MaxwellA  /> The opposite may be true, however, if the senior court has only a minor discussion of the topic (for example, if it is a secondary consideration in the case).<ref name=MaxwellA  /> A information retrieval system must also be aware of the authority of the jurisdiction. A case from a binding authority is most likely of more value than one from a non-binding authority.\n\nAdditionally, the intentions of the user may determine which cases they find valuable. For instance, where a legal professional is attempting to argue a specific interpretation of law, he might find a minor court\'s decision which supports his position more valuable than a senior courts position which does not.<ref name=MaxwellA  /> He may also value similar positions from different areas of law, different jurisdictions, or dissenting opinions.<ref name=MaxwellA />\n\nOvercoming these problems can be made more difficult because of the large number of cases available. The number of legal cases available via electronic means is constantly increasing (in 2003, US appellate courts handed down approximately 500 new cases per day<ref name=Jackson />), meaning that an accurate legal information retrieval system must incorporate methods of both sorting past data and managing new data.<ref name=Jackson /><ref>Maxwell, K.T., and Schafer, B. 2007, p.1</ref>\n\n== Techniques ==\n\n===Boolean searches===\n\n[[Boolean search]]es, where a user may specify terms such as use of specific words or judgments by a specific court, are the most common type of search available via legal information retrieval systems. They are widely implemented by services such as [[Westlaw]], [[LexisNexis]], and [[Findlaw]].  However, they overcome few of the problems discussed above. \n\nThe recall and precision rates of these searches vary depending on the implementation and searches analyzed. One study found a basic boolean search\'s [[recall rate]] to be roughly 20%, and its precision rate to be roughly 79%.<ref name="Blair, D.C. 1985, p.293"/> Another study implemented a generic search (that is, not designed for legal uses) and found a recall rate of 56% and a precision rate of 72% among legal professionals. Both numbers increased when searches were run by non-legal professionals, to a 68% recall rate and 77% precision rate. This is likely explained because of the use of complex legal terms by the legal professionals.<ref>Saravanan M., et al. 2009, p. 116</ref>\n\n===Manual classification===\n\nIn order to overcome the limits of basic boolean searches, information systems have attempted to classify case laws and statutes into more computer friendly structures. Usually, this results in the creation of an [[ontology]] to classify the texts, based on the way a legal professional might think about them.<ref name="Maxwell, K.T. 2008, p. 2">Maxwell, K.T., and Schafer, B. 2008, p. 2</ref> These attempt to link texts on the basis of their type, their value, and/or their topic areas. Most major legal search providers now implement some sort of classification search, such as [[Westlaw]]\'s \xe2\x80\x9cNatural Language\xe2\x80\x9d<ref name=WL>Westlaw Research, http://www.westlaw.com</ref> or [[LexisNexis]]\' Headnote<ref name=LN>Lexis Research, http://www.lexisnexis.com</ref> searches. Additionally, both of these services allow browsing of their classifications, via Westlaw\'s West Key Numbers<ref name=WL /> or Lexis\' Headnotes.<ref name=LN /> Though these two search algorithms are proprietary and secret, it is known that they employ manual classification of text (though this may be computer-assisted).<ref name="Maxwell, K.T. 2008, p. 2"/>\n\nThese systems can help overcome the majority of problems inherent in legal information retrieval systems, in that manual classification has the greatest chances of identifying landmark cases and understanding the issues that arise in the text.<ref name="Maxwell, K.T. 2008, p. 3">Maxwell, K.T., and Schafer, B. 2008, p. 3</ref> In one study, ontological searching resulted in a precision rate of 82% and a recall rate of 97% among legal professionals.<ref>Saravanan, M. et al.  2009, p. 116</ref> The legal texts included, however, were carefully controlled to just a few areas of law in a specific jurisdiction.<ref>Saravanan, M. et al. 2009, p. 103</ref>\n\nThe major drawback to this approach is the requirement of using highly skilled legal professionals and large amounts of time to classify texts.<ref name="Maxwell, K.T. 2008, p. 3"/><ref>Schweighofer, E. and Liebwald, D. 2008, p. 108</ref> As the amount of text available continues to increase, some have stated their belief that manual classification is unsustainable.<ref>Maxwell, K.T., and Schafer, B. 2008, p. 4</ref>\n\n===Natural language processing===\n\nIn order to reduce the reliance on legal professionals and the amount of time needed, efforts have been made to create a system to automatically classify legal text and queries.<ref name=Jackson /><ref name=AshleyA>Ashley, K.D. and Bruninghaus, S. 2009, p. 125</ref><ref name=Gelbart>Gelbart, D. and Smith, J.C. 1993, p. 142</ref> Adequate translation of both would allow accurate information retrieval without the high cost of human classification. These automatic systems generally employ [[Natural Language Processing]] (NLP) techniques that are adapted to the legal domain, and also require the creation of a legal [[ontology]]. Though multiple systems have been postulated,<ref name=Jackson /><ref name=AshleyA /><ref name=Gelbart /> few have reported results. One system, \xe2\x80\x9cSMILE,\xe2\x80\x9d which attempted to automatically extract classifications from case texts, resulted in an [[f-measure]] (which is a calculation of both recall rate and precision) of under 0.3 (compared to perfect f-measure of 1.0).<ref name=AshleyB >Ashley, K.D. and Bruninghaus, S. 2009, p. 159</ref> This is probably much lower than an acceptable rate for general usage.<ref name=AshleyB /><ref>Maxwell, K.T., and Schafer, B. 2009, p. 3</ref>\n\nDespite the limited results, many theorists predict that the evolution of such systems will eventually replace manual classification systems.<ref>Maxwell, K.T., and Schafer, B. 2009, p. 9</ref><ref>Ashley, K.D. and Bruninghaus, S. 2009, p. 126</ref>\n\n== Notes ==\n{{Reflist|2}}\n\n==References==\n{{Refbegin}}\n*{{cite journal\n|author     = Maxwell, K.T., and Schafer, B.\n|year       = 2008\n|title      = Concept and Context in Legal Information Retrieval\n|url        = http://portal.acm.org/citation.cfm?id=1564016\n|journal    = Frontiers in Artificial Intelligence and Applications\n|volume     = 189\n|pages      = 63\xe2\x80\x9372\n|publisher  = IOS Press\n|accessdate = 2009-11-07\n}}\n*{{cite journal\n|author     = Jackson, P. et al.\n|year       = 1998\n|title      = Information extraction from case law and retrieval of prior cases by partial parsing and query generation\n|url        = http://portal.acm.org/citation.cfm?id=288627.288642\n|journal    = Conference on Information and Knowledge Management\n|pages      = 60\xe2\x80\x9367\n|publisher  = ACM\n|accessdate = 2009-11-07\n}}\n*{{cite journal\n|author     = Blair, D.C., and Maron, M.E.\n|year       = 1985\n|title      = An evaluation of retrieval effectiveness for a full-text document-retrieval\n|url        = http://portal.acm.org/citation.cfm?id=3166.3197&coll=GUIDE&dl=GUIDE&CFID=61732097&CFTOKEN=95519997\n|journal    = Communications of the ACM\n|volume     = 28\n|issue      = 3 \n|pages      = 289\xe2\x80\x93299\n|publisher  = ACM\n|accessdate = 2009-11-07\n|doi=10.1145/3166.3197\n}}\n*{{cite journal\n|author     = Peters, W. et al.\n|year       = 2007\n|title      = The structuring of legal knowledge in LOIS\n|url        = http://www.springerlink.com/content/d04l7h2507700g45/\n|journal    = Artificial Intelligence and Law\n|volume     = 15\n|issue      = 2\n|pages      = 117\xe2\x80\x93135\n|publisher  = Springer Netherlands\n|accessdate = 2009-11-07\n|doi=10.1007/s10506-007-9034-4\n}}\n*{{cite journal\n|author     = Saravanan, M. et al.\n|year       = 2007\n|title      = Improving legal information retrieval using an ontological framework \n|url        = http://www.springerlink.com/content/h66412k08h855626/\n|journal    = Artificial Intelligence and Law\n|volume     = 17\n|issue      = 2\n|pages      = 101\xe2\x80\x93124\n|publisher  = Springer Netherlands\n|accessdate = 2009-11-07\n|doi=10.1007/s10506-009-9075-y\n}}\n*{{cite journal\n|author     = Schweighofer, E. and Liebwald, D.\n|year       = 2007\n|title      = Advanced lexical ontologies and hybrid knowledge based systems: First steps to a dynamic legal electronic commentary\n|url        = http://www.springerlink.com/content/v62v7131x10413v0/\n|journal    = Artificial Intelligence and Law\n|volume     = 15\n|issue      = 2\n|pages      = 103\xe2\x80\x93115\n|publisher  = Springer Netherlands\n|accessdate = 2009-11-07\n|doi=10.1007/s10506-007-9029-1\n}}\n*{{cite journal\n|author     = Gelbart, D. and Smith, J.C.\n|year       = 1993\n|title      = FLEXICON: an evaluation of a statistical ranking model adapted to intelligent legal text management\n|url        = http://portal.acm.org/citation.cfm?id=158994\n|journal    = International Conference on Artificial Intelligence and Law\n|pages      = 142\xe2\x80\x93151\n|publisher  = ACM\n|accessdate = 2009-11-07\n}}\n*{{cite journal\n|author     = Ashley, K.D. and Bruninghaus, S.\n|year       = 2009\n|title      = Automatically classifying case texts and predicting outcomes\n|url        = http://www.springerlink.com/content/lhg8837331hgu024/\n|journal    = Artificial Intelligence and Law\n|volume     = 17\n|issue      = 2\n|pages      = 125\xe2\x80\x93165\n|publisher  = Springer Netherlands\n|accessdate = 2009-11-07\n|doi=10.1007/s10506-009-9077-9\n}}\n{{Refend}}\n\n{{DEFAULTSORT:Legal Information Retrieval}}\n[[Category:Information retrieval]]\n[[Category:Natural language processing]]\n[[Category:Legal research]]'
p270
sg6
S'Legal information retrieval'
p271
ssI229
(dp272
g2
S'http://en.wikipedia.org/wiki/Web of Knowledge'
p273
sg4
S"{{Mergeto|Web of Science|date=June 2014|discuss=Talk:Web of Science#Merge}}\n[[File:Web of Science Logo.png|thumb|The current Web of Science logo]]\n[[Image:ISI Web of Knowledge updated.png|thumb|400px|An example search result from Web of Knowledge version 3.0]]\n\n'''Web of Knowledge''' (formerly known as [[Institute for Scientific Information|ISI]] Web of Knowledge) is an academic [[citation index]]ing and search service, which is combined with web linking and is provided by [[Thomson Reuters]]. Web of Knowledge covers the sciences, [[social science]]s, arts and [[humanities]]. It provides [[bibliography|bibliographic]] content and tools to access, analyze, and manage research information. Multiple databases can be searched simultaneously.<ref name=describe/><ref name=tutor>[http://science.thomsonreuters.com/tutorials/wok4/wok4tut3.html Tutorial]. ISI Web of Knowledge. Thomson Reuters. 2010. Accessed on 2010-06-24</ref>\n\n==Overview==\nWeb of Knowledge is described as a unifying research tool which enables the user to acquire, analyze, and disseminate database information in a timely manner. This is accomplished because of the creation of a common vocabulary, called [[Ontology (information science)|ontology]], for varied search terms and varied data. Moreover, search terms generate related information across categories.\n\nAcceptable content for Web of Knowledge is determined by an evaluation and selection process based on the following criteria: impact, influence, timeliness, [[peer review]], and geographic representation.<ref name=describe/>\n\n===Search and analysis===\n<!-- Deleted image removed: [[File:ISI Web of knowledge logo.jpg|thumb||Former Web of Knowledge logo]] -->\n\nWeb of Knowledge employs various search and analysis capabilities. First, citation indexing is employed, which is enhanced by the capability to search for results across disciplines. The influence, impact, history, and [[methodology]] of an idea can be followed from its first instance, notice, or referral to the present day. This technology points to a deficiency with the [[Index term|keyword]]-only method of searching. \n\nSecond, subtle trends and patterns relevant to the literature or research of interest, become apparent. Broad trends indicate significant topics of the day, as well as the history relevant to both the work at hand, and particular areas of study. \n\nThird, trends can be [[mathematical modeling|graphically]] represented.<ref name=describe>[http://thomsonreuters.com/content/science/pdf/Web_of_Knowledge_factsheet.pdf Overview and Description]. ISI Web of Knowledge. Thomson Reuters. 2010. Accessed on 2010-06-24</ref><ref>{{cite web|url=http://wokinfo.com/realfacts/qualityandquantity/|title=Web of Knowledge > Real Facts > Quality and Quantity|accessdate = 2010-05-05}}</ref>\n\n=== Content ===\nThe combined databases includes the following:\n*23,000 [[Academic journal|academic]] and [[scientific journal]]s (including [[Web of Science]] journal listings)\n*23,000,000 [[patent]]s\n*110,000 conference [[proceedings]]\n*9,000 websites\n*Coverage from the year 1900 to present day (with Web of Science)\n*Over 40 million source items\n*Integrated and simultaneous searching across multiple databases<ref name=describe/>\n\n=== Included databases ===\nThe Web of Knowledge suite encompasses the following databases:<ref name=dbase-List>{{Cite web| last =''ISI Web of Knowledge''| title =Suite of databases| publisher =Thomson Reuters| year =2010| url = http://thomsonreuters.com/products_services/science/science_products/a-z/isi_web_of_knowledge?parentKey=555184 | format =List of databases that are part of the Web of Knowledge suite.| accessdate =2010-06-24}}</ref><ref name=AtoZ>{{Cite web| last = ISI Web of Knowledge platform| title =Available databases A to Z| publisher =Thomson Reuters| year =2010| url =http://wokinfo.com/products_tools/products/ | format =Choose databases on method of discovery and analysis| accessdate =2010-06-24}}</ref><ref>[http://wokinfo.com/media/pdf/SSR1103443WoK5-2_web3.pdf Thomson Reuters Web of Knowledge. Thomson Reuters, 2013.]</ref>\n{{columns-list|colwidth=30em|\n*[[Biological Abstracts]]\n*[[Biosis Previews]] \n*[[CAB Abstracts]]\n*[[CAB Direct|CAB Global Health]]\n*[[Chinese Science Citation Database]]\n*[[Conference Proceedings Citation Index]] \n*[[Current Contents|Current Contents Connect]]\n*[[Data Citation Index]]\n*[[Derwent Innovations Index]]\n*[[Essential Science Indicators]]\n*[[Food Science and Technology Abstracts]]\n*[[Inspec]] \n*[[ISI Highly Cited]]\n*[[Journal Citation Reports]]\n*[[MEDLINE]] \n*[[Web of Science]]\n**[[Arts & Humanities Citation Index]]\n**[[Book Citation Index]] \n**[[Current Chemical Reactions]]\n**[[Index Chemicus]]\n**[[Science Citation Index Expanded]]\n**[[Social Sciences Citation Index]]\n*[[The Zoological Record]]\n}}\n\n==See also==\n*[[List of academic journal search engines]]\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n* {{Official website|http://wokinfo.com/}}\n\n{{Thomson Reuters}}\n\n[[Category:Bibliographic databases]]\n[[Category:Online databases]]\n[[Category:Thomson Reuters]]\n[[Category:Citation indices]]\n[[Category:Scholarly search services]]"
p274
sg6
S'Web of Knowledge'
p275
ssI103
(dp276
g2
S'http://en.wikipedia.org/wiki/Exploratory search'
p277
sg4
S'\'\'\'Exploratory search\'\'\' is a specialization of information exploration which represents the activities carried out by searchers who are either:<nowiki>[3]</nowiki>\n* a) unfamiliar with the domain of their goal (i.e. need to learn about the topic in order to understand how to achieve their goal)\n* b) unsure about the ways to achieve their goals (either the technology or the process)\n* c) or even unsure about their goals in the first place.\n\nConsequently, exploratory search covers a broader class of activities than typical [[information retrieval]], such as investigating, evaluating, comparing, and synthesizing, where new information is sought in a defined conceptual area; [[exploratory data analysis]] is another example of an information exploration activity. Typically, therefore, such users generally combine querying and browsing strategies to foster learning and investigation.\n\n==History==\nExploratory search is a topic that has grown from the fields of [[information retrieval]] and [[information seeking]] but has become more concerned with alternatives to the kind of search that has received the majority of focus (returning the most relevant documents to a [[Google]]-like keyword search). The research is motivated by questions like "what if the user doesn\'t know which keywords to use?" or "what if the user isn\'t looking for a single answer?" Consequently, research has begun to focus on defining the broader set of \'\'information behaviors\'\' in order to learn about the situations when a user is, or feels, limited by only having the ability to perform a keyword search.\n\nIn the last few years, a series of workshops have been held at various related and key events. In 2005, the [http://research.microsoft.com/~ryenw/xsi/index.html Exploratory Search Interfaces workshop] focused on beginning to define some of the key challenges in the field. Since then a series of other workshops have been held at related conferences: [http://research.microsoft.com/~ryenw/eess/index.html Evaluating Exploratory Search] at [http://www.sigir2006.org SIGIR06] and [http://research.microsoft.com/~ryenw/esi/index.html Exploratory Search and HCI] at [http://www.chi2007.org CHI07] (in order to meet with the experts in [[human\xe2\x80\x93computer interaction]]).\n\nIn March 2008, an [http://www.sciencedirect.com/science/journal/03064573 \'\'Information Processing and Management\'\' special issue]<nowiki>[2]</nowiki> has focused particularly on the challenges of evaluating exploratory search, given the reduced assumptions that can be made about scenarios of use.\n\nIn June 2008, the [[National Science Foundation]] sponsored an [http://www.ils.unc.edu/ISSS_workshop/ invitational workshop] to identify a research agenda for exploratory search and similar fields for the coming years.\n\n==Research challenges==\n\n===Important scenarios===\nWith the majority of research in the [[information retrieval]] community focusing on typical keyword search scenarios, one challenge for exploratory search is to further understand the scenarios of use for when keyword search is not sufficient. An example scenario, often used to motivate the research by [http://mspace.fm mSpace] states: if a user does not know much about classical music, how should they even begin to find a piece that they might like.\n\n===Designing new interfaces===\nWith one of the motivations being to support users when keyword search is not enough, some research has focused on identifying alternative user interfaces and interaction models that support the user in different ways. An example is [[Faceted classification|faceted search]] which presents diverse category-style options to the user, so that they can choose from a list instead of guess a possible keyword query.\n\nMany of the [[human\xe2\x80\x93computer information retrieval|interactive forms of search]], including [[faceted browser]]s, are being considered for their support of exploratory search conditions.\n\nComputational cognitive models of exploratory search have been developed to capture the cognitive complexities involved in exploratory search. Model-based dynamic presentation of information cues are proposed to facilitate exploratory search performance.<ref>Fu, W.-T., Kannampalill, T. G., & Kang, R. (2010). Facilitating exploratory search by model-based navigational cues. In Proceedings of the ACM International conference on Intelligent User Interface. 199-208.  http://portal.acm.org/citation.cfm?id=1719970.1719998</ref>\n\n===Evaluating interfaces===\nAs the tasks and goals involved with exploratory search are largely undefined or unpredictable, it is very hard to evaluate systems with the measures often used in information retrieval. Accuracy was typically used to show that a user had found a correct answer, but when the user is trying to summarize a domain of information, the \'\'correct\'\' answer is near impossible to identify, if not entirely subjective (for example: possible hotels to stay in Paris). In exploration, it is also arguable that spending more time (where time efficiency is typically desirable) researching a topic shows that a system provides increased support for investigation. Finally, and perhaps most importantly, giving study participants a well specified task could immediately prevent them from exhibiting exploratory behavior.\n\n===Models of exploratory search behavior===\nThere has been recent attempts to develop process model of exploratory search behavior, especially in social information system (e.g., see [[models of collaborative tagging]].<ref>{{Citation\n  | doi = 10.1145/1460563.1460600\n  | last1 = Fu  | first1 = Wai-Tat\n  | title = The Microstructures of Social Tagging: A Rational Model\n  | journal = Proceedings of the ACM 2008 conference on Computer Supported Cooperative Work. \n  | pages = 66\xe2\x80\x9372\n  | date = April 2008\n  | url = http://portal.acm.org/citation.cfm?id=1460600\n  | isbn = 978-1-60558-007-4 }}\n</ref>\n.<ref>{{Citation\n  | last1 = Fu  | first1 = Wai-Tat\n  | title = A Semantic Imitation Model of Social Tagging\n  | journal = Proceedings of the IEEE conference on Social Computing\n  | pages = 66\xe2\x80\x9372\n  | date = Aug 2009\n  | url = http://www.humanfactors.illinois.edu/Reports&PapersPDFs/IEEESocialcom09/A%20Semantic%20Imitation%20Model%20of%20Social%20Tag%20Choices%20(2).pdf }}</ref> The process model assumes that user-generated information cues, such as social tags, can act as navigational cues that facilitate exploration of information that others have found and shared with other users on a social information system (such as [[social bookmarking]] system). These models provided extension to existing process model of information search that characterizes information-seeking behavior in traditional fact-retrievals using search engines.<ref>\n{{Citation\n  | last1 = Fu  | first1 = Wai-Tat\n  | last2 = Pirolli  | first2 = Peter\n  | title = SNIF-ACT: a cognitive model of user navigation on the world wide web\n  | journal = Human-Computer Interaction\n  | pages = 335\xe2\x80\x93412\n  | year = 2007\n  | url = http://portal.acm.org/citation.cfm?id=1466608\n  | volume = 22}}</ref><ref>Kitajima, M., Blackmon, M. H., & Polson, P. G. (2000). A comprehension-based\nmodel of Web navigation and its application to Web usability analysis. In S. Mc-\nDonald, Y. Waern, & G. Cockton (Eds.), People and computers XIV\xe2\x80\x94Usability or else!\nNew York: Springer-Verlag.</ref><ref>Miller, C. S., & Remington, R.W. (2004). Modeling information navigation: Implications\nfor information architecture. Human Computer Interaction, 19, 225\xe2\x80\x93271.</ref>\nRecent development in exploratory search is often concentrated in predicting user\'s search intents in interaction with the user.<ref>\n{{Citation\n  | last1 = Ruotsalo  | first1 = Tuukka\n  | last2 = Athukorala  | first2 = Kumaripaba\n  | last3 = Glowacka  | first3 = Dorota\n  | last4 = Konuyshkova  | first4 = Ksenia\n  | last5 = Oulasvrita  | first5 = Antti\n  | last6 = Kaipiainen  | first6 = Samuli\n  | last7 = Kaski  | first7 = Samuel\n  | last8 = Jacucci  | first8 = Giulio\n  | title = Supporting exploratory search tasks with interactive user modeling\n  | journal = Proceedings of the 76th Annual Meeting of the American Society for Information Science and Technology ASIS&T\n  | year = 2013}}\n</ref>\nSuch predictive user modeling, also referred as intent modeling, can help users to get accustomed to a body of domain knowledge and help users to make sense of the potential directions to be explored around their initial, often vague, expression of information needs\n<ref>\n{{Citation\n  | last1 = Ruotsalo  | first1 = Tuukka\n  | last2 = Peltonen  | first2 = Jaakko\n  | last3 = Eugster | first3 = Manuel J.A.\n  | last4 = Glowacka  | first4 = Dorota\n  | last5 = Konuyshkova  | first5 = Ksenia\n  | last6 = Athukorala  | first6 = Kumaripaba\n  | last7 = Kosunen | first7 = Ilkka    \n  | last8 = Reijonen  | first8 = Aki\n  | last9 = Myllym\xc3\xa4ki | first9 = Petri\n  | last10 = Kaski  | first10 = Samuel\n  | last11 = Jacucci  | first11 = Giulio\n  | title = Directing Exploratory Search with Interactive Intent Modeling\n  | journal = Proceedings of the ACM Conference of Information and Knowledge Management CIKM\n  | year = 2013}}\n</ref>\n.<ref>\n{{Citation\n  | last1 = Glowacka  | first1 = Dorota\n  | last2 = Ruotsalo  | first2 = Tuukka\n  | last3 = Konuyshkova  | first3 = Ksenia\n  | last4 = Athukorala  | first4 = Kumaripaba\n  | last5 = Kaski  | first5 = Samuel\n  | last6 = Jacucci  | first6 = Giulio\n  | title = Directing exploratory search: Reinforcement learning from user interactions with keywords\n  | journal = Proceedings of the ACM Conference of Intelligent User Interfaces IUI\n  | url = http://dl.acm.org/citation.cfm?id=2449413\n  | pages = 117\xe2\x80\x93128 \n  | year = 2013}}\n</ref>\n\n==Major figures==\n\nKey figures, including experts from both [[information seeking]] and [[human\xe2\x80\x93computer interaction]], are:\n*[http://research.microsoft.com/~ryenw Ryen White]\n*[http://ils.unc.edu/~march Gary Marchionini]\n*[http://comminfo.rutgers.edu/~belkin/belkin.html Nicholas Belkin]\n*[http://users.ecs.soton.ac.uk/mc m.c. schraefel]\n*[[Marcia Bates]]\n\n==References==\n<References/>\n#White, R.W., Kules, B., Drucker, S.M., and schraefel, m.c. (2006). \'\'Supporting Exploratory Search\'\', Introduction to Special Section of Communications of the ACM, Vol. 49, Issue 4, (2006), pp.&nbsp;36\xe2\x80\x9339.\n#Ryen W. White, Gary Marchionini, Gheorghe Muresan (2008). \'\'Evaluating exploratory search systems: Introduction to special topic issue of information processing and management\'\' Vol. 44, Issue 2, (2008), pp.&nbsp;433\xe2\x80\x93436\n#Ryen W. White and Resa A. Roth (2009). \'\'Exploratory Search: Beyond the Query-Response Paradigm\'\', San Rafael, CA: Morgan and Claypool.\n#P. Papadakos, S. Kopidaki, N. Armenatzoglou and Y. Tzitzikas (2009). \'\'Exploratory Web Searching with Dynamic Taxonomies and Results Clustering\'\',13th European Conference on Digital Libraries (ECDL\'09), Corfu, Greece, Sep-Oct 2009\n\n{{DEFAULTSORT:Exploratory Search}}\n[[Category:Human\xe2\x80\x93computer interaction]]\n[[Category:Information retrieval]]\n[[Category:Information science]]'
p278
sg6
S'Exploratory search'
p279
ssI232
(dp280
g2
S'http://en.wikipedia.org/wiki/Query by humming'
p281
sg4
S'{{inline|date=August 2012}}\n\'\'\'Query by humming\'\'\' (\'\'\'QbH\'\'\') is a music retrieval system that branches off the original classification systems of title, artist, composer, and genre. It normally applies to songs or other music with a distinct single theme or melody. The system involves taking a user-hummed [[melody]] (input [[Information retrieval|query]]) and comparing it to an existing [[database]].  The system then returns a ranked list of music closest to the input query. \n\nOne example of this would be a system involving a [[portable media player]] with a built-in [[microphone]] that allows for faster [[Search engine technology|searching]] through [[Digital media|media]] files.\n\nThe [[MPEG-7]] standard includes provisions for QbH music searches.\n\n== Examples of QbH systems ==\nSoundHound and Midomi are the only commercially available query by humming services available online at Midomi.com or on the mobile app called SoundHound. \nBoth are powered by the same backend and are capable of recognizing humming and singing as well as recorded tracks. \nFor the singing and humming search, the searchable database is based on Midomi.com\'s user contributions. Midomi has collected about one million tracks based on user contributions in multiple languages, making it the largest database of its kind by a large margin. The top four languages are: English, Japanese, Chinese and Spanish. \n\n"Musipedia" is an example of a QbH system that uses a variety of input methods such as humming, tapping the keyboard, keyboard search (a virtual piano keyboard), draw notes, and a contour search, using [[Parsons_code|Parsons Code]] to encode the music pieces.\n\n[[Tunebot]] is a music search engine that uses queries from humming, lyrics, and melody. People can contribute to the database and expand the variety of searchable songs. Tunebot also serves as the back-end for a game called [[Karaoke Callout]], in which players\' performances are compared by the engine with songs in the database.\n\n== External links ==\n===Online demos===\n* [http://www.midomi.com/ Midomi]\n* [http://www.soundhound.com/ SoundHound (mobile app)] \n* [http://www.musipedia.org/query_by_humming.0.html QbH system] from Musipedia\n* [http://querybyhum.cs.nyu.edu/ QbH research project at NYU]\n* [http://www.sloud.com/technology/query_by_humming/ Query by Humming at Sloud Inc], [http://www.sloud.com/ QbH applet (Active X)] \n* [http://www.musicline.de/de/melodiesuche/input Musicline QbH based on technology from Fraunhofer Institut] {{de icon}}\n* [http://maart.sourceforge.net/ MaART at Sourceforge]\n* [http://tunebot.cs.northwestern.edu/ Tunebot at Northwestern University]\n\n===General info and articles===\n* {{Wayback|url=http://mirsystems.info/index.php?id=mirsystems|title=Comprehensive list of Music Information Retrieval systems (apparently last updated ca 2003)|date=20081221191111}}\n* [http://www.cs.cornell.edu/zeno/papers/humming/humming.html Query By Humming \xe2\x80\x93 Musical Information Retrieval in an Audio Database], paper by Asif Ghias, Jonathan Logan, David Chamberlin, Brian C. Smith; [[ACM Multimedia]] 1995\n* [http://cs.nyu.edu/~eugenew/publications/humming-summary.pdf A survey presentation of QBH by Eugene Weinstein, 2006]\n* [http://www.dlib.org/dlib/may97/meldex/05witten.html The New Zealand Digital Library MELody inDEX], article by Rodger J. McNab, Lloyd A. Smith, David Bainbridge and Ian H. Witten; [[D-Lib Magazine]] 1997\n* [http://deepblue.lib.umich.edu/bitstream/handle/2027.42/35292/10373_ftp.pdf?sequence=1 Name that Tune: A Pilot Study in Finding a Melody from a Sung Query], article by Bryan Pardo, Jonah Shifrin, and William Birmingham, Journal of the American Society for Information Science and Technology, vol. 55 (4), pp. 283-300, 2004\n\n[[Category:Music search engines]]\n[[Category:Acoustic fingerprinting]]'
p282
sg6
S'Query by humming'
p283
ssI106
(dp284
g2
S'http://en.wikipedia.org/wiki/Policy framework'
p285
sg4
S"{{refimprove|date=March 2009}}\nA '''policy framework''' is a logical structure that is established to organize policy documentation into groupings and categories that make it easier for employees to find and understand the contents of various [[policy]] documents. Policy frameworks can also be used to help in the planning and development of the policies for an organization.\n\n==Principles==\n[[State Services Commission]] of [[New Zealand]] outlines eleven principles of policy framework as below.<ref>http://www.ssc.govt.nz/Documents/policy_framework_for_Government_.htm</ref>\n\n===Availability===\nGovernment departments should make information available easily, widely and equitably to the people of New Zealand (except where reasons preclude such availability as specified in legislation).....\n\n===Coverage===\nGovernment departments should make the following information increasingly available on an electronic basis:\n* all published material or material already in the public domain\n* all policies that could be released publicly\n* all information created or collected on a statutory basis (subject to commercial sensitivity and privacy considerations)\n* all documents that the public may be required to complete\n* corporate documentation in which the public would be interested\n\n===Pricing=== \na) Free dissemination of Government-held information is appropriate where:\n* dissemination to a target audience is desirable for a public policy purpose, or\n* a charge to recover the cost of dissemination is not feasible or cost-effective\n\nb) Pricing to recover the cost of dissemination is appropriate where:\n* there is no particular public policy reason to disseminate the information, and \n* a charge to recover the cost of dissemination is both feasible and cost effective\n\nc) Pricing to recover the cost of transformation is appropriate where:\n* pricing to recover the cost of dissemination is appropriate, and\n* there is an avoidable cost involved in transforming the information from the form in which it is held into a form preferred by the recipient, where it is feasible and cost-effective to recover in addition to the cost of dissemination\n\nd) Pricing to recover the full costs of information production and dissemination is appropriate where:\n* the information is created for the commercial purpose of sale at a profit, and \n* to do so would not breach the other pricing principles\n\n===Ownership===\nGovernment-held information, created or collected by any person employed or engaged by the Crown is a strategic resource 'owned' by the Government as a steward on behalf of the public.\n\n===Stewardship===\nGovernment departments are stewards of Government-held information, and it is their responsibility to implement good information management.\n\n===Collection===\nGovernment departments should only collect information for specified public policy, operational business or legislative purposes.\n\n===Copyright===\nInformation created by departments is subject to Crown copyright but where wide dissemination is desirable, the Crown should permit use of its copyrights subject to acknowledgement of source.\n \n===Preservation===\nGovernment-held information should be preserved only where a public business need, legislative or policy requirement, or a historical or archival reason, exists.\n\n===Quality===\nThe key qualities underpinning Government-held information include accuracy, relevancy, timeliness, consistency and collection without bias so that the information supports the purposes for which it is collected.\n\n===Integrity===\nThe integrity of Government-held information will be achieved when:\n* all guarantees and conditions surrounding the information are met\n* the principles are clear and communicated\n* any situation relating to Government-held information is handled openly and consistently\n* those affected by changes to Government-held information are consulted on those changes\n* those charged as independent guardians of the public interest  (e.g. the Ombudsman) have confidence in the ability of departments to manage the information well\n* there are minimum exceptions to the principles.\n\n===Privacy===\nThe principles of the Privacy Act 1993 apply.\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Policy Framework}}\n[[Category:Information retrieval]]\n[[Category:Government of New Zealand]]"
p286
sg6
S'Policy framework'
p287
ssI235
(dp288
g2
S'http://en.wikipedia.org/wiki/SensMe'
p289
sg4
S'{{Refimprove|date=April 2010}}\n{{Infobox Software|\n|name                   = SensMe\n|logo                   = [[File:SensMelogo.png|center|64px|SensMe Logo]]\n|screenshot             = [[File:SensMe.jpg|200px|center]]\n|caption                = Screenshot of SensMe on Media Go\n|developer              = [[Sony Corporation]]\n|genre                  = Mood detection software for music data.\n|platform               = [[Sony Walkman]] MP3/4 players<br />[[Sony Ericsson]]<br />[[Media Go]]<br />[[PlayStation Portable]]\n|license                = [[Proprietary software|Proprietary]]\n|website                = [http://www.sonyericsson.com/cws/support/phones/detailed/whatissenseme/w910i?cc=gb&lc=en http://www.sonyericsson.com]\n}}\n\n\'\'\'SensMe\'\'\' is a [[Proprietary software|proprietary]] music mood and tempo detection system created by [[Sony|Sony Corporation]], and employed in numerous Sony branded products, most notably the [[Walkman]] MP3/MP4 players (E <ref>[http://presscentre.sony.eu/content/detail.aspx?ReleaseID=6052&NewsAreaId=2 Your cool, colourful music partner Feature-packed WALKMAN\xc2\xae E450 Video MP3 player from Sony with premium sound for young music fans (15 July 2010 )]</ref> and S series<ref>[http://presscentre.sony.eu/content/detail.aspx?ReleaseID=6203&NewsAreaId=2 Sony introduces super-slim WALKMAN\xc2\xae S750 (15 September 2010)]</ref>), [[Media Go]], [[PlayStation Portable]], and [[Sony Ericsson]] phone series, .\n\n==Technical specifications==\n\n\'\'SensMe\'\' works by mapping music to a dual axis map based on the mood and tempo of music tracks.<ref>What is SensMe? http://www.sonyericsson.com/cws/support/phones/detailed/whatissenseme/w980</ref> Mood and tempo is determined by using the appropriate Sony compatible software which analyzes music tracks individually and computes the relevant track information. Analyzed tracks can then be plotted onto an intuitive dual axis map through which the music library on the device can be navigated, and playlists can be generated based on relative speed and mood. The horizontal axis is based on mood and the vertical axis is based on [[tempo]].\n\n==PlayStation Portable==\n\nSensMe was made available on the PlayStation Portable as of system software version 6.10.<ref name="update610" /> It can be downloaded via the [[XrossMediaBar|XMB]] or by using a computer.<ref name="pspdownload">SensMe PSP Download http://www.playstation.com/psp-app/sensme/en/</ref> The application features twelve channels by which music is categorized. These include Favorites, Newly Added, Dance, Extreme, Lounge, Emotional, Mellow, Upbeat, Relax, Energetic, Morning/Day/Night/Midnight, and Shuffle All.\n\n===PlayStation Portable Version History===\n{| class="wikitable"\n!width="180"|Version<br> Release date (UTC)\n!class="unsortable"|Description\n|-\n|align=center|\'\'\'1.50\'\'\'<br>March 31, 2010\n|\n* Music tracks transferred using a PlayStation 3 system or music management application other than Media Go are now also categorized into channels.\n* Users can now add music tracks to a block list so they do not play.\n* Users can now activate or deactivate the [Dynamic Normalizer] feature.\n|-\n|align=center|\'\'\'1.01\'\'\'<br>October 22, 2009\n|\n* Descriptions of some menu items in some languages have been revised.\n|-\n|align=center|\'\'\'1.00\'\'\'<br>October 1, 2009\n|\n* Initial release.\n|}\n\n==SensMe compatible products==\n* [[Walkman]]\n* [[Media Go]]\n* [[PlayStation Portable]]<ref name="update610">PSP Firmware Update (v6.10) http://blog.us.playstation.com/2009/09/psp-firmware-update-v6-10/</ref>\n\n[[File:Sony Ericsson W760i running SensMe.JPG|thumb|right|Screenshot of SensMe on a Sony Ericsson]]\n\n===Sony Ericsson handsets===\n*\'\'[[Sony Ericsson Aino|Aino]]\'\'\n*\'\'[http://www.sony.co.uk/product/nws-s-series/nwz-s639f Sony NWZ-S639F Media Player]\'\'\n*\'\'[[Sony Ericsson Elm|elm]]\'\'\n*\'\'[[Sony Ericsson W380|W380]]\'\'\n*\'\'[[Sony Ericsson W518a|W508]]\'\'\n*\'\'[[Sony Ericsson W518a|W518a]]\'\'\n*\'\'[[Sony Ericsson W595|W595]]\'\'\n*\'\'[[Sony Ericsson W705|W705]]\'\'\n*\'\'[[Sony Ericsson W705|W715]]\'\'\n*\'\'[[Sony Ericsson W760|W760]]\'\'\n*\'\'[[Sony Ericsson W890i|W890i]]\'\'\n*\'\'[[Sony Ericsson W902|W902]]\'\'\n*\'\'[[Sony Ericsson W910|W910i]]\'\'\n*\'\'[[Sony Ericsson W980|W980]]\'\'\n*\'\'[[Sony Ericsson W995|W995]]\'\'\n*\'\'[[Sony Ericsson Xperia X10|Xperia X10]]\'\'\n*\'\'[[Sony Ericsson Xperia Neo|Xperia Neo]]\'\'\n*\'\'[[Sony Ericsson Xperia Play|Xperia Play]]\'\'\n*\'\'[[Sony Ericsson Xperia ray|Xperia Ray]]\'\'\n===Sony handsets===\n*\'\'[[Sony Xperia E|Xperia E]]\'\'\n*\'\'[[Sony Xperia M|Xperia M]]\'\'\n*\'\'[[Sony Xperia Sola|Xperia Sola]]\'\'\n* [[Sony_Xperia_L|Xperia L]]\n*\'\'[[Sony Xperia S|Xperia S]]\'\'\n*\'\'[[Sony Xperia P|Xperia P]]\'\'\n*\'\'[[Sony Xperia U|Xperia U]]\'\'\n*\'\'[[Sony Xperia T|Xperia T]]\'\'\n*\'\'[[Sony Xperia TX|Xperia TX]]\'\'\n*\'\'[[Sony Xperia TL|Xperia TL]]\'\'\n*\'\'[[Sony Xperia tipo|Xperia tipo]]\'\'\n*\'\'[[Sony Xperia T|Xperia Go]]\'\'\n*\'\'[[Sony Xperia V|Xperia V]]\'\'\n*\'\'[[Sony Xperia Z|Xperia Z]]\'\'\n*\'\'[[Sony Xperia Z1|Xperia Z1]]\'\'\n*\'\'[[Sony Xperia Z1 Compact|Xperia Z1 Compact]]\'\'\n*\'\'[[Sony Xperia Z Ultra|Xperia Z Ultra]]\'\'\n*\'\'[[Sony Xperia Z1f|Xperia Z1f/Z1s]]\'\'\n*\'\'[[Sony Xperia ZL|Xperia ZL]]\'\'\n*\'\'[[Sony Xperia ZL|Xperia SP]]\'\'\n*\'\'[[Sony Xperia Z2|Xperia Z2]]\'\'\n*\'\'[[Sony Xperia Z3|Xperia Z3]]\'\'\n*\'\'[[Sony Xperia Z3 Compact|Xperia Z3 Compact]]\'\'\n*\'\'[http://www.sonyericsson.com/cws/products/mobilephones/overview/zylo?cc=ph&lc=en#view=features_specifications Zylo (W20i)]\'\'\n\n==References==\n{{reflist}}\n\n[[Category:Sony software]]\n[[Category:Music search engines]]'
p290
sg6
S'SensMe'
p291
ssI109
(dp292
g2
S'http://en.wikipedia.org/wiki/Stemming'
p293
sg4
S'{{Expert-subject|date=October 2010}}\n{{for|the skiing technique|Stem (skiing)}}\n\'\'\'Stemming\'\'\' is the term used in [[linguistic morphology]] and [[information retrieval]] to describe the process for reducing inflected (or sometimes derived) words to their [[word stem]], base or [[root (linguistics)|root]] form\xe2\x80\x94generally a written word form. The stem needs not to be identical to the [[morphological root]] of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. [[Algorithm]]s for stemming have been studied in [[computer science]] since the 1960s. Many [[search engine]]s treat words with the same stem as [[synonym]]s as a kind of [[query expansion]], a process called conflation.\n\nStemming programs are commonly referred to as stemming algorithms or stemmers.\n\n==Examples==\nA stemmer for English, for example, should identify the [[string literal|string]] "cats" (and possibly "catlike", "catty" etc.) as based on the root "cat", and "stemmer", "stemming", "stemmed" as based on "stem". A stemming algorithm reduces the words "fishing", "fished", and "fisher" to the root word, "fish". On the other hand, "argue", "argued", "argues", "arguing", and "argus" reduce to the stem "argu" (illustrating the case where the stem is not itself a word or root) but "argument" and "arguments" reduce to the stem "argument".<!-- using the Porter algorithm -->\n\n==History==\nThe first published stemmer was written by [[Julie Beth Lovins]] in 1968.<ref>{{cite journal |first=Julie Beth |last=Lovins |year=1968 |title=Development of a Stemming Algorithm |journal=Mechanical Translation and Computational Linguistics |volume=11 |pages=22\xe2\x80\x9331 }}</ref> This paper was remarkable for its early date and had great influence on later work in this area.\n\nA later stemmer was written by [[Martin Porter]] and was published in the July 1980 issue of the journal \'\'Program\'\'. This stemmer was very widely used and became the de facto standard algorithm used for English stemming. Dr. Porter received the [[Tony Kent Strix award]] in 2000 for his work on stemming and information retrieval.\n\nMany implementations of the Porter stemming algorithm were written and freely distributed; however, many of these implementations contained subtle flaws. As a result, these stemmers did not match their potential. To eliminate this source of error, Martin Porter released an official [http://tartarus.org/~martin/PorterStemmer/ free-software implementation] of the algorithm around the year 2000. He extended this work over the next few years by building [[Snowball programming language|Snowball]], a framework for writing stemming algorithms, and implemented an improved English stemmer together with stemmers for several other languages.\n\n==Algorithms==\nThere are several types of stemming algorithms which differ in respect to performance and accuracy and how certain stemming obstacles are overcome.\n\n===Lookup algorithms===\nA simple stemmer looks up the inflected form in a [[lookup table]]. The advantages of this approach is that it is simple, fast, and easily handles exceptions. The disadvantages are that all inflected forms must be explicitly listed in the table: new or unfamiliar words are not handled, even if they are perfectly regular (e.g. iPads ~ iPad), and the table may be large. For languages with simple morphology, like English, table sizes are modest, but highly inflected languages like Turkish may have hundreds of potential inflected forms for each root.\n\nA lookup approach may use preliminary part-of-speech tagging to avoid overstemming.<ref>Yatsko, V. A.; [http://yatsko.zohosites.com/y-stemmer.html \'\'Y-stemmer\'\']</ref>\n\n====The production technique====\nThe lookup table used by a stemmer is generally produced semi-automatically. For example, if the word is "run", then the inverted algorithm might automatically generate the forms "running", "runs", "runned", and "runly". The last two forms are valid constructions, but they are unlikely.\n\n===Suffix-stripping algorithms===\nSuffix stripping algorithms do not rely on a lookup table that consists of inflected forms and root form relations. Instead, a typically smaller list of "rules" is stored which provides a path for the algorithm, given an input word form, to find its root form. Some examples of the rules include:\n* if the word ends in \'ed\', remove the \'ed\'\n* if the word ends in \'ing\', remove the \'ing\'\n* if the word ends in \'ly\', remove the \'ly\'\n\nSuffix stripping approaches enjoy the benefit of being much simpler to maintain than brute force algorithms, assuming the maintainer is sufficiently knowledgeable in the challenges of linguistics and morphology and encoding suffix stripping rules. Suffix stripping algorithms are sometimes regarded as crude given the poor performance when dealing with exceptional relations (like \'ran\' and \'run\'). The solutions produced by suffix stripping algorithms are limited to those [[lexical category|lexical categories]] which have well known suffixes with few exceptions. This, however, is a problem, as not all parts of speech have such a well formulated set of rules. Lemmatisation attempts to improve upon this challenge.\n\nPrefix stripping may also be implemented. Of course, not all languages use prefixing or suffixing.\n\n====Additional algorithm criteria====\nSuffix stripping algorithms may differ in results for a variety of reasons. One such reason is whether the algorithm constrains whether the output word must be a real word in the given language. Some approaches do not require the word to actually exist in the language lexicon (the set of all words in the language). Alternatively, some suffix stripping approaches maintain a database (a large list) of all known morphological word roots that exist as real words. These approaches check the list for the existence of the term prior to making a decision. Typically, if the term does not exist, alternate action is taken. This alternate action may involve several other criteria. The non-existence of an output term may serve to cause the algorithm to try alternate suffix stripping rules.\n\nIt can be the case that two or more suffix stripping rules apply to the same input term, which creates an ambiguity as to which rule to apply. The algorithm may assign (by human hand or stochastically) a priority to one rule or another. Or the algorithm may reject one rule application because it results in a non-existent term whereas the other overlapping rule does not. For example, given the English term \'\'friendlies\'\', the algorithm may identify the \'\'ies\'\' suffix and apply the appropriate rule and achieve the result of \'\'friendl\'\'. \'\'friendl\'\' is likely not found in the lexicon, and therefore the rule is rejected.\n\nOne improvement upon basic suffix stripping is the use of suffix substitution. Similar to a stripping rule, a substitution rule replaces a suffix with an alternate suffix. For example, there could exist a rule that replaces \'\'ies\'\' with \'\'y\'\'. How this affects the algorithm varies on the algorithm\'s design. To illustrate, the algorithm may identify that both the \'\'ies\'\' suffix stripping rule as well as the suffix substitution rule apply. Since the stripping rule results in a non-existent term in the lexicon, but the substitution rule does not, the substitution rule is applied instead. In this example, \'\'friendlies\'\' becomes \'\'friendly\'\' instead of \'\'friendl\'\'.\n\nDiving further into the details, a common technique is to apply rules in a cyclical fashion (recursively, as computer scientists would say). After applying the suffix substitution rule in this example scenario, a second pass is made to identify matching rules on the term \'\'friendly\'\', where the \'\'ly\'\' stripping rule is likely identified and accepted. In summary, \'\'friendlies\'\' becomes (via substitution) \'\'friendly\'\' which becomes (via stripping) \'\'friend\'\'.\n\nThis example also helps illustrate the difference between a rule-based approach and a brute force approach. In a brute force approach, the algorithm would search for \'\'friendlies\'\' in the set of hundreds of thousands of inflected word forms and ideally find the corresponding root form \'\'friend\'\'. In the rule-based approach, the three rules mentioned above would be applied in succession to converge on the same solution. Chances are that the rule-based approach would be slower, as lookup algorithms have a direct access to the solution, while rule-based should try several options, and combinations of them, and then choose which result seems to be the best.\n\n===Lemmatisation algorithms===\nA more complex approach to the problem of determining a stem of a word is [[lemmatisation]]. This process involves first determining the [[part of speech]] of a word, and applying different normalization rules for each part of speech. The part of speech is first detected prior to attempting to find the root since for some languages, the stemming rules change depending on a word\'s part of speech.\n\nThis approach is highly conditional upon obtaining the correct lexical category (part of speech). While there is overlap between the normalization rules for certain categories, identifying the wrong category or being unable to produce the right category limits the added benefit of this approach over suffix stripping algorithms. The basic idea is that, if the stemmer is able to grasp more information about the word being stemmed, then it can apply more accurate normalization rules (which unlike suffix stripping rules can also modify the stem).\n\n===Stochastic algorithms===\n[[Stochastic]] algorithms involve using probability to identify the root form of a word. Stochastic algorithms are trained (they "learn") on a table of root form to inflected form relations to develop a probabilistic model. This model is typically expressed in the form of complex linguistic rules, similar in nature to those in suffix stripping or lemmatisation. Stemming is performed by inputting an inflected form to the trained model and having the model produce the root form according to its internal ruleset, which again is similar to suffix stripping and lemmatisation, except that the decisions involved in applying the most appropriate rule, or whether or not to stem the word and just return the same word, or whether to apply two different rules sequentially, are applied on the grounds that the output word will have the highest probability of being correct (which is to say, the smallest probability of being incorrect, which is how it is typically measured).\n\nSome lemmatisation algorithms are stochastic in that, given a word which may belong to multiple parts of speech, a probability is assigned to each possible part. This may take into account the surrounding words, called the context, or not. Context-free grammars do not take into account any additional information. In either case, after assigning the probabilities to each possible part of speech, the most likely part of speech is chosen, and from there the appropriate normalization rules are applied to the input word to produce the normalized (root) form.\n\n===\'\'n\'\'-gram analysis===\nSome stemming techniques use the [[n-gram]] context of a word to choose the correct stem for a word.<ref name="Workshop2006">{{cite book|author=Cross-Language Evaluation Forum. Workshop|title=Accessing Multilingual Information Repositories: 6th Workshop of the Cross-Language Evaluation Forum, CLEF 2005, Vienna, Austria, 21-23 September, 2005, Revised Selected Papers|url=http://books.google.com/books?id=jWKzNGr1H0AC&pg=PA159|date=20 September 2006|publisher=Springer|isbn=978-3-540-45697-1|pages=159\xe2\x80\x93}}</ref>\n\n===Hybrid approaches===\nHybrid approaches use two or more of the approaches described above in unison. A simple example is a [[probabilistic suffix tree|suffix tree]] algorithm which first consults a lookup table using brute force. However, instead of trying to store the entire set of relations between words in a given language, the lookup table is kept small and is only used to store a minute amount of "frequent exceptions" like "ran => run". If the word is not in the exception list, apply suffix stripping or lemmatisation and output the result.\n\n===Affix stemmers===\nIn [[linguistics]], the term [[affix]] refers to either a [[prefix]] or a [[suffix]]. In addition to dealing with suffixes, several approaches also attempt to remove common prefixes. For example, given the word \'\'indefinitely\'\', identify that the leading "in" is a prefix that can be removed. Many of the same approaches mentioned earlier apply, but go by the name \'\'\'affix stripping\'\'\'. A study of affix stemming for several European languages can be found here.<ref>Jongejan, B.; and Dalianis, H.; \'\'Automatic Training of Lemmatization Rules that Handle Morphological Changes in pre-, in- and Suffixes Alike\'\', in the \'\'Proceeding of the ACL-2009, Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Singapore, August 2\xe2\x80\x937, 2009\'\', pp. 145-153\n[http://www.aclweb.org/anthology/P/P09/P09-1017.pdf]</ref>\n\n===Matching algorithms===\nSuch algorithms use a stem database (for example a set of documents that contain stem words). These stems, as mentioned above, are not necessarily valid words themselves (but rather common sub-strings, as the "brows" in "browse" and in "browsing"). In order to stem a word the algorithm tries to match it with stems from the database, applying various constraints, such as on the relative length of the candidate stem within the word (so that, for example, the short prefix "be", which is the stem of such words as "be", "been" and "being", would not be considered as the stem of the word "beside").\n\n==Language challenges==\nWhile much of the early academic work in this area was focused on the English language (with significant use of the Porter Stemmer algorithm), many other languages have been investigated.<ref>Dolamic, Ljiljana; and Savoy, Jacques; [http://clef.isti.cnr.it/2007/working_notes/DolamicCLEF2007.pdf \'\'Stemming Approaches for East European Languages (CLEF 2007)\'\']</ref><ref>Savoy, Jacques; [http://portal.acm.org/citation.cfm?doid=1141277.1141523 \'\'Light Stemming Approaches for the French, Portuguese, German and Hungarian Languages\'\'], ACM Symposium on Applied Computing, SAC 2006, ISBN 1-59593-108-2</ref><ref>Popovi\xc4\x8d, Mirko; and Willett, Peter (1992); [http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199206%2943:5%3C384::AID-ASI6%3E3.0.CO;2-L/abstract \'\'The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data\'\'], Journal of the [[American Society for Information Science]], Volume 43, Issue 5 (June), pp. 384\xe2\x80\x93390</ref><ref>[http://staff.science.uva.nl/~mdr/Publications/Files/clef2005-proc-adhoc.pdf \'\'Stemming in Hungarian at CLEF 2005\'\']</ref><ref>Viera, A. F. G. & Virgil, J. (2007); [http://InformationR.net/ir/12-3/paper315.html \'\'Uma revis\xc3\xa3o dos algoritmos de radicaliza\xc3\xa7\xc3\xa3o em l\xc3\xadngua portuguesa\'\'], Information Research, 12(3), paper 315</ref>\n\nHebrew and Arabic are still considered difficult research languages for stemming. English stemmers are fairly trivial (with only occasional problems, such as "dries" being the third-person singular present form of the verb "dry", "axes" being the plural of "axe" as well as "axis"); but stemmers become harder to design as the morphology, orthography, and character encoding of the target language becomes more complex. For example, an Italian stemmer is more complex than an English one (because of a greater number of verb inflections), a Russian one is more complex (more noun [[declension]]s), a Hebrew one is even more complex (due to [[nonconcatenative morphology]], a writing system without vowels, and the requirement of prefix stripping: Hebrew stems can be two, three or four characters, but not more), and so on.\n\n===Multilingual stemming===\nMultilingual stemming applies morphological rules of two or more languages simultaneously instead of rules for only a single language when interpreting a search query. Commercial systems using multilingual stemming exist{{CN|date=October 2013}}.\n\n==Error metrics==\nThere are two error measurements in stemming algorithms, overstemming and understemming. Overstemming is an error where two separate inflected words are stemmed to the same root, but should not have been\xe2\x80\x94a [[false positive]]. Understemming is an error where two separate inflected words should be stemmed to the same root, but are not\xe2\x80\x94a [[false negative]]. Stemming algorithms attempt to minimize each type of error, although reducing one type can lead to increasing the other.\n\nFor example, the widely used Porter stemmer stems "universal", "university", and "universe" to "univers". This is a case of overstemming: though these three words are etymologically related, their modern meanings are in widely different domains, so treating them as synonyms in a search engine will likely reduce the relevance of the search results.\n\nAn example of understemming in the Porter stemmer is "alumnus" \xe2\x86\x92 "alumnu", "alumni" \xe2\x86\x92 "alumni", "alumna"/"alumnae" \xe2\x86\x92 "alumna".  This English word keeps Latin morphology, and so these near-synonyms are not conflated.\n\n==Applications==\nStemming is used as an approximate method for grouping words with a similar basic meaning together. For example, a text mentioning "daffodils" is probably closely related to a text mentioning "daffodil" (without the s). But in some cases, words with the same morphological stem have [[idiom]]atic meanings which are not closely related: a user searching for "marketing" will not be satisfied by most documents mentioning "markets" but not "marketing".\n\n===Information retrieval===\nStemmers are common elements in [[Information Retrieval|query systems]] such as [[World Wide Web|Web]] [[search engine]]s. The effectiveness of stemming for English query systems were soon found to be rather limited, however, and this has led early [[information retrieval]] researchers to deem stemming irrelevant in general.<ref>Baeza-Yates, Ricardo; and Ribeiro-Neto, Berthier (1999); \'\'Modern Information Retrieval\'\', ACM Press/Addison Wesley</ref> An alternative approach, based on searching for [[n-gram]]s rather than stems, may be used instead. Also, recent research has shown greater benefits for retrieval in other languages.<ref>Kamps, Jaap; Monz, Christof; de Rijke, Maarten; and Sigurbj\xc3\xb6rnsson, B\xc3\xb6rkur (2004); \'\'Language-Dependent and Language-Independent Approaches to Cross-Lingual Text Retrieval\'\', in Peters, C.; Gonzalo, J.; Braschler, M.; and Kluck, M. (eds.); \'\'Comparative Evaluation of Multilingual Information Access Systems\'\', Springer Verlag, pp. 152\xe2\x80\x93165</ref><ref>Airio, Eija (2006); \'\'Word Normalization and Decompounding in Mono- and Bilingual IR\'\', Information Retrieval \'\'\'9\'\'\':249\xe2\x80\x93271</ref>\n\n===Domain Analysis===\nStemming is used to determine domain vocabularies in [[domain analysis]].\n<ref>Frakes, W.; Prieto-Diaz, R.; & Fox, C. (1998); \'\'DARE: Domain Analysis and Reuse Environment\'\', Annals of Software Engineering (5), pp. 125-141</ref>\n\n===Use in commercial products===\nMany commercial companies have been using stemming since at least the 1980s and have produced algorithmic and lexical stemmers in many languages.<ref>[http://www.dtsearch.co.uk/language.htm \'\'Language Extension Packs\'\'], dtSearch</ref><ref>[http://technet2.microsoft.com/Office/en-us/library/87065c9d-d39d-479d-909b-02160ec6d7791033.mspx?mfr=true \'\'Building Multilingual Solutions by using Sharepoint Products and Technologies\'\'], Microsoft Technet</ref>\n\nThe Snowball stemmers have been compared with commercial lexical stemmers with varying results.<ref>[http://clef.isti.cnr.it/2003/WN_web/19.pdf CLEF 2003: Stephen Tomlinson compared the Snowball stemmers with the Hummingbird lexical stemming (lemmatization) system]</ref><ref>[http://clef.isti.cnr.it/2004/working_notes/WorkingNotes2004/21.pdf CLEF 2004: Stephen Tomlinson "Finnish, Portuguese and Russian Retrieval with Hummingbird SearchServer"]</ref>\n\n[[Google search]] adopted word stemming in 2003.<ref>[http://www.google.com/support/bin/static.py?page=searchguides.html&ctx=basics#stemming \'\'The Essentials of Google Search\'\'], Web Search Help Center, [[Google|Google Inc.]]</ref> Previously a search for "fish" would not have returned "fishing". Other software search algorithms vary in their use of word stemming. Programs that simply search for substrings obviously will find "fish" in "fishing" but when searching for "fishes" will not find occurrences of the word "fish".\n\n==See also==\n* [[Root (linguistics)]] - linguistic definition of the term "root"\n* [[Stem (linguistics)]] - linguistic definition of the term "stem"\n* [[Morphology (linguistics)]]\n* [[Lemma (morphology)]] - linguistic definition\n* [[Lemmatization]]\n* [[Lexeme]]\n* [[Inflection]]\n* [[Derivation (linguistics)|Derivation]] - stemming is a form of reverse derivation\n* [[Natural language processing]] - stemming is generally regarded as a form of NLP\n* [[Text mining]] - stemming algorithms play a major role in commercial NLP software\n* [[Computational linguistics]]\n\n{{Natural Language Processing}}\n\n==References==\n{{reflist|2}}\n\n==Further reading==\n{{refbegin|2}}\n* Dawson, J. L. (1974); \'\'Suffix Removal for Word Conflation\'\', Bulletin of the Association for Literary and Linguistic Computing, 2(3): 33\xe2\x80\x9346\n* Frakes, W. B. (1984); \'\'Term Conflation for Information Retrieval\'\', Cambridge University Press\n* Frakes, W. B. & Fox, C. J. (2003); \'\'Strength and Similarity of Affix Removal Stemming Algorithms\'\', SIGIR Forum, 37: 26\xe2\x80\x9330\n* Frakes, W. B. (1992); \'\'Stemming algorithms, Information retrieval: data structures and algorithms\'\', Upper Saddle River, NJ: Prentice-Hall, Inc.\n* Hafer, M. A. & Weiss, S. F. (1974); \'\'Word segmentation by letter successor varieties\'\', Information Processing & Management 10 (11/12), 371\xe2\x80\x93386\n* Harman, D. (1991); \'\'How Effective is Suffixing?\'\', Journal of the American Society for Information Science 42 (1), 7\xe2\x80\x9315\n* Hull, D. A. (1996); \'\'Stemming Algorithms&nbsp;\xe2\x80\x93 A Case Study for Detailed Evaluation\'\', JASIS, 47(1): 70\xe2\x80\x9384\n* Hull, D. A. & Grefenstette, G. (1996); \'\'A Detailed Analysis of English Stemming Algorithms\'\', Xerox Technical Report\n* Kraaij, W. & Pohlmann, R. (1996); \'\'Viewing Stemming as Recall Enhancement\'\', in Frei, H.-P.; Harman, D.; Schauble, P.; and Wilkinson, R. (eds.); \'\'Proceedings of the 17th ACM SIGIR conference held at Zurich, August 18\xe2\x80\x9322\'\', pp.&nbsp;40\xe2\x80\x9348\n* Krovetz, R. (1993); \'\'Viewing Morphology as an Inference Process\'\', in \'\'Proceedings of ACM-SIGIR93\'\', pp.&nbsp;191\xe2\x80\x93203\n* Lennon, M.; Pierce, D. S.; Tarry, B. D.; & Willett, P. (1981); \'\'An Evaluation of some Conflation Algorithms for Information Retrieval\'\', Journal of Information Science, 3: 177\xe2\x80\x93183\n* Lovins, J. (1971); \'\'[http://www.eric.ed.gov/sitemap/html_0900000b800c571a.html Error Evaluation for Stemming Algorithms as Clustering Algorithms]\'\', JASIS, 22: 28\xe2\x80\x9340\n* Lovins, J. B. (1968); \'\'Development of a Stemming Algorithm\'\', Mechanical Translation and Computational Linguistics, 11, 22\xe2\x80\x9431\n* Jenkins, Marie-Claire; and Smith, Dan (2005); [http://www.uea.ac.uk/polopoly_fs/1.85493!stemmer25feb.pdf \'\'Conservative Stemming for Search and Indexing\'\']\n* Paice, C. D. (1990); \'\'[http://www.comp.lancs.ac.uk/computing/research/stemming/paice/article.htm Another Stemmer]\'\', SIGIR Forum, 24: 56\xe2\x80\x9361\n* Paice, C. D. (1996) \'\'[http://www3.interscience.wiley.com/cgi-bin/abstract/57804/ABSTRACT Method for Evaluation of Stemming Algorithms based on Error Counting]\'\', JASIS, 47(8): 632\xe2\x80\x93649\n* Popovi\xc4\x8d, Mirko; and Willett, Peter (1992); [http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199206%2943:5%3C384::AID-ASI6%3E3.0.CO;2-L/abstract \'\'The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data\'\'], Journal of the [[American Society for Information Science]], Volume 43, Issue 5 (June), pp.&nbsp;384\xe2\x80\x93390\n* Porter, Martin F. (1980); \'\'[http://telemat.det.unifi.it/book/2001/wchange/download/stem_porter.html An Algorithm for Suffix Stripping]\'\', Program, 14(3): 130\xe2\x80\x93137\n* Savoy, J. (1993); \'\'[http://www3.interscience.wiley.com/cgi-bin/abstract/10049824/ABSTRACT?CRETRY=1&SRETRY=0 Stemming of French Words Based on Grammatical Categories]\'\' Journal of the American Society for Information Science, 44(1), 1\xe2\x80\x939\n* Ulmschneider, John E.; & Doszkocs, Tamas (1983); \'\'[http://www.eric.ed.gov/sitemap/html_0900000b8007ea83.html A Practical Stemming Algorithm for Online Search Assistance]\'\', Online Review, 7(4), 301\xe2\x80\x93318\n* Xu, J.; & Croft, W. B. (1998); \'\'[http://portal.acm.org/citation.cfm?doid=267954.267957 Corpus-Based Stemming Using Coocurrence of Word Variants]\'\', ACM Transactions on Information Systems, 16(1), 61\xe2\x80\x9381\n{{refend}}\n\n==External links==\n*[http://opennlp.apache.org/index.html Apache OpenNLP] includes Porter and Snowball stemmers\n* [http://smile-stemmer.appspot.com SMILE Stemmer] - free online service, includes Porter and Paice/Husk\' Lancaster stemmers (Java API)\n* [http://code.google.com/p/ir-themis/ Themis] - open source IR framework, includes Porter stemmer implementation (PostgreSQL, Java API)\n* [http://snowball.tartarus.org Snowball] - free stemming algorithms for many languages, includes source code, including stemmers for five romance languages\n* [http://www.iveonik.com/blog/2011/08/snowball-stemmers-on-csharp-free-download/ Snowball on C#] - port of Snowball stemmers for C# (14 languages)\n* [http://snowball.tartarus.org/wrappers/guide.html Python bindings to Snowball API]\n* [http://locknet.ro/archive/2009-10-29-ann-ruby-stemmer.html Ruby-Stemmer] - Ruby extension to Snowball API\n* [http://pecl.php.net/package/stem/ PECL] - PHP extension to the Snowball API\n* [http://www.oleandersolutions.com/stemming.html Oleander Porter\'s algorithm] - stemming library in C++ released under BSD\n* [http://www.cs.waikato.ac.nz/~eibe/stemmers/index.html Unofficial home page of the Lovins stemming algorithm] - with source code in a couple of languages\n* [http://www.tartarus.org/~martin/PorterStemmer/index.html Official home page of the Porter stemming algorithm] - including source code in several languages\n* [http://www.comp.lancs.ac.uk/computing/research/stemming/index.htm Official home page of the Lancaster stemming algorithm] - Lancaster University, UK\n* [http://www.cmp.uea.ac.uk/Research/stemmer/ Official home page of the UEA-Lite Stemmer ] - University of East Anglia, UK\n* [http://www.comp.lancs.ac.uk/computing/research/stemming/general/index.htm Overview of stemming algorithms]\n* [http://code.google.com/p/ptstemmer/ PTStemmer] - A Java/Python/.Net stemming toolkit for the Portuguese language\n* [http://mazko.github.com/jssnowball/ jsSnowball] - open source JavaScript implementation of Snowball stemming algorithms for many languages\n* [http://trimc-nlp.blogspot.com/2013/08/snowball-stemmer-for-java.html Snowball Stemmer] - implementation for Java\n* [http://hlt.di.fct.unl.pt/luis/hindi_stemmer/ hindi_stemmer] - open source stemmer for Hindi\n* [http://hlt.di.fct.unl.pt/luis/czech_stemmer/ czech_stemmer] - open source stemmer for Czech\n* [http://www.comp.leeds.ac.uk/eric/sawalha08coling.pdf Comparative Evaluation of Arabic Language Morphological Analysers and Stemmers]\n* [https://github.com/rdamodharan/tamil-stemmer Tamil Stemmer]\n\n{{FOLDOC}}\n\n[[Category:Linguistic morphology]]\n[[Category:Natural language processing]]\n[[Category:Tasks of natural language processing]]\n[[Category:Computational linguistics]]\n[[Category:Information retrieval]]'
p294
sg6
S'Stemming'
p295
ssI238
(dp296
g2
S'http://en.wikipedia.org/wiki/MusicRadar (service)'
p297
sg4
S"{{multiple issues|\n{{advert|date=August 2013}}\n{{cleanup|reason=Syntax, capitals|date=August 2013}}\n{{fanpov|date=August 2013}}\n{{Orphan|date=August 2013}}\n}}\n{{Infobox Website\n|name=MusicRadar(\xe9\x9f\xb3\xe4\xb9\x90\xe9\x9b\xb7\xe8\xbe\xbe)\n|logo=\n|screenshot=\n|caption=\n|url=http://www.doreso.com/\n|commercial=Yes\n|type=[[Music]] [[website]]\n|registration=Optional\n|owner=Shanghai Yinlong Information Technology Co., LTD\n|author=Shanghai Yinlong Information Technology Co., LTD\n|launch date=January 2013\n|current status=\n|revenue=}}\n\n'''Music radar''' is a sound-to-sound music search engine, which allows users to obtain more detailed information of music/songs by singing/humming or by recording original music. It is available on [[App Store (iOS)|App Store]]<ref>https://itunes.apple.com/cn/app/yin-le-lei-da/id635262613</ref> for [[iPhone]] and [[Google Play]]<ref>https://play.google.com/store/apps/details?id=com.voicedragon.musicclient.googleplay</ref> for [[Android (operating system)|Android]] mobiles. Music radar was launched by Shanghai Yinlong Information Technology Co., LTD in Jan. 2013.<ref>http://www.doreso.com/</ref>\n\n==Features==\nThe app (MusicRadar) currently has three ways of searching music: by identifying recorded original music fragment; by humming or singing the melody using microphone; and by direct input of the name of song or singer. Users could share their searching results on [[Facebook]], [[Twitter]] or other SNS website.\n\n==History==\nThe music radar team got the 1st place on Query by Singing/Humming (QBSH) task, Music Information Retrieval Evaluation eXchange (MIREX) 2012.<ref>http://www.music-ir.org/mirex/wiki/2012:Main_Page</ref> The app was launched for business intention at the end of January 2013, supporting query by singing/humming & audio fingerprinting. After two months, the app has reached its first one million user milestone in April, 2013. In May 2013, Music radar announces that it has integrated deep learning techniques in its query by singing/humming module to promote the recognize rate and reduce the user\xe2\x80\x99s waiting time. In July 2013, Music radar released China's first cloud based music recognizing openAPI to public.<ref>[http://roll.sohu.com/20130731/n383076514.shtml 2013\xe5\xb9\xb47\xe6\x9c\x88\xef\xbc\x8c\xe9\x9f\xb3\xe4\xb9\x90\xe9\x9b\xb7\xe8\xbe\xbe\xe5\x8f\x91\xe5\xb8\x83\xe4\xba\x86\xe5\x9b\xbd\xe5\x86\x85\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe2\x80\x9c\xe9\x9f\xb3\xe9\xa2\x91\xe6\xa3\x80\xe7\xb4\xa2\xe5\xbc\x80\xe6\x94\xbe\xe4\xba\x91\xe5\xb9\xb3\xe5\x8f\xb0\xe2\x80\x9d\xef\xbc\x8c\xe6\x8f\x90\xe4\xbe\x9b\xe5\xbc\x80\xe6\x94\xbe\xe7\x9a\x84\xe9\x9f\xb3\xe9\xa2\x91\xe6\xa3\x80\xe7\xb4\xa2API\xe3\x80\x82]</ref>\n\n==References==\n{{reflist}}\n\n\n\n[[Category:Acoustic fingerprinting]]\n[[Category:Music search engines]]\n[[Category:IOS software]]\n[[Category:Android (operating system) software]]"
p298
sg6
S'MusicRadar (service)'
p299
ssI112
(dp300
g2
S'http://en.wikipedia.org/wiki/Literature-based discovery'
p301
sg4
S'\'\'\'Literature-based discovery\'\'\' refers to the use of papers and other [[Academic publishing|academic publications]] (the "literature") to find new relationships between existing knowledge (the "discovery"). The technique was pioneered by [[Don R. Swanson]] in the 1980s and has since seen widespread use. \n\nLiterature-based discovery does not generate new knowledge through laboratory experiments, as is customary for [[empirical]] sciences. Instead it seeks to connect existing knowledge from empirical results by bringing to light relationships that are implicated and "neglected".<ref>{{cite journal | last1 = Swanson | first1 = Don | year = 1988 | title = Migraine and Magnesium: Eleven Neglected Connections | url = | journal = Perspectives in Biology and Medicine | volume = 31 | issue = 4| pages = 526\xe2\x80\x93557 }}</ref> It is marked by [[empiricism]] and [[rationalism]] in concert or [[consilience]].\n\n==Swanson linking==\n[[File:Swanson linking.jpg|thumb|Swanson linking example diagram]]\n\'\'Swanson linking\'\' is a term proposed in 2003<ref>Stegmann J, Grohmann G. Hypothesis generation guided by co-word clustering. Scientometrics. 2003;56:111\xe2\x80\x93135. As quoted by Bekhuis</ref> that refers to connecting two pieces of knowledge previously thought to be unrelated.<ref>{{cite journal|last=Bekhuis|first=Tanja|title=Conceptual biology, hypothesis discovery, and text mining: Swanson\'s legacy|publisher=BioMed Central Ltd.|year=2006|pmc=1459187|pmid=16584552|doi=10.1186/1742-5581-3-2|volume=3|journal=Biomed Digit Libr|pages=2}}</ref> For example, it may be known that illness A is caused by chemical B, and that drug C is known to reduce the amount of chemical B in the body. However, because the respective articles were published separately from one another (called "disjoint data"), the relationship between illness A and drug C may be unknown. \'\'Swanson linking\'\' aims to find these relationships and report them.\n\n==See also==\n*[[Arrowsmith System]]\n*[[Implicature]]\n*[[Latent semantic indexing]]\n*[[Metaphor]]\n\n==References==\n* Chen, Ran; Hongfei Lin & Zhihao Yang (2011). "Passage retrieval based hidden knowledge discovery from biomedical literature." \'\'Expert Systems with Applications: An International Journal\'\' (August, 2011), vol. 38, no. 8, pp.&nbsp;9958\xe2\x80\x939964.\n*:  \'\'\'Abstract\'\'\': [...] automatic extraction of the implicit biological relationship from biomedical literature contributes to building the biomedical hypothesis that can be explored further experimentally. This paper presents a passage retrieval based method which can explore the hidden connection from MEDLINE records. [...] Experimental results show this method can significantly improve the hidden knowledge discovery performance. @ [http://portal.acm.org/citation.cfm?id=1967763.1968003&coll=DL&dl=GUIDE&CFID=23143258&CFTOKEN=52033794 ACM DL]\n\n; Further readings\n* [[Patrick Wilson (librarian)|Wilson, Patrick]] (1977). \'\'Public Knowledge, Private Ignorance: Toward a Library and Information Policy\'\'. Greenwood Publishing Group. p.&nbsp;156. ISBN 0-8371-9485-7.\n\n; Footnotes\n{{reflist}}\n\n[[Category:Information retrieval]]\n[[Category:Medical research]]\n\n\n{{science-stub}}'
p302
sg6
S'Literature-based discovery'
p303
ssI115
(dp304
g2
S'http://en.wikipedia.org/wiki/Latent semantic analysis'
p305
sg4
S'{{mergefrom|Latent semantic indexing|date=July 2012}}\n{{semantics}}\n\'\'\'Latent semantic analysis\'\'\' (\'\'\'LSA\'\'\') is a technique in [[natural language processing]], in particular in [[vectorial semantics]], of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.  LSA assumes that words that are close in meaning will occur in similar pieces of text.  A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called [[singular value decomposition]] (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.  Words are then compared by taking the cosine of the angle between the two vectors (or the [[dot product]] between the [[Unit vector|normalizations]] of the two vectors) formed by any two rows.  Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.<ref>{{cite journal | title=Latent Semantic Analysis | author=Susan T. Dumais |year=2005 | doi=10.1002/aris.1440380105 | journal=Annual Review of Information Science and Technology | volume=38 | pages=188}}</ref>\n\nAn information retrieval method using latent semantic structure was patented in 1988 ([http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=4839853 US Patent 4,839,853]) by [[Scott Deerwester]], [[Susan Dumais]], [[George Furnas]], [[Richard Harshman]], [[Thomas Landauer]], [[Karen Lochbaum]] and [[Lynn Streeter]]. In the context of its application to [[information retrieval]], it is sometimes called [[Latent semantic indexing|Latent Semantic Indexing \'\'\'(LSI)\'\'\']].<ref>{{cite web | url=http://lsa.colorado.edu/ | title=The Latent Semantic Indexing home page}}</ref>\n\n== Overview ==\n\n=== Occurrence matrix ===\nLSA can use a [[term-document matrix]] which describes the occurrences of terms in documents; it is a [[sparse matrix]] whose rows correspond to [[terminology|terms]] and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is [[tf-idf]] (term frequency\xe2\x80\x93inverse document frequency): the element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.\n\nThis matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.\n\n=== Rank lowering ===\nAfter the construction of the occurrence matrix, LSA finds a [[low-rank approximation]]<ref>Markovsky I. (2012) Low-Rank Approximation: Algorithms, Implementation, Applications, Springer, 2012, ISBN 978-1-4471-2226-5 {{page needed|date=January 2012}}</ref> to the [[term-document matrix]]. There could be various reasons for these approximations:\n\n* The original term-document matrix is presumed too large for the computing resources; in this case, the approximated low rank  matrix is interpreted as an \'\'approximation\'\' (a "least and necessary evil").\n* The original term-document matrix is presumed \'\'noisy\'\': for example, anecdotal instances of terms are to be eliminated. From this point of view, the approximated matrix is interpreted as a \'\'de-noisified matrix\'\' (a better matrix than the original).\n* The original term-document matrix is presumed overly [[Sparse matrix|sparse]] relative to the "true" term-document matrix.  That is, the original matrix lists only the words actually \'\'in\'\' each document, whereas we might be interested in all words \'\'related to\'\' each document\xe2\x80\x94generally a much larger set due to [[synonymy]].\n\nThe consequence of the rank lowering is that some dimensions are combined and depend on more than one term:\n\n:: {(car), (truck), (flower)} -->  {(1.3452 * car + 0.2828 * truck), (flower)}\n\nThis mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also mitigates the problem with [[polysemy]], since components of polysemous words that point in the "right" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.\n\n=== Derivation ===\nLet <math>X</math> be a matrix where element <math>(i,j)</math> describes the occurrence of term <math>i</math> in document <math>j</math> (this can be, for example, the frequency). <math>X</math> will look like this:\n\n:<math>\n\\begin{matrix} \n & \\textbf{d}_j \\\\\n & \\downarrow \\\\\n\\textbf{t}_i^T \\rightarrow &\n\\begin{bmatrix} \nx_{1,1} & \\dots & x_{1,n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nx_{m,1} & \\dots & x_{m,n} \\\\\n\\end{bmatrix}\n\\end{matrix}\n</math>\n\nNow a row in this matrix will be a vector corresponding to a term, giving its relation to each document:\n\n:<math>\\textbf{t}_i^T = \\begin{bmatrix} x_{i,1} & \\dots & x_{i,n} \\end{bmatrix}</math>\n\nLikewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term:\n\n:<math>\\textbf{d}_j = \\begin{bmatrix} x_{1,j} \\\\ \\vdots \\\\ x_{m,j} \\end{bmatrix}</math>\n\nNow the [[dot product]] <math>\\textbf{t}_i^T \\textbf{t}_p</math> between two term vectors gives the [[correlation]] between the terms over the documents. The [[matrix product]] <math>X X^T</math> contains all these dot products. Element <math>(i,p)</math> (which is equal to element <math>(p,i)</math>) contains the dot product <math>\\textbf{t}_i^T \\textbf{t}_p</math> (<math> = \\textbf{t}_p^T \\textbf{t}_i</math>). Likewise, the matrix <math>X^T X</math> contains the dot products between all the document vectors, giving their correlation over the terms: <math>\\textbf{d}_j^T \\textbf{d}_q = \\textbf{d}_q^T \\textbf{d}_j</math>.\n\nNow, from the theory of linear algebra, there exists a decomposition of <math>X</math> such that <math>U</math> and <math>V</math> are [[orthogonal matrix|orthogonal matrices]] and <math>\\Sigma</math> is a [[diagonal matrix]]. This is called a [[singular value decomposition]] (SVD):\n\n:<math>\n\\begin{matrix}\nX = U \\Sigma V^T\n\\end{matrix}\n</math>\n\nThe matrix products giving us the term and document correlations then become\n\n:<math>\n\\begin{matrix}\nX X^T &=& (U \\Sigma V^T) (U \\Sigma V^T)^T = (U \\Sigma V^T) (V^{T^T} \\Sigma^T U^T) = U \\Sigma V^T V \\Sigma^T U^T = U \\Sigma \\Sigma^T U^T \\\\\nX^T X &=& (U \\Sigma V^T)^T (U \\Sigma V^T) = (V^{T^T} \\Sigma^T U^T) (U \\Sigma V^T) = V \\Sigma^T U^T U \\Sigma V^T = V \\Sigma^T \\Sigma V^T\n\\end{matrix}\n</math>\n\nSince <math>\\Sigma \\Sigma^T</math> and <math>\\Sigma^T \\Sigma</math> are diagonal we see that <math>U</math> must contain the [[eigenvector]]s of <math>X X^T</math>, while <math>V</math> must be the eigenvectors of <math>X^T X</math>. Both products have the same non-zero eigenvalues, given by the non-zero entries of <math>\\Sigma \\Sigma^T</math>, or equally, by the non-zero entries of <math>\\Sigma^T\\Sigma</math>. Now the decomposition looks like this:\n\n:<math>\n\\begin{matrix} \n & X & & & U & & \\Sigma & & V^T \\\\\n & (\\textbf{d}_j) & & & & & & & (\\hat{\\textbf{d}}_j) \\\\\n & \\downarrow & & & & & & & \\downarrow \\\\\n(\\textbf{t}_i^T) \\rightarrow \n&\n\\begin{bmatrix} \nx_{1,1} & \\dots & x_{1,n} \\\\\n\\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\\\\nx_{m,1} & \\dots & x_{m,n} \\\\\n\\end{bmatrix}\n&\n=\n&\n(\\hat{\\textbf{t}}_i^T) \\rightarrow\n&\n\\begin{bmatrix} \n\\begin{bmatrix} \\, \\\\ \\, \\\\ \\textbf{u}_1 \\\\ \\, \\\\ \\,\\end{bmatrix} \n\\dots\n\\begin{bmatrix} \\, \\\\ \\, \\\\ \\textbf{u}_l \\\\ \\, \\\\ \\, \\end{bmatrix}\n\\end{bmatrix}\n&\n\\cdot\n&\n\\begin{bmatrix} \n\\sigma_1 & \\dots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\dots & \\sigma_l \\\\\n\\end{bmatrix}\n&\n\\cdot\n&\n\\begin{bmatrix} \n\\begin{bmatrix} & & \\textbf{v}_1 & & \\end{bmatrix} \\\\\n\\vdots \\\\\n\\begin{bmatrix} & & \\textbf{v}_l & & \\end{bmatrix}\n\\end{bmatrix}\n\\end{matrix}\n</math>\n\nThe values <math>\\sigma_1, \\dots, \\sigma_l</math> are called the singular values, and <math>u_1, \\dots, u_l</math> and <math>v_1, \\dots, v_l</math> the left and right singular vectors.\nNotice the only part of <math>U</math> that contributes to <math>\\textbf{t}_i</math> is the <math>i\\textrm{\'th}</math> row.\nLet this row vector be called <math>\\hat{\\textrm{t}}_i</math>.\nLikewise, the only part of <math>V^T</math> that contributes to <math>\\textbf{d}_j</math> is the <math>j\\textrm{\'th}</math> column, <math>\\hat{ \\textrm{d}}_j</math>.\nThese are \'\'not\'\' the eigenvectors, but \'\'depend\'\' on \'\'all\'\' the eigenvectors.\n\nIt turns out that when you select the <math>k</math> largest singular values, and their corresponding singular vectors from <math>U</math> and <math>V</math>, you get the rank <math>k</math> approximation to <math>X</math> with the smallest error ([[Frobenius norm]]). This approximation has a minimal error.  But more importantly we can now treat the term and document vectors as a "semantic space". The vector <math>\\hat{\\textbf{t}}_i</math> then has <math>k</math> entries mapping it to a lower-dimensional space dimensions. These new dimensions do not relate to any comprehensible concepts. They are a lower-dimensional approximation of the higher-dimensional space. Likewise, the vector <math>\\hat{\\textbf{d}}_j</math> is an approximation in this lower-dimensional space. We write this approximation as\n\n:<math>X_k = U_k \\Sigma_k V_k^T</math>\n\nYou can now do the following:\n* See how related documents <math>j</math> and <math>q</math> are in the low-dimensional space by comparing the vectors <math>\\Sigma_k \\hat{\\textbf{d}}_j </math> and <math>\\Sigma_k \\hat{\\textbf{d}}_q </math> (typically by [[vector space model|cosine similarity]]).\n* Comparing terms <math>i</math> and <math>p</math> by comparing the vectors <math>\\Sigma_k \\hat{\\textbf{t}}_i^T </math> and <math>\\Sigma_k \\hat{\\textbf{t}}_p^T </math>.\n* Documents and term vector representations can be clustered using traditional clustering algorithms like k-means using similarity measures like cosine.\n* Given a query, view this as a mini document, and compare it to your documents in the low-dimensional space.\n\nTo do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents:\n\n:<math>\\hat{\\textbf{d}}_j = \\Sigma_k^{-1} U_k^T \\textbf{d}_j</math>\n\nNote here that the inverse of the diagonal matrix <math>\\Sigma_k</math> may be found by inverting each nonzero value within the matrix.\n\nThis means that if you have a query vector <math>q</math>, you must do the translation <math>\\hat{\\textbf{q}} = \\Sigma_k^{-1} U_k^T \\textbf{q}</math> before you compare it with the document vectors in the low-dimensional space. You can do the same for pseudo term vectors:\n\n:<math>\\textbf{t}_i^T = \\hat{\\textbf{t}}_i^T \\Sigma_k V_k^T</math>\n\n:<math>\\hat{\\textbf{t}}_i^T = \\textbf{t}_i^T V_k^{-T} \\Sigma_k^{-1} = \\textbf{t}_i^T V_k \\Sigma_k^{-1}</math>\n\n:<math>\\hat{\\textbf{t}}_i = \\Sigma_k^{-1}  V_k^T \\textbf{t}_i</math>\n\n== Applications ==\n\nThe new low-dimensional space typically can be used to:\n* Compare the documents in the low-dimensional space ([[data clustering]], [[document classification]]).\n* Find similar documents across languages, after analyzing a base set of translated documents ([[cross language retrieval]]).\n* Find relations between terms ([[synonymy]] and [[polysemy]]).\n* Given a query of terms, translate it into the low-dimensional space, and find matching documents ([[information retrieval]]).\n* Find the best similarity between small groups of terms, in a semantic way (i.e. in a context of a knowledge corpus), as for example in multi choice questions [[Multiple choice question|MCQ]] answering model.<ref name="Alain2009">{{cite journal | url=http://hal.archives-ouvertes.fr/docs/00/38/41/43/PDF/eLSA1-brm20.pdf |format=PDF| title=Effect of tuned parameters on an LSA multiple choice questions answering model | author=Alain Lifchitz, Sandra Jhean-Larose, Guy Denhi\xc3\xa8re | journal=Behavior Research Methods | volume=41 | issue=4 | pages=1201\xe2\x80\x931209 | year=2009  | doi=10.3758/BRM.41.4.1201 | pmid=19897829 }}</ref>\n\nSynonymy and polysemy are fundamental problems in [[natural language processing]]: \n* Synonymy is the phenomenon where different words describe the same idea. Thus, a query in a search engine may fail to retrieve a relevant document that does not contain the words which appeared in the query. For example, a search for "doctors" may not return a document containing the word "[[physicians]]", even though the words have the same meaning.\n* Polysemy is the phenomenon where the same word has multiple meanings. So a search may retrieve irrelevant documents containing the desired words in the wrong meaning. For example, a botanist and a computer scientist looking for the word "tree" probably desire different sets of documents.\n\n=== Commercial applications ===\n\nLSA has been used to assist in performing [[prior art]] searches for [[patents]].<ref name="Gerry2007">{{Cite journal | author=Gerry J. Elman | title=Automated Patent Examination Support - A proposal | journal=Biotechnology Law Report | date=October 2007 | doi=10.1089/blr.2007.9896 | volume=26 | issue=5 | pages=435 | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref>\n\n=== Applications in human memory ===\n\nThe use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of [[free recall]] and memory search.  There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words.  These findings are referred to as the [[Semantic Proximity Effect]].<ref>{{cite journal | url=http://psycnet.apa.org/journals/xlm/25/4/923.pdf |format=PDF| title=Contextual Variability and Serial Position Effects in Free Recall | author=Marc W. Howard and Michael J. Kahana |year=1999}}</ref>\n\nWhen participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list.  These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall.<ref>{{cite journal | url=https://memory.psych.upenn.edu/files/pubs/ZaroEtal06.pdf |format=PDF| title=Temporal Associations and Prior-List Intrusions in Free Recall | author=Franklin M. Zaromb et al. | booktitle=Interspeech\'2005|year=2006}}</ref>\n\nAnother model, termed [[Word Association Spaces]] (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs.<ref>{{cite web|last=Nelson|first=Douglas|title=The University of South Florida Word Association, Rhyme and Word Fragment Norms|url=http://w3.usf.edu/FreeAssociation/Intro.html|accessdate=5/8/2011}}</ref>\n\n== Implementation ==\n\nThe [[Singular Value Decomposition|SVD]] is typically computed using large matrix methods (for example, [[Lanczos method]]s) but may also be computed incrementally and with greatly reduced resources via a [[neural network]]-like approach, which does not require the large, full-rank matrix to be held in memory.<ref name="Genevi2005">{{cite conference | url=http://www.dcs.shef.ac.uk/~genevieve/gorrell_webb.pdf |format=PDF| title=Generalized Hebbian Algorithm for Latent Semantic Analysis | author=Genevi\xc3\xa8ve Gorrell and Brandyn Webb | booktitle=Interspeech\'2005 |year=2005}}</ref>\nA fast, incremental, low-memory, large-matrix SVD algorithm has recently been developed.<ref name="brand2006">{{cite journal | url=http://www.merl.com/reports/docs/TR2006-059.pdf |format=PDF| title=Fast Low-Rank Modifications of the Thin Singular Value Decomposition | author=Matthew Brand | journal=Linear Algebra and Its Applications | volume=415 | pages=20\xe2\x80\x9330 | year=2006 | doi=10.1016/j.laa.2005.07.021 }}</ref> [http://web.mit.edu/~wingated/www/resources.html MATLAB] and [http://radimrehurek.com/gensim Python] implementations of these fast algorithms are available. Unlike Gorrell and Webb\'s (2005) stochastic approximation, Brand\'s algorithm (2003) provides an exact solution.\nIn recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality.<ref>doi: 10.1109/ICCSNT.2011.6182070</ref>\n\n== Limitations ==\nSome of LSA\'s drawbacks include:\n\n* The resulting dimensions might be difficult to interpret. For instance, in\n:: {(car), (truck), (flower)} \xe2\x86\xa6  {(1.3452 * car + 0.2828 * truck), (flower)}\n:the (1.3452 * car + 0.2828 * truck) component could be interpreted as "vehicle". However, it is very likely that cases close to\n:: {(car), (bottle), (flower)} \xe2\x86\xa6  {(1.3452 * car + 0.2828 * \'\'\'bottle\'\'\'), (flower)}\n:will occur. This leads to results which can be justified on the mathematical level, but have no interpretable meaning in natural language.\n\n* LSA cannot capture [[polysemy]] (i.e., multiple meanings of a word){{Citation needed|date=October 2013}}.  Each occurrence of a word is treated as having the same meaning due to the word being represented as a single point in space.  For example, the occurrence of "chair" in a document containing "The Chair of the Board" and in a separate document containing "the chair maker" are considered the same.  The behavior results in the vector representation being an \'\'average\'\' of all the word\'s different meanings in the corpus, which can make it difficult for comparison.  However, the effect is often lessened due to words having a [[word sense disambiguation|predominant sense]] throughout a corpus (i.e. not all meanings are equally likely).\n\n* Limitations of [[bag of words model]] (BOW), where a text is represented as an unordered collection of words.\n\n* To address some of the limitation of [[bag of words model]] (BOW), [[N-gram|multi-gram]] dictionary can be used to find direct and indirect association as well as [[Higher-order statistics|higher-order]] [[co-occurrence]]s among terms.<ref>[http://www.translational-medicine.com/content/12/1/324 J Transl Med. 2014 Nov 27;12(1):324.]</ref>\n\n* The [[probabilistic model]] of LSA does not match observed data: LSA assumes that words and documents form a joint [[normal distribution|Gaussian]] model ([[ergodic hypothesis]]), while a [[Poisson distribution]] has been observed.  Thus, a newer alternative is [[probabilistic latent semantic analysis]], based on a [[multinomial distribution|multinomial]] model, which is reported to give better results than standard LSA.<ref name="Thomas1999">{{cite conference | url=http://www.cs.brown.edu/people/th/papers/Hofmann-UAI99.pdf |format=PDF| title=Probabilistic Latent Semantic Analysis | author=Thomas Hofmann | booktitle=Uncertainty in Artificial Intelligence |year=1999}}</ref>\n\n== See also ==\n* [[Compound term processing]]\n* [[Explicit semantic analysis]]\n* [[Latent semantic mapping]]\n* [[Latent Semantic Structure Indexing]]\n* [[Principal components analysis]]\n* [[Probabilistic latent semantic analysis]]\n* [[Spamdexing]]\n* [[Topic model]]\n** [[Latent Dirichlet allocation]]\n* [[Vectorial semantics]]\n* [[Coh-Metrix]]\n\n== References ==\n{{Reflist}}\n* {{cite journal\n | url=http://lsa.colorado.edu/papers/dp1.LSAintro.pdf\n |format=PDF| title=Introduction to Latent Semantic Analysis\n | author=[[Thomas Landauer]], Peter W. Foltz, & Darrell Laham\n | journal=Discourse Processes\n | volume=25\n | pages=259\xe2\x80\x93284\n |year=1998\n | doi=10.1080/01638539809545028\n | issue=2\xe2\x80\x933\n}}\n* {{cite journal\n | url=http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf \n |format=PDF| title=Indexing by Latent Semantic Analysis\n | author=[[Scott Deerwester]], [[Susan Dumais|Susan T. Dumais]], [[George Furnas|George W. Furnas]], [[Thomas Landauer|Thomas K. Landauer]], [[Richard Harshman]]\n | journal=Journal of the American Society for Information Science\n | volume=41\n | issue=6\n | pages=391\xe2\x80\x93407\n | year=1990 \n | doi=10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9\n}} Original article where the model was first exposed.\n* {{cite journal\n | url=http://citeseer.ist.psu.edu/berry95using.html\n | title=Using Linear Algebra for Intelligent Information Retrieval\n | author=Michael Berry, [[Susan Dumais|Susan T. Dumais]], Gavin W. O\'Brien\n |year=1995\n}} [http://lsirwww.epfl.ch/courses/dis/2003ws/papers/ut-cs-94-270.pdf (PDF)]. Illustration of the application of LSA to document retrieval.\n* {{cite web\n | url=http://iv.slis.indiana.edu/sw/lsa.html\n | title=Latent Semantic Analysis\n | publisher=InfoVis\n}}\n* {{cite web\n | url=http://cran.at.r-project.org/web/packages/lsa/index.html\n | title=An Open Source LSA Package for R\n | publisher=CRAN\n | author=Fridolin Wild\n | date=November 23, 2005\n | accessdate=2006-11-20\n}}\n* {{ cite web\n | url=http://www.welchco.com/02/14/01/60/96/02/2901.HTM\n | title=A Solution to Plato\'s Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge\n | author=[[Thomas Landauer]], [[Susan Dumais|Susan T. Dumais]]\n | accessdate=2007-07-02\n}}\n\n==External links==\n\n===Articles on LSA===\n* [http://www.scholarpedia.org/article/Latent_semantic_analysis Latent Semantic Analysis], a scholarpedia article on LSA written by Tom Landauer, one of the creators of LSA.\n\n===Talks and demonstrations===\n* [http://videolectures.net/slsfs05_hofmann_lsvm/ LSA Overview], talk by Prof. [http://www.cs.brown.edu/~th/ Thomas Hofmann] describing LSA, its applications in Information Retrieval, and its connections to [[probabilistic latent semantic analysis]].\n* [http://www.semanticsearchart.com/researchLSA.html Complete LSA sample code in C# for Windows]. The demo code includes enumeration of text files, filtering stop words, stemming, making a document-term matrix and SVD.\n\n===Implementations===\n\nDue to its cross-domain applications in [[Information Retrieval]], [[Natural Language Processing]] (NLP), [[Cognitive Science]] and [[Computational Linguistics]], LSA has been implemented to support many different kinds of applications.\n* [http://www.d.umn.edu/~tpederse/senseclusters.html Sense Clusters], an Information Retrieval-oriented perl implementation of LSA\n* [http://code.google.com/p/airhead-research/ S-Space Package], a Computational Linguistics and Cognitive Science-oriented Java implementation of LSA\n* [http://code.google.com/p/semanticvectors/ Semantic Vectors] applies Random Projection, LSA, and Reflective Random Indexing to [[Lucene]] term-document matrices\n* [http://infomap-nlp.sourceforge.net/ Infomap Project], an NLP-oriented C implementation of LSA (superseded by semanticvectors project)\n* [http://scgroup20.ceid.upatras.gr:8000/tmg/index.php/Main_Page Text to Matrix Generator], A MATLAB Toolbox for generating term-document matrices from text collections, with support for LSA\n* [[Gensim]] contains a fast, online Python implementation of LSA for matrices larger than RAM.\n\n{{DEFAULTSORT:Latent Semantic Analysis}}\n[[Category:Information retrieval]]\n[[Category:Natural language processing]]\n[[Category:Latent variable models]]\n\n[[fa:\xd8\xa2\xd9\x86\xd8\xa7\xd9\x84\xdb\x8c\xd8\xb2 \xd9\xbe\xd9\x86\xd9\x87\xd8\xa7\xd9\x86 \xd9\x85\xd9\x81\xd9\x87\xd9\x88\xd9\x85\xdb\x8c \xd8\xa7\xd8\xad\xd8\xaa\xd9\x85\xd8\xa7\xd9\x84\xdb\x8c]]'
p306
sg6
S'Latent semantic analysis'
p307
ssI118
(dp308
g2
S'http://en.wikipedia.org/wiki/Masterseek'
p309
sg4
S'{{refimprove|date=July 2014}}\n{{Infobox company\n| name     = Masterseek\n| logo     = [[Image:masterseek logo.png|260px]]\n| type     = [[Private company|Private]]\n| traded_as        = \n| foundation       = [[Denmark]] (1999)\n| founder          = [[Rasmus Refer]]\n| location_city    = [[New York City]]\n| location_country = {{nowrap|United States}}\n| area_served      = Worldwide\n| key_people       = Rasmus Refer <small>(Co-Founder, [[Chief executive officer|CEO]])</small><br />J\xc3\xb8rgen Trygved <small>(Co-Founder)</small><br />Qasim Raza <small>([[Chief technology officer|CTO]])</small><br />Robert Perz <small>(COO, 2005-08)</small>\n| industry         = Internet<br>Computer software\n| products         = [[Business-to-business|B2B]] [[Search Engine]]\n| revenue          = \n| operating_income =\n| net_income       = \n| assets           = \n| num_employees    =\n| subsid           = [[Accoona]]\n| homepage         = {{URL|http://www.masterseek.com/}}\n| intl = yes\n}}\n\n\'\'\'Masterseek Corp.\'\'\' is a [[Business-to-business|B2B]] (Business to Business) [[search engine]] founded in [[Denmark]] in 1999.<ref name="hartzer"/> It currently hosts over 83 million worldwide company profiles from 75 countries,<ref name="usti"/> and business subscribers are given complete control over their corporate profiles.<ref name="hartzer"/> According to the amount of listed profiles, they are the largest B2B search engine worldwide.<ref name="seochat"/>\n\n==Founding==\n<!-- Deleted image removed: [[File: Rasmus refer.png|thumb|150px|left|[[Rasmus Refer]]]] -->\nMasterseek was founded in Denmark by [[Rasmus Refer]] in 1999.<ref name="hartzer"/> Their Denmark headquarters is located at Bredgade 29, DK-1260 Kbh. K, and they also have a current headquarters in [[New York City]], at 82 [[Wall Street]].<ref name="csc"/>\n\nAccording to its executives, Masterseek utilizes a business model based on an annual business subscription fee of USD $149, in return for which subscribers receive full editing control over their corporate profile, content and advertising, and control over widgets and embedded video, among other factors.<ref name="betaversion"/>\n\n==Finances==\nAs of June 2008, accountancy firm Horwart International had approximated the raw market value of the Masterseek company at $150 million.<ref name="investors"/> The company remains privately owned, but also in June 2008, it sold 10% of its authorized stocks to a range of foreign investors.<ref name="investors"/> The company announced on January 31, 2009 that they company was again offering a limited number of shares for sale in order to raise $4\xe2\x80\x936 million in order to gain a listing on the [[Swedish people|Swedish]] marketplace AktieTorget. Founder Refer also announced there were plans for an [[Initial Public Offering|IPO]].<ref name="ipo"/> By October 2009, they had signed with the Swedish-based company Thenberg & Kinde Fondkommission AB for financing.<ref name="seochat"/>\n\nIn the Danish company register the name Masterseek is coming up in three bankruptcies and one compulsory dissolution.<ref name="businessdk-cheats" />\n\n==Statistics==\nIn June 2008, the company stated it had 50 million company profiles, from over 75 countries, and handled 90,000 B2B searches daily.<ref name="hartzer"/><ref name="ipo"/><ref name="strengthen"/> The company stated they had 82 million profiles on March 21, 2011, with an average of 300,000 new profiles added monthly.<ref name="betaversion"/>\n\n==Acquiring Accoona==\n[[File:accoona logo.png|right|220px]]\nOn October 30, 2008, it was announced that Masterseek had acquired the B2B search engine [[Accoona]].<ref name="hartzer"/><ref name="paidcontent"/> \n"When [Business Insider] first heard about the money-losing Jersey City-based startup filing for IPO last year, [their] impulse was to run away screaming."<ref name="accoonnaddead"/> The search engine had been fairly successful in the United States and [[China]],<ref name="search"/> where it had an exclusive partnership with \'\'[[China Daily News]]\'\'.<ref name="accoonnaddead"/> On August 3, 2006, \'\'[[Time Magazine|TIME]]\'\' had dubbed Accoona one of its "50 best websites," illustrating how the search engine used [[artificial intelligence]] to "understand" the meaning of keyword queries.<ref name="coolest"/> Accoona had run into difficulties and gone defunct by early October 2008, withdrawing its [[Initial Public Offering|IPO]].<ref name="accoonnaddead"/>\n\nAfter Masterseek bought the remaining search engine codes, domain name, and assets,<ref name="hartzer"/> Accoona was integrated with Masterseek, and re-launched in the USA and China. It was launched in Europe in January 2009.<ref name="search"/> Accoona information was also integrated into the Masterseek search engine.<ref name="hartzer"/>\n\n==Technology==\nThe Masterseek search engine relies on web crawlers that automatically collect and sort company details from the internet.<ref name="strengthen"/> Searches can look up company profiles, contact information, and descriptions of products and services. Searches can be global, national, regional, or involved local markets. Hits are listed by relevance according to search terms. There are different search options, including a specific product search, company searches, and people searches. Results can be displayed in most languages.<ref name="seochat"/> The search engine also offers MasterRank, a point system for ranking corporate websites.<ref name="Bussinessweek"/>\n\n===Beta version===\nMasterseek released a new [[Beta Version]] of its search engine on March 25, 2011. Before then, it was accessible to business owners, managers, and other professionals, but the Beta Version made searching the site free and open to the public.<ref name="betaversion"/>\n\n== Warnings against Masterseek Corp. ==\nIn December 2009 the Swedish [[Financial Supervisory Authority (Sweden)|Financial Supervisory Authority]] issued a warning against Masterseek Corp. to warn investors <ref name="fise" /><ref name="businessdk" />\n\nIn February 2010 the  issued a warning against Masterseek and related companies Bark Group and Blogger Wave.<ref name="shareholdersdk" />\n\n==Sponsorships==\nOn July 5, 2007, Masterseek announced they were cosponsors to [[Team CSC]], Denmark\'s cycling team, beginning with the team\'s involvement in the [[Tour de France]]. The Masterseek name began to be displayed on the team\'s apparel that week, with the Tour\'s start in [[London]].<ref name="csc"/>\n\n==Management==\n*[[Rasmus Refer]] - Founder, Director, President of Technology\n*Qasim Raza - Chief Technology Officer<ref name="Bussinessweek"/>\n\n==See also==\n*[[Accoona]]\n*[[Business-to-business|B2B]]\n*[[Search engines]]\n\n== References ==\n{{reflist|2| refs =\n\n<ref name="shareholdersdk">{{cite news\n|url=http://www.shareholders.dk/art/templates/pressemeddelelse.aspx?articleid=512&zoneid=29\n|title=Danish Shareholders Association warns against Bark Group, Blogger Wave and Masterseek\n|publisher=[http://shareholders.dk/ Dansk Aktion\xc3\xa6rforening]\n|accessdate=2014-07-27}}</ref>\n\n<ref name="businessdk">{{cite news\n|url=http://www.business.dk/digital/it-firma-snyder-investorer\n|title=IT-firma snyder investorer (IT firm cheating investors)\n|publisher=[http://business.dk/ Business.dk]\n|accessdate=2014-07-03}}</ref>\n\n<ref name="businessdk-cheats">{{cite news\n|url=http://bizzen.blogs.business.dk/2010/02/09/plattenslagere-skamrider-danske-varem%C3%A6rker-som-carlsberg-danisco-og-coop/\n|title=Plattenslagere skamrider danske varem\xc3\xa6rker som Carlsberg, Danisco og Coop (Cheats shame rides Danish brands)\n|publisher=[http://business.dk/ Business.dk]\n|accessdate=2014-07-25}}</ref>\n\n<ref name="fise">{{cite news\n|url=http://www.fi.se/Folder-EN/Startpage/Register/Investor-alerts/Warning-list/Warning-against-Masterseek-Corp/\n|title=Warning against Masterseek Corp.\n|publisher=Finansinspektionen\n|accessdate=2014-07-03}}</ref>\n\n<ref name="Bussinessweek">{{cite news\n|url=http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=29327873\n|title=Masterseek Corp.\n|last=\n|first=\n|date=\n|publisher=\'\'[[Businessweek]]\'\'\n|accessdate=2011-05-08}}</ref>\n\n<ref name="coolest">{{cite news\n|url=http://www.time.com/time/business/article/0,8599,1222614,00.html\n|title=50 Coolest Websites: 2006\n|last=Buechner\n|first=Maryanne\n|date=August 3, 2006\n|publisher=\'\'[[Time Magazine|TIME]]\'\'\n|accessdate=2011-05-08}}</ref>\n\n<ref name="strengthen">{{cite news\n|url=http://www.reuters.com/article/2008/06/26/idUS149456+26-Jun-2008+MW20080626\n|title=Global Business Search Engine to Strengthen Its Advertising Network for B2B Search\n|last=\n|first=\n|date=June 26, 2008\n|publisher=\'\'[[Reuters]]\'\'\n|accessdate=2011-05-08}}</ref>\n\n<ref name="investors">{{cite news\n|url=http://www.reuters.com/article/2008/06/27/idUS85249+27-Jun-2008+MW20080627\n|title=Search Engine is Looking for Strategic Investors\n|last=\n|first=Masterseek\n|date=June 27, 2008\n|publisher=\'\'[[Reuters]]\'\'\n|accessdate=2011-05-08}}</ref>\n\n<ref name="accoonnaddead">{{cite news\n|url=http://www.businessinsider.com/2008/10/dead-search-engine-accoona-officially-dead\n|title=Dead Search Engine Accoona Officially Dead\n|last=Krangel\n|first=Eric\n|date=October 3, 2008\n|publisher=\'\'[[Business Insider]]\'\'\n|accessdate=2011-05-08}}</ref>\n\n<ref name="hartzer">{{cite news\n|url=https://www.billhartzer.com/pages/b2b-search-engine-accoona-acquired-by-masterseek/\n|title=B2B Search Engine Accoona Acquired by Masterseek\n|last=Hartzer\n|first=Bill\n|date=November 5, 2008\n|publisher=BillHartzer.com: Search Engine Marketing\n|accessdate=2011-05-08}}</ref>\n\n<ref name="search">{{cite news\n|url=http://blog.searchenginewatch.com/081105-115108\n|title=Accoona Acquired by Masterseek\n|last=Johnson\n|first=Nathania \n|date=November 5, 2008\n|publisher=\'\'[[Search Engine Watch]]\'\'\n|accessdate=2011-05-08}}</ref>\n\n<ref name="paidcontent">{{cite news\n|url=http://paidcontent.org/article/419-almost-dead-search-engine-accoona-bought-by-denmarks-masterseek/\n|title=Almost-Dead Search Engine Accoona Bought by Denmark\'s Masterseek\n|last=\n|first=\n|date=November 2008\n|publisher=\'\'Paid Content\'\'\n|accessdate=2011-05-08}}</ref>\n\n<ref name="ipo">{{cite news\n|url=http://www.reuters.com/article/2009/01/31/idUS85625+31-Jan-2009+MW20090131\n|title=Masterseek.com Is Planning for an IPO\n|last=\n|first=Masterseek\n|date=January 31, 2009\n|publisher=\'\'[[Reuters]]\'\'\n|accessdate=2011-05-08}}</ref>\n\n<ref name="seochat">{{cite news\n|url=http://www.seochat.com/c/a/Search-Engine-News/Masterseek-a-Global-Business-Search-Engine/\n|title=Masterseek: a Global Business Search Engine\n|last=Morgan\n|first=KC\n|date=October 27, 2009\n|publisher=\'\'SEOchat\'\'\n|accessdate=2011-05-08}}</ref>\n\n<ref name="betaversion">{{cite news\n|url=http://www.wiredprnews.com/2011/03/21/masterseek-expeced-to-release-beta-version-of-search-engine-continue-dominance-in-business-to-business-search_2011032117895.html\n|title=Masterseek Expected to Release Beta Version of Search Engine\n|last=\n|first=\n|date=March 21, 2011\n|publisher=WirePRNews\n|accessdate=2011-05-08}}</ref>\n\n<ref name="usti">{{cite news\n|url=http://usbusinesstimes.com/internet/3424-business-search-powerhouse-masterseek-partners-with-cutting-edge-job-database-simply-hired.html\n|title=Masterseek.com Partners with Simply Hired\n|last=\n|first=admin\n|date=April 16, 2011\n|publisher=\'\'US Business Times\'\'\n|accessdate=2011-05-08}}</ref>\n\n<ref name="csc">{{cite news\n|url=http://www.global-business-profiles.com/masterseek-cosponsors-team-csc/\n|title=Masterseek Cosponsors Team Csc\n|last=\n|first=\n|date=May 4, 2011\n|publisher=Global Business Profiles\n|accessdate=2011-05-08}}</ref>\n\n}}\n\n==External links==\n*{{Official website|http://www.masterseek.com/}}\n*[http://twitter.com/#!/masterseek_tw1 Masterseek] on [[Twitter]]\n\n[[Category:Internet search engines]]\n[[Category:Web service providers]]\n[[Category:Internet properties established in 1999]]\n[[Category:Business services companies established in 1999]]\n[[Category:Information retrieval]]\n[[Category:Online companies]]\n[[Category:Software companies established in 1999]]\n\n[[da:Dansk Aktion\xc3\xa6rforening|Danish Shareholders Association]]'
p310
sg6
S'Masterseek'
p311
ssI121
(dp312
g2
S'http://en.wikipedia.org/wiki/Ness Computing'
p313
sg4
S'{{Notability|Companies|date=July 2011}}\n\n\'\'\'Ness Computing\'\'\' is a personal search company. It was acquired by OpenTable in 2014 and is being shut down in April.<ref>{{cite web|last=Lunden|first=Ingrid|title=OpenTable Buys Ness For $17.3M|url=http://techcrunch.com/2014/02/06/opentable-ness/|work=TechCrunch|accessdate=26 March 2014}}</ref> \n\nIt was founded in October 2009 by Corey Reese,<ref>http://www.linkedin.com/in/coreyreese</ref> Paul Twohey,<ref>http://www.linkedin.com/in/twohey</ref> Nikhil Raghavan,<ref>http://www.linkedin.com/in/nikhilraghavan</ref> and Steven Schlansker.<ref>http://www.linkedin.com/in/stevenschlansker</ref> The company is headquartered in Los Altos, California.\n\nThe company, whose mission is to make search personal, is sometimes referred to as the "Palantir for fun". It aims to help people make decisions about dining, nightlife, entertainment, shopping, music, travel and more. \n\nNess\' mission is to make search personal. The company refers to its technology as the "Likeness Engine", a combination of a [[recommendation engine]] that uses [[machine learning]] to look at data from diverse sources and a traditional [[search engine]] that serves up results based on these signals. \n\nThe free Ness Dining App (for iPhone) has been referred to as the [[Netflix]] <ref>http://eater.com/archives/2011/08/26/ness-iphone-app-recommends-restaurants-using-likeness-score.php</ref> or [[Pandora]] <ref>http://gigaom.com/2011/08/25/ness-restaurant-app/</ref> for restaurants. Based on a user\'s ratings and preferences, the service will deliver recommendations for a particular time, location, price range, and cuisine preference. Users may view the menu for a place via SinglePlatform,<ref>http://www.singleplatform.com/</ref> browse [[Instagram]] photos tagged at the restaurant, and make reservations in the app via [[OpenTable]]. The app is free and available in the [[App Store (iOS)]].\n\n==References==\n{{Reflist}}\n\n[[Category:Information retrieval]]\n[[Category:Software companies based in California]]'
p314
sg6
S'Ness Computing'
p315
ssI124
(dp316
g2
S'http://en.wikipedia.org/wiki/Automatic Content Extraction'
p317
sg4
S"{{Multiple issues|\n{{citation style|date=December 2011}}\n{{technical|date=October 2012}}\n{{abbreviations|date=October 2012}}\n}}\n'''Automatic Content Extraction (ACE)''' is a program for developing advanced [[Information extraction]] [[technologies]]. Given a text in [[natural language]], the ACE challenge is to detect:\n# '''entities''' mentioned in the text, such as: persons, organizations, locations, facilities, weapons, vehicles, and geo-political entities.\n# '''relations''' between entities, such as: person A is the manager of company B. Relation types include: role, part, located, near, and social.\n# '''events''' mentioned in the text, such as: interaction, movement, transfer, creation and destruction.\n\nThis program began with a [[pilot study]] in 1999.\n\nWhile the ACE program is directed toward extraction of information from [[Sound|audio]] and [[image]] sources in addition to pure text, the research effort is restricted to information extraction from text. The actual [[transduction (machine learning)|transduction]] of audio and image data into text is not part of the ACE research effort, although the processing of ASR and OCR output from such transducers is.\n\nThe program relates to [[English language|English]], [[Arabic language|Arabic]] and [[Chinese language|Chinese]] texts.\n\nThe effort involves:\n* defining the research tasks in detail,\n* collecting and annotating data needed for training, development, and evaluation,\n* supporting the research with evaluation tools and [[research workshop]]s.\n\nIn general objective, the ACE program is motivated by and addresses the same issues as the MUC program that preceded it. The ACE program, however, defines the research objectives in terms of the target objects (i.e., the entities, the relations, and the events) rather than in terms of the words in the text. For example, the so-called \xe2\x80\x9cnamed entity\xe2\x80\x9d task, as defined in MUC, is to identify those words (on the page) that are names of entities. In ACE, on the other hand, the corresponding task is to identify the entity so named. This is a different task, one that is more abstract and that involves inference more explicitly in producing an\nanswer. In a real sense, the task is to detect things that \xe2\x80\x9caren\xe2\x80\x99t there\xe2\x80\x9d.\n\nThe ACE corpus is one of the standard benchmarks for testing new information extraction [[algorithm]]s.\n\n==References==\n* [http://www.citeulike.org/user/erelsegal-halevi/article/10003935 George Doddington@NIS T, Alexis Mitchell@LD C, Mark Przybocki@NIS T, Lance Ramshaw@BB N, Stephanie Strassel@LD C, Ralph Weischedel@BB N. The automatic content extraction (ACE) program\xe2\x80\x93tasks, data, and evaluation. 2004]\n\n==External links==\n* [http://www.itl.nist.gov/iaui/894.02/related_projects/muc/ MUC] - ACE's predecessor.\n* [http://projects.ldc.upenn.edu/ace/ ACE] (LDC)\n* [http://www.itl.nist.gov/iad/894.01/tests/ace/ ACE] (NIST)\n\n[[Category:Information retrieval]]"
p318
sg6
S'Automatic Content Extraction'
p319
ssI127
(dp320
g2
S'http://en.wikipedia.org/wiki/Macroglossa Visual Search'
p321
sg4
S'{{Infobox Website\n| name           = Macroglossa\n| logo           = [[File:Macroglossa Visual Search Engine Logo, 2012.gif]]\n| screenshot     = \n| caption        = Macroglossa logo\n| url            = [http://www.macroglossa.com macroglossa.com]\n| type           = [[Visual search engine|Visual]] [[Search engine|Search Engine]]\n| language       = English\n| registration   = optional\n| author         = MVE\n| launch date    = 2010\n| current status = beta 0.1\n| slogan         = search is visual\n| alexa          = {{IncreaseNegative}} 2,828,096 ({{as of|2014|4|1|alt=April 2014}})<ref name="alexa">{{cite web|url= http://www.alexa.com/siteinfo/macroglossa.com |title= Macroglossa.com Site Info | publisher= [[Alexa Internet]] |accessdate= 2014-04-01 }}</ref><!--Updated monthly by OKBot.-->\n}}\n\'\'\'Macroglossa\'\'\' is a [[visual search engine]] based on the comparison of images,<ref>Nicola Mattina. "[http://blog.wired.it/startupcloud/2010/12/29/macroglossa-usare-le-immagini-per-effettuare-ricerche-sul-web.html Macroglossa: usare le immagini per effettuare ricerche sul web]", Wired.it, Retrieved December 29, 2010.</ref><ref>GreatStartups.com . "[http://greatstartups.com/2010/10/13/macroglossa-com-whats-in-the-picture/ Macroglossa.com-What\xe2\x80\x99s In The Picture ]", greatstartups.com, Retrieved October 13, 2010.</ref> coming from an Italian Group. The development of the project began in 2009. In April 2010 is released the first public [[Alpha stage#Alpha|alpha]].<ref>Liva Judic. "[http://searchenginewatch.com/article/2050950/Macroglossas-Visual-Search-Engine-fails-to-meet-basic-expectations Macroglossa\'s Visual Search Engine fails to meet basic expectations ]", SEW - searchenginewatch, Retrieved April 26, 2010.</ref>\nUsers can upload photos or images that they aren\'t sure what they are to determine what the images contain. Macroglossa compares images to return search results based on specific search categories. \nThe engine does not use technologies and solutions such as [[Optical character recognition|OCR]], [[Tag (metadata)|tags]], vocabulary trees. The comparison is directly based on the contents of the image which the user wants to know more.\n\nInteresting features are the categorization of the elements, the ability to search specific portions of the image or start a search from a video file,<ref>Mve. "[http://www.macroglossa.com/press_macrog_eng_a2dot0.pdf - Macroglossa PR]",  - Retrieved 2011.</ref> but the main function is to simulate a digital eye on trying to find similarities of an unknown subject. This feature makes the engine unique.\n\nThis technology has several advantages. First, it allows users to pull results from collections of visual content<ref>Make Use OF . "[http://www.makeuseof.com/dir/macroglossa-identify-objects-in-image/ - MacroGlossa: Find Similar Images & Identify Objects In Image ]",  - Makeuseof.com. 2010.</ref> without using tags for search. Second, the visuals can be [[Crowdsourcing|crowd sourced]]. In fact by being a search engine, rather than simply a tool, Macroglossa should be able to crowdsourced and scale its recognition vocabulary faster than anyone else and a technology like this would increase the cognitive and spatial skills in [[humanoid]] robotics.<ref>J. Sturm, A. Visser. "[http://cvpr.in.tum.de/old/pub/pub/sturm09ras.pdf An appearance-based visual compass for mobile robots ]", Appearance-based, mobile robot localization, active vision, machine learning. 2000.</ref> In addition Macroglosssa can also be used as a Reverse Image Search to find [[orphan works]] and possible violations of copyright of images.\n\nMacroglossa supports all popular image extensions such [[Jpg|jpeg]], [[Portable Network Graphics|png]], [[BMP file format|bmp]], [[gif]] and video formats such [[Audio Video Interleave|avi]], [[.mov|mov]], [[mp4]], [[m4v]], [[3gp]], [[wmv]], [[mpeg]].\n\nMacroglossa enters [[Beta stage#Beta beta|beta]] stage in September 2011<ref>Mve. "[http://www.macroglossa.com/disclaimer.html - macroglossa.com]",  - Releases and Features. 2011.</ref> and at the same time open to the public the opportunity to use the developed [[Interface (object-oriented programming)|interfaces]] ( Api for web and mobile applications ) in order to expand the use of the engine in the [[Business-to-business|B2B]] and [[Business-to-consumer|B2C]] fields. Macroglossa becomes a [[Software as a service|SaaS]].\n\n[[Api|API]] are distributed on three levels : free, basic, and premium. The free API has limited use, but basic and premium do not. The premium API also offers custom services allowing customers to extend and mold the features offered by computer vision.<ref>J. R. Mart\xc3\xadnez-de Dios, C. Serna y A. Ollero. "[http://grvc.us.es/publica/revistas/documentos/FishFarms.pdf Computer vision and robotics techniques in fish farms ]", Robotica. Vo. 21. No. 3. Editor Cambridge University Press. June 2003.</ref>\n\n==References==\n{{reflist}}\n\n==Notes==\n* \'\'Wired.it, Retrieved December 29, 2010 :\'\' Macroglossa is an Italian project born from a passion for research and innovation by the MVE group of independent developers. The startup has developed a visual search engine based on the comparison of the subjects in the images. The owners of the project define it as "a sort of digital eye can capture, compare and draw conclusions." The purpose of this service is to provide a new type of research within the network. The search engine allows you to upload a picture on the platform and look for similar images on the web. The engine is not based on text tags and does not use OCR to extract strings from images to locate the target. Everything focuses on the key points of the image uploaded by the user. The aim is to give as much information as possible on the results obtained. Each image has a direct result of the source.\n\n==External links==\n* [http://www.macroglossa.com Macroglossa] home page\n* Macroglossa [http://www.macroglossa.com/api.html Api program]\n* Macroglossa on [http://www.killerstartups.com/Search/macroglossa-com-carry-out-visual-searches Killer Startups]\n* [http://yourstory.in/2011/07/macroglossa-reaches-alpha-version-4-0-a-picture-search-engine/ Yourstory.in] talks about Macroglossa\n\n[[Category:Information retrieval]]\n[[Category:Internet search engines]]\n[[Category:Data search engines]]\n[[Category:Multimedia]]\n[[Category:Image search]]'
p322
sg6
S'Macroglossa Visual Search'
p323
ss.

