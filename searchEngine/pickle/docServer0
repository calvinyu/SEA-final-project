(dp0
I0
(dp1
S'url'
p2
S'http://en.wikipedia.org/wiki/Category:Searching'
p3
sS'text'
p4
S"{{Commons category|Searching}}\n[[Category:Information retrieval]]\n<!-- used to be on cat search engines that was merged into this, but didn't think it really applied... [[Category:Information technology]] -->"
p5
sS'title'
p6
S'Category:Searching'
p7
ssI129
(dp8
g2
S'http://en.wikipedia.org/wiki/Preference learning'
p9
sg4
V'''Preference learning''' is a subfield in [[machine learning]] in which the goal is to learn a predictive [[Preference (economics)|preference]] model from observed preference information.<ref>[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The\u000aMIT Press ISBN 9780262018258.</ref> In the view of [[supervised learning]], preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items.\u000a\u000aWhile the concept of preference learning has been emerged for some time in many fields such as [[economics]],<ref name="SHOG00" /> it's a relatively new topic in [[Artificial Intelligence]] research. Several workshops have been discussing preference learning and related topics in the past decade.<ref name="WEB:WORKSHOP" />\u000a\u000a==Tasks==\u000a\u000aThe main task in preference learning concerns problems in "[[learning to rank]]". According to different types of preference information observed, the tasks are categorized as three main problems in the book ''Preference Learning'':<ref name="FURN11" />\u000a\u000a===Label ranking===\u000a\u000aIn label ranking, the model has an instance space <math>X=\u005c{x_i\u005c}\u005c,\u005c!</math> and a finite set of labels <math>Y=\u005c{y_i|i=1,2,\u005ccdots,k\u005c}\u005c,\u005c!</math>. The preference information is given in the form <math>y_i \u005csucc_{x} y_j\u005c,\u005c!</math> indicating instance <math>x\u005c,\u005c!</math> shows preference in <math>y_i\u005c,\u005c!</math> rather than <math>y_j\u005c,\u005c!</math>. A set of preference information is used as training data in the model. The task of this model is to find a preference ranking among the labels for any instance.\u000a\u000aIt was observed some conventional [[Classification in machine learning|classification]] problems can be generalized in the framework of label ranking problem:<ref name="HARP03" /> if a training instance <math>x\u005c,\u005c!</math> is labeled as class <math>y_i\u005c,\u005c!</math>, it implies that <math>\u005cforall j \u005cneq i, y_i \u005csucc_{x} y_j\u005c,\u005c!</math>. In [[Multi-label classification|multi-label]] situation, <math>x\u005c,\u005c!</math> is associated with a set of labels <math>L \u005csubseteq Y\u005c,\u005c!</math> and thus the model can extract a set of preference information <math>\u005c{y_i \u005csucc_{x} y_j | y_i \u005cin L, y_j \u005cin Y\u005cbackslash L\u005c}\u005c,\u005c!</math>. Training a preference model on this preference information and the classification result of an instance is just the corresponding top ranking label.\u000a\u000a===Instance ranking===\u000a\u000aInstance ranking also has the instance space <math>X\u005c,\u005c!</math> and label set <math>Y\u005c,\u005c!</math>. In this task, labels are defined to have a fixed order <math>y_1 \u005csucc y_2 \u005csucc \u005ccdots \u005csucc y_k\u005c,\u005c!</math> and each instance <math>x_l\u005c,\u005c!</math> is associated with a label <math>y_l\u005c,\u005c!</math>. Giving a set of instances as training data, the goal of this task is to find the ranking order for a new set of instances.\u000a\u000a===Object ranking===\u000a\u000aObject ranking is similar to instance ranking except that no labels are associated with instances. Given a set of pairwise preference information in the form <math>x_i \u005csucc x_j\u005c,\u005c!</math> and the model should find out a ranking order among instances.\u000a\u000a==Techniques==\u000a\u000aThere are two practical representations of the preference information <math>A \u005csucc B\u005c,\u005c!</math>. One is assigning <math>A\u005c,\u005c!</math> and <math>B\u005c,\u005c!</math> with two real numbers <math>a\u005c,\u005c!</math> and <math>b\u005c,\u005c!</math> respectively such that <math>a > b\u005c,\u005c!</math>. Another one is assigning a binary value <math>V(A,B) \u005cin \u005c{0,1\u005c}\u005c,\u005c!</math> for all pairs <math>(A,B)\u005c,\u005c!</math> denoting whether <math>A \u005csucc B\u005c,\u005c!</math> or <math>B \u005csucc A\u005c,\u005c!</math>. Corresponding to these two different representations, there are two different techniques applied to the learning process.\u000a\u000a===Utility function===\u000a\u000aIf we can find a mapping from data to real numbers, ranking the data can be solved by ranking the real numbers. This mapping is called [[utility function]]. For label ranking the mapping is a function <math>f: X \u005ctimes Y \u005crightarrow \u005cmathbb{R}\u005c,\u005c!</math> such that <math>y_i \u005csucc_x y_j \u005cRightarrow f(x,y_i) > f(x,y_j)\u005c,\u005c!</math>. For instance ranking and object ranking, the mapping is a function <math>f: X \u005crightarrow \u005cmathbb{R}\u005c,\u005c!</math>.\u000a\u000aFinding the utility function is a [[Regression analysis|regression]] learning problem which is well developed in machine learning.\u000a\u000a===Preference relations===\u000a\u000aThe binary representation of preference information is called preference relation. For each pair of alternatives (instances or labels), a binary predicate can be learned by conventional supervising learning approach. Fürnkranz, Johannes and Hüllermeier proposed this approach in label ranking problem.<ref name="FURN03" /> For object ranking, there is an early approach by Cohen et al.<ref name="COHE98" />\u000a\u000aUsing preference relations to predict the ranking will not be so intuitive. Since preference relation is not transitive, it implies that the solution of ranking satisfying those relations would sometimes be unreachable, or there could be more than one solution. A more common approach is to find a ranking solution which is maximally consistent with the preference relations. This approach is a natural extension of pairwise classification.<ref name="FURN03" />\u000a\u000a==Uses==\u000a\u000aPreference learning can be used in ranking search results according to feedback of user preference. Given a query and a set of documents, a learning model is used to find the ranking of documents corresponding to the relevance with this query. More discussions on research in this field can be found in Tie-Yan Liu's survey paper.<ref name="LIU09" />\u000a\u000aAnother application of preference learning is [[recommender systems]].<ref name="GEMM09" /> Online store may analyze customer's purchase record to learn a preference model and then recommend similar products to customers. Internet content providers can make use of user's ratings to provide more user preferred contents.\u000a\u000a==See also==\u000a*[[Learning to rank]]\u000a\u000a==References==\u000a\u000a{{Reflist|\u000arefs=\u000a\u000a<ref name="SHOG00">{{\u000acite journal\u000a|last       = Shogren\u000a|first      = Jason F.\u000a|coauthors  = List, John A.; Hayes, Dermot J.\u000a|year       = 2000\u000a|title      = Preference Learning in Consecutive Experimental Auctions\u000a|url        = http://econpapers.repec.org/article/oupajagec/v_3a82_3ay_3a2000_3ai_3a4_3ap_3a1016-1021.htm\u000a|journal    = American Journal of Agricultural Economics\u000a|volume     = 82\u000a|pages      = 1016\u20131021\u000a|doi=10.1111/0002-9092.00099\u000a}}</ref>\u000a\u000a<ref name="WEB:WORKSHOP">{{\u000acite web\u000a|title      = Preference learning workshops\u000a|url        = http://www.preference-learning.org/#Workshops\u000a}}</ref>\u000a\u000a<ref name="FURN11">{{\u000acite book\u000a|last       = F&uuml;rnkranz\u000a|first      = Johannes\u000a|coauthors  = H&uuml;llermeier, Eyke\u000a|year       = 2011\u000a|title      = Preference Learning\u000a|url        = http://books.google.com/books?id=nc3XcH9XSgYC\u000a|chapter    = Preference Learning: An Introduction\u000a|chapterurl = http://books.google.com/books?id=nc3XcH9XSgYC&pg=PA4\u000a|publisher  = Springer-Verlag New York, Inc.\u000a|pages      = 3\u20138\u000a|isbn       = 978-3-642-14124-9\u000a}}</ref>\u000a\u000a<ref name="HARP03">{{\u000acite journal\u000a|last       = Har-peled\u000a|first      = Sariel\u000a|coauthors  = Roth, Dan; Zimak, Dav\u000a|year       = 2003\u000a|title      = Constraint classification for multiclass classification and ranking\u000a|journal    = In Proceedings of the 16th Annual Conference on Neural Information Processing Systems, NIPS-02\u000a|pages      = 785\u2013792\u000a}}</ref>\u000a\u000a<ref name="FURN03">{{\u000acite journal\u000a|last       = F&uuml;rnkranz\u000a|first      = Johannes\u000a|coauthors  = H&uuml;llermeier, Eyke\u000a|year       = 2003\u000a|title      = Pairwise Preference Learning and Ranking\u000a|journal    = Proceedings of the 14th European Conference on Machine Learning\u000a|pages      = 145\u2013156\u000a}}</ref>\u000a\u000a<ref name="COHE98">{{\u000acite journal\u000a|last       = Cohen\u000a|first      = William W.\u000a|coauthors  = Schapire, Robert E.; Singer, Yoram\u000a|year       = 1998\u000a|title      = Learning to order things\u000a|url        = http://dl.acm.org/citation.cfm?id=302528.302736\u000a|journal    = In Proceedings of the 1997 Conference on Advances in Neural Information Processing Systems\u000a|pages      = 451\u2013457\u000a}}</ref>\u000a\u000a<ref name="LIU09">{{\u000acite journal\u000a|last       = Liu\u000a|first      = Tie-Yan\u000a|year       = 2009\u000a|title      = Learning to Rank for Information Retrieval\u000a|url        = http://dl.acm.org/citation.cfm?id=1618303.1618304\u000a|journal    = Foundations and Trends in Information Retrieval\u000a|volume     = 3\u000a|issue      = 3\u000a|pages      = 225\u2013331\u000a|doi        = 10.1561/1500000016\u000a}}</ref>\u000a\u000a<ref name="GEMM09">{{\u000acite journal\u000a|last       = Gemmis\u000a|first      = Marco De\u000a|author2=Iaquinta, Leo |author3=Lops, Pasquale |author4=Musto, Cataldo |author5=Narducci, Fedelucio |author6= Semeraro,Giovanni \u000a|year       = 2009\u000a|title      = Preference Learning in Recommender Systems\u000a|url        = http://www.ecmlpkdd2009.net/wp-content/uploads/2008/09/preference-learning.pdf#page=45\u000a|journal    = PREFERENCE LEARNING\u000a|volume     = 41\u000a|pages      = 387\u2013407\u000a|doi=10.1007/978-3-642-14125-6_18\u000a}}</ref>\u000a\u000a}}\u000a\u000a==External links==\u000a*[http://www.preference-learning.org/ Preference Learning site]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Machine learning]]
p10
sg6
S'Preference learning'
p11
ssI3
(dp12
g2
S'http://en.wikipedia.org/wiki/Subsetting'
p13
sg4
VIn research communities (for example, [[earth science]]s), '''subsetting''' is the process of retrieving just the parts of large files which are of interest for a specific purpose. This occurs usually in a client\u2014server setting, where the extraction of the parts of interest occurs on the server before the data is sent to the client over a network. The main purpose of subsetting is to save bandwidth on the network and storage space on the client computer.\u000a\u000aSubsetting may be favorable for the following reasons:<ref name="Institute2012">{{cite book|author=SAS Institute|title=SAS/ETS 12.1 User's Guide|url=http://books.google.com/books?id=OE0UfAhit4kC&pg=PA70|date=1 August 2012|publisher=SAS Institute|isbn=978-1-61290-379-8|pages=70}}</ref>\u000a* restrict the time range\u000a* select [[Cross-sectional data|cross section]]s of data\u000a* select particular kinds of [[time series]]\u000a* exclude particular obersvations\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a\u000a==External links==\u000a*[http://www.subset.org/index.jsp Subset.org]\u000a\u000a[[Category:Information retrieval]]\u000a\u000a{{Statistics-stub}}
p14
sg6
S'Subsetting'
p15
ssI132
(dp16
g2
S'http://en.wikipedia.org/wiki/Search engine technology'
p17
sg4
V{{multiple issues|\u000a{{Refimprove|date=May 2014}}\u000a{{Tone|article|date=January 2013}}\u000a}}\u000aA search engine is a type of computer software used to search data in the form of text or a database for specified information.<ref>{{cite web|title=Define search engine|url=http://www.webopedia.com/TERM/S/search_engine.html|accessdate=1 June 2014}}</ref>\u000a\u000aSearch engines normally consist of spiders (also known as bots) which roam the web searching for links and keywords. They send collected data back to the indexing software which categorizes and adds the links to databases with their related keywords. When you specify a search term the engine does not scan the whole web but extracts related links from the database.\u000a\u000a==History of Search Technology==\u000a\u000a{{Empty section|date=July 2014}}\u000a\u000a== The Memex ==\u000a\u000aThe concept of hypertext and a memory extension originates from an article that was published in [[The Atlantic Monthly]] in July 1945 written by [[Vannevar Bush]], titled [[As We May Think]].  Within this article Vannevar urged scientists to work together to help build a body of knowledge for all mankind. He then proposed the idea of a virtually limitless, fast, reliable, extensible, associative memory storage and retrieval system. He named this device a [[memex]].<ref>{{cite journal|last1=Yeo|first1=Richard|title=Before Memex: Robert Hooke, John Locke, and Vannevar Bush on External Memory|journal=Science in Context|date=30 January 2007|volume=20|issue=01|page=21|doi=10.1017/S0269889706001128}}</ref>\u000a\u000aBush regarded the notion of \u201cassociative indexing\u201d as his key conceptual contri- bution. As he explained, this was \u201ca provision whereby any item may be caused at will to select immediately and automatically another. This is the essential feature of the memex. The process of tying two items together is the important thing.\u201d This \u201clinking\u201d (as we now say) constituted a \u201ctrail\u201d of documents that could be named, coded, and found again. Moreover, after the original two items were coupled, \u201cnumerous items\u201d could be \u201cjoined together to form a trail\u201d; they could be \u201creviewed in turn, rapidly or slowly, by deflecting a lever like that used for turning the pages of a book. It is exactly as though the physical items had been gathered together from widely separated sources and bound together to form a new book\u201d<ref>{{cite journal|title=Before Memex: Robert Hooke, John Locke, and Vannevar Bush on External Memory|journal=Science in Context|date=30 January 2007|volume=20|issue=01|pages=21\u201347|doi=10.1017/S0269889706001128|accessdate=1 June 2014|postscript=The example Bush gives is a quest to find information on the relative merits of the Turkish short bow and the English long bow in the crusades}}</ref>\u000a\u000aAll of the documents used in the memex would be in the form of microfilm copy acquired as such or, in the case of personal records, transformed to microfilm by the machine itself. Memex would also employ new retrieval techniques based on a new kind of associative indexing the basic idea of which is a provision whereby any item may be caused at will to select immediately and automatically another to create personal "trails" through linked documents. The new procedures, that Bush anticipated facilitating information storage and retrieval would lead to the development of wholly new forms of encyclopedia.\u000a\u000aThe most important mechanism, conceived by Bush and considered as closed to the modern hypertext systems is the associative trail. It would be a way to create a new linear sequence of microfilm frames across any arbitrary sequence of microfilm frames by creating a chained sequence of links in the way just described, along with personal comments and side trails.\u000aThe essential feature of the memex [is] the process of tying two items together\u2026 When the user is building a trail, he names it in his code book, and taps it out on his keyboard. Before him are the two items to be joined, projected onto adjacent viewing positions. At the bottom of each there are a number of blank code spaces, and a pointer is set to indicate one of these on each item. The user taps a single key, and the items are permanently joined\u2026 Thereafter, at any time, when one of these items is in view, the other can be instantly recalled merely by tapping a button below the corresponding code space.\u000a\u000aIn the article of Bush is not described any automatic search, nor any universal metadata scheme such as a standard library classification or a hypertext element set. Instead, when the user made an entry, such as a new or annotated manuscript, or image, he was expected to index and describe it in his personal code book. Later on, by consulting his code book, the user could retrace annotated and generated entries.\u000a\u000aIn 1965 Bush took part in the project INTREX of MIT, for developing technology for mechanization the processing of information for library use. In his 1967 essay titled "Memex Revisited", he pointed out that the development of the digital computer, the transistor, the video, and other similar devices had heightened the feasibility of such mechanization, but costs would delay its achievements. He was right again.\u000a\u000aTed Nelson, who later did pioneering work with first practical hypertext system and coined the term "hypertext" in the 1960s, credited Bush as his main influence.<ref>{{cite web|title=The MEMEX of Vannevar Bush|url=http://history-computer.com/Internet/Dreamers/Bush.html}}</ref>\u000a\u000a== SMART ==\u000a\u000aGerard Salton, who died on August 28 of 1995, was the father of modern search technology. His teams at Harvard and Cornell developed the SMART informational retrieval system. Salton\u2019s Magic Automatic Retriever of Text included important concepts like the vector space model, Inverse Document Frequency (IDF), Term Frequency (TF), term discrimination values, and relevancy feedback mechanisms.\u000a\u000aHe authored a 56 page book called A Theory of Indexing which explained many of his tests upon which search is still largely based.\u000a\u000a== String Search Engines ==\u000a\u000aIn 1987 an article was published detailing the development of a character string search engine (SSE) for rapid text retrieval on a double-metal 1.6-\u03bcm n-well CMOS solid-state circuit with 217,600 transistors lain out on a 8.62x12.76-mm die area. The SSE accommodated a novel string-search architecture which combines a 512-stage finite-state automaton (FSA) logic with a content addressable memory (CAM) to achieve an approximate string comparison of 80 million strings per second. The CAM cell consisted of four conventional static RAM (SRAM) cells and a read/write circuit. Concurrent comparison of 64 stored strings with variable length was achieved in 50 ns for an input text stream of 10 million characters/s, permitting performance despite the presence of single character errors in the form of character codes. Furthermore, the chip allowed nonanchor string search and variable-length `don't care' (VLDC) string search.<ref>{{cite journal|last=Yamada|first=H.|author2=Hirata, M. |author3=Nagai, H. |author4= Takahashi, K. |title=A high-speed string-search engine|journal=IEEE Journal of Solid-State Circuits|date=Oct 1987|volume=22|issue=5|pages=829\u2013834|doi=10.1109/JSSC.1987.1052819|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1052819&url=http%3A%2F%2Fieeexplore.ieee.org%2Fstamp%2Fstamp.jsp%3Ftp%3D%26arnumber%3D1052819|accessdate=30 May 2014|publisher=IEEE}}</ref>\u000a\u000a<!-- Potential source for article expansion:  http://ieeexplore.ieee.org/search/searchresult.jsp?queryText%3Dsearch-engine&sortType=asc_p_Publication_Year&pageNumber=1&resultAction=SORT -->\u000a\u000a== Web Search Engines ==\u000a\u000a=== Archie ===\u000a\u000aThe first web search engines was Archie, created in 1990<ref name="intelligent-technologies">{{cite book|author1=Priti Srinivas Sajja|author2=Rajendra Akerkar|title=Intelligent technologies for web applications|date=2012|publisher=CRC Press|location=Boca Raton|isbn=978-1-4398-7162-1|page=87|url=http://books.google.com/books?id=HqXxoWK7tucC&pg=PA87&lpg=PA87&dq=the+University+of+Nevada+System+Computing+Services+group+developed+Veronica.&source=bl&ots=Xt7TQz0a6Y&sig=vusKa34uORNCBI6lT3-sEy5qv-Q&hl=en&sa=X&ei=KzqOU7PCDcOlyATtt4L4DA&ved=0CEoQ6AEwBQ#v=onepage&q=the%20University%20of%20Nevada%20System%20Computing%20Services%20group%20developed%20Veronica.&f=false|accessdate=3 June 2014}}</ref> by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program "archives," but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on. For more information on where Archie is today, see:\u000ahttp://www.bunyip.com/products/archie/\u000a\u000aThe primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: Some administrator decides that he wants to make files available from his computer. He sets up a program on his computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, he or she connects to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol.\u000a\u000aInitially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, "anonymous" FTP sites became repositories for files, allowing all users to post and retrieve them.\u000a\u000aEven with archive sites, many important files were still scattered on small FTP servers. Unfortunately, these files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file.\u000a\u000aArchie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.<ref name="wileyhistory">{{cite web|title=A History of Search Engines|url=http://www.wiley.com/legacy/compbooks/sonnenreich/history.html|publisher=Wiley|accessdate=1 June 2014}}</ref>\u000a\u000a=== Veronica ===\u000a\u000aIn 1993, the University of Nevada System Computing Services group developed Veronica.<ref name="intelligent-technologies"/> It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.<ref name="wileyhistory"/>\u000a\u000a=== The Lone Wanderer ===\u000a\u000aThe World Wide Web Wanderer, developed by Matthew Gray in 1993<ref>{{cite book|author1=Priti Srinivas Sajja|author2=Rajendra Akerkar|title=Intelligent technologies for web applications|date=2012|publisher=CRC Press|location=Boca Raton|isbn=978-1-4398-7162-1|page=86|url=http://books.google.com/books?id=HqXxoWK7tucC&pg=PA87&lpg=PA87&dq=the+University+of+Nevada+System+Computing+Services+group+developed+Veronica.&source=bl&ots=Xt7TQz0a6Y&sig=vusKa34uORNCBI6lT3-sEy5qv-Q&hl=en&sa=X&ei=KzqOU7PCDcOlyATtt4L4DA&ved=0CEoQ6AEwBQ#v=onepage&q=the%20University%20of%20Nevada%20System%20Computing%20Services%20group%20developed%20Veronica.&f=false|accessdate=3 June 2014}}</ref> was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database.\u000a\u000aMatthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of time a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained.\u000a\u000aIn response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways.\u000a\u000aALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot doesn't run about eating up Net bandwidth.  Unfortunately, the disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they don't submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.<ref name="wileyhistory"/>\u000a\u000a=== Excite ===\u000a\u000aExcite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\u000aTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.<ref name="wileyhistory"/>\u000a\u000aExcite was the first serious commercial search engine which launched in 1995.<ref>{{cite web|title=The Major Search Engines|url=http://www.pccua.edu/kholland/major_search_engines.htm|accessdate=1 June 2014|date=21 January 2014}}</ref> It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million.\u000a\u000a=== Yahoo! ===\u000a\u000aIn April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos.\u000a\u000aAs the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory.\u000a\u000aThe Wanderer captured only URLs, which made it difficult to find things that weren\u2019t explicitly described by their URL. Because URLs are rather cryptic to begin with, this didn\u2019t help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites.\u000a\u000a=== Lycos ===\u000a\u000aAt Carnegie Mellon University during the July of 1994, Michael Mauldin, on leave from CMU,developed the Lycos search engine.\u000a\u000a== Types of Web Search Engines ==\u000a\u000aSearch engines on the web are sites enriched with facility to search the content stored on other sites.  There is difference in the way various search engines work, but they all perform three basic tasks.<ref>{{cite book|author1=Priti Srinivas Sajja|author2=Rajendra Akerkar|title=Intelligent technologies for web applications|date=2012|publisher=CRC Press|location=Boca Raton|isbn=978-1-4398-7162-1|page=85|url=http://books.google.com/books?id=HqXxoWK7tucC&pg=PA87&lpg=PA87&dq=the+University+of+Nevada+System+Computing+Services+group+developed+Veronica.&source=bl&ots=Xt7TQz0a6Y&sig=vusKa34uORNCBI6lT3-sEy5qv-Q&hl=en&sa=X&ei=KzqOU7PCDcOlyATtt4L4DA&ved=0CEoQ6AEwBQ#v=onepage&q=the%20University%20of%20Nevada%20System%20Computing%20Services%20group%20developed%20Veronica.&f=false|accessdate=3 June 2014}}</ref>\u000a\u000a# Finding and selecting full or partial content based on the keywords provided.\u000a# Maintaining index of the content and referencing to the location they find\u000a# Allowing users to look for words or combinations of words found in that index.\u000a\u000aThe process begins when a user enters a query statement into the system through the interface provided.\u000a\u000a{| class="wikitable"\u000a|-\u000a! Type\u000a! Example\u000a! Description\u000a|-\u000a| Conventional\u000a| librarycatalog\u000a| Search by keyword, title, author, etc.\u000a|-\u000a| Text-based\u000a| Lexis-Nexis,Google,Yahoo!\u000a| Search by keywords. Limited search using queries in natural language.\u000a|-\u000a| Multimedia\u000a| QBIC, WebSeek, SaFe\u000a| Search by visual appearance (shapes, colors,..)\u000a|-\u000a| Q/A\u000a| [[Stack Exchange]], NSIR\u000a| Search in (restricted) natural language\u000a|-\u000a| Clustering Systems\u000a| Vivisimo, Clusty\u000a|\u000a|-\u000a| Research Systems\u000a| Lemur, Nutch\u000a|\u000a|}\u000a\u000aThere are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two.\u000a\u000aCrawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine.\u000a\u000aHuman-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index.\u000a\u000aIn both cases, when you query a search engine to locate information, you're actually searching through the index that the search engine has created \u2014you are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index hasn't been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated.\u000a\u000aSo why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for.\u000a\u000aOne of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing.\u000a\u000aAnother common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered "important" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking.\u000a\u000aModern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. [[Google]]), database or structured data search engines (e.g. [[Dieselpoint]]), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and [[Yahoo!]], utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity.\u000a\u000a==Search engine categories==\u000a\u000a===Web search engines===\u000aSearch engines that are expressly designed for searching web pages, documents, and images were developed to facilitate searching through a large, nebulous blob of unstructured resources. They are engineered to follow a multi-stage process: crawling the infinite stockpile of pages and documents to skim the figurative foam from their contents, indexing the foam/buzzwords in a sort of semi-structured form (database or something), and at last, resolving user entries/queries to return mostly relevant results and links to those skimmed documents or pages from the inventory.\u000a\u000a====Crawl====\u000aIn the case of a wholly textual search, the first step in classifying web pages is to find an \u2018index item\u2019 that might relate expressly to the \u2018search term.\u2019 In the past, search engines began with a small list of URLs as a so-called seed list, fetched the content, and parsed the links on those pages for relevant information, which subsequently provided new links. The process was highly cyclical and continued until enough pages were found for the searcher\u2019s use.\u000aThese days, a continuous crawl method is employed as opposed to an incidental discovery based on a seed list. The crawl method is an extension of aforementioned discovery method. Except there is no seed list, because the system never stops worming.\u000a\u000aMost search engines use sophisticated scheduling algorithms to \u201cdecide\u201d when to revisit a particular page, to appeal to its relevance. These algorithms range from constant visit-interval with higher priority for more frequently changing pages to adaptive visit-interval based on several criteria such as frequency of chance, popularity, and overall quality of site. The speed of the web server running the page as well as resource constraints like amount of hardware or bandwidth also figure in.\u000a\u000a====Link map====\u000aThe pages that are discovered by web crawls are often distributed and fed into another computer that creates a veritable map of resources uncovered. The bunchy clustermass looks a little like a graph, on which the different pages are represented as small nodes that are connected by  links between the pages. \u000aThe excess of data is stored in multiple data structures that permit quick access to said data by certain algorithms that compute the popularity score of pages on the web based on how many links point to a certain web page, which is how people can access any number of resources concerned with diagnosing psychosis. Another example would be the accessibility/rank of web pages containing information on Mohamed Morsi versus the very best attractions to visit in Cairo after simply entering \u2018Egypt\u2019 as a search term. One such algorithm, [[PageRank]], proposed by Google founders Larry Page and Sergey Brin, is well known and has attracted a lot of attention because it highlights repeat mundanity of web searches courtesy of students that don\u2019t know how to properly research subjects on Google.\u000aThe idea of doing link analysis to compute a popularity rank is older than PageRank. Other variants of the same idea are currently in use \u2013 grade schoolers do the same sort of computations in picking kickball teams. But in all seriousness, these ideas can be categorized into three main categories: rank of individual pages and nature of web site content. Search engines often differentiate between internal links and external links, because web masters and mistresses are not strangers to shameless self-promotion. Link map data structures typically store the anchor text embedded in the links as well, because anchor text can often provide a \u201cvery good quality\u201d summary of a web page\u2019s content.\u000a\u000a===Database Search Engines===\u000aSearching for text-based content in databases presents a few special challenges from which a number of specialized search engines flourish. Databases can be slow when solving complex queries (with multiple logical or string matching arguments). Databases allow pseudo-logical queries which full-text searches do not use. There is no crawling necessary for a database since the data is already structured. However, it is often necessary to index the data in a more economized form to allow a more expeditious search.\u000a\u000a===Mixed Search Engines===\u000aSometimes, data searched contains both database content and web pages or documents. Search engine technology has developed to respond to both sets of requirements. Most mixed search engines are large Web search engines, like Google. They search both through structured and unstructured data sources. Take for example, the word \u2018ball.\u2019 In its simplest terms, it returns more than 40 variations on Wikipedia alone. Did you mean a ball, as in the social gathering/dance? A soccer ball? The ball of the foot? Pages and documents are crawled and indexed in a separate index. Databases are indexed also from various sources. Search results are then generated for users by querying these multiple indices in parallel and compounding the results according to \u201crules.\u201d\u000a\u000a<!-- \u000aWorking on article, loosely pasting in snippets of information to use in improving \u000aarticle later, leaving all this in comments while I work on it\u000a\u000aLOTS OF WORK TO DO\u000a\u000aPotential sections to research into..\u000a\u000a== Models of Information Retrieval ==\u000a=== Boolean Model ===\u000a=== Vector Model ===\u000a\u000a== Document Preprocessing ==\u000a# Tokenization ===\u000a# Stemming ===\u000a# The Porter Algorithm\u000a# Storing, indexing, and searching text\u000a#Inverted indexes\u000a\u000a== Word Distributions ==\u000aThe Zipf distribution\u000aThe Benford distribution\u000aHeap's law. TF*IDF. Vector space similarity and ranking.\u000a\u000a== Retrieval evaluation ==\u000a Precision and Recall. F-measure. Reference collections. The TREC conferences.\u000a\u000a== Automated indexing/labeling ==\u000a. Compression and coding. Optimal codes.\u000a\u000a== String matching ==\u000a. Approximate matching.\u000a\u000a== Query expansion ==. Relevance feedback.\u000a\u000a== Text classification ==\u000a. Naive Bayes. Feature selection. Decision trees.\u000a\u000aLinear classifiers. k-nearest neighbors. Perceptron. Kernel methods. Maximum-margin classifiers. Support vector machines. Semi-supervised learning.\u000aLexical semantics and Wordnet.\u000aLatent semantic indexing. Singular value decomposition. Vector space clustering. k-means clustering. EM clustering.\u000aRandom graph models. Properties of random graphs: clustering coefficient, betweenness, diameter, giant connected component, degree distribution.\u000aSocial network analysis. Small worlds and scale-free networks. Power law distributions. Centrality.\u000aGraph-based methods. Harmonic functions. Random walks. PageRank. Hubs and authorities. Bipartite graphs. HITS. Models of the Web.\u000a\u000aCrawling the web. Webometrics. Measuring the size of the web. The Bow-tie-method.\u000aHypertext retrieval. Web-based IR. Document closures. Focused crawling.\u000aQuestion answering\u000aBurstiness. Self-triggerability\u000aInformation extraction\u000aAdversarial IR. Human behavior on the web. Text summarization\u000a\u000a== Search Engine Parts ==\u000a\u000aThere are three main parts to every search engine: Spider, Index, and Web Interface.\u000a\u000a=== Spider === \u000a   \u000aA spider crawls the web. It follows links and scans web pages. All search engines have periods of deep crawl and quick crawl. During a deep crawl, the spider follows all links it can find and scans web pages in their entirety. During a quick crawl, the spider does not follow all links and may not scan pages in their entirety.\u000a\u000aThe job of the spider is to discover new pages and to collect copies of those pages, which are then analyzed in the index.\u000a\u000a==== Crawl Rate ====\u000a\u000aPages that are considered important get crawled frequently. The crawl rate depends directly on link popularity and domain authority.\u000a\u000aIf many links point to a website, it may be an important site, so it makes sense to crawl it more often than a site with fewer links. This is also a money-saving issue. If search engines were to crawl all sites at an equal rate, it would take more time overall and cost more as a result.\u000a\u000a=== Index ===\u000a\u000aThe index is the place where search engines keep basic copies of web pages and sort search results. When you a do a search, search engines do not search the web; they show results from their index. The number of pages in the index does not represent the entire web, but the number of pages that the spider has discovered, scanned and saved.\u000a\u000aThe index is the place where search engineers apply algorithms, and it is the place where rankings are partially determined. Search engineers may choose to apply an algorithm to the entire index, or only to a portion of it.\u000a\u000a==== Datacenters and Different Indexes ====\u000a\u000aSearch engines have multiple datacenters around the world. When you enter a search term, your query is directed to the closest datacenter.\u000a\u000aDifferent datacenters may have slightly different indexes, especially during an update. As a result, search results may differ depending on your location.\u000a\u000a== History ==\u000a\u000a=== Meta Tags ===\u000a\u000aMeta tags were designed to help search engines sort web pages. Pages included keywords in meta tags telling search engines about the contents of each page. For a short time meta tags worked and helped search engines serve relevant results, but over time marketers learned they could easily rank by stuffing those tags with keywords.\u000a\u000aAs a result, search engine optimization in those days became about cramming "loans, loans, loans, loans, loans" into the meta tag. Search engines got spammed beyond being of any use, and many faced an exodus of users as a result.\u000a\u000aYahoo started as web directory in 1994 and outsourced their search until 2004. Google launched in 1996 and did not have a successful business model until 2001. Microsoft did not come on the search engine scene until 2003.\u000aor more information on search engine history, you may want to investigate Search Engine History, a site entirely devoted to this topic. It also touches on the history of search engine optimization. Additionally, Web Master World has an excellent thread that covers the history of SEO.\u000a\u000aWeb Interface\u000a\u000aWhen you search using a web interface (like Google.com), in many cases results are already presorted to a certain extent. The degree to which results are presorted depends on the complexity of the algorithm. If the time to apply an algorithm to the index is considerable, then that algorithm is applied in advance. On the other hand, some algorithms are applied at the time when the search query is requested.\u000a\u000aSearch queries go through analysis to determine the possible intent behind the query. Google is currently leading in this area.\u000a\u000aStop Words\u000a\u000a"Stop words" are words that are frequently used in the English language. Those words include a, the, all, also, but, down, full, much etc. They are words that are used by everyone regardless of the topic. Generally, search engines ignore "stop" words and will usually correct your search to exclude them. For example, when you search for "cat and dog" search engines will exclude "and" and only search for "cat" "dog."\u000a\u000aGoogle does use stop words to an extent.\u000a\u000aKeyword Density\u000a\u000aKeyword density is a measure of how often a word appears on the page in relation to other words. It is an over-hyped measurement that doesn\u2019t help in search rankings. Search engines use far more than keyword density for on-page analysis. Their technology includes the location of terms on the page, word proximity and natural language processing.\u000a\u000aGoogle has purchased Applied Semantics for its AdSense Network, but may also be using this technology for on-page analysis. Additionally, please keep in mind that one of Google\u2019s current projects involves scanning thousands of books, from which it may learn more about natural language patterns.\u000a\u000aLocation of Terms on The Page\u000a\u000aBy analyzing how terms are located in relation to each other on the page, search engines can determine partial relevancy of the page. The closer terms are to each other, the more relevant a page is.\u000a\u000aIn many cases, keywords appear separately from each other throughout the page. This is considered normal in most cases, but be sure to include a term together at least once in the title, heading or paragraph.\u000a\u000aLink Analysis\u000a\u000aLink analysis is at the core of all search engine relevancy. Apart from Page Rank and general link popularity, Google looks at: link anchor text, the page from which the link comes, age of the link, location of the link, title of the page from which the link comes, authority of the linking page and more.\u000a\u000aLinks are the biggest quality indicators that search engines have at the moment. Before search engines existed, and before the web was commercialized it was much harder to find information. All you had to rely on was links. There were few if any spammers, and people who found interesting sites shared those sites with others by placing a link. Also, the first web pages and servers were universities and colleges; this is why Google is biased toward .edu domains \u2013 they were the first on the scene, and usually contain quality content and resources.\u000a\u000aAs the web became commercial and Google\u2019s Page Rank well known, links became a form of advertising, where a link could be bought or artificially made by spammers. This is the reason for Google\u2019s bias toward older links and links from trusted domains.\u000a\u000aYahoo put less weight on link analysis than Google, while Ask.com is more about "authoritative hubs." Ask.com generally has a harder time ranking documents unless there\u2019s a community around a topic.\u000a\u000aSize and Length of the Page\u000a\u000aThere\u2019s no "best" page copy length for ranking on search results. Search engines have specifically addressed this issue, and both long content and short content have equal chances to rank.\u000a\u000aBehavioral Feedback\u000a\u000aAll major search engines such as Google, Yahoo, Live and Ask collect user feedback about web pages. They look at search queries, prior search queries, time interval between those queries and semantic relationships in order to learn more about intent. They also track click through rates for different listings. If, for example, users click on a listing and then go back right away, search engines may remove that listing and artificially lower its position for one or more keywords.\u000a\u000aThis brings up the fact that user experience is becoming an important part of SEO. As search engines collect more data, they are constantly learning to interpret it. As they get better at it, retaining users on your pages for a certain time period (maybe a benchmark for an industry) may become an important factor in the SEO game.\u000a\u000aBehavior feedback is currently used in personalized search.\u000a<ref>{{cite web|url=http://www.seochat.com/c/a/search-engine-news/the-history-of-search-and-search-technology/|accessdate=1 June 2014}}</ref>\u000a-->\u000a\u000a==See also==\u000a*[[Database search engine]]\u000a*[[Enterprise search]]\u000a*[[Search engine]]\u000a*[[Disambiguation]]\u000a*[[Search engine indexing]]\u000a*[[Web crawler]]\u000a*[[Structured Search]]\u000a\u000a==External links==\u000a* [http://www.searchtools.com/info/database-search.html Searching for Text Information in Databases]\u000a* [http://www.urbandictionary.com/define.php?term=Searchency Searchency]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a{{DEFAULTSORT:Search Engine Technology}}\u000a[[Category:Internet search engines]]\u000a[[Category:Information retrieval]]
p18
sg6
S'Search engine technology'
p19
ssI6
(dp20
g2
S'http://en.wikipedia.org/wiki/IFACnet'
p21
sg4
V'''IFACnet''', the KnowledgeNet for Professional Accountants, is the global, multilingual search engine developed by the [[International Federation of Accountants]] (IFAC) and its members to provide professional accountants worldwide with one-stop access to [[good practice guidance]], articles, management tools and other resources. This enterprise search engine was launched on October 2, 2006 by INDEZ. Originally marketed to professional accountants in business, IFACnet was expanded in March 2007 to provide resources and information relevant to small and medium accounting practices. It now includes resources and information for accountants in all sectors of the profession.\u000a\u000aThe following 31 organizations participate in IFACnet:\u000a\u000a*[[American Institute of Certified Public Accountants]] (AICPA)\u000a*[[Association of Chartered Certified Accountants]] (ACCA)\u000a*[[Canadian Institute of Chartered Accountants]]\u000a*[[Certified General Accountants Association of Canada]]\u000a*[[Chartered Institute of Management Accountants]] (CIMA)\u000a*[[Chartered Institute of Public Finance and Accountancy]]\u000a*[[CMA Canada]]\u000a*[[Compagnie Nationale des Commissaires aux Comptes]]\u000a*[[Conseil Supérieur de l'Ordre des Experts-Comptables]]\u000a*[[Consiglio Nazionale Dottori Commercialisti]]\u000a*[[CPA Australia]]\u000a*[[Délégation Internationale Pour l'Audit et la Comptabilité]]\u000a*[[Hong Kong Institute of Certified Public Accountants]] (HKICPA)\u000a*[[International Federation of Accountants]]  (IFAC)\u000a*[[Institut der Wirtschaftspruefer in Deutschland]] e.V. (IDW)\u000a*[[Institute of Certified Public Accountants in Ireland]]\u000a*[[Institute of Certified Public Accountants of Singapore]]\u000a*[[Institute of Chartered Accountants of Australia]]\u000a*[[Institute of Chartered Accountants in England & Wales]] (ICAEW)\u000a*[[Institute of Chartered Accountants in Ireland]]\u000a*[[Institute of Chartered Accountants of India]]\u000a*[[Institute of Chartered Accountants of Pakistan]]\u000a*[[Institute of Chartered Accountants of Scotland]] (ICAS)\u000a*[[Institute of Management Accountants]]\u000a*[[Japanese Institute of Certified Public Accountants]] (JICPA)\u000a*[[Koninklijk Nederlands Instituut van Registeraccountants]] (Royal NIVRA)\u000a*[[Malaysian Institute of Accountants]]\u000a*[[Malta Institute of Accountants]]\u000a*[[National Association of State Boards of Accountancy]] (NASBA)\u000a*[[South African Institute of Chartered Accountants]] (SAICA)\u000a*[[Union of Chambers of Certified Public Accountants of Turkey]] (TÜRMOB)\u000a\u000a==External links==\u000a*[http://www.ifacnet.com/ IFACnet - A KnowledgeNet for Professional Accountants]\u000a*[http://www.ifac.org/ International Federation of Accountants Homepage]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]\u000a[[Category:Accounting organizations]]
p22
sg6
S'IFACnet'
p23
ssI135
(dp24
g2
S'http://en.wikipedia.org/wiki/Vocabulary mismatch'
p25
sg4
S'\'\'\'Vocabulary mismatch\'\'\' is a common phenomenon in the usage of natural languages, occurring when different people name the same thing or concept differently.\n\nFurnas et al. (1987) were perhaps the first to quantitatively study the vocabulary mismatch problem.<ref>Furnas, G., et al, The Vocabulary Problem in Human-System Communication, Communications of the ACM, 1987, 30(11), pp. 964-971.</ref>  Their results show that on average 80% of the times different people (experts in the same field) will name the same thing differently.  There are usually tens of possible names that can be attributed to the same thing.  This research motivated the work on [[latent semantic indexing]].\n\nThe vocabulary mismatch between user created queries and relevant documents in a corpus causes the term mismatch problem in [[information retrieval]].  Zhao and Callan (2010)<ref>Zhao, L. and Callan, J., Term Necessity Prediction, Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM 2010). Toronto, Canada, 2010.</ref> were perhaps the first to quantitatively study the vocabulary mismatch problem in a retrieval setting.  Their results show that an average query term fails to appear in 30-40% of the documents that are relevant to the user query.  They also showed that this probability of mismatch is a central probability in one of the fundamental probabilistic retrieval models, the [[Binary Independence Model]].  They developed novel term weight prediction methods that can lead to potentially 50-80% accuracy gains in retrieval over strong keyword retrieval models.  Further research along the line shows that expert users can use Boolean Conjunctive Normal Form expansion to improve retrieval performance by 50-300% over unexpanded keyword queries.<ref name="cnf">Zhao, L. and Callan, J., Automatic term mismatch diagnosis for selective query expansion, SIGIR 2012.</ref>\n\n== Techniques that solve mismatch ==\nZhao provided a survey of common techniques that can solve mismatch in the dissertation on term mismatch.<ref>Zhao, L., Modeling and Solving Term Mismatch in Full-text Retrieval, PhD Dissertation, Carnegie Mellon University, 2012. [http://www.cs.cmu.edu/~lezhao/thesis/diss-Le.pdf URL] retrieved 9/3/2012.</ref>\n\n===Stemming===\n\n===Full-text indexing versus only indexing keywords or abstracts===\n\n===Usages of inlink anchor text or other social tagging===\n\n===Query expansion===\nA recent study by Zhao and Callan (2012)<ref name="cnf"/> using expert created manual [[Conjunctive normal form]] queries has shown that searchonym expansion in the Boolean conjunctive normal form is much more effective than the traditional bag of word expansion e.g. [[Rocchio algorithm|Rocchio expansion]].\n\nA wiki website called [http://www.wikiquery.org WikiQuery] has been developed by one of the authors of the above study, which helps users create, store and share effective Conjunctive normal form queries.\n\n===Translation based models===\n\n== References ==\n\n{{Reflist}}\n\n[[language code:Title]]\n\n[[Category:Linguistic research]]\n[[Category:Information retrieval]]\n[[Category:Natural language processing]]'
p26
sg6
S'Vocabulary mismatch'
p27
ssI9
(dp28
g2
S'http://en.wikipedia.org/wiki/Bioinformatic Harvester'
p29
sg4
VThe '''Bioinformatic Harvester''' is a bioinformatic meta [[search engine]] created by the [[European Molecular Biology Laboratory]]<ref>{{Cite journal|title= 	Information retrieval on Internet using meta-search engines: A review|authors=Manoj, M, Elizabeth, Jacob |date=Oct 2008|publisher=CSIR|pages=739\u2013746|issn=0022-4456\u000a|journal=JSIR |volume=67 (10)}}</ref> and subsequently hosted and further developed by KIT [[Karlsruhe Institute of Technology]] for [[gene]]s and protein-associated information. Harvester currently works for [[human]], [[mouse]], [[rat]], [[zebrafish]], [[drosophila]] and [[arabidopsis thaliana]] based information. Harvester cross-links >50 popular bioinformatic resources and allows cross searches. Harvester serves tens of thousands of pages every day to scientists and physicians.\u000a\u000a{{Infobox software\u000a| name                  = Bioinformatic Harvester\u000a|developer              = Urban Liebel, Björn Kindler\u000a|latest release version = 4\u000a|latest release date    = {{release date and age|2011|05|24}}\u000a|operating_system       = Web based\u000a|genre                  = Bioinformatics tool\u000a|license                = Public Domain\u000a|website                = http://harvester.kit.edu\u000a}}\u000a\u000a== How Harvester works ==\u000a\u000aHarvester collects information from [[protein]] and gene databases along with information from so called "prediction servers." Prediction server e.g. provide online sequence analysis for a single protein. Harvesters search index is based on the [[International Protein Index|IPI]] and [[UniProt]] protein information collection. The collections consists of:\u000a\u000a* ~72.000 human, ~57.000 mouse, ~41.000 rat, ~51.000 zebrafish, ~35.000 arabidopsis protein pages, which cross-link ~50 major bioinfiormatic resources.\u000a\u000a<!-- Deleted image removed: [[Image:harvester-kit.JPG|thumb| A screenshot of the [http://harvester.kit.edu/ Harvester search engine]]] -->\u000a\u000a== Harvester crosslinks several types of information ==\u000a\u000a===Text based information===\u000afrom the following databases:\u000a\u000a* [[UniProt]], world largest protein database\u000a* [[SOURCE]], convenient gene information overview\u000a* [[Simple Modular Architecture Research Tool]] (SMART),\u000a* [[SOSUI]], predicts transmembrane domains\u000a* [[PSORT]], predicts protein localisation\u000a* [[HomoloGene]], compares proteins from different species\u000a* [[gfp-cdna]], protein localisation with fluorescence microscopy\u000a* [[International Protein Index]] (IPI).\u000a\u000a=== Databases rich in graphical elements ===\u000a...are not collected, but crosslinked via [[iframe]]s. Iframes are transparent windows within a [[HTML]] pages. The iframe windows allows up-to-date viewing of the "iframed," linked databases. Several such iframes are combined on a Harvester protein page. This method allows convenient comparison of information from several databases.\u000a\u000a* NCBI-[[BLAST]], an algorithm for comparing biological sequences from the [[National Center for Biotechnology Information|NCBI]].\u000a* [[Ensembl]], automatic gene annotation by the EMBL-[[European Bioinformatics Institute|EBI]] and [[Sanger Institute]]\u000a* [[FlyBase]] is a database of model organism ''[[Drosophila melanogaster]]''.\u000a* [[GoPubMed]] is a knowledge-based search engine for biomedical texts.\u000a* [[Information Hyperlinked over Proteins|iHOP]], information hyperlinked over proteins via gene/protein synonyms\u000a* [[Mendelian Inheritance in Man]] project catalogues all the known diseases.\u000a* [[RZPD]], German resources Center for genome research in Berlin/Heidelberg.\u000a* [[STRING]], Search Tool for the Retrieval of Interacting Genes/Proteins, developed by [[EMBL]], [[Swiss Institute of Bioinformatics|SIB]] and [[University of Zurich|UZH]].\u000a* [[Zebrafish Information Network]].\u000a* [http://locate.imb.uq.edu.au/ LOCATE] subcellular localization database (mouse).\u000a\u000a=== Access from external application ===\u000a\u000a* [[Genome browser]], working draft assemblies for genomes [[University of California, Santa Cruz|UCSC]]\u000a* [[Google Scholar]]\u000a* [[Mitocheck]]\u000a* [[PolyMeta]], meta search engine for Google, Yahoo, MSN, Ask, Exalead, AllTheWeb, GigaBlast\u000a\u000a== What one can find ==\u000a\u000aHarvester allows a combination of different search terms and single words.\u000a\u000aSearch Examples:\u000a\u000a* Gene-name: "golga3"\u000a* Gene-alias: "ADAP-S ADAS ADHAPS ADPS" (one gene name is sufficient)\u000a* Gene-Ontologies: "Enzyme linked receptor protein signaling pathway"\u000a* [[UniGene|Unigene]]-Cluster: "Hs.449360"\u000a\u000a* Go-annotation: "intra-Golgi transport"\u000a* Molecular function: "protein kinase binding"\u000a* Protein: "Q9NPD3"\u000a* Protein domain: "SH2 sar"\u000a* Protein Localisation: "endoplasmic reticulum"\u000a\u000a* Chromosome: "2q31"\u000a* Disease relevant: use the word "diseaselink"\u000a* Combinations: "golgi diseaselink" (finds all golgi proteins associated with a disease)\u000a* [[mRNA]]: "AL136897"\u000a\u000a* Word: "Cancer"\u000a* Comment: "highly expressed in heart"\u000a* Author: "Merkel, Schmidt"\u000a* Publication or project: "[[cDNA]] sequencing project"\u000a\u000a==See also==\u000a\u000a* [[Biological database]]s\u000a* [[Entrez]]\u000a* [[European Bioinformatics Institute]]\u000a* [[HPRD|Human Protein Reference Database]]\u000a* [[Metadata]]\u000a* [[Sequence profiling tool]]\u000a\u000a== Literature ==\u000a*{{cite journal |author=Liebel U, Kindler B, Pepperkok R |title='Harvester': a fast meta search engine of human protein resources |journal=Bioinformatics |volume=20 |issue=12 |pages=1962\u20133 |date=August 2004 |pmid=14988114 |doi=10.1093/bioinformatics/bth146 |url=http://bioinformatics.oxfordjournals.org/cgi/pmidlookup?view=long&pmid=14988114}}\u000a*{{cite journal |author=Liebel U, Kindler B, Pepperkok R |title=Bioinformatic "Harvester": a search engine for genome-wide human, mouse, and rat protein resources |journal=Meth. Enzymol. |volume=404 |issue= |pages=19\u201326 |year=2005 |pmid=16413254 |doi=10.1016/S0076-6879(05)04003-6 |url=http://linkinghub.elsevier.com/retrieve/pii/S0076-6879(05)04003-6}}\u000a\u000a== Notes and references ==\u000a<references/>\u000a\u000a== External links ==\u000a* http://harvester.kit.edu Bioinformatic Harvester V at KIT [[Karlsruhe Institute of Technology]]\u000a* [http://harvester42.fzk.de Harvester42] at KIT - integrating 50 general search engines\u000a\u000a[[Category:Bioinformatics software]]\u000a[[Category:Biological databases]]\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]\u000a[[Category:Biology websites]]
p30
sg6
S'Bioinformatic Harvester'
p31
ssI138
(dp32
g2
S'http://en.wikipedia.org/wiki/Okapi BM25'
p33
sg4
VIn [[information retrieval]], '''Okapi BM25''' is a [[ranking function]] used by [[search engine]]s to rank matching documents according to their [[Relevance (information retrieval)|relevance]] to a given search query. It is based on the [[Probabilistic relevance model|probabilistic retrieval framework]] developed in the 1970s and 1980s by [[Stephen E. Robertson]], [[Karen Spärck Jones]], and others.\u000a\u000aThe name of the actual ranking function is BM25. To set the right context, however, it usually referred to as "Okapi BM25", since the Okapi information retrieval system, implemented at [[London]]'s [[City University, London|City University]] in the 1980s and 1990s, was the first system to implement this function.\u000a\u000aBM25, and its newer variants, e.g. BM25F (a version of BM25 that can take document structure and anchor text into account), represent state-of-the-art [[TF-IDF]]-like retrieval functions used in document retrieval, such as [[web search]].\u000a\u000a== The ranking function ==\u000a\u000aBM25 is a [[Bag of words model|bag-of-words]] retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of the inter-relationship between the query terms within a document (e.g., their relative proximity). It is not a single function, but actually a whole family of scoring functions, with slightly different components and parameters. One of the most prominent instantiations of the function is as follows.\u000a\u000aGiven a query <math>Q</math>, containing keywords <math>q_1, ..., q_n</math>, the BM25 score of a document <math>D</math> is:\u000a\u000a:<math> \u005ctext{score}(D,Q) = \u005csum_{i=1}^{n} \u005ctext{IDF}(q_i) \u005ccdot \u005cfrac{f(q_i, D) \u005ccdot (k_1 + 1)}{f(q_i, D) + k_1 \u005ccdot (1 - b + b \u005ccdot \u005cfrac{|D|}{\u005ctext{avgdl}})},</math>\u000a\u000awhere <math>f(q_i, D)</math> is <math>q_i</math>'s [[term frequency]] in the document <math>D</math>, <math>|D|</math> is the length of the document <math>D</math> in words, and <math>avgdl</math> is the average document length in the text collection from which documents are drawn. <math>k_1</math> and <math>b</math> are free parameters, usually chosen, in absence of an advanced optimization, as <math>k_1 \u005cin [1.2,2.0]</math> and <math>b = 0.75</math>.<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze. ''An Introduction to Information Retrieval'', Cambridge University Press, 2009, p. 233.</ref> <math>\u005ctext{IDF}(q_i)</math> is the IDF ([[inverse document frequency]]) weight of the query term <math>q_i</math>. It is usually computed as:\u000a\u000a:<math>\u005ctext{IDF}(q_i) = \u005clog \u005cfrac{N - n(q_i) + 0.5}{n(q_i) + 0.5},</math>\u000a\u000awhere <math>N</math> is the total number of documents in the collection, and <math>n(q_i)</math> is the number of documents containing <math>q_i</math>.\u000a\u000aThere are several interpretations for IDF and slight variations on its formula. In the original BM25 derivation, the IDF component is derived from the [[Binary Independence Model]].\u000a\u000aPlease note that the above formula for IDF shows potentially major drawbacks when using it for terms appearing in more than half of the corpus documents. These terms' IDF is negative, so for any two almost-identical documents, one which contains the term and one which does not contain it, the latter will possibly get a larger score.\u000aThis means that terms appearing in more than half of the corpus will provide negative contributions to the final document score. This is often an undesirable behavior, so many real-world applications would deal with this IDF formula in a different way:\u000a\u000a* Each summand can be given a floor of 0, to trim out common terms;\u000a* The IDF function can be given a floor of a constant <math>\u005cepsilon</math>, to avoid common terms being ignored at all;\u000a* The IDF function can be replaced with a similarly shaped one which is non-negative, or strictly positive to avoid terms being ignored at all.\u000a\u000a== IDF information theoretic interpretation ==\u000aHere is an interpretation from information theory. Suppose a query term <math>q</math> appears in <math>n(q)</math> documents. Then a randomly picked document <math>D</math> will contain the term with probability <math>\u005cfrac{n(q)}{N}</math> (where <math>N</math> is again the cardinality of the set of documents in the collection). Therefore, the [[information]] content of the message "<math>D</math> contains <math>q</math>" is:\u000a\u000a:<math>-\u005clog \u005cfrac{n(q)}{N} = \u005clog \u005cfrac{N}{n(q)}.</math>\u000a\u000aNow suppose we have two query terms <math>q_1</math> and <math>q_2</math>. If the two terms occur in documents entirely independently of each other, then the probability of seeing both <math>q_1</math> and <math>q_2</math> in a randomly picked document <math>D</math> is:\u000a\u000a:<math>\u005cfrac{n(q_1)}{N} \u005ccdot \u005cfrac{n(q_2)}{N},</math>\u000a\u000aand the information content of such an event is:\u000a\u000a:<math>\u005csum_{i=1}^{2} \u005clog \u005cfrac{N}{n(q_i)}.</math>\u000a\u000aWith a small variation, this is exactly what is expressed by the IDF component of BM25.\u000a\u000a== Modifications ==\u000a* At the extreme values of the coefficient <math>b</math> BM25 turns into ranking functions known as '''BM11''' (for <math>b=1</math>) and '''BM15''' (for <math>b=0</math>).<ref>http://xapian.org/docs/bm25.html</ref>\u000a* '''BM25F'''<ref>Hugo Zaragoza, Nick Craswell, Michael Taylor, Suchi Saria, and Stephen Robertson. [http://trec.nist.gov/pubs/trec13/papers/microsoft-cambridge.web.hard.pdf ''Microsoft Cambridge at TREC-13: Web and HARD tracks.''] In Proceedings of TREC-2004.</ref>  is a modification of BM25 in which the document is considered to be composed from several fields (such as headlines, main text, anchor text) with possibly different degrees of importance.\u000a* '''BM25+'''<ref>Yuanhua Lv and ChengXiang Zhai. [http://sifaka.cs.uiuc.edu/~ylv2/pub/cikm11-lowerbound.pdf ''Lower-bounding term frequency normalization.''] In Proceedings of CIKM'2011, pages 7-16.</ref> is an extension of BM25. BM25+ was developed to address one deficiency of the standard BM25 in which the component of term frequency normalization by document length is not properly lower-bounded; as a result of this deficiency, long documents which do match the query term can often be scored unfairly by BM25 as having a similar relevancy to shorter documents that do not contain the query term at all. The scoring formula of BM25+ only has one additional free parameter <math>\u005cdelta</math> (a default value is <math>1.0</math> in absence of a training data) as compared with BM25:\u000a\u000a:<math> \u005ctext{score}(D,Q) = \u005csum_{i=1}^{n} \u005ctext{IDF}(q_i) \u005ccdot \u005cleft[ \u005cfrac{f(q_i, D) \u005ccdot (k_1 + 1)}{f(q_i, D) + k_1 \u005ccdot (1 - b + b \u005ccdot \u005cfrac{|D|}{\u005ctext{avgdl}})} + \u005cdelta \u005cright]</math>\u000a\u000a== Footnotes ==\u000a{{Reflist}}\u000a\u000a== References ==\u000a* {{cite conference|author=Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford | title=Okapi at TREC-3 | conference=[http://trec.nist.gov/pubs/trec3/t3_proceedings.html Proceedings of the Third Text REtrieval Conference (TREC 1994)]|location=Gaithersburg, USA|date=November 1994|url=http://trec.nist.gov/pubs/trec3/papers/city.ps.gz}}\u000a\u000a* {{cite conference|author=Stephen E. Robertson, Steve Walker, and Micheline Hancock-Beaulieu|title=Okapi at TREC-7|conference=[http://trec.nist.gov/pubs/trec7/t7_proceedings.html Proceedings of the Seventh Text REtrieval Conference]|location=Gaithersburg, USA|date=November 1998|url=http://trec.nist.gov/pubs/trec7/papers/okapi_proc.pdf.gz}}\u000a\u000a* {{cite doi|10.1016/S0306-4573(00)00015-7}}\u000a\u000a* {{cite doi|10.1016/S0306-4573(00)00016-9}}\u000a\u000a== External links ==\u000a* {{cite book|last1=Robertson|first1=Stephen|last2=Zaragoza|first2=Hugo|title=The Probabilistic Relevance Framework: BM25 and Beyond|date=2009|publisher=NOW Publishers, Inc.|isbn=978-1-60198-308-4|url=http://staff.city.ac.uk/~sb317/papers/foundations_bm25_review.pdf}}\u000a\u000a[[Category:Ranking functions]]\u000a[[Category:Information retrieval]]
p34
sg6
S'Okapi BM25'
p35
ssI12
(dp36
g2
S'http://en.wikipedia.org/wiki/BASE (search engine)'
p37
sg4
S"{{multiple issues|\n{{notability|Web|date=February 2012}}\n{{refimprove|date=June 2009}}\n{{primary sources|date=February 2012}}\n{{one source|date=February 2012}}\n{{no footnotes|date=February 2012}}\n}}\n\n'''BASE''' ('''Bielefeld Academic Search Engine''') is a multi-disciplinary [[search engine]] to scholarly internet resources, created by [[Bielefeld University]] Library in [[Bielefeld]], [[Germany]]. It is based on search technology provided by [[Fast Search & Transfer]] (FAST), a [[Norway|Norwegian]] company. It [[Web harvesting|harvests]] OAI metadata from scientific [[Digital repository|digital repositories]] that implement the [[Open Archives Initiative Protocol for Metadata Harvesting]] (OAI-PMH), and are [[Index (search engine)|indexed]] using FAST's software. In addition to OAI [[metadata]], the library indexes selected web sites and local data collections, all of which can be searched via a single search interface.\n\nIt allows those who use the search engine to search metadata, when available, as well as conducting [[full text search]]es. It contrasts with commercial search engines in multiple ways, including in the types and kinds of resources it searches and the information it offers about the results it finds. Where available, [[Bibliographic database|bibliographic data]] is provided, and the results may be sorted by multiple fields, such as by author or year of publication.\n\n== See also ==\n* [[List of academic databases and search engines]]\n\n==External links==\n* [http://www.base-search.net/ BASE search]\n\n[[Category:Internet search engines]]\n[[Category:Information retrieval]]\n[[Category:Open access (publishing)]]\n[[Category:Bibliographic databases]]\n\n\n{{software-stub}}"
p38
sg6
S'BASE (search engine)'
p39
ssI141
(dp40
g2
Vhttp://en.wikipedia.org/wiki/Human\u2013computer information retrieval
p41
sg4
V'''Human\u2013computer information retrieval''' ('''HCIR''') is the study of [[information retrieval]] techniques that bring human intelligence into the [[search engine|search]] process. The fields of [[human\u2013computer interaction]] (HCI) and information retrieval (IR) have both developed innovative techniques to address the challenge of navigating complex information spaces, but their insights have often failed to cross disciplinary borders. Human\u2013computer information retrieval has emerged in academic research and industry practice to bring together research in the fields of IR and HCI, in order to create new kinds of search systems that depend on continuous human control of the search process.\u000a\u000a== History ==\u000a\u000aThis term ''human\u2013computer information retrieval'' was coined by Gary Marchionini in a series of lectures delivered between 2004 and 2006.<ref name=march2006>Marchionini, G. (2006). Toward Human-Computer Information Retrieval Bulletin, in June/July 2006 Bulletin of the American Society for Information Science. Available online at http://www.asis.org/Bulletin/Jun-06/marchionini.html.</ref> Marchionini\u2019s main thesis is that "HCIR aims to empower people to explore large-scale information bases but demands that people also take responsibility for this control by expending cognitive and physical energy."\u000a\u000aIn 1996 and 1998, a pair of workshops at the [[University of Glasgow]] on [[information retrieval]] and [[human\u2013computer interaction]] sought to address the overlap between these two fields. Marchionini notes the impact of the [[World Wide Web]] and the sudden increase in [[information literacy]] \u2013 changes that were only embryonic in the late 1990s.\u000a\u000aA few workshops have focused on the intersection of IR and HCI. The Workshop on Exploratory Search, initiated by the [[University of Maryland Human-Computer Interaction Lab]] in 2005, alternates between the [[Association for Computing Machinery]] [[Special Interest Group on Information Retrieval]] (SIGIR) and [[CHI (conference)|Special Interest Group on Computer-Human Interaction]] (CHI) conferences. Also in 2005, the [[European Science Foundation]] held an Exploratory Workshop on Information Retrieval in Context. Then, the first Workshop on Human Computer Information Retrieval was held in 2007 at the [[Massachusetts Institute of Technology]].\u000a\u000a== What is HCIR? ==\u000a\u000aHCIR includes various aspects of IR and HCI. These include [[exploratory search]], in which users generally combine querying and browsing strategies to foster learning and investigation; information retrieval in context (i.e., taking into account aspects of the user or environment that are typically not reflected in a query); and interactive information retrieval, which Peter Ingwersen defines as "the interactive communication processes that occur during the retrieval of information by involving all the major participants in information retrieval (IR), i.e. the user, the intermediary, and the IR system."<ref name=ingwer1992>Ingwersen, P. (1992). Information Retrieval Interaction. London: Taylor Graham. Available online at http://vip.db.dk/pi/iri/index.htm.</ref>\u000a\u000aA key concern of HCIR is that IR systems intended for human users be implemented and evaluated in a way that reflects the needs of those users.<ref>{{cite web|title=Mira working group (1996). Evaluation Frameworks for Interactive Multimedia Information Retrieval Applications|url=http://www.dcs.gla.ac.uk/mira/}}</ref>\u000a\u000aMost modern IR systems employ a [[ranking|ranked]] retrieval model, in which the documents are scored based on the [[probability]] of the document\u2019s [[relevance]] to the query.<ref>Grossman, D. and Frieder, O. (2004). Information Retrieval Algorithms and Heuristics. </ref> In this model, the system only presents the top-ranked documents to the user. This systems are typically evaluated based on their [[Information_retrieval#Average precision of precision and recall|mean average precision]] over a set of benchmark queries from organizations like the [[Text Retrieval Conference]] (TREC).\u000a\u000aBecause of its emphasis in using human intelligence in the information retrieval process, HCIR requires different evaluation models \u2013 one that combines evaluation of the IR and HCI components of the system. A key area of research in HCIR involves evaluation of these systems. Early work on interactive information retrieval, such as Juergen Koenemann and [[Nicholas J. Belkin]]\u2019s 1996 study of different levels of interaction for automatic query reformulation, leverage the standard IR measures of [[Information_retrieval#Precision|precision]] and [[Information_retrieval#Recall|recall]] but apply them to the results of multiple iterations of user interaction, rather than to a single query response.<ref name=koene1996>Koenemann, J. and Belkin, N. J. (1996). A case for interaction: a study of interactive information retrieval behavior and effectiveness. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems: Common Ground (Vancouver, British Columbia, Canada, April 13\u201318, 1996). M. J. Tauber, Ed. CHI \u201896. ACM Press, New York, NY, 205-212. Available online at http://sigchi.org/chi96/proceedings/papers/Koenemann/jk1_txt.htm.\u000a</ref> Other HCIR research, such as Pia Borlund\u2019s IIR evaluation model, applies a methodology more reminiscent of HCI, focusing on the characteristics of users, the details of experimental design, etc.<ref name=borlund2003>Borlund, P. (2003). The IIR evaluation model: a framework for evaluation of interactive information retrieval systems. Information Research, 8(3), Paper 152. Available online at http://informationr.net/ir/8-3/paper152.html.</ref>\u000a\u000a== Goals ==\u000a\u000aMarchionini put forth the following goals towards a system where the user has more control in determining relevant results.<ref name=march2006/>\u000a\u000aSystems should\u000a*no longer only deliver the relevant documents, but must also provide semantic information along with those documents\u000a*increase user responsibility as well as control; that is, information systems require human intellectual effort\u000a*have flexible architectures so they may evolve and adapt to increasingly more demanding and knowledgeable user bases\u000a*aim to be part of information ecology of personal and [[Collective memory|shared memories]] and tools rather than discrete standalone services\u000a*support the entire [[information life cycle]] (from creation to preservation) rather than only the dissemination or use phase\u000a*support tuning by end users and especially by information professionals who add value to information resources\u000a*be engaging and fun to use\u000a\u000aIn short, information retrieval systems are expected to operate in the way that good libraries do. Systems should help users to bridge the gap between data or information (in the very narrow, granular sense of these terms) and knowledge (processed data or information that provides the context necessary to inform the next iteration of an information seeking process). That is, good libraries provide both the information a patron needs as well as a partner in the learning process \u2014 the [[information professional]] \u2014 to navigate that information, make sense of it, preserve it, and turn it into knowledge (which in turn creates new, more informed information needs).\u000a\u000a== Techniques ==\u000a\u000aThe techniques associated with HCIR emphasize representations of information that use human intelligence to lead the user to relevant results. These techniques also strive to allow users to explore and digest the dataset without penalty, i.e., without expending unnecessary costs of time, mouse clicks, or context shift.\u000a\u000aMany [[search engines]] have features that incorporate HCIR techniques. [[Spelling suggestion]]s and [[query expansion|automatic query reformulation]] provide mechanisms for suggesting potential search paths that can lead the user to relevant results. These suggestions are presented to the user, putting control of selection and interpretation in the user\u2019s hands.\u000a\u000a[[Faceted search]] enables users to navigate information [[hierarchy|hierarchically]], going from a category to its sub-categories, but choosing the order in which the categories are presented. This contrasts with traditional [[Taxonomy (general)|taxonomies]] in which the hierarchy of categories is fixed and unchanging. [[Faceted classification|Faceted navigation]], like taxonomic navigation, guides users by showing them available categories (or facets), but does not require them to browse through a hierarchy that may not precisely suit their needs or way of thinking.<ref>Hearst, M. (1999). User Interfaces and Visualization, Chapter 10 of Baeza-Yates, R. and Ribeiro-Neto, B., Modern Information Retrieval.</ref>\u000a\u000a[[Lookahead]] provides a general approach to penalty-free exploration. For example, various [[web applications]] employ [[Ajax (programming)|AJAX]] to automatically complete query terms and suggest popular searches. Another common example of lookahead is the way in which search engines annotate results with summary information about those results, including both static information (e.g., [[metadata]] about the objects) and "snippets" of document text that are most pertinent to the words in the search query.\u000a\u000a[[Relevance feedback]] allows users to guide an IR system by indicating whether particular results are more or less relevant.<ref>Rocchio, J. (1971). Relevance feedback in information retrieval. In: Salton, G (ed), The SMART Retrieval System.</ref>\u000a\u000aSummarization and [[analytics]] help users digest the results that come back from the query. Summarization here is intended to encompass any means of [[aggregate data|aggregating]] or [[data compression|compressing]] the query results into a more human-consumable form. Faceted search, described above, is one such form of summarization. Another is [[cluster analysis|clustering]], which analyzes a set of documents by grouping similar or co-occurring documents or terms. Clustering allows the results to be partitioned into groups of related documents. For example, a search for "java" might return clusters for [[Java (programming language)]], [[Java|Java (island)]], or [[Java (coffee)]].\u000a\u000a[[information visualization|Visual representation of data]] is also considered a key aspect of HCIR. The representation of summarization or analytics may be displayed as tables, charts, or summaries of aggregated data. Other kinds of [[information visualization]] that allow users access to summary views of search results include [[tag clouds]] and [[treemapping]].\u000a\u000a== References ==\u000a\u000a<References/>\u000a\u000a==External links==\u000a*{{cite web|url=https://sites.google.com/site/hcirworkshop/ |title=Workshops on Human Computer Information Retrieval}}\u000a*{{cite web|url=http://www.chiir.org/ |title=ACM SIGIR Conference on Human Information Interaction and Retrieval (CHIIR)}}\u000a\u000a{{DEFAULTSORT:Human-computer information retrieval}}\u000a[[Category:Information retrieval]]\u000a[[Category:Human\u2013computer interaction]]
p42
sg6
VHuman\u2013computer information retrieval
p43
ssI15
(dp44
g2
S'http://en.wikipedia.org/wiki/Latent semantic mapping'
p45
sg4
S"'''Latent semantic mapping (LSM)''' is a data-driven framework to model globally meaningful relationships implicit in large volumes of (often textual) data. It is a generalization of [[latent semantic analysis]]. In information retrieval, LSA enables retrieval on the basis of conceptual content, instead of merely matching words between queries and documents.\n\nLSM was derived from earlier work on latent semantic analysis.  There are 3 main characteristics of latent semantic analysis: Discrete entities, usually in the form of words and documents, are mapped onto continuous vectors, the mapping involves a form of global correlation pattern, and dimensionality reduction is an important aspect of the analysis process. These constitute generic properties, and have been identified as potentially useful in a variety of different contexts.  This usefulness has encouraged great interest in LSM. The intended product of latent semantic mapping, is a data-driven framework for modeling relationships in large volumes of data.\n\n[[Mac OS X v10.5]] and later includes a [[Software framework|framework]] implementing latent semantic mapping.<ref>[http://developer.apple.com/documentation/TextFonts/Reference/LatentSemanticMapping/index.html API Reference: Latent Semantic Mapping Framework Reference<!-- Bot generated title -->]</ref>\n\n== See also ==\n* [[Latent semantic analysis]]\n\n== Notes ==\n{{reflist}}\n\n== References ==\n* {{cite journal\n | url=http://ieeexplore.ieee.org/iel5/79/32367/01511825.pdf\n | title=Latent semantic mapping [information retrieval]\n | author=Bellegarda, J.R.\n | date=2005\n}}\n* {{cite conference\n | url=https://www.securecms.com/ICASSP2006/Tutorial_06.asp\n | title=Latent semantic mapping: Principles and applications\n | author=J. Bellegarda\n | booktitle=ICASSP 2006\n | date=2006\n}}\n\n[[Category:Information retrieval]]\n[[Category:Natural language processing]]\n\n\n{{semantics-stub}}\n{{compu-stub}}"
p46
sg6
S'Latent semantic mapping'
p47
ssI144
(dp48
g2
S'http://en.wikipedia.org/wiki/Information retrieval'
p49
sg4
V{{Information science}}\u000a\u000a'''Information retrieval''' ('''IR''') is the activity of obtaining [[information]] resources relevant to an information need from a collection of information resources.  Searches can be based on [[metadata]] or on [[Full text search|full-text]] (or other content-based) indexing.\u000a\u000aAutomated information retrieval systems are used to reduce what has been called "[[information overload]]". Many universities and [[public library|public libraries]] use IR systems to provide access to books, journals and other documents. [[Web search engine]]s are the most visible [[Information retrieval applications|IR applications]].\u000a\u000a== Overview ==\u000a\u000aAn information retrieval process begins when a user enters a [[query string|query]] into the system. Queries are formal statements of [[information need]]s, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of [[relevance|relevancy]].\u000a\u000aAn object is an entity that is represented by information in a [[database]]. User queries are matched against the database information. Depending on the [[Information retrieval applications|application]] the data objects may be, for example, text documents, images,<ref name=goodron2000>{{cite journal |first=Abby A. |last=Goodrum |title=Image Information Retrieval: An Overview of Current Research |journal=Informing Science |volume=3 |number=2 |year=2000 }}</ref> audio,<ref name=Foote99>{{cite journal |first=Jonathan |last=Foote |title=An overview of audio information retrieval |journal=Multimedia Systems |year=1999 |publisher=Springer }}</ref> [[mind maps]]<ref name=Beel2009>{{cite journal |first=Jöran |last=Beel |first2=Bela |last2=Gipp |first3=Jan-Olaf |last3=Stiller |contribution=Information Retrieval On Mind Maps - What Could It Be Good For? |contribution-url=http://www.sciplore.org/publications_en.php |title=Proceedings of the 5th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom'09) |year=2009 |publisher=IEEE |place=Washington, DC }}</ref> or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.\u000a\u000aMost IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.<ref name="Frakes1992">{{cite book |last=Frakes |first=William B. |title=Information Retrieval Data Structures & Algorithms |publisher=Prentice-Hall, Inc. |year=1992 |isbn=0-13-463837-9 |url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes }}</ref>\u000a\u000a== History ==\u000a{{Rquote|right|But do you know that, although I have kept the diary [on a phonograph] for months past, it never once struck me how I was going to find any particular part of it in case I wanted to look it up?|[[John Seward|Dr Seward]]| [[Bram Stoker]]'s ''[[Dracula]]'',\u000a 1897}}\u000aThe idea of using computers to search for relevant pieces of information was popularized in the article ''[[As We May Think]]'' by [[Vannevar Bush]] in 1945.<ref name="Singhal2001">{{cite journal |last=Singhal |first=Amit |title=Modern Information Retrieval: A Brief Overview |journal=Bulletin of the IEEE Computer Society Technical Committee on Data Engineering |volume=24 |issue=4 |pages=35\u201343 |year =2001 |url=http://singhal.info/ieee2001.pdf }}</ref> The first automated information retrieval systems were introduced in the 1950s and 1960s. By 1970 several different techniques had been shown to perform well on small [[text corpora]] such as the Cranfield collection (several thousand documents).<ref name="Singhal2001" /> Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.\u000a\u000aIn 1992, the US Department of Defense along with the [[National Institute of Standards and Technology]] (NIST), cosponsored the [[Text Retrieval Conference]] (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that [[scalability|scale]] to huge corpora. The introduction of [[web search engine]]s has boosted the need for very large scale retrieval systems even further.\u000a\u000a== Model types ==\u000a[[File:Information-Retrieval-Models.png|thumb|500px|Categorization of IR-models (translated from [[:de:Informationsrückgewinnung#Klassifikation von Modellen zur Repräsentation natürlichsprachlicher Dokumente|German entry]], original source [http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&lng=eng&id= Dominik Kuropka]).]]\u000aFor effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.\u000a\u000a=== First dimension: mathematical basis ===\u000a* ''Set-theoretic'' models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are:\u000a** [[Standard Boolean model]]\u000a** [[Extended Boolean model]]\u000a** [[Fuzzy retrieval]]\u000a* ''Algebraic models'' represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value.\u000a** [[Vector space model]]\u000a** [[Generalized vector space model]]\u000a** [[Topic-based vector space model|(Enhanced) Topic-based Vector Space Model]]\u000a** [[Extended Boolean model]]\u000a** [[Latent semantic indexing]] a.k.a. [[latent semantic analysis]]\u000a* ''Probabilistic models'' treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the [[Bayes' theorem]] are often used in these models.\u000a** [[Binary Independence Model]]\u000a** [[Probabilistic relevance model]] on which is based the [[Probabilistic relevance model (BM25)|okapi (BM25)]] relevance function\u000a** [[Uncertain inference]]\u000a** [[Language model]]s\u000a** [[Divergence-from-randomness model]]\u000a** [[Latent Dirichlet allocation]]\u000a* ''Feature-based retrieval models'' view documents as vectors of values of ''feature functions'' (or just ''features'') and seek the best way to combine these features into a single relevance score, typically by [[learning to rank]] methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just a yet another feature.\u000a\u000a=== Second dimension: properties of the model ===\u000a* ''Models without term-interdependencies'' treat different terms/words as independent. This fact is usually represented in vector space models by the [[orthogonality]] assumption of term vectors or in probabilistic models by an [[Independence (mathematical logic)|independency]] assumption for term variables.\u000a* ''Models with immanent term interdependencies'' allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by [[dimension reduction|dimensional reduction]]) from the [[co-occurrence]] of those terms in the whole set of documents.\u000a* ''Models with transcendent term interdependencies'' allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example a human or sophisticated algorithms.)\u000a\u000a== Performance and correctness measures ==\u000a{{main|Precision and recall}}\u000a\u000aMany different measures for evaluating the performance of information retrieval systems have been proposed. The measures require a collection of documents and a query. All common measures described here assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice queries may be [[ill-posed]] and there may be different shades of relevancy.\u000a\u000a=== Precision ===\u000a\u000aPrecision is the fraction of the documents retrieved that are [[Relevance (information retrieval)|relevant]] to the user's information need.\u000a\u000a:<math> \u005cmbox{precision}=\u005cfrac{|\u005c{\u005cmbox{relevant documents}\u005c}\u005ccap\u005c{\u005cmbox{retrieved documents}\u005c}|}{|\u005c{\u005cmbox{retrieved documents}\u005c}|} </math>\u000a\u000aIn [[binary classification]], precision is analogous to [[positive predictive value]]. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called ''precision at n'' or ''P@n''.\u000a\u000aNote that the meaning and usage of "precision" in the field of Information Retrieval differs from the definition of [[accuracy and precision]] within other branches of science and [[statistics]].\u000a\u000a=== Recall ===\u000a\u000aRecall is the fraction of the documents that are relevant to the query that are successfully retrieved.\u000a\u000a:<math>\u005cmbox{recall}=\u005cfrac{|\u005c{\u005cmbox{relevant documents}\u005c}\u005ccap\u005c{\u005cmbox{retrieved documents}\u005c}|}{|\u005c{\u005cmbox{relevant documents}\u005c}|} </math>\u000a\u000aIn binary classification, recall is often called [[sensitivity and specificity|sensitivity]]. So it can be looked at as ''the probability that a relevant document is retrieved by the query''.\u000a\u000aIt is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.\u000a\u000a=== Fall-out ===\u000aThe proportion of non-relevant documents that are retrieved, out of all non-relevant documents available:\u000a\u000a:<math> \u005cmbox{fall-out}=\u005cfrac{|\u005c{\u005cmbox{non-relevant documents}\u005c}\u005ccap\u005c{\u005cmbox{retrieved documents}\u005c}|}{|\u005c{\u005cmbox{non-relevant documents}\u005c}|} </math>\u000a\u000aIn binary classification, fall-out is closely related to [[sensitivity and specificity|specificity]] and is equal to <math>(1-\u005cmbox{specificity})</math>. It can be looked at as ''the probability that a non-relevant document is retrieved by the query''.\u000a\u000aIt is trivial to achieve fall-out of 0% by returning zero documents in response to any query.\u000a\u000a=== F-measure ===\u000a{{main|F-score}}\u000aThe weighted [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score is:\u000a\u000a:<math>F = \u005cfrac{2 \u005ccdot \u005cmathrm{precision} \u005ccdot \u005cmathrm{recall}}{(\u005cmathrm{precision} + \u005cmathrm{recall})}.\u005c,</math>\u000a\u000aThis is also known as the <math>F_1</math> measure, because recall and precision are evenly weighted.\u000a\u000aThe general formula for non-negative real <math>\u005cbeta</math> is:\u000a:<math>F_\u005cbeta = \u005cfrac{(1 + \u005cbeta^2) \u005ccdot (\u005cmathrm{precision} \u005ccdot \u005cmathrm{recall})}{(\u005cbeta^2 \u005ccdot \u005cmathrm{precision} + \u005cmathrm{recall})}\u005c,</math>.\u000a\u000aTwo other commonly used F measures are the <math>F_{2}</math> measure, which weights recall twice as much as precision, and the <math>F_{0.5}</math> measure, which weights precision twice as much as recall.\u000a\u000aThe F-measure was derived by van Rijsbergen (1979) so that <math>F_\u005cbeta</math> "measures the effectiveness of retrieval with respect to a user who attaches <math>\u005cbeta</math> times as much importance to recall as precision".  It is based on van Rijsbergen's effectiveness measure <math>E = 1 - \u005cfrac{1}{\u005cfrac{\u005calpha}{P} + \u005cfrac{1-\u005calpha}{R}}</math>.  Their relationship is <math>F_\u005cbeta = 1 - E</math> where <math>\u005calpha=\u005cfrac{1}{1 + \u005cbeta^2}</math>.\u000a\u000a=== Average precision ===\u000a<!-- [[Average precision]] redirects here -->\u000aPrecision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision <math>p(r)</math> as a function of recall <math>r</math>. Average precision computes the average value of <math>p(r)</math> over the interval from <math>r=0</math> to <math>r=1</math>:<ref name="zhu2004">{{cite journal |first=Mu |last=Zhu |contribution=Recall, Precision and Average Precision |contribution-url=http://sas.uwaterloo.ca/stats_navigation/techreports/04WorkingPapers/2004-09.pdf |year=2004 }}</ref>\u000a:<math>\u005coperatorname{AveP} = \u005cint_0^1 p(r)dr</math>\u000aThat is the area under the precision-recall curve.\u000aThis integral is in practice replaced with a finite sum over every position in the ranked sequence of documents:\u000a:<math>\u005coperatorname{AveP} = \u005csum_{k=1}^n P(k) \u005cDelta r(k)</math>\u000awhere <math>k</math> is the rank in the sequence of retrieved documents, <math>n</math> is the number of retrieved documents, <math>P(k)</math> is the precision at cut-off <math>k</math> in the list, and <math>\u005cDelta r(k)</math> is the change in recall from items <math>k-1</math> to <math>k</math>.<ref name="zhu2004" />\u000a\u000aThis finite sum is equivalent to:\u000a:<math> \u005coperatorname{AveP} = \u005cfrac{\u005csum_{k=1}^n (P(k) \u005ctimes \u005coperatorname{rel}(k))}{\u005cmbox{number of relevant documents}} \u005c!</math>\u000awhere <math>\u005coperatorname{rel}(k)</math> is an indicator function equaling 1 if the item at rank <math>k</math> is a relevant document, zero otherwise.<ref name="Turpin2006">{{cite journal |last=Turpin |first=Andrew |last2=Scholer |first2=Falk |title=User performance versus precision measures for simple search tasks |journal=Proceedings of the 29th Annual international ACM SIGIR Conference on Research and Development in information Retrieval (Seattle, WA, August 06\u201311, 2006) |publisher=ACM |location=New York, NY |pages=11\u201318 |doi=10.1145/1148170.1148176 |year=2006 |isbn=1-59593-369-7 }}</ref> Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.\u000a\u000aSome authors choose to interpolate the <math>p(r)</math> function to reduce the impact of "wiggles" in the curve.<ref name=voc2010>{{cite journal |last=Everingham |first=Mark |last2=Van Gool |first2=Luc |last3=Williams |first3=Christopher K. I. |last4=Winn |first4=John |last5=Zisserman |first5=Andrew |title=The PASCAL Visual Object Classes (VOC) Challenge |journal=International Journal of Computer Vision |volume=88 |issue=2 |pages=303\u2013338 |publisher=Springer |date=June 2010 |url=http://pascallin.ecs.soton.ac.uk/challenges/VOC/pubs/everingham10.pdf |accessdate=2011-08-29 |doi=10.1007/s11263-009-0275-4 }}</ref><ref name="nlpbook">{{cite book |last=Manning |first=Christopher D. |last2=Raghavan |first2=Prabhakar |last3=Schütze |first3=Hinrich |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008 |url=http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html }}</ref> For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}:<ref name="voc2010" /><ref name="nlpbook" />\u000a:<math>\u005coperatorname{AveP} = \u005cfrac{1}{11} \u005csum_{r \u005cin \u005c{0, 0.1, \u005cldots, 1.0\u005c}} p_{\u005coperatorname{interp}}(r)</math>\u000awhere <math>p_{\u005coperatorname{interp}}(r)</math> is an interpolated precision that takes the maximum precision over all recalls greater than <math>r</math>:\u000a:<math>p_{\u005coperatorname{interp}}(r) = \u005coperatorname{max}_{\u005ctilde{r}:\u005ctilde{r} \u005cgeq r} p(\u005ctilde{r})</math>.\u000a\u000aAn alternative is to derive an analytical <math>p(r)</math> function by assuming a particular parametric distribution for the underlying decision values. For example, a ''binormal precision-recall curve'' can be obtained by assuming decision values in both classes to follow a Gaussian distribution.<ref>K.H. Brodersen, C.S. Ong, K.E. Stephan, J.M. Buhmann (2010). [http://icpr2010.org/pdfs/icpr2010_ThBCT8.28.pdf The binormal assumption on precision-recall curves]. ''Proceedings of the 20th International Conference on Pattern Recognition'', 4263-4266.</ref>\u000a\u000a=== R-Precision ===\u000a\u000aPrecision at '''R'''-th position in the ranking of results for a query that has '''R''' relevant documents. This measure is highly correlated to Average Precision. Also, Precision is equal to Recall at the '''R'''-th position.\u000a\u000a=== Mean average precision ===\u000a<!-- [[Mean average precision]] redirects here -->\u000aMean average precision for a set of queries is the mean of the average precision scores for each query.\u000a:<math> \u005coperatorname{MAP} = \u005cfrac{\u005csum_{q=1}^Q \u005coperatorname{AveP(q)}}{Q} \u005c!</math>\u000awhere ''Q'' is the number of queries.\u000a\u000a=== Discounted cumulative gain ===\u000a{{main|Discounted cumulative gain}}\u000aDCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.\u000a\u000aThe DCG accumulated at a particular rank position <math>p</math> is defined as:\u000a\u000a:<math> \u005cmathrm{DCG_{p}} = rel_{1} + \u005csum_{i=2}^{p} \u005cfrac{rel_{i}}{\u005clog_{2}i}. </math>\u000a\u000aSince result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p (<math>IDCG_p</math>), which normalizes the score:\u000a\u000a:<math> \u005cmathrm{nDCG_{p}} = \u005cfrac{DCG_{p}}{IDCG{p}}. </math>\u000a\u000aThe nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the <math>DCG_p</math> will be the same as the <math>IDCG_p</math> producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.\u000a\u000a=== Other Measures ===\u000a* [[Mean reciprocal rank]]\u000a* [[Spearman's rank correlation coefficient]]\u000a\u000a=== Timeline ===\u000a\u000a* Before the '''1900s'''\u000a*: '''1801''': [[Joseph Marie Jacquard]] invents the [[Jacquard loom]], the first machine to use punched cards to control a sequence of operations.\u000a*: '''1880s''': [[Herman Hollerith]] invents an electro-mechanical data tabulator using punch cards as a machine readable medium.\u000a*: '''1890''' Hollerith [[Punched cards|cards]], [[keypunch]]es and [[Tabulating machine|tabulators]] used to process the [[1890 US Census]] data.\u000a* '''1920s-1930s'''\u000a*: [[Emanuel Goldberg]] submits patents for his "Statistical Machine\u201d a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents.\u000a* '''1940s\u20131950s'''\u000a*: '''late 1940s''': The US military confronted problems of indexing and retrieval of wartime scientific research documents captured from Germans.\u000a*:: '''1945''': [[Vannevar Bush]]'s ''[[As We May Think]]'' appeared in ''[[Atlantic Monthly]]''.\u000a*:: '''1947''': [[Hans Peter Luhn]] (research engineer at IBM since 1941) began work on a mechanized punch card-based system for searching chemical compounds.\u000a*: '''1950s''': Growing concern in the US for a "science gap" with the USSR motivated, encouraged funding and provided a backdrop for mechanized literature searching systems (Allen Kent ''et al.'') and the invention of citation indexing ([[Eugene Garfield]]).\u000a*: '''1950''': The term "information retrieval" appears to have been coined by [[Calvin Mooers]].<ref>Mooers, Calvin N.; ''Theory Digital Handling Non-numerical Information'' (Zator Technical Bulletin No. 48) 5, cited in "information, n.". OED Online. December 2011. Oxford University Press.</ref>\u000a*: '''1951''': Philip Bagley conducted the earliest experiment in computerized document retrieval in a master thesis at [[MIT]].<ref name="Doyle1975">{{cite book |last=Doyle |first=Lauren |last2=Becker |first2=Joseph |title=Information Retrieval and Processing |publisher=Melville |year=1975 |pages=410 pp. |isbn=0-471-22151-1 }}</ref>\u000a*: '''1955''': Allen Kent joined [[Case Western Reserve University]], and eventually became associate director of the Center for Documentation and Communications Research. That same year, Kent and colleagues published a paper in American Documentation describing the precision and recall measures as well as detailing a proposed "framework" for evaluating an IR system which included statistical sampling methods for determining the number of relevant documents not retrieved.\u000a*: '''1958''': International Conference on Scientific Information Washington DC included consideration of IR systems as a solution to problems identified. See: ''Proceedings of the International Conference on Scientific Information, 1958'' (National Academy of Sciences, Washington, DC, 1959)\u000a*: '''1959''': [[Hans Peter Luhn]] published "Auto-encoding of documents for information retrieval."\u000a* '''1960s''':\u000a*: '''early 1960s''': [[Gerard Salton]] began work on IR at Harvard, later moved to Cornell.\u000a*: '''1960''': [[Melvin Earl Maron]] and John Lary<!-- sic --> Kuhns<ref name="Maron2008">{{cite journal |title=An Historical Note on the Origins of Probabilistic Indexing |last=Maron | first=Melvin E. |journal=Information Processing and Management |volume=44 |year=2008 |pages=971\u2013972 |url=http://yunus.hacettepe.edu.tr/~tonta/courses/spring2008/bby703/maron-on-probabilistic%20indexing-2008.pdf |doi=10.1016/j.ipm.2007.02.012 |issue=2 }}</ref> published "On relevance, probabilistic indexing, and information retrieval" in the Journal of the ACM 7(3):216\u2013244, July 1960.\u000a*: '''1962''':\u000a*:* [[Cyril W. Cleverdon]] published early findings of the Cranfield studies, developing a model for IR system evaluation. See: Cyril W. Cleverdon, "Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems". Cranfield Collection of Aeronautics, Cranfield, England, 1962.\u000a*:* Kent published ''Information Analysis and Retrieval''.\u000a*: '''1963''':\u000a*:* Weinberg report "Science, Government and Information" gave a full articulation of the idea of a "crisis of scientific information."  The report was named after Dr. [[Alvin Weinberg]].\u000a*:* Joseph Becker and [[Robert M. Hayes]] published text on information retrieval. Becker, Joseph; Hayes, Robert Mayo. ''Information storage and retrieval: tools, elements, theories''. New York, Wiley (1963).\u000a*: '''1964''':\u000a*:* [[Karen Spärck Jones]] finished her thesis at Cambridge, ''Synonymy and Semantic Classification'', and continued work on [[computational linguistics]] as it applies to IR.\u000a*:* The [[National Bureau of Standards]] sponsored a symposium titled "Statistical Association Methods for Mechanized Documentation." Several highly significant papers, including G. Salton's first published reference (we believe) to the [[SMART Information Retrieval System|SMART]] system.\u000a*:'''mid-1960s''':\u000a*::* National Library of Medicine developed [[MEDLARS]] Medical Literature Analysis and Retrieval System, the first major machine-readable database and batch-retrieval system.\u000a*::* Project Intrex at MIT.\u000a*:: '''1965''': [[J. C. R. Licklider]] published ''Libraries of the Future''.\u000a*:: '''1966''': [[Don Swanson]] was involved in studies at University of Chicago on Requirements for Future Catalogs.\u000a*: '''late 1960s''': [[F. Wilfrid Lancaster]] completed evaluation studies of the MEDLARS system and published the first edition of his text on information retrieval.\u000a*:: '''1968''':\u000a*:* Gerard Salton published ''Automatic Information Organization and Retrieval''.\u000a*:* John W. Sammon, Jr.'s RADC Tech report "Some Mathematics of Information Storage and Retrieval..." outlined the vector model.\u000a*:: '''1969''': Sammon's "A nonlinear mapping for data structure analysis" (IEEE Transactions on Computers) was the first proposal for visualization interface to an IR system.\u000a* '''1970s'''\u000a*: '''early 1970s''':\u000a*::* First online systems\u2014NLM's AIM-TWX, MEDLINE; Lockheed's Dialog; SDC's ORBIT.\u000a*::* [[Theodor Nelson]] promoting concept of [[hypertext]], published ''Computer Lib/Dream Machines''.\u000a*: '''1971''': [[Nicholas Jardine]] and [[Cornelis J. van Rijsbergen]] published "The use of hierarchic clustering in information retrieval", which articulated the "cluster hypothesis."<ref>{{cite journal|author=N. Jardine, C.J. van Rijsbergen|title=The use of hierarchic clustering in information retrieval|journal=Information Storage and Retrieval|volume=7|issue=5|pages=217\u2013240|date=December 1971|doi=10.1016/0020-0271(71)90051-9}}</ref>\u000a*: '''1975''': Three highly influential publications by Salton fully articulated his vector processing framework and term discrimination model:\u000a*::* ''A Theory of Indexing'' (Society for Industrial and Applied Mathematics)\u000a*::* ''A Theory of Term Importance in Automatic Text Analysis'' ([[JASIS]] v. 26)\u000a*::* ''A Vector Space Model for Automatic Indexing'' ([[Communications of the ACM|CACM]] 18:11)\u000a*: '''1978''': The First [[Association for Computing Machinery|ACM]] [[Special Interest Group on Information Retrieval|SIGIR]] conference.\u000a*: '''1979''': C. J. van Rijsbergen published ''Information Retrieval'' (Butterworths). Heavy emphasis on probabilistic models.\u000a* '''1980s'''\u000a*: '''1980''': First international ACM SIGIR conference, joint with British Computer Society IR group in Cambridge.\u000a*: '''1982''': [[Nicholas J. Belkin]], Robert N. Oddy, and Helen M. Brooks proposed the ASK (Anomalous State of Knowledge) viewpoint for information retrieval. This was an important concept, though their automated analysis tool proved ultimately disappointing.\u000a*: '''1983''': Salton (and Michael J. McGill) published ''Introduction to Modern Information Retrieval'' (McGraw-Hill), with heavy emphasis on vector models.\u000a*: '''1985''': David Blair and [[Bill Maron]] publish: An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System\u000a*: '''mid-1980s''': Efforts to develop end-user versions of commercial IR systems.\u000a*:: '''1985\u20131993''': Key papers on and experimental systems for visualization interfaces.\u000a*:: Work by [[Donald B. Crouch]], [[Robert R. Korfhage]], Matthew Chalmers, Anselm Spoerri and others.\u000a*: '''1989''': First [[World Wide Web]] proposals by [[Tim Berners-Lee]] at [[CERN]].\u000a* '''1990s'''\u000a*: '''1992''': First [[Text Retrieval Conference|TREC]] conference.\u000a*: '''1997''': Publication of [[Robert R. Korfhage|Korfhage]]'s ''Information Storage and Retrieval''<ref name="Korfhage1997">{{cite book |last=Korfhage |first=Robert R. |title=Information Storage and Retrieval |publisher=Wiley |year=1997 |pages=368 pp. |isbn=978-0-471-14338-3 |url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471143383,descCd-authorInfo.html }}</ref> with emphasis on visualization and multi-reference point systems.\u000a*: '''late 1990s''': Web [[Web search engine|search engines]] implementation of many features formerly found only in experimental IR systems. Search engines become the most common and maybe best instantiation of IR models.\u000a\u000a== Awards in the field ==\u000a\u000a* [[Tony Kent Strix award]]\u000a* [[Gerard Salton Award]]\u000a\u000a==See also==\u000a\u000a{{col-begin}}\u000a{{col-break}}\u000a\u000a* [[Adversarial information retrieval]]\u000a* [[Collaborative information seeking]]\u000a* [[Controlled vocabulary]]\u000a* [[Cross-language information retrieval]]\u000a* [[Data mining]]\u000a* [[European Summer School in Information Retrieval]]\u000a* [[Human\u2013computer information retrieval]]\u000a* [[Information extraction]]\u000a* [[Information Retrieval Facility]]\u000a* [[Knowledge visualization]]\u000a* [[Multimedia Information Retrieval]]\u000a* [[List of information retrieval libraries]]\u000a{{col-break}}\u000a* [[Personal information management]]\u000a* [[Relevance (Information Retrieval)]]\u000a* [[Relevance feedback]]\u000a* [[Rocchio Classification]]\u000a* [[Index (search engine)|Search index]]\u000a* [[Social information seeking]]\u000a* [[Special Interest Group on Information Retrieval]]\u000a* [[Structured Search]]\u000a* [[Subject indexing]]\u000a* [[Temporal information retrieval]]\u000a* [[Tf-idf]]\u000a* [[XML-Retrieval]]\u000a* Key-objects\u000a\u000a{{col-end}}\u000a\u000a== References ==\u000a{{reflist}}\u000a\u000a==External links==\u000a{{wikiquote}}\u000a* [http://www.acm.org/sigir/ ACM SIGIR: Information Retrieval Special Interest Group]\u000a* [http://irsg.bcs.org/ BCS IRSG: British Computer Society - Information Retrieval Specialist Group]\u000a* [http://trec.nist.gov Text Retrieval Conference (TREC)]\u000a* [http://www.isical.ac.in/~fire Forum for Information Retrieval Evaluation (FIRE)]\u000a* [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval] (online book) by [[C. J. van Rijsbergen]]\u000a* [http://ir.dcs.gla.ac.uk/wiki/ Information Retrieval Wiki]\u000a* [http://ir-facility.org/ Information Retrieval Facility]\u000a* [http://www.nonrelevant.net Information Retrieval @ DUTH]\u000a* [http://nlp.stanford.edu/IR-book/ Introduction to Information Retrieval (online book) by Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, Cambridge University Press. 2008.  ]\u000a\u000a{{DEFAULTSORT:Information Retrieval}}\u000a[[Category:Articles with inconsistent citation formats]]\u000a[[Category:Information retrieval| ]]\u000a[[Category:Natural language processing]]
p50
sg6
S'Information retrieval'
p51
ssI18
(dp52
g2
S'http://en.wikipedia.org/wiki/Poliqarp'
p53
sg4
S"'''Poliqarp''' is an [[open source]] [[search engine]] designed to process [[text corpus|text corpora]], among others the [[National Corpus of Polish]] created at the Institute of Computer Science, [[Polish Academy of Sciences]].\n\n==Features==\n\n* Custom [[query language]].\n* Two-level [[regular expressions]]:\n** operating at the level of characters in words\n** operating at the level of words in statements/paragraphs\n* Good performance\n* Compact corpus representation (compared to similar projects)\n* Portability across operating systems: [[Linux]]/[[BSD]]/[[Win32]]\n* Lack of portability across [[endianness]] (current release works only on little endian devices)\n\n==External links==\n\n* [http://www.korpus.pl/index.php?lang=en&page=welcome Polish corpus website (in English)]\n* [http://poliqarp.sourceforge.net/ Project website on SourceForge]\n* [http://poliqarp.suxx.pl/ Search plugin for Firefox]\n<br />\n[[Category:Information retrieval]]"
p54
sg6
S'Poliqarp'
p55
ssI147
(dp56
g2
S'http://en.wikipedia.org/wiki/Concept search'
p57
sg4
VA '''[[concept]] search''' (or conceptual search) is an automated [[information retrieval]] method that is used to search electronically stored [[unstructured data|unstructured text]] (for example, [[digital archive]]s, email, scientific literature, etc.) for information that is conceptually similar to the information provided in a search query.  In other words, the ''ideas'' expressed in the information retrieved in response to a concept search query are relevant to the ideas contained in the text of the query.\u000a\u000a__TOC__\u000a\u000a==Why Concept Search?==\u000aConcept search techniques were developed because of limitations imposed by classical Boolean [[Search algorithm|keyword search]] technologies when dealing with large, unstructured digital collections of text.  Keyword searches often return results that include many non-relevant items ([[false positive]]s) or that exclude too many relevant items (false negatives) because of the effects of [[synonymy]] and [[polysemy]].  Synonymy means that one of two or more words in the same language have the same meaning, and polysemy means that many individual words have more than one meaning.\u000a\u000aPolysemy is a major obstacle for all computer systems that attempt to deal with human language.  In English, most frequently used terms have several common meanings.  For example, the word fire can mean: a combustion activity; to terminate employment; to launch, or to excite (as in fire up).  For the 200 most-polysemous terms in English, the typical verb has more than twelve common meanings, or senses.  The typical noun from this set has more than eight common senses.  For the 2000 most-polysemous terms in English, the typical verb has more than eight common senses and the typical noun has more than five.<ref>Bradford, R. B., Word Sense Disambiguation, [[Content Analyst Company]], LLC, U.S. Patent 7415462, 2008.</ref>\u000a\u000aIn addition to the problems of polysemous and synonymy, keyword searches can exclude inadvertently [[misspelled]] words as well as the variations on the [[Stemming|stems]] (or roots) of words (for example, strike vs. striking).  Keyword searches are also susceptible to errors introduced by [[optical character recognition]] (OCR) scanning processes, which can introduce [[random error]]s into the text of documents (often referred to as [[noisy text]]) during the scanning process.\u000a\u000aA concept search can overcome these challenges by employing [[word sense disambiguation]] (WSD),<ref>R. Navigli, [http://www.dsi.uniroma1.it/~navigli/pubs/ACM_Survey_2009_Navigli.pdf Word Sense Disambiguation: A Survey], ACM Computing Surveys, 41(2), 2009.</ref> and other techniques, to help it derive the actual meanings of the words, and their underlying concepts, rather than by simply matching character strings like keyword search technologies.\u000a\u000a==Approaches to Concept Search==\u000aIn general, information retrieval research and technology can be divided into two broad categories: semantic and statistical. Information retrieval systems that fall into the semantic category will attempt to implement some degree of syntactic and [[Semantic analysis (machine learning)|semantic analysis]] of the [[natural language]] text that a human user would provide (also see [[computational linguistics]]).  Systems that fall into the statistical category will find results based on statistical measures of how closely they match the query.  However, systems in the semantic category also often rely on statistical methods to help them find and retrieve information.<ref>Greengrass, E., Information Retrieval: A Survey, 2000.</ref>\u000a\u000aEfforts to provide information retrieval systems with semantic processing capabilities have basically used three different approaches:\u000a\u000a* Auxiliary structures\u000a* Local [[co-occurrence]] statistics\u000a* Transform techniques (particularly [[matrix decomposition]]s)\u000a\u000a===Auxiliary Structures===\u000aA variety of techniques based on Artificial Intelligence (AI) and [[Natural language processing|Natural Language Processing]] (NLP) have been applied to semantic processing, and most of them have relied on the use of auxiliary structures such as [[controlled vocabularies]] and [[Ontology (information science)|ontologies]].  Controlled vocabularies (dictionaries and thesauri), and ontologies allow broader terms, narrower terms, and related terms to be incorporated into queries.<ref>Dubois, C., The Use of Thesauri in Online Retrieval, Journal of Information Science, 8(2), 1984 March, pp. 63-66.</ref> Controlled vocabularies are one way to overcome some of the most severe constraints of Boolean keyword queries.  Over the years, additional auxiliary structures of general interest, such as the large synonym sets of [[WordNet]], have been constructed.<ref>Miller, G., Special Issue, [http://www.mit.edu/~6.863/spring2009/readings/5papers.pdf WordNet: An On-line Lexical Database], Intl. Journal of Lexicography, 3(4), 1990.</ref>  It was shown that concept search that is based on auxiliary structures, such as [[WordNet]], can be efficiently implemented by reusing retrieval models and data structures of classical [[Information Retrieval]].<ref>Fausto Giunchiglia, Uladzimir Kharkevich, and Ilya Zaihrayeu. [http://www.ulakha.com/concept-search-eswc2009.html Concept Search], In Proceedings of European Semantic Web Conference, 2009.</ref>  Later approaches have implemented grammars to expand the range of semantic constructs.  The creation of data models that represent sets of concepts within a specific domain (''domain ontologies''), and which can incorporate the relationships among terms, has also been implemented in recent years.\u000a\u000aHandcrafted controlled vocabularies contribute to the efficiency and comprehensiveness of information retrieval and related text analysis operations, but they work best when topics are narrowly defined and the terminology is standardized.  Controlled vocabularies require extensive human input and oversight to keep up with the rapid evolution of language.  They also are not well suited to the growing volumes of unstructured text covering an unlimited number of topics and containing thousands of unique terms because new terms and topics need to be constantly introduced.  Controlled vocabularies are also prone to capturing a particular world view at a specific point in time, which makes them difficult to modify if concepts in a certain topic area change.<ref name="Bradford, R. B. 2008">Bradford, R. B., Why LSI? [[Latent Semantic Indexing]] and Information Retrieval, White Paper, [[Content Analyst Company]], LLC, 2008.</ref>\u000a\u000a===Local Co-occurrence Statistics===\u000aInformation retrieval systems incorporating this approach count the number of times that groups of terms appear together (co-occur) within a [[sliding window]] of terms or sentences (for example, ± 5 sentences or ± 50 words) within a document.  It is based on the idea that words that occur together in similar contexts have similar meanings.  It is local in the sense that the sliding window of terms and sentences used to determine the co-occurrence of terms is relatively small.\u000a\u000aThis approach is simple, but it captures only a small portion of the semantic information contained in a collection of text.  At the most basic level, numerous experiments have shown that approximately only ¼ of the information contained in text is local in nature.<ref>Landauer, T., and Dumais, S., A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge, Psychological Review, 1997, 104(2), pp. 211-240.</ref>   In addition, to be most effective, this method requires prior knowledge about the content of the text, which can be difficult with large, unstructured document collections.<ref name="Bradford, R. B. 2008"/>\u000a\u000a===Transform Techniques===\u000aSome of the most powerful approaches to semantic processing are based on the use of mathematical transform techniques.  [[Matrix decomposition]] techniques have been the most successful.  Some widely used matrix decomposition techniques include the following:<ref>Skillicorn, D., Understanding Complex Datasets: Data Mining with Matrix Decompositions, CRC Publishing, 2007.</ref>\u000a\u000a* [[Independent component analysis]]\u000a* Semi-discrete decomposition\u000a* [[Non-negative matrix factorization]]\u000a* [[Singular value decomposition]]\u000a\u000aMatrix decomposition techniques are data-driven, which avoids many of the drawbacks associated with auxiliary structures.  They are also global in nature, which means they are capable of much more robust information extraction and representation of semantic information than techniques based on local co-occurrence statistics.<ref name="Bradford, R. B. 2008"/>\u000a\u000aIndependent component analysis is a technique that creates sparse representations in an automated fashion,<ref>Honkela, T., Hyvarinen, A. and Vayrynen, J. WordICA - Emergence of linguistic representations for words by independent component analysis. Natural Language Engineering, 16(3):277-308, 2010</ref> and the semi-discrete and non-negative matrix approaches sacrifice accuracy of representation in order to reduce computational complexity.<ref name="Bradford, R. B. 2008"/>\u000a\u000aSingular value decomposition (SVD) was first applied to text at Bell Labs in the late 1980s. It was used as the foundation for a technique called [[Latent semantic indexing|Latent Semantic Indexing]] (LSI) because of its ability to find the semantic meaning that is latent in a collection of text.  At first, the SVD was slow to be adopted because of the resource requirements needed to work with large datasets.  However, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.  LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.<ref>Dumais, S., Latent Semantic Analysis, ARIST Review of Information Science and Technology, vol. 38, Chapter 4, 2004.</ref>\u000a\u000a==Uses of Concept Search==\u000a* '''[[eDiscovery]]''' - Concept-based search technologies are increasingly being used for Electronic Document Discovery (EDD or eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is much more efficient than traditional linear review techniques.  Concept-based searching is becoming accepted as a reliable and efficient search method that is more likely to produce relevant results than keyword or Boolean searches.<ref>Magistrate Judge John M. Facciola of the U.S. District Court for the District of Washington, D.C.\u000aDisability Rights Council v. Washington Metropolitan Transit Authority, 242 FRD 139 (D. D.C. 2007), citing George L. Paul & Jason R. Baron, "Information Inflation: Can the Legal System Adapt?" 13 Rich. J.L. & Tech. 10 (2007).</ref>\u000a\u000a* '''[[Enterprise Search]] and Enterprise Content Management (ECM)''' - Concept search technologies are being widely used in enterprise search.  As the volume of information within the enterprise grows, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis has become essential.  In 2004 the Gartner Group estimated that professionals spend 30 percent of their time searching, retrieving, and managing information.<ref name="Laplanche, R. 2004">Laplanche, R., Delgado, J., Turck, M., Concept Search Technology Goes Beyond Keywords, Information Outlook, July 2004.</ref>  The research company IDC found that a 2,000-employee corporation can save up to $30 million per year by reducing the time employees spend trying to find information and duplicating existing documents.<ref name="Laplanche, R. 2004"/>\u000a\u000a* '''[[Content-based image retrieval|Content-Based Image Retrieval (CBIR)]]''' - Content-based approaches are being used for the semantic retrieval of digitized images and video from large visual corpora.  One of the earliest content-based image retrieval systems to address the semantic problem was the ImageScape search engine.  In this system, the user could make direct queries for multiple visual objects such as sky, trees, water, etc. using spatially positioned icons in a WWW index containing more than ten million images and videos using keyframes.  The system used information theory to determine the best features for minimizing uncertainty in the classification.<ref name="Lew, M. S. 2006">Lew, M. S., Sebe, N., Djeraba, C., Jain, R., Content-based Multimedia Information Retrieval: State of the Art and Challenges, ACM Transactions on Multimedia Computing, Communications, and Applications, February 2006.</ref>  The semantic gap is often mentioned in regard to CBIR.  The semantic gap refers to the gap between the information that can be extracted from visual data and the interpretation that the same data have for a user in a given situation.<ref>Datta R., Joshi, D., Li J., Wang, J. Z., [http://infolab.stanford.edu/~wangz/project/imsearch/review/JOUR/datta.pdf Image Retrieval: Ideas, Influences, and Trends of the New Age], ACM Computing Surveys, Vol. 40, No. 2, April 2008.</ref>  The [http://www.liacs.nl/~mir ACM SIGMM Workshop on Multimedia Information Retrieval] is dedicated to studies of CBIR.\u000a\u000a* '''Multimedia and Publishing''' - Concept search is used by the multimedia and publishing industries to provide users with access to news, technical information, and subject matter expertise coming from a variety of unstructured sources.  Content-based methods for multimedia information retrieval (MIR) have become especially important when text annotations are missing or incomplete.<ref name="Lew, M. S. 2006"/>\u000a\u000a* '''Digital Libraries and Archives''' - Images, videos, music, and text items in digital libraries and digital archives are being made accessible to large groups of users (especially on the Web) through the use of concept search techniques.  For example, the Executive Daily Brief (EDB), a business information monitoring and alerting product developed by EBSCO Publishing, uses concept search technology to provide corporate end users with access to a digital library containing a wide array of business content.  In a similar manner, the [[Music Genome Project]] spawned Pandora, which employs concept searching to spontaneously create individual music libraries or ''virtual'' radio stations.\u000a\u000a* '''Genomic Information Retrieval (GIR)''' - Genomic Information Retrieval (GIR) uses concept search techniques applied to genomic literature databases to overcome the ambiguities of scientific literature.\u000a\u000a* '''Human Resources Staffing and Recruiting''' - Many human resources staffing and recruiting organizations have adopted concept search technologies to produce highly relevant resume search results that provide more accurate and relevant candidate resumes than loosely related keyword results.\u000a\u000a==Effective Concept Searching==\u000aThe effectiveness of a concept search can depend on a variety of elements including the dataset being searched and the search engine that is used to process queries and display results. However, most concept search engines work best for certain kinds of queries:\u000a\u000a* Effective queries are composed of enough text to adequately convey the intended concepts.  Effective queries may include full sentences, paragraphs, or even entire documents.  Queries composed of just a few words are not as likely to return the most relevant results.\u000a\u000a* Effective queries do not include concepts in a query that are not the object of the search.  Including too many unrelated concepts in a query can negatively affect the relevancy of the result items.  For example, searching for information about ''boating on the Mississippi River'' would be more likely to return relevant results than a search for ''boating on the Mississippi River on a rainy day in the middle of the summer in 1967.''\u000a\u000a* Effective queries are expressed in a full-text, natural language style similar in style to the documents being searched.  For example, using queries composed of excerpts from an introductory science textbook would not be as effective for concept searching if the dataset being searched is made up of advanced, college-level science texts.  Substantial queries that better represent the overall concepts, styles, and language of the items for which the query is being conducted are generally more effective.\u000a\u000aAs with all search strategies, experienced searchers generally refine their queries through multiple searches, starting with an initial ''seed'' query to obtain conceptually relevant results that can then be used to compose and/or refine additional queries for increasingly more relevant results.  Depending on the search engine, using query concepts found in result documents can be as easy as selecting a document and performing a ''find similar'' function.  Changing a query by adding terms and concepts to improve result relevance is called ''[[query expansion]]''.<ref>[[Stephen Robertson (computer scientist)|Robertson, S. E.]], [[Karen Spärck Jones|Spärck Jones, K.]], Simple, Proven Approaches to Text Retrieval, Technical Report, University of Cambridge Computer Laboratory, December 1994.</ref> The use of [[ontology (information science)|ontologies]] such as WordNet has been studied to expand queries with conceptually-related words.<ref>Navigli, R., Velardi, P. [http://www.dcs.shef.ac.uk/~fabio/ATEM03/navigli-ecml03-atem.pdf An Analysis of Ontology-based Query Expansion Strategies]. ''Proc. of Workshop on Adaptive Text Extraction and Mining (ATEM 2003)'', in the ''14th European Conference on Machine Learning (ECML 2003)'', Cavtat-Dubrovnik, Croatia, September 22-26th, 2003, pp.&nbsp;42\u201349</ref>\u000a\u000a==Relevance Feedback==\u000a[[Relevance feedback]] is a feature that helps users determine if the results returned for their queries meet their information needs.  In other words, relevance is assessed relative to an information need, not a query.  A document is relevant if it addresses the stated information need, not because it just happens to contain all the words in the query.<ref name="Manning, C. D. 2008">Manning, C. D., Raghavan P., Schütze H., Introduction to Information Retrieval, Cambridge University Press, 2008.</ref>   It is a way to involve users in the retrieval process in order to improve the final result set.<ref name="Manning, C. D. 2008"/> Users can refine their queries based on their initial results to improve the quality of their final results.\u000a\u000aIn general, concept search relevance refers to the degree of similarity between the concepts expressed in the query and the concepts contained in the results returned for the query.  The more similar the concepts in the results are to the concepts contained in the query, the more relevant the results are considered to be.  Results are usually ranked and sorted by relevance so that the most relevant results are at the top of the list of results and the least relevant results are at the bottom of the list.\u000a\u000aRelevance feedback has been shown to be very effective at improving the relevance of results.<ref name="Manning, C. D. 2008"/>   A concept search decreases the risk of missing important result items because all of the items that are related to the concepts in the query will be returned whether or not they contain the same words used in the query.<ref name="Laplanche, R. 2004"/>\u000a\u000a[[Ranking]] will continue to be a part of any modern information retrieval system.  However, the problems of heterogeneous data, scale, and non-traditional discourse types reflected in the text, along with the fact that search engines will increasingly be integrated components of complex information management processes, not just stand-alone systems, will require new kinds of system responses to a query.  For example, one of the problems with ranked lists is that they might not reveal relations that exist among some of the result items.<ref name="Callan, J. 2007">Callan, J., Allan, J., Clarke, C. L. A., Dumais, S., Evans, D., A., Sanderson, M., Zhai, C., Meeting of the MINDS: An Information Retrieval Research Agenda, ACM, SIGIR Forum, Vol. 41 No. 2, December 2007.</ref>\u000a\u000a==Guidelines for Evaluating a Concept Search Engine==\u000a# Result items should be relevant to the information need expressed by the concepts contained in the query statements, even if the terminology used by the result items is different from the terminology used in the query.\u000a# Result items should be sorted and ranked by relevance.\u000a# Relevant result items should be quickly located and displayed.  Even complex queries should return relevant results fairly quickly.\u000a# Query length should be ''non-fixed'', i.e., a query can be as long as deemed necessary.  A sentence, a paragraph, or even an entire document can be submitted as a query.\u000a# A concept query should not require any special or complex syntax.  The concepts contained in the query can be clearly and prominently expressed without using any special rules.\u000a# Combined queries using concepts, keywords, and metadata should be allowed.\u000a# Relevant portions of result items should be usable as query text simply by selecting the item and telling the search engine to ''find similar'' items.\u000a# Query-ready indexes should be created relatively quickly.\u000a# The search engine should be capable of performing Federated searches.  Federated searching enables concept queries to be used for simultaneously searching multiple datasources for information, which are then merged, sorted, and displayed in the results.\u000a# A concept search should not be affected by misspelled words, typographical errors, or OCR scanning errors in either the query text or in the text of the dataset being searched.\u000a\u000a==Search Engine Conferences and Forums==\u000aFormalized search engine evaluation has been ongoing for many years.  For example, the [[Text Retrieval Conference|Text REtrieval Conference (TREC)]] was started in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.  Most of today's commercial search engines include technology first developed in TREC.<ref>Croft, B., Metzler, D., Strohman, T., Search Engines, Information Retrieval in Practice, Addison Wesley, 2009.</ref>\u000a\u000aIn 1997, a Japanese counterpart of TREC was launched, called National Institute of Informatics Test Collection for IR Systems (NTCIR).  NTCIR conducts a series of evaluation workshops for research in information retrieval, question answering, text summarization, etc.  A European series of workshops called the Cross Language Evaluation Forum (CLEF) was started in 2001 to aid research in multilingual information access.  In 2002, the Initiative for the Evaluation of XML Retrieval (INEX) was established for the evaluation of content-oriented XML retrieval systems.\u000a\u000aPrecision and recall have been two of the traditional performance measures for evaluating information retrieval systems.  Precision is the fraction of the retrieved result documents that are relevant to the user's information need.  Recall is defined as the fraction of relevant documents in the entire collection that are returned as result documents.<ref name="Manning, C. D. 2008"/>\u000a\u000aAlthough the workshops and publicly available test collections used for search engine testing and evaluation have provided substantial insights into how information is managed and retrieved, the field has only scratched the surface of the challenges people and organizations face in finding, managing, and, using information now that so much information is available.<ref name="Callan, J. 2007"/>   Scientific data about how people use the information tools available to them today is still incomplete because experimental research methodologies haven\u2019t been able to keep up with the rapid pace of change. Many challenges, such as contextualized search, personal information management, information integration, and task support, still need to be addressed.<ref name="Callan, J. 2007"/>\u000a\u000a==See also==\u000a* [[approximate string matching]]\u000a* [[Compound term processing]]\u000a* [[Concept mining]]\u000a* [[Computational linguistics]]\u000a* [[Information extraction]]\u000a* [[Latent semantic indexing]]\u000a* [[Latent semantic analysis]]\u000a* [[Semantic network]]\u000a* [[Semantic search]]\u000a* [[Semantic Web]]\u000a* [[Statistical semantics]]\u000a* [[Text mining]]\u000a* [[Word Sense Disambiguation]]\u000a\u000a==References==\u000a{{Reflist|2}}\u000a\u000a==External links==\u000a* [http://trec.nist.gov/ Text Retrieval Conference (TREC)]\u000a* [http://research.nii.ac.jp/ntcir/ National Institute of Informatics Test Collection for IR Systems (NTCIR)]\u000a* [http://www.clef-campaign.org/ Cross Language Evaluation Forum (CLEF)]\u000a* [http://inex.is.informatik.uni-duisburg.de/ Initiative for the Evaluation of XML Retrieval (INEX)]\u000a\u000a[[Category:Information retrieval]]
p58
sg6
S'Concept search'
p59
ssI21
(dp60
g2
S'http://en.wikipedia.org/wiki/Key Word in Context'
p61
sg4
V'''KWIC''' is an acronym for '''Key Word In Context''', the most common format for [[concordance (publishing)|concordance]] lines. The term KWIC was first coined by [[Hans Peter Luhn]].<ref>Manning, C. D., Schütze, H.: "Foundations of Statistical Natural Language Processing", p.35. The MIT Press, 1999</ref> The system was based on a concept called ''keyword in titles'' which was first proposed for Manchester libraries in 1864 by [[Andrea Crestadoro]].<ref name="index">{{cite book|title=Advanced Indexing and Abstracting Practices|url=http://books.google.co.uk/books?id=nIUkl7bLzYUC&pg=PA41&dq=Andrea+Crestadoro#v=onepage&q=Andrea%20Crestadoro&f=false}}</ref>\u000a\u000aA '''KWIC''' index is formed by sorting and aligning the words within an article title to allow each word (except the [[stop words]]) in titles to be searchable alphabetically in the index. It was a useful indexing method for technical manuals before computerized [[full text search]] became common.\u000a\u000aFor example, a search query including all of the words in the title statement of this article ("KWIC is an acronym for Key Word In Context, the most common format for concordance lines") and the [[Wikipedia:Slogans|Wikipedia slogan]] in English ("the free encyclopedia"), searched against this very webpages, might yield a KWIC index as follows. A KWIC index usually uses a wide layout to allow the display of maximum 'in context' information (not shown in the following example).\u000a\u000a{| nowrap\u000a|-\u000a|align=right|KWIC is an\u000a|'''acronym''' for Key Word In Context, ...\u000a|page 1\u000a|-\u000a|align=right|... Key Word In Context, the most \u000a|'''common''' format for concordance lines.\u000a|page 1\u000a|-\u000a|align=right|... the most common format for \u000a|'''concordance''' lines.\u000a|page 1\u000a|-\u000a|align=right|... is an acronym for Key Word In \u000a|'''Context''', the most common format ...\u000a|page 1\u000a|-\u000a|align=right|Wikipedia, The Free \u000a|'''Encyclopedia'''\u000a|page 0\u000a|-\u000a|align=right|... In Context, the most common \u000a|'''format''' for concordance lines.\u000a|page 1\u000a|-\u000a|align=right|Wikipedia, The \u000a|'''Free''' Encyclopedia\u000a|page 0\u000a|-\u000a|align=right|KWIC is an acronym for \u000a|'''Key''' Word In Context, the most ...\u000a|page 1\u000a|-\u000a|&nbsp;\u000a|'''KWIC''' is an acronym for Key Word ...\u000a|page 1\u000a|-\u000a|align=right|... common format for concordance \u000a|'''lines'''.\u000a|page 1\u000a|-\u000a|align=right|... for Key Word In Context, the \u000a|'''most''' common format for concordance ...\u000a|page 1\u000a|-\u000a|&nbsp;\u000a|'''Wikipedia''', The Free Encyclopedia\u000a|page 0\u000a|-\u000a|align=right|KWIC is an acronym for Key\u000a|'''Word''' In Context, the most common ...\u000a|page 1\u000a|}\u000a\u000aA KWIC index is a special case of a '''permuted index'''. This term refers to the fact that it indexes all [[cyclic permutation]]s of the headings. Books composed of many short sections with their own descriptive headings, most notably collections of [[Manual page (Unix)|manual pages]], often ended with a '''permuted index''' section, allowing the reader to easily find a section by any word from its heading. This practice, also known as '''KWOC''' (\u201c'''Key Word Out of Context'''\u201d), is no longer common.\u000a\u000a==References in Literature==\u000a\u000a''Note: The first reference does not show the KWIC index unless you pay to view the paper. The second reference does not even list the paper at all.''\u000a\u000a* [[David Parnas|David L. Parnas]] uses a KWIC Index as an example on how to perform modular design in his paper [http://portal.acm.org/citation.cfm?id=361623&coll=ACM&dl=ACM&CFID=9516243&CFTOKEN=98251202 ''On the Criteria To Be Used in Decomposing Systems into Modules''], available as an [http://www.acm.org/classics/may96/ ACM Classic Paper]\u000a* Christopher D. Manning and Hinrich Schütze describe a KWIC index and computer concordancing in section 1.4.5 of their book ''Foundations of Statistical Natural Language Processing''\u000a\u000a==References==\u000a{{reflist|2}}\u000a\u000a==See also==\u000a* <tt>[[Ptx (Unix)|ptx]]</tt>, a Unix command-line utility producing a [[permuted index]]\u000a*[[Concordancer]]\u000a*[[Concordance (publishing)]]\u000a*[[Burrows\u2013Wheeler transform]]\u000a*[[Hans Peter Luhn]]\u000a*[[Suffix tree]]\u000a\u000a[[Category:Indexing]]\u000a[[Category:Information retrieval]]\u000a[[Category:Reference]]\u000a[[Category:Searching]]
p62
sg6
S'Key Word in Context'
p63
ssI150
(dp64
g2
S'http://en.wikipedia.org/wiki/Stop words'
p65
sg4
V{{distinguish|Safeword}}\u000aIn [[computing]], '''stop words''' are words which are filtered out before or after [[Natural language processing|processing of natural language]] data (text).<ref>{{cite doi|10.1017/CBO9781139058452.002}}</ref> There is no single universal list of stop words used by all [[Natural language processing|processing of natural language]] tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these '''stop words''' to support [[phrase search]].\u000a\u000aAny group of words can be chosen as the stop words for a given purpose. For some [[search engine]]s, these are some of the most common, short [[function word]]s, such as ''the'', ''is'', ''at'', ''which'', and ''on''. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as '[[The Who]]', '[[The The]]', or '[[Take That]]'. Other search engines remove some of the most common words\u2014including [[lexical word]]s, such as "want"\u2014from a query in order to improve performance.<ref>[http://blog.stackoverflow.com/2008/12/podcast-32 Stackoverflow]: "One of our major performance optimizations for the "related questions" query is removing the top 10,000 most common English dictionary words (as determined by Google search) before submitting the query to the SQL Server 2008 full text engine. It\u2019s shocking how little is left of most posts once you remove the top 10k English dictionary words. This helps limit and narrow the returned results, which makes the query dramatically faster".</ref>\u000a\u000a[[Hans Peter Luhn]], one of the pioneers in [[information retrieval]], is credited with coining the phrase and using the concept. {{Citation needed|date=March 2013}}\u000a\u000a== See also ==\u000a{{Div col|cols=3}}\u000a* [[Text mining]]\u000a* [[Concept mining]]\u000a* [[Information extraction]]\u000a* [[Natural language processing]]\u000a* [[Query expansion]]\u000a* [[Stemming]]\u000a* [[Index (search engine)|Search engine indexing]]\u000a* [[Poison words]]\u000a* [[Function words]]\u000a{{Div col end}}\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://xpo6.com/list-of-english-stop-words/  List of English Stop Words (PHP array, CSV) ]\u000a* [http://dev.mysql.com/doc/refman/5.5/en/fulltext-stopwords.html  Full-Text Stopwords in MySQL ]\u000a* [http://www.textfixer.com/resources/common-english-words.txt English Stop Words (CSV)]\u000a* [http://mail.sarai.net/private/prc/Week-of-Mon-20080204/001656.html Hindi Stop Words]\u000a* [http://solariz.de/de/deutsche_stopwords.htm German Stop Words],[http://aniol-consulting.de/uebersicht-deutscher-stop-words/ German Stop Words and phrases], another list of [http://www.ranks.nl/stopwords/german.html German stop words]\u000a* [[:pl:Wikipedia:Stopwords|Polish Stop Words]]\u000a* [https://code.google.com/p/stop-words/ Collection of stop words in 29 languages]\u000a* [http://www.text-analytics101.com/2014/10/all-about-stop-words-for-text-mining.html A Detailed Explanation of Stop Words by Kavita Ganesan]\u000a\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Searching]]\u000a{{Natural Language Processing}}\u000a{{SearchEngineOptimization}}
p66
sg6
S'Stop words'
p67
ssI24
(dp68
g2
S'http://en.wikipedia.org/wiki/Poison words'
p69
sg4
S"{{Missing information|Examples of poison words|date=September 2008}}\n{{Unreferenced|date=December 2007}}\n\n'''Poison words''', or '''forbidden words''', is the name given to words or phrases that trigger suspicion, mistrust and loss of respect, or are of inappropriate character for a given web site in its consideration for a [[search engine]].\n\nThere is no definite list of poison words which all natural language processing tools incorporate. \n\nThis is different from harmless but useless words that are called [[Stop words]].\n\nAdult (obscene) words can put a web page in an adult category where it is filtered out by various filters at search engines, so this is one set of poison words. But some consider any words that lower your ranking in a search engine as poison words. Some people consider any words that encourage ads to pervade a whole site and displace much higher earning ads as poison words.\n\n== See also ==\n\n* [[Bayesian poisoning]]\n* [[Natural language processing]]\n* [[Text mining]]\n* [[Index (search engine)|Search engine indexing]]\n\n== External links ==\n\n\n{{SearchEngineOptimization}}\n\n[[Category:Information retrieval]]\n[[Category:Searching]]"
p70
sg6
S'Poison words'
p71
ssI153
(dp72
g2
S'http://en.wikipedia.org/wiki/Swiftype'
p73
sg4
V{{Infobox company\u000a|name             = Swiftype\u000a|logo             =Black_Swiftype_Logo.png \u000a|type             = [[Privately held company|Private]]\u000a|industry         = [[Software]] <br/> [[Information Technology]] <br/> [[Search Engines]]\u000a|area_served      = Worldwide\u000a|location_city    = [[San Francisco, California|San Francisco]], [[California (state)|California]]\u000a|location_country = U.S.\u000a|founders       = {{unbulleted list|Matt Riley, Quin Hoxie}}\u000a|key_people       = {{unbulleted list|Matt Riley (CEO), Quin Hoxie (CTO)}}\u000a|services         = {{unbulleted list|[[vertical search]], [[eCommerce]] search, database search, website search, [[enterprise search]], [[search engines]], [[fulltext search]], [[faceted search]], [[concept search]], [[real-time search]]}}\u000a|genre                  = [[Search algorithm|Search]] and [[index (search engine)|index]]\u000a|num_employees    = 25\u000a|foundation       = [[San Francisco, California|San Francisco]], [[California (state)|California]], [[U.S.A]] [[January 2012]]\u000a|homepage         = {{URL|https://www.swiftype.com/|Swiftype.com}}\u000a|intl             = yes\u000a|footnotes             = {{unbulleted list|[http://www.crunchbase.com/organization/swiftype Crunchbase] [http://www.Swiftype.com Official Website]}}\u000a|alt = Black text and red icon edition of the full Swiftype logo|products = {{unbulleted list|[[vertical search]], [[eCommerce]] search, database search, website search, [[enterprise search]], [[search engines]], [[fulltext search]], [[faceted search]], [[concept search]], [[real-time search]]}}}}\u000a\u000a'''Swiftype''' is a company that sells [[search engines]] for websites and mobile applications (also known as [[enterprise search]]) and creates a [[PageRank]] specific to individual websites and mobile applications.<ref name="TechCrunch-Ha">{{cite news|last1=Ha|first1=Anthony|title=Y Combinator-Backed Swiftype Builds Site Search That Doesn\u2019t Suck|url=http://techcrunch.com/2012/05/08/swiftype-launch/|accessdate=21 July 2014|publisher=TechCrunch|date=May 8, 2012|ref=TechCrunch-Ha}}</ref><ref name=BetaBeat>{{cite news|last1=Roy|first1=Jessica|title=Can This Y Combinator Startup Solve the Site Search Problem?|url=http://betabeat.com/2012/05/can-this-y-combinator-startup-solve-the-site-search-problem/|accessdate=21 July 2014|publisher=BetaBeat|date=July 21, 2014|ref=BetaBeat}}</ref><ref name="Crunchbase">{{cite web|url=http://www.crunchbase.com/organization/swiftype|website=Crunchbase|accessdate=19 July 2014|title = <nowiki>Swiftype | CrunchBase</nowiki>}}</ref><ref name=VatorNews>{{Cite news|url = http://vator.tv/news/2013-08-15-swiftype-bags-17m-from-big-names-for-better-search|title = Swiftype bags $1.7M from big names for better search|last = Marino|first = Faith|date = August 15, 2013|work = |accessdate = July 21, 2014|ref = VatorNews|publisher = VatorNews}}</ref>  The company is based in [[San Francisco, CA]] and is funded mainly through [[venture capital]].<ref name=Crunchbase />\u000a\u000a==History==\u000aSwiftype was founded in 2012 by former [[Scribd]] engineers Matt Riley and Quin Hoxie.<ref name=Crunchbase /> The two met while working on an internal search tool for [[Scribd]].<ref name="TechCrunch-Ha" /><ref name="BetaBeat" /><ref name=Forbes>{{cite news|last1=Casserly|first1=Meghan|title=Site Search (Should Be) Sexy: How Swiftype Raised $1.7M In Seed Funding From SV Bigwigs|url=http://www.forbes.com/sites/meghancasserly/2013/08/15/site-search-should-be-sexy-how-swiftype-raised-1-7-in-seed-funding-from-sv-bigwigs/|accessdate=19 July 2014|publisher=Forbes|ref = Forbes|date=2013-08-15}}</ref> Swiftype participated in [[Y Combinator (company)|Y Combinator]] in 2012 and received investment from a number of prominent sources.<ref name=VentureBeat>{{Cite news|url = http://venturebeat.com/2013/08/15/yc-startup-swiftype-raises-1-7m-seed-round-from-andreessen-nea-kleiner/|title = YC startup Swiftype raises $1.7M seed round from Andreessen; NEA; Kleiner|last = Grant|first = Rebecca|date = August 15, 2013|work = |accessdate = July 21, 2014|publisher = VentureBeat|ref = VentureBeat}}</ref><ref name=VatorNews /><ref name="TechCrunch-Yang">{{cite news|last1=Yang|first1=Anthony|title=Site Search Engine Creator Swiftype Raises $1.7M From A16Z, Others|url=http://techcrunch.com/2013/08/15/swiftype-1-7m/|accessdate=19 July 2014|publisher=TechCrunch|date=2013-08-15|ref = TechCrunch-Yang}}</ref><ref name=AllThingsD>{{cite news|last1=Gannes|first1=Liz|title=Swiftype Raises $1.7M for Smarter Site Search|url=http://allthingsd.com/20130815/swiftype-raises-1-7m-for-site-search/|accessdate=19 July 2014|publisher=All Things D|ref = AllThingsD|date=2013-08-15}}</ref> In September 2013, the company obtained [[Series A]] funding.<ref name=Crunchbase /><ref name=VatorNews /><ref name=VentureBeat /><ref name=TechCrunch-Yang /><ref name=AllThingsD /><ref name=StartUpBeatBeat>{{Cite news|url = http://startupbeat.com/2013/08/26/swiftype-wants-to-dramatically-improve-search-on-websites-and-mobile-apps-of-all-types-and-sizes-id3402/|title = Swiftype wants to dramatically improve search on websites and mobile apps of all types and sizes|last = Editor|first = |date = August 26, 2013|work = |accessdate = July 21, 2014|publisher = StartUpBeat|ref = StartUpBeat}}</ref>\u000a\u000aAs of August 2013, Swiftype had over 70,000 websites using their search bar, powering over 130 million queries per month.<ref name=Forbes /><ref name=AllThingsD />\u000a\u000a==Features==\u000aSwiftype is available as an [[API]] or [[web crawler]] based engine.<ref name=TechCrunch-Yang />  The company also offers a VIP-approved [[WordPress]] Plugin, a [[Shopify]] App, and a [[Magento]] extension.<ref>{{Cite web|url = https://apps.shopify.com/swiftype|title = Autocomplete & Site Search by Swiftype \u2013 Ecommerce Plugins for Online Stores \u2013 Shopify App Store|date = 2014-09-26|accessdate = 2014-09-26|website = Shopify App Store|publisher = Shopify|last = |first = }}</ref><ref>{{Cite web|url = http://www.magentocommerce.com/magento-connect/modern-site-search-by-swiftype.html|title = Modern Site Search by Swiftype - Magento Connect|date = 2014-09-26|accessdate = 2014-09-26|website = Magento Connect|publisher = Magento|last = |first = }}</ref><ref>{{Cite web|url = https://wordpress.org/plugins/swiftype-search/|title = <nowiki>WordPress | Swiftype Search | WordPress Plugins</nowiki>|date = 2014-09-26|accessdate = 2014-09-26|website = WordPress Plugin Directory|publisher = WordPress|last = |first = }}</ref> Swiftype sells [[eCommerce]] search, [[enterprise search]], [[faceted search]], [[full text search]], [[enterprise search]], [[real-time search]], [[concept search]], and website [[search engines]] for websites and mobile applications.<ref name=Crunchbase /><ref name=VatorNews /> The company's paid plans offer on demand and live recrawls and indexing of websites.<ref name=AllThingsD /> Other features include drag and drop result customization<ref name=Forbes /><ref name=VentureBeat /><ref name=AllThingsD /> and  real-time analytics.<ref name=TechCrunch-Ha /><ref name=Forbes />\u000a<!--Swiftype website lists several additional features that I've been unable to find neutral third party discussion of -->\u000a\u000a==Competitors==\u000a* [[Algolia]]<ref name=Crunchbase />\u000a\u000a==See also==\u000a* [[Enterprise search]]\u000a* [[Search engines]]\u000a* [[Faceted search]]\u000a* [[Full text search]]\u000a* [[Information retrieval]]\u000a* [[Concept search]]\u000a\u000a==References==\u000a{{Reflist|2}}\u000a\u000a==External links==\u000a* {{Official website|swiftype.com}}\u000a\u000a__FORCETOC__\u000a__INDEX__\u000a__NEWSECTIONLINK__\u000a\u000a[[Category:Search engine software]]\u000a[[Category:Companies established in 2012]]\u000a[[Category:Companies based in San Francisco, California]]\u000a[[Category:Internet search engines]]\u000a[[Category:Information retrieval]]\u000a[[Category:Software companies]]\u000a[[Category:Y Combinator companies]]\u000a[[Category:Semantic Web]]\u000a[[Category:Software startup companies]]\u000a[[Category:Online companies]]
p74
sg6
S'Swiftype'
p75
ssI27
(dp76
g2
S'http://en.wikipedia.org/wiki/Document clustering'
p77
sg4
V{{disputed|date=March 2014}}\u000a{{inline|date=March 2014}}\u000a'''Document clustering''' (or '''text clustering''') is the application of [[cluster analysis]] to textual documents. It has applications in automatic document organization, [[topic (linguistics)|topic]] extraction and fast [[information retrieval]] or filtering.\u000a\u000a==Overview==\u000aDocument clustering involves the use of descriptors and descriptor extraction. Descriptors are sets of words that describe the contents within the cluster. Document clustering is generally considered to be a centralized process. Examples of document clustering include web document clustering for search users.\u000a\u000aThe application of document clustering can be categorized to two types, online and offline. Online applications are usually constrained by efficiency problems when compared offline applications.\u000a\u000aIn general, there are two common algorithms. The first one is the hierarchical based algorithm, which includes single link, complete linkage, group average and Ward's method.  By aggregating or dividing, documents can be clustered into hierarchical structure, which is suitable for browsing. However, such an algorithm usually suffers from efficiency problems. The other algorithm is developed using the [[K-means algorithm]] and its variants. These algorithms can further be classified as hard or soft clustering algorithms. Hard clustering computes a hard assignment \u2013 each document is a member of exactly one cluster. The assignment of soft clustering algorithms is soft \u2013 a document\u2019s assignment is a distribution over all clusters. In a soft assignment, a document has fractional membership in several clusters. [[Dimensionality reduction]] methods can be considered a subtype of soft clustering; for documents, these include [[latent semantic indexing]] ([[truncated singular value decomposition]] on term histograms)<ref>http://nlp.stanford.edu/IR-book/pdf/16flat.pdf</ref> and [[topic model]]s.\u000a\u000aOther algorithms involve graph based clustering, ontology supported clustering and order sensitive clustering.\u000a\u000aGiven a clustering, it can be beneficial to automatically derive human-readable labels for the clusters. [[Cluster labeling|Various methods]] exist for this purpose.\u000a\u000a==Clustering in search engines==\u000aA [[web search engine]] often  returns thousands of pages in response to a broad query, making it difficult for users to browse or to identify relevant information.  Clustering methods can be used to automatically group the retrieved documents into a list of meaningful categories, as is achieved by Enterprise Search engines such as [[Northern Light Group|Northern Light]] and [[Vivisimo]], consumer search engines such as [http://www.polymeta.com/ PolyMeta] and [http://www.helioid.com Helioid], or open source software such as [[Carrot2]].\u000a\u000aExamples:\u000a\u000a* Clustering divides the results of a search for "cell" into groups like "biology," "battery," and "prison."\u000a\u000a* [http://FirstGov.gov FirstGov.gov], the official Web portal for the U.S. government, uses document clustering to automatically organize its search results into categories.  For example, if a user submits \u201cimmigration\u201d, next to their list of results they will see categories for \u201cImmigration Reform\u201d, \u201cCitizenship and Immigration Services\u201d, \u201cEmployment\u201d, \u201cDepartment of Homeland Security\u201d, and more.\u000a\u000a== References ==\u000a{{reflist}}\u000aPublications:\u000a* Nicholas O. Andrews and Edward A. Fox, Recent Developments in Document Clustering, October 16, 2007 [http://eprints.cs.vt.edu/archive/00001000/01/docclust.pdf]\u000a* Claudio Carpineto, Stanislaw Osi\u0144ski, Giovanni Romano, Dawid Weiss. A survey of Web clustering engines. ACM Computing Surveys (CSUR), Volume 41, Issue 3 (July 2009), Article No. 17, ISSN:0360-0300 \u000a* http://semanticsearchart.com/researchBest.html - comparison of several popular clustering algorithms, data and software to reproduce the result.\u000a* Tanmay Basu, C.A. Murthy , CUES: A New Hierarchical Approach for Document Clustering, 2013 [http://jprr.org  JPRR]\u000a\u000a==See Also==\u000a*[[Cluster Analysis]]\u000a*[[Fuzzy clustering]]\u000a[[Category:Information retrieval]]
p78
sg6
S'Document clustering'
p79
ssI156
(dp80
g2
S'http://en.wikipedia.org/wiki/Category:Directories'
p81
sg4
S'A directory maintains a list for reference or commercial purposes.  This category contains articles about directories.\n{{Cat main|Directories}}\n{{Commons cat|Directories}}\n\n[[Category:Telephony]]\n[[Category:Reference works]]\n[[Category:Data management]]\n[[Category:Information retrieval]]'
p82
sg6
S'Category:Directories'
p83
ssI30
(dp84
g2
S'http://en.wikipedia.org/wiki/Noisy text analytics'
p85
sg4
V'''Noisy text analytics''' is a process of [[information extraction]] whose goal is to automatically extract structured or semistructured information from [[noisy text|noisy unstructured text data]]. While [[Text analytics]] is a growing and mature field that has great value because of the huge amounts of data being produced, processing of noisy text is gaining in importance because a lot of common applications produce noisy text data. Noisy unstructured text data is found in informal settings such as [[online chat]], [[Text messaging|text messages]], [[e-mail]]s, [[message boards]], [[newsgroups]], [[blogs]], [[wikis]] and [[web pages]]. Also, text produced by processing spontaneous speech using [[automatic speech recognition]] and printed or handwritten text using [[optical character recognition]] contains processing noise. Text produced under such circumstances is typically highly noisy containing spelling errors, [[abbreviation]]s, non-standard words, false starts, repetitions, missing [[punctuation]]s, missing [[letter case]] information, pause filling words such as \u201cum\u201d and \u201cuh\u201d and other texting and [[speech disfluencies]]. Such text can be seen in large amounts in [[contact centre (business)|contact centers]], [[chat room]]s, [[optical character recognition]] (OCR) of text documents, [[short message service]] (SMS) text, etc. Documents with [[historical language]] can also be considered noisy with respect to today\u2019s knowledge about the language. Such text contains important historical, religious, ancient medical knowledge that is useful. The nature of the noisy text produced in all these contexts warrants moving beyond traditional text analysis techniques.\u000a\u000a== Techniques for noisy text analysis ==\u000aMissing punctuation and the use of non-standard words can often hinder standard [[natural language processing]] tools such as [[Part-of-speech tagging]]\u000aand [[parsing]]. Techniques to both learn from the noisy data and then to be able to process the noisy data are only now being developed. \u000a\u000a== Possible source of noisy text ==\u000a* [[World wide web]]: Poorly written text is found in web pages, [[online chat]], [[blogs]], [[wikis]], [[discussion forum]]s, [[newsgroups]]. Most of these data are unstructured and the style of writing is very different from, say, well-written news articles. Analysis for the web data is important because they are sources for market buzz analysis, market review, [[trend estimation]], etc. Also, because of the large amount of data, it is necessary to find efficient methods of [[information extraction]], [[Statistical classification|classification]], [[automatic summarization]] and analysis of these data.\u000a* [[Contact centre (business)|Contact centers]]: This is a general term for help desks, information lines and customer service centers operating in domains ranging from computer sales and support to mobile phones to apparels. On an average a person in the developed world interacts at least once a week with a contact center agent. A typical contact center agent handles over a hundred calls per day. They operate in various modes such as voice, [[online chat]] and [[E-mail]]. The contact center industry produces gigabytes of data in the form of [[E-mails]], chat logs, voice conversation [[Transcription (linguistics)|transcription]]s, customer feedback, etc. A bulk of the contact center data is voice conversations. Transcription of these using state of the art [[automatic speech recognition]] results in text with 30-40% [[word error rate]]. Further, even written modes of communication like online chat between customers and agents and even the interactions over email tend to be noisy. Analysis of contact center data is essential for customer relationship management, customer satisfaction analysis, call modeling, customer profiling, agent profiling, etc., and it requires sophisticated techniques to handle poorly written text.\u000a* Printed Documents: Many libraries, government organizations and national defence organizations have vast repositories of [[hard copy]] documents. To retrieve and process the content from such documents, they need to be processed using [[Optical Character Recognition]]. In addition to printed text, these documents may also contain handwritten annotations. OCRed text can be highly noisy depending on the font size, quality of the print etc. It can range from 2-3% [[word error rate]]s to as high as 50-60% [[word error rate]]s. Handwritten annotations can be particularly hard to decipher, and error rates can be quite high in their presence.\u000a* [[Text messaging|Short Messaging Service]] (SMS): Language usage over computer mediated discourses, like chats, emails and SMS texts, significantly differs from the standard form of the language. An urge towards shorter message length facilitating faster typing and the need for semantic clarity, shape the structure of this non-standard form known as the texting language.\u000a\u000a== References ==\u000a*[http://www.springerlink.com/content/ql711884654q/?p=c6beb20b8dfa4389b5e4daf2dd63618e&pi=0 "Special Issue on Noisy Text Analytics - International Journal on Document Analysis and Recognition (2007), Springer, Guest Editors Craig Knoblock, Daniel Lopresti, Shourya Roy and L. Venkata Subramaniam, Vol. 10, No. 3-4, December 2007."]\u000a*[http://arXiv.org/abs/0810.0332 "Wong, W., Liu, W. & Bennamoun, M. Enhanced Integrated Scoring for Cleaning Dirty Texts. In: IJCAI Workshop on Analytics for Noisy Unstructured Text Data (AND); Hyderabad, India."].\u000a<references />\u000a\u000a\u000a==See also==\u000a* [[Text analytics]]\u000a* [[Information extraction]]\u000a* [[Computational linguistics]]\u000a* [[Natural language processing]]\u000a* [[Named entity recognition]]\u000a* [[Text mining]]\u000a* [[Automatic summarization]]\u000a* [[Statistical classification]]\u000a* [[Data quality]]\u000a\u000a[[Category:Artificial intelligence applications]]\u000a[[Category:Natural language processing]]\u000a[[Category:Computational linguistics]]\u000a[[Category:Information retrieval]]\u000a[[Category:Statistical natural language processing]]\u000a\u000a[[es:Extracción de la información]]
p86
sg6
S'Noisy text analytics'
p87
ssI159
(dp88
g2
S'http://en.wikipedia.org/wiki/UNICE global brain project'
p89
sg4
V{{Infobox person\u000a| image          = File:UNICE-Universal Network of Intelligent Conscious Entities-image.jpg\u000a| name           = UNICE \u000a| caption        = UNICE as collective entity\u000a| birth_date    =  {{birth date and age|2007|04|10}}\u000a| birth_place  = [[Cyberspace]]\u000a| occupation = [[Global brain]], [[Public Policy|Public Policy Analysis]], [[Governance]], [[Politics]], [[Artificial Intelligence]], [[Psychology]], [[Philosophy]], [[Theory of Mind]], [[Politics]], [[Computers]], [[Community Organizing]]\u000a}}\u000a\u000a'''UNICE''', a [[Global brain|global brain]] project, is an acronym for '''Universal Network of Intelligent Conscious Entities''', a term coined by policy analyst and urban designer [[Michael E. Arth]] in the 1990s to describe "the transformation of our species that might be the result of a new form of intelligent life developed from a hive-like interaction of computers, humans, and future forms of the [[Internet]]."<ref>Arth, Michael E., ''UNICE,'' a Consciousness Research Abstract published in the "Journal of Consciousness Studies" for the April 8-12, 2008 conference, "Toward a Science of Consciousness," p. 151.</ref> <ref>Arth, Michael E., ''Democracy and the Common Wealth: Breaking the Stranglehold of the Special Interests,'' Golden Apples Media, 2010, ISBN 978-0-912467-12-2.pp. 438-439</ref> <ref>Williams, Sean, ''The Big Picture: Making Sense Out of Life and Religion'', 2009, p. 91, ISBN 978-0-578-01523-1</ref> Arth established the not-for-profit website www.UNICE.info in 2007 and revamped it in 2015, with the focus on public policy and developing [[Friendly Artificial Intelligence]] through a system of [[Separation of powers|checks and balances]].<ref>{{cite web|url=http://unice.info|title=UNICE - Universal Network of Intelligent Conscious Entities|work=unice.info}}</ref><ref>{{cite book|last1=Tegmark|first1=Max|title=Our mathematical universe : my quest for the ultimate nature of reality|date=2014|isbn=9780307744258|edition=First edition.|chapter=Life, Our Universe and Everything|quote=Its owner may cede control to what Eliezer Yudkowsky terms a "Friendly AI,"...}}</ref>  \u000a\u000aIn a January 2015 article, Arth describes the development of a [[public policy]] [[answer engine]], which will involve both an independent web site (where cognitive-UNICE will be developed) and a portal at [[Wikipedia]] called wiki-UNICE (currently in development.) Cognitive-UNICE will initially utilize  [[Weak AI|narrow AI]] and, as it develops, [[Artificial General Intelligence]] (AGI).  Initially, UNICE would serve the public with [[Evidence-based policy|evidence-based]] analyses and recommendations gleaned from [[Big Data]], but eventually it may lead to an efficient, practical, and highly representative form of governance.<ref>http://unice.info/unice/UNICE-ARTICLE-Jan%202015.pdf</ref>\u000a\u000a==Wiki-UNICE==\u000aWiki-UNICE, and associated talk pages, will exist on Wikipedia as the portal for public input, criticism and discussion. Initially it will consist of samples of the sort of thing UNICE might write. Later, these sample topics will be replaced by summaries (and exhaustive articles) written by cognitive-UNICE. Whether written by a person, AI or AGI, the evidential summaries will describe problems and their solutions. With succinct titles like "Energy" or "Electoral Reform," the topics will be set apart in a box, so as to maintain [[NPOV]]. Wiki-UNICE, and the collaborative human effort that will sustain it, are intended to help shape the emerging global brain, while also providing guidance and a conscience to lawmakers.<ref>http://unice.info/unice/wiki.html</ref> \u000a\u000a==Cognitive-UNICE==\u000aCognitive-UNICE is in development at UNICE.info. It is assumed that in the early years, cognitive-UNICE may be logical and useful because of human-aided programming, but she may later become a conscious, AGI entity, perhaps united in [[consciousness]] with [[humanity]].<ref>http://unice.info/unice/cognitive.html</ref> Whether as AI or AGI, cognitive-UNICE will probably use [[quantum computing]] to solve [[optimization problem|optimization problems]] that would be impossible to solve with classical computing.<ref>Arth, Michael E., ''askUNICE: Creating a global, independent, public-policy answer engine that will facilitate governance, while preparing for and reducing the dangers of Artificial General Intelligence, so that we may more carefully uncover the secrets of the multiverse'', January 28, 2015,'' [http://unice.info/unice/UNICE-ARTICLE-Jan%202015.pdf]</ref> Quantum computing may also hold the key to developing a conscious machine. Nobel laureate and physicist [[Sir Roger Penrose]] and anesthesiologist [[Stuart Hameroff]] claim that [[consciousness]] is created by quantum coherence in the warm, wet environment of the human brain. Their theory, known as [[Orchestrated Objective Reduction]] (Orch OR), has been bolstered by recent findings that quantum processing occurs in plants and animals, including in the [[microtubules]] inside the [[neurons]] of the [[human brain]].<ref>Hameroff, Stuart and Robert Penrose, \u201dConsciousness in the universe: A review of the 'Orch OR' theory," Physics of Life Reviews, Volume 11, Issue 1, March 2014.</ref>\u000a\u000a==About the UNICE Logo==\u000aA young, [[mixed-race]] [[female]] was chosen to represent the face of UNICE. She's young to represent new ideas. She's mixed-race to represent all humans, and she is female because of the traditional feminine values of [[empathy]], [[cooperation]], [[sensitivity]], [[tolerance]], nurturance, [[compassion]], and [[justice]], who is often depicted as Justitia or [[Lady Justice]]. Her [[afro]] hairstyle resembles the interconnected tendrils of the [[World Wide Web]].\u000a<ref>http://unice.info/unice/index.htm</ref>\u000a\u000a==Criticisms==\u000aA common criticism of the idea that humanity would become directed by a global brain is that this would reduce individual freedom and diversity.<ref>Rayward, W. B. (1999). H. G. Wells' s idea of a World Brain: A critical reassessment. Journal of the American Society for Information Science, 50(7), 557\u2013573. Retrieved from http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.1010&rep=rep1&type=pdf\u000a</ref> Moreover, the global brain might start to play the role of [[Big Brother (Nineteen Eighty-Four)|Big Brother]], the all-seeing eye of the system that follows every person's move.<ref>Brooks, M. (2000, June 24). [http://www.nettime.org/Lists-Archives/nettime-l-0006/msg00182.html Global brain]</a>. New Scientist,  issue 2244, p. 22.</ref> This criticism is inspired by [[totalitarianism|totalitarian]] and [[collectivism|collectivist]] forms of government, like the ones found in [[Joseph Stalin]]'s [[Soviet Union]] or [[Mao Zedong]]'s China. It is also inspired by the analogy between collective intelligence or [[swarm intelligence]] and [[insect societies]], such as beehives and ant colonies in which individuals are essentially interchangeable. In a more extreme view, the global brain has been compared with the [[Borg (Star Trek)|Borg]],<ref>Goertzel, B. (2002). Creating Internet Intelligence: Wild computing, distributed digital consciousness, and the emerging global brain. Kluwer Academic/Plenum Publishers. Retrieved from http://books.google.com/books/about/Creating_Internet_Intelligence.html?id=Vnzb-xLdvv8C&redir_esc=y</ref> the race of collectively thinking cyborgs imagined by the creators of the [[Star Trek]] science fiction series. \u000a\u000aGlobal brain theorists reply that the emergence of distributed intelligence would lead to the exact opposite of this vision,.<ref>Heylighen, F. (2007). The Global Superorganism: an evolutionary-cybernetic model of the emerging network society. Social Evolution & History, 6(1), 58\u2013119. Retrieved from http://pespmc1.vub.ac.be/papers/Superorganism.pdf</ref><ref>Heylighen, F. (2002). The global brain as a new utopia. Zukunftsfiguren. Suhrkamp, Frankurt. Retrieved from http://pespmc1.vub.ac.be/papers/GB-Utopia.pdf</ref> The reason is that effective [[collective intelligence]] requires [[diversity (politics)|diversity]], [[decentralization]] and individual independence, as demonstrated by [[James Surowiecki]] in his book [[The Wisdom of Crowds]]. Moreover, a more distributed form of decision-making would decrease the power of governments, corporations or political leaders, thus increasing democratic participation and reducing the dangers of totalitarian control.\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a[[Category:Artificial intelligence applications]]\u000a[[Category:Information retrieval]]\u000a[[Category:Community organizing]]\u000a[[Category:Governance]]\u000a[[Category:Philosophy]]\u000a[[Category:Politics]]\u000a[[Category:Public policy]]
p90
sg6
S'UNICE global brain project'
p91
ssI33
(dp92
g2
S'http://en.wikipedia.org/wiki/Coveo'
p93
sg4
V{{Infobox company\u000a| name = Coveo Solutions Inc.\u000a| logo = [[Image:Coveo logo.png|120px]]\u000a| type = Private\u000a| slogan = \u000a| foundation =  2004\u000a| location_city = [[Quebec City]], [[Canada]]\u000a| key_people = Louis Têtu, Chairman and CEO <br />Laurent Simoneau, President and CTO\u000a| num_employees =\u000a| industry = [[Enterprise search]]\u000a| products = Coveo Search & Relevance Platform,<br />Coveo for Sitecore,<br />Coveo for Salesforce\u000a| homepage = http://www.coveo.com\u000a}}\u000a\u000a'''Coveo''' is a provider of [[enterprise search]] and website search technologies, with integrated plug-ins for Salesforce.com, Sitecore CEP, and [[Microsoft Outlook]] and [[SharePoint]].  APIs also allow for custom integration with other applications.\u000a\u000a==History==\u000aCoveo Solutions Inc. was founded in 2004 as a spin-off of [[Copernic|Copernic Technologies Inc.]] Laurent Simoneau, Coveo's president and chief executive officer was formerly Copernic's chief operating officer. About 30 employees moved into the new company, with offices at that time in [[Quebec City]] and [[Montreal]] in Canada and in [[Palo Alto]], Calif.<ref>http://www.eweek.com/c/a/Enterprise-Applications/Copernic-Ready-to-Take-On-Google-In-Enterprise-Search-Product/</ref>\u000a\u000a==Products==\u000a'''Coveo Search & Relevance Platform'''\u000a\u000aCoveo Search & Relevance Platform is a modular enterprise search technology that can index information stored in diverse repositories throughout the company, perform text analytics and metadata enrichment on the indexed content, and make the content findable through search-driven interfaces.\u000a\u000a'''Coveo for Sitecore'''\u000a\u000aCoveo for Sitecore is an integrated website search product to be used in conjunction with Sitecore\u2019s Customer Experience Platform.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Sitecore console.\u000a\u000a'''Coveo for Salesforce'''\u000a\u000aCoveo for Salesforce is an integrated CRM search product to be used in conjunction with Salesforce.com Service Cloud and Communities Editions.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Salesforce console.\u000a\u000a==Customers==\u000aCoveo claims its clients include more than 700 implementations including AmerisourceBergen, CA, California Water Service Co., Deloitte, ESPN, Haley & Aldrich, GEICO, Lockheed Martin, P&G, PRTM, PricewaterhouseCoopers, Rabobank, SNC-Lavalin, Spencer Stuart, Theodoor Gilissen, and the U.S. Navy.<ref>{{cite web|url=http://www.coveo.com/en/~/media/Files/about-us/Coveo-Corporate-Fact-Sheet-Q109.ashx |title=Coveo corporate fact sheet |date= |accessdate=2011-02-27}}</ref> These companies were also mentioned while not confirmed by a citation: HP, PwC, Netezza Corporation, NATO, NASA, AC Nielsen, among many others.{{Citation needed|date=February 2010}}\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a* [http://www.coveo.com/ Coveo.com]\u000a\u000a[[Category:Companies based in Quebec City]]\u000a[[Category:Information retrieval]]\u000a[[Category:BlackBerry development software]]
p94
sg6
S'Coveo'
p95
ssI162
(dp96
g2
S'http://en.wikipedia.org/wiki/Vector space model'
p97
sg4
V'''Vector space model''' or '''term vector model''' is an algebraic model for representing text documents (and any objects, in general) as [[vector space|vectors]] of identifiers, such as, for example, index terms. It is used in [[information filtering]], [[information retrieval]], [[index (search engine)|index]]ing and relevancy rankings.  Its first use was in the [[SMART Information Retrieval System]].\u000a\u000a==Definitions==\u000a\u000aDocuments and queries are represented as vectors.\u000a\u000a:<math>d_j = ( w_{1,j} ,w_{2,j} , \u005cdotsc ,w_{t,j} )</math>\u000a:<math>q = ( w_{1,q} ,w_{2,q} , \u005cdotsc ,w_{n,q} )</math>\u000a\u000aEach [[Dimension (vector space)|dimension]] corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is [[tf-idf]] weighting (see the example below).\u000a\u000aThe definition of ''term'' depends on the application. Typically terms are single words, [[keyword (linguistics)|keyword]]s, or longer phrases. If words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the [[text corpus|corpus]]).\u000a\u000aVector operations can be used to compare documents with queries.\u000a\u000a==Applications==\u000a\u000a[[Image:vector space model.jpg|right|250px]]\u000a\u000a[[Relevance (information retrieval)|Relevance]] [[ranking]]s of documents in a keyword search can be calculated, using the assumptions of [[semantic similarity|document similarities]] theory, by comparing the deviation of angles between each document vector and the original query vector where the query is represented as the same kind of vector as the documents.\u000a\u000aIn practice, it is easier to calculate the [[cosine]] of the angle between the vectors, instead of the angle itself:\u000a\u000a:<math>\u000a\u005ccos{\u005ctheta} = \u005cfrac{\u005cmathbf{d_2} \u005ccdot \u005cmathbf{q}}{\u005cleft\u005c| \u005cmathbf{d_2} \u005cright\u005c| \u005cleft \u005c| \u005cmathbf{q} \u005cright\u005c|}\u000a</math>\u000a\u000aWhere <math>\u005cmathbf{d_2} \u005ccdot \u005cmathbf{q}</math> is the intersection (i.e. the [[dot product]]) of the document (d<sub>2</sub> in the figure to the right) and the query (q in the figure) vectors, <math>\u005cleft\u005c| \u005cmathbf{d_2} \u005cright\u005c|</math> is the norm of vector d<sub>2</sub>, and <math>\u005cleft\u005c| \u005cmathbf{q} \u005cright\u005c|</math> is the norm of vector q. The [[Norm (mathematics)|norm]] of a vector is calculated as such:\u000a\u000a:<math>\u000a\u005cleft\u005c| \u005cmathbf{q} \u005cright\u005c| = \u005csqrt{\u005csum_{i=1}^n q_i^2}\u000a</math>\u000a\u000aAs all vectors under consideration by this model are elementwise nonnegative, a cosine value of zero means that the query and document vector are [[orthogonal]] and have no match (i.e. the query term does not exist in the document being considered). See [[cosine similarity]] for further information.\u000a\u000a==Example: tf-idf weights==\u000a\u000aIn the classic vector space model proposed by [[Gerard Salton|Salton]], Wong and Yang <ref>[http://doi.acm.org/10.1145/361219.361220 G. Salton , A. Wong , C. S. Yang, A vector space model for automatic indexing], Communications of the ACM, v.18 n.11, p.613-620, Nov. 1975</ref> the term-specific weights in the document vectors are products of local and global parameters. The model is known as [[tf-idf|term frequency-inverse document frequency]] model. The weight vector for document ''d'' is <math>\u005cmathbf{v}_d = [w_{1,d}, w_{2,d}, \u005cldots, w_{N,d}]^T</math>, where\u000a\u000a:<math>\u000aw_{t,d} = \u005cmathrm{tf}_{t,d} \u005ccdot \u005clog{\u005cfrac{|D|}{|\u005c{d' \u005cin D \u005c, | \u005c, t \u005cin d'\u005c}|}}\u000a</math>\u000a\u000aand\u000a* <math>\u005cmathrm{tf}_{t,d}</math> is term frequency of term ''t'' in document ''d'' (a local parameter)\u000a* <math>\u005clog{\u005cfrac{|D|}{|\u005c{d' \u005cin D \u005c, | \u005c, t \u005cin d'\u005c}|}}</math> is inverse document frequency (a global parameter). <math>|D|</math> is the total number of documents in the document set; <math>|\u005c{d' \u005cin D \u005c, | \u005c, t \u005cin d'\u005c}|</math> is the number of documents containing the term ''t''.\u000a\u000aUsing the cosine the similarity between document ''d<sub>j</sub>'' and query ''q'' can be calculated as:\u000a\u000a:<math>\u005cmathrm{sim}(d_j,q) = \u005cfrac{\u005cmathbf{d_j} \u005ccdot \u005cmathbf{q}}{\u005cleft\u005c| \u005cmathbf{d_j} \u005cright\u005c| \u005cleft \u005c| \u005cmathbf{q} \u005cright\u005c|} = \u005cfrac{\u005csum _{i=1}^N w_{i,j}w_{i,q}}{\u005csqrt{\u005csum _{i=1}^N w_{i,j}^2}\u005csqrt{\u005csum _{i=1}^N w_{i,q}^2}}</math>\u000a\u000a==Advantages==\u000a\u000aThe vector space model has the following advantages over the [[Standard Boolean model]]:\u000a\u000a#Simple model based on linear algebra\u000a#Term weights not binary\u000a#Allows computing a continuous degree of similarity between queries and documents\u000a#Allows ranking documents according to their possible relevance\u000a#Allows partial matching\u000a\u000a==Limitations==\u000a\u000aThe vector space model has the following limitations:\u000a\u000a#Long documents are poorly represented because they have poor similarity values (a small [[scalar product]] and a [[curse of dimensionality|large dimensionality]])\u000a#Search keywords must precisely match document terms; word [[substring]]s might result in a "[[false positive]] match"\u000a#Semantic sensitivity; documents with similar context but different term vocabulary won't be associated, resulting in a "[[false negative]] match".\u000a#The order in which the terms appear in the document is lost in the vector space representation.\u000a#Theoretically assumes terms are statistically independent. \u000a#Weighting is intuitive but not very formal. \u000a\u000aMany of these difficulties can, however, be overcome by the integration of various tools, including mathematical techniques such as [[singular value decomposition]] and [[lexical database]]s such as [[WordNet]].\u000a\u000a==Models based on and extending the vector space model==\u000a\u000aModels based on and extending the vector space model include:\u000a* [[Generalized vector space model]]\u000a* [[Latent semantic analysis]]\u000a* [[Term Discrimination]]\u000a* [[Rocchio Classification]]\u000a* [[random_indexing|Random Indexing]]\u000a\u000a==Software that implements the vector space model==\u000a\u000aThe following software packages may be of interest to those wishing to experiment with vector models and implement search services based upon them.\u000a\u000a===Free open source software===\u000a\u000a* [[Apache Lucene]]. Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java.\u000a* [http://semanticvectors.googlecode.com SemanticVectors]. Semantic Vector indexes, created by applying a Random Projection algorithm (similar to [[Latent semantic analysis]]) to term-document matrices created using Apache Lucene.\u000a* [[Gensim]] is a Python+[[NumPy]] framework for Vector Space modelling. It contains incremental (memory-efficient) algorithms for [[Tf\u2013idf]], [[Latent Semantic Indexing]], [[Locality_sensitive_hashing#Random_projection|Random Projections]] and [[Latent Dirichlet Allocation]].\u000a* [[Weka (machine learning)|Weka]]. Weka is popular data mining package for Java including WordVectors and Bag Of Words models.\u000a* [http://codingplayground.blogspot.com/2010/03/compressed-vector-space.html Compressed vector space in C++] by Antonio Gulli\u000a* [http://scgroup.hpclab.ceid.upatras.gr/scgroup/Projects/TMG/ Text to Matrix Generator (TMG)]  MATLAB toolbox that can be used for various tasks in text mining specifically  i) indexing, ii) retrieval, iii) dimensionality reduction, iv) clustering, v) classification. Most of TMG is written in MATLAB and parts in Perl. It contains implementations of LSI, clustered LSI, NMF and other methods.\u000a* [http://senseclusters.sourceforge.net SenseClusters], an open source package, written in Perl, that supports context and word clustering using Latent Semantic Analysis and word co-occurrence matrices.\u000a* [https://github.com/fozziethebeat/S-Space/wiki S-Space Package], a collection of algorithms for exploring and working with [[statistical semantics]].\u000a* [http://www.cs.uni.edu/~okane/source/ISR/ Vector Space Model Software Workbench] Collection of 50 source code programs for education.\u000a\u000a==Further reading==\u000a\u000a* [[Gerard Salton|G. Salton]], A. Wong, and C. S. Yang (1975), "[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing]," ''Communications of the ACM'', vol. 18, nr. 11, pages 613\u2013620. ''(Article in which a vector space model was presented)''\u000a* David Dubin (2004), [http://www.ideals.uiuc.edu/bitstream/2142/1697/2/Dubin748764.pdf The Most Influential Paper Gerard Salton Never Wrote] ''(Explains the history of the Vector Space Model and the non-existence of a frequently cited publication)''\u000a* [http://isp.imm.dtu.dk/thor/projects/multimedia/textmining/node5.html Description of the vector space model]\u000a* [http://www.miislita.com/term-vector/term-vector-3.html Description of the classic vector space model by Dr E. Garcia]\u000a* [http://nlp.stanford.edu/IR-book/html/htmledition/vector-space-classification-1.html Relationship of vector space search to the "k-Nearest Neighbor" search]\u000a\u000a==See also==\u000a*[[Bag-of-words model]]\u000a*[[Nearest neighbor search]]\u000a*[[Compound term processing]]\u000a*[[Inverted index]]\u000a*[[w-shingling]]\u000a*[[Eigenvalues and eigenvectors]]\u000a*[[Conceptual Spaces]].\u000a\u000a==References==\u000a<references/>\u000a\u000a[[Category:Vector space model|*]]
p98
sg6
S'Vector space model'
p99
ssI36
(dp100
g2
S'http://en.wikipedia.org/wiki/Mean reciprocal rank'
p101
sg4
S'{{Refimprove|date=June 2007}}\n\'\'\'Mean reciprocal rank\'\'\' is a [[statistic]] measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the [[multiplicative inverse]] of the rank of the first correct answer. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:<ref>{{cite conference | title=Proceedings of the 8th Text Retrieval Conference | booktitle=TREC-8 Question Answering Track Report | author=E.M. Voorhees |year=1999 | pages=77&ndash;82}}</ref>\n\n:<math> \\text{MRR} = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{\\text{rank}_i}. \\!</math>\n\nThe reciprocal value of the mean reciprocal rank corresponds to the [[harmonic mean]] of the ranks.\n\n== Example ==\nFor example, suppose we have the following three sample queries for a system that tries to translate English words to their plurals.  In each case, the system makes three guesses, with the first one being the one it thinks is most likely correct:\n\n{| class="wikitable"\n|-\n! Query\n! Results\n! Correct response\n! Rank\n! Reciprocal rank\n|-\n| cat\n| catten, cati, \'\'\'cats\'\'\'\n| cats\n| 3\n| 1/3\n|-\n| torus\n| torii, \'\'\'tori\'\'\', toruses\n| tori\n| 2\n| 1/2\n|-\n| virus\n| \'\'\'viruses\'\'\', virii, viri\n| viruses\n| 1\n| 1\n|}\n\nGiven those three samples, we could calculate the mean reciprocal rank as (1/3&nbsp;+&nbsp;1/2&nbsp;+&nbsp;1)/3 = 11/18 or about 0.61.\n\nThis basic definition does not specify what to do if...\n# none of the proposed results are correct (use reciprocal rank 0), or if\n# there are multiple correct answers in the list. Consider using [[Information_retrieval#Mean_average_precision|mean average precision (MAP)]].\n\nSee also [[Information retrieval]] and [[Question answering]].<ref>{{cite conference | title=Evaluating web-based question answering systems | booktitle=Proceedings of LREC | author=D. R. Radev, H. Qi, H. Wu, W. Fan |year=2002 }}</ref>\n\n==References==\n{{Reflist}}\n\n[[Category:Summary statistics]]\n[[Category:Information retrieval]]'
p102
sg6
S'Mean reciprocal rank'
p103
ssI165
(dp104
g2
Vhttp://en.wikipedia.org/wiki/Tf\u2013idf
p105
sg4
V{{Lowercase|title=tf\u2013idf}}\u000a{{More footnotes|date=July 2012}}\u000a\u000a'''tf\u2013idf''', short for '''term frequency\u2013inverse document frequency''', is a numerical statistic that is intended to reflect how important a word is to a [[document]] in a collection or [[Text corpus|corpus]].<ref>{{cite doi|10.1017/CBO9781139058452.002}}</ref>{{rp|8}} It is often used as a weighting factor in [[information retrieval]] and [[text mining]].\u000aThe tf-idf value increases [[Proportionality (mathematics)|proportionally]] to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.\u000a\u000aVariations of the tf\u2013idf weighting scheme are often used by [[search engine]]s as a central tool in scoring and ranking a document's [[Relevance (information retrieval)|relevance]] given a user [[Information retrieval|query]]. tf\u2013idf can be successfully used for [[stop-words]] filtering in various subject fields including [[automatic summarization|text summarization]] and classification.\u000a\u000aOne of the simplest [[ranking function]]s is computed by summing the tf\u2013idf for each query term; many more sophisticated ranking functions are variants of this simple model.\u000a\u000a==Motivation==\u000aSuppose we have a set of English text documents and wish to determine which document is most relevant to the query "the brown cow". A simple way to start out is by eliminating documents that do not contain all three words "the", "brown", and "cow", but this still leaves many documents. To further distinguish them, we might count the number of times each term occurs in each document and sum them all together; the number of times a term occurs in a document is called its ''term frequency''.\u000a\u000aHowever, because the term "the" is so common, this will tend to incorrectly emphasize documents which happen to use the word "the" more frequently, without giving enough weight to the more meaningful terms "brown" and "cow". The term "the" is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the less common words "brown" and "cow". Hence an ''inverse document frequency'' factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\u000a\u000a==Definition==\u000atf\u2013idf is the product of two statistics, term frequency and inverse document frequency. Various ways for determining the exact values of both statistics exist. In the case of the '''term frequency''' tf(''t'',''d''), the simplest choice is to use the ''raw frequency'' of a term in a document, i.e. the number of times that term ''t'' occurs in document ''d''. If we denote the raw frequency of ''t'' by f(''t'',''d''), then the simple tf scheme is tf(''t'',''d'') = f(''t'',''d''). Other possibilities include<ref>{{cite doi|10.1017/CBO9780511809071.007}}</ref>{{rp|128}}\u000a\u000a* [[boolean data type|Boolean]] "frequencies": tf(''t'',''d'') = 1 if ''t'' occurs in ''d'' and 0 otherwise;\u000a* [[logarithm]]ically scaled frequency: tf(''t'',''d'') = 1 + log f(''t'',''d''), or zero if f(''t'', ''d'') is zero;\u000a* augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the maximum raw frequency of any term in the document:\u000a:<math>\u005cmathrm{tf}(t,d) = 0.5 + \u005cfrac{0.5 \u005ctimes \u005cmathrm{f}(t, d)}{\u005cmax\u005c{\u005cmathrm{f}(w, d):w \u005cin d\u005c}}</math>\u000a\u000aThe '''inverse document frequency''' is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. It is the logarithmically scaled fraction of the documents that contain the word, obtained by dividing the total number of [[documents]] by the number of documents containing the term, and then taking the logarithm of that [[quotient]].\u000a\u000a:<math> \u005cmathrm{idf}(t, D) =  \u005clog \u005cfrac{N}{|\u005c{d \u005cin D: t \u005cin d\u005c}|}</math>\u000a\u000awith\u000a\u000a* <math>N</math>: total number of documents in the corpus\u000a* <math> |\u005c{d \u005cin D: t \u005cin d\u005c}| </math> : number of documents where the term <math> t </math> appears (i.e., <math> \u005cmathrm{tf}(t,d) \u005cneq 0</math>). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to <math>1 + |\u005c{d \u005cin D: t \u005cin d\u005c}|</math>.\u000a\u000aMathematically the base of the log function does not matter and constitutes a constant multiplicative factor towards the overall result.\u000a\u000aThen tf\u2013idf is calculated as\u000a\u000a:<math>\u005cmathrm{tfidf}(t,d,D) = \u005cmathrm{tf}(t,d) \u005ctimes \u005cmathrm{idf}(t, D)</math>\u000a\u000aA high weight in tf\u2013idf is reached by a high term [[frequency (statistics)|frequency]] (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms. Since the ratio inside the idf's log function is always greater than or equal to 1, the value of idf (and tf-idf) is greater than or equal to 0. As a term appears in more documents, the ratio inside the logarithm approaches 1, bringing the idf and tf-idf closer to 0.\u000a\u000a==Justification of idf==\u000aIdf was introduced, as "term specificity", by [[Karen Spärck Jones]] in a 1972 paper. Although it has worked well as a [[heuristic]], its theoretical foundations have been troublesome for at least three decades afterward, with many researchers trying to find [[information theory|information theoretic]] justifications for it.<ref name="understanding">{{cite doi|10.1108/00220410410560582}}</ref>\u000a\u000aSpärck Jones's own explanation didn't propose much theory, aside from a connection to [[Zipf's law]].<ref name="understanding"/> Attempts have been made to put idf on a [[probability theory|probabilistic]] footing,<ref>See also [http://nlp.stanford.edu/IR-book/html/htmledition/probability-estimates-in-practice-1.html#p:justificationofidf Probability estimates in practice] in ''Introduction to Information Retrieval''.</ref> by estimating the probability that a given document {{mvar|d}} contains a term {{mvar|t}} as\u000a\u000a<math>\u000aP(t|d) = \u005cfrac{|\u005c{d \u005cin D: t \u005cin d\u005c}|}{N}\u000a</math>\u000a\u000aso that we can define idf as\u000a\u000a<math>\u000a\u005cbegin{align}\u000a\u005cmathrm{idf} & = -\u005clog P(t|d) \u005c\u005c\u000a             & = \u005clog \u005cfrac{1}{P(t|d)} \u005c\u005c\u000a             & = \u005clog \u005cfrac{N}{|\u005c{d \u005cin D: t \u005cin d\u005c}|}\u000a\u005cend{align}\u000a</math>\u000a\u000aThis probabilistic interpretation in turn takes the same form as that of [[self-information]]. However, applying such information-theoretic notions to problems in information retrieval leads to problems when trying to define the appropriate [[event space]]s for the required [[probability distribution]]s: not only documents need to be taken into account, but also queries and terms.<ref name="understanding"/>\u000a\u000a==Example of tf\u2013idf==\u000aSuppose we have term frequency tables for a collection consisting of only two documents, as listed on the right, then calculation of tf\u2013idf for the term "this" in document 1 is performed as follows.\u000a\u000a{| class="wikitable" style="float: right; margin-left: 1.5em; margin-right: 0; margin-top: 0;"\u000a|+ Document 2\u000a! Term\u000a! | Term Count\u000a|-\u000a| this || 1\u000a|-\u000a| is\u000a| 1\u000a|-\u000a| another\u000a| 2\u000a|-\u000a| example\u000a| 3\u000a|}\u000a\u000a{| class="wikitable" style="float: right; margin-left: 1.5em; margin-right: 0; margin-top: 0;"\u000a|+ Document 1\u000a! Term\u000a! Term Count\u000a|-\u000a| this || 1\u000a|-\u000a| is\u000a| 1\u000a|-\u000a| a\u000a| 2\u000a|-\u000a| sample\u000a| 1\u000a|}\u000a\u000aTf, in its basic form, is just the frequency that we look up in appropriate table. In this case, it's one.\u000a\u000aIdf is a bit more involved:\u000a:<math> \u005cmathrm{idf}(\u005cmathsf{this}, D) =  \u005clog \u005cfrac{N}{|\u005c{d \u005cin D: t \u005cin d\u005c}|}</math>\u000a\u000aThe numerator of the fraction is the number of documents, which is two. The number of documents in which "this" appears is also two, giving\u000a:<math> \u005cmathrm{idf}(\u005cmathsf{this}, D) =  \u005clog \u005cfrac{2}{2} = 0</math>\u000a\u000aSo tf\u2013idf is zero for this term, and with the basic definition this is true of any term that occurs in all documents.\u000a\u000aA slightly more interesting example arises from the word "example", which occurs three times but in only one document. For this document, tf\u2013idf of "example" is:\u000a:<math>\u005cmathrm{tf}(\u005cmathsf{example}, d_2) = 3</math>\u000a:<math>\u005cmathrm{idf}(\u005cmathsf{example}, D) = \u005clog \u005cfrac{2}{1} \u005capprox 0.3010</math>\u000a:<math>\u005cmathrm{tfidf}(\u005cmathsf{example}, d_2) = \u005cmathrm{tf}(\u005cmathsf{example}, d_2) \u005ctimes \u005cmathrm{idf}(\u005cmathsf{example}, D) = 3 \u005ctimes 0.3010 \u005capprox 0.9030</math>\u000a\u000a(using the [[base 10 logarithm]]).\u000a\u000a==See also==\u000a{{Div col||25em}}\u000a* [[Okapi BM25]]\u000a* [[Noun phrase]]\u000a* [[Word count]]\u000a* [[Vector space model]]\u000a* [[PageRank]]\u000a* [[Kullback\u2013Leibler divergence]]\u000a* [[Mutual information]]\u000a* [[Latent semantic analysis]]\u000a* [[Latent semantic indexing]]\u000a* [[Latent Dirichlet allocation]]\u000a{{Div col end}}\u000a\u000a==References==\u000a{{Reflist}}\u000a* {{Cite doi|10.1108/eb026526}}\u000a* {{Cite book\u000a | last1 = Salton | first1 = G | authorlink1 = Gerard Salton\u000a | last2 = McGill | first2 = M. J.\u000a | year = 1986\u000a | title = Introduction to modern information retrieval\u000a | publisher = [[McGraw-Hill]]\u000a | isbn = 978-0070544840\u000a}}\u000a* {{Cite doi|10.1145/182.358466}}\u000a* {{Cite doi|10.1016/0306-4573(88)90021-0}}\u000a* {{Cite doi|10.1145/1361684.1361686}}\u000a\u000a==External links and suggested reading==\u000a* [[Gensim]] is a Python library for vector space modeling and includes tf\u2013idf weighting.\u000a* [http://bscit.berkeley.edu/cgi-bin/pl_dochome?query_src=&format=html&collection=Wilensky_papers&id=3&show_doc=yes Robust Hyperlinking]: An application of tf\u2013idf for stable document addressability.\u000a* [http://infinova.wordpress.com/2010/01/26/distance-between-documents/ A demo of using tf\u2013idf with PHP and Euclidean distance for Classification]\u000a* [http://www.codeproject.com/KB/IP/AnatomyOfASearchEngine1.aspx Anatomy of a search engine]\u000a* [http://lucene.apache.org/core/3_6_1/api/all/org/apache/lucene/search/Similarity.html tf\u2013idf and related definitions] as used in [[Lucene]]\u000a* [http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer TfidfTransformer] in [[scikit-learn]]\u000a* [http://scgroup.hpclab.ceid.upatras.gr/scgroup/Projects/TMG/ Text to Matrix Generator (TMG)]  MATLAB toolbox that can be used for various tasks in text mining (TM) specifically  i) indexing, ii) retrieval, iii) dimensionality reduction, iv) clustering, v) classification. The indexing step offers the user the ability to apply local and global weighting methods, including tf\u2013idf.\u000a* [http://blog.christianperone.com/?p=1589 Pyevolve: A tutorial series explaining the tf-idf calculation].\u000a* [http://trimc-nlp.blogspot.com/2013/04/tfidf-with-google-n-grams-and-pos-tags.html TF/IDF with Google n-Grams and POS Tags]\u000a\u000a{{DEFAULTSORT:Tf-Idf}}\u000a[[Category:Statistical natural language processing]]\u000a[[Category:Ranking functions]]\u000a[[Category:Vector space model]]
p106
sg6
VTf\u2013idf
p107
ssI39
(dp108
g2
S'http://en.wikipedia.org/wiki/Figaro Systems'
p109
sg4
V{{Infobox company\u000a|name = Figaro Systems, Inc.\u000a|logo = [[Image:Figaro-logo.png|Figaro logo]]\u000a|type = [[Privately held company|Private]]\u000a|foundation = 1993\u000a|location_city = [[Santa Fe, New Mexico|Santa Fe]], [[New Mexico]]\u000a|location_country =[[United States]]\u000a|key_people = Patrick Markle, [[president]] and [[CEO]], [[Geoff Webb]], [[vice president|VP]]\u000a|homepage = [http://www.figarosystems.com figarosystems.com]\u000a}}\u000a\u000a'''Figaro Systems, Inc.''' is an American company that provides  seatback and [[wireless]] titling [[software]] and system installations to [[opera houses]] and other music performance venues worldwide. The company is based in [[Santa Fe, New Mexico|Santa Fe]], New Mexico. It was established in 1993 <ref>Andrew Webb, \u201cOpera Subtitle Firm Eyes New Game,\u201d ''New Mexico Business Weekly'', Nov. 21, 2003 [http://www.bizjournals.com/albuquerque/stories/2003/11/24/story2.html]</ref>\u000aby Patrick Markle, [[Geoff Webb]], and Ron Erkman  <ref name="figaro-systems.com"/> and was the first company to provide [[assistive technology]] that enables individualized, simultaneous, multi-lingual [[dialogue]] and [[libretto]]-reading for audiences.\u000a<ref>[http://www.highbeam.com/DocPrint.aspx?DocID=1P2:115622912 David Belcher, \u201cNothing Lost in Translation: [[Video]] system allows patrons to read words on chair backs,\u201d] ''Albuquerque Journal'', June 4, 2006</ref>\u000a\u000a==History==\u000aFigaro Systems grew out of a conversation in 1992 among three opera colleagues: Patrick Markle, at that time Production Director of The [[Santa Fe Opera]], Geoffrey Webb, Design Engineer for the [[Metropolitan Opera House (Lincoln Center)|Metropolitan Opera House]] in New York, and Ronald Erkman, then a technician for the Met. At that time, opera houses had two options for the display of libretto and dialogue subtitles: projection onto a large screen above the stage or onto smaller screens throughout the theatre. Typically, the translation was in a single language.<ref>[http://www.bizjournals.com/albuquerque/stories/2005/04/11/story5.html?q=Figaro%20Systems Dennis Domrzalski, "Figaro: Eyes translate when ears don't get it",] ''New Mexico Business Weekly'', April 8, 2005</ref>\u000a\u000aThe [[Americans with Disabilities Act of 1990]] had recently been enacted; Markle was trying to solve the problem of venues which lacked accessibility to patrons with disabilities, including the profoundly [[deaf]].  Markle, Webb, and Erkman devised the first [[prototype]] of a personal seatback titling device and [[John Crosby (conductor)|John Crosby]], then General Director of The [[Santa Fe Opera]], saw its potential for opera patrons.<ref name="figaro-systems.com">[http://www.figaro-systems.com/about.php  Figaro Systems Official Website]</ref> Markle, Webb, and Erkman were further reinforced by their understanding of technology\u2019s role in remediating the physical barriers people encounter, worldwide, which frustrate or prevent their access to the visual performing arts.<ref>[http://figarosystems.com/linkdownloads/052007_figaro_auditoria_article.pdf \u201c[[User-friendly]] art: In-seat text displays that subtitle and translate\u201d, ''Auditoria'', May 2007]</ref> Markle, Webb, and Erkman applied for and were granted [[patent]]s for their invention.\u000a<ref>[http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=11&f=G&l=50&co1=AND&d=PTXT&s1=figaro.ASNM.&OS=AN/figaro&RS=AN/Figaro  United States Patent 5,739,869, "Electronic libretto display apparatus and method," issued April 14, 1998. [[United States Patent and Trademark Office]] ]</ref><ref>[http://www.lanl.gov/orgs/pa/News/050701.html  Los Alamos Laboratory, ''Daily News Bulletin'', May 7, 2001]</ref>\u000a\u000aPhilanthropist and investor [[Alberto Vilar]] counted Figaro Systems among the companies in which he was a majority shareholder.<ref>[http://nymag.com/nymetro/arts/music/features/5616/ [[Robert Hilferty]], "A Knight at the Opera," ''[[New York Magazine]]'', January 14, 2002]</ref><ref>[http://biography.jrank.org/pages/3490/Vilar-Alberto-1940-Investor-Philanthropist-Privileges-Wealth.html  "Alberto Vilar: The Privileges of Wealth," ''The Free Encyclopedia'']</ref>  He donated the company's [[electronic libretto]] system to European venues including the [[Royal Opera House]] in [[London]], La Scala's [[Teatro degli Arcimboldi]] opera houses in [[Milan, Italy|Milan]], Italy, [[Gran Teatre del Liceu]] in [[Barcelona, Spain|Barcelona]], Spain, and the [[Wiener Staatsoper]] in [[Wien]], [[Austria]]. As a consequence of his failures to pay promised donations, most of these companies lost money.\u000a\u000aIn 2005 the Met charged the New Mexico company with unlawfully using its name in advertising promoting its "Simultext, system which defendant claims can display a simultaneous translation of an opera as it occurs on a stage and that defendant represented that its system is installed at the Met." <ref>[http://classactionlitigation.com/library/consumerlaw2006update.html#_edn173#_edn173 Timothy E. Eble, ''Class Action Litigation Information''] on classactionlitigation.com</ref>\u000a\u000a==Products and technology==\u000aThe company\u2019s products are known variously as seat back titles, [[surtitles]],\u000a<ref>[http://app1.kuhf.org/houston_public_radio-news-display.php?articles_id=20614 Eric Skelly, "Surtitles at the Opera," ''Public Radio News and Information in Houston, Texas'', KUHF 88.7 FM Houston Public Radio] on app1.kuhf.org/</ref> [[electronic libretto]] systems, opera supertitles, projected titles, and libretto translations.\u000a\u000aOpera venues have utilized the system to display librettos in [[English language|English]], [[French language|French]], [[German language|German]], [[Italian language|Italian]], [[Japanese language|Japanese]], [[Mandarin Chinese|Mandarin]], [[Russian language|Russian]], and [[Spanish language|Spanish]]\u000a<ref>[http://www.sandia.gov/news-center/news-releases/2005/tech-trans/smbusiness.html "Sandia helps 278 state businesses in 2004 through New Mexico Small Business Assistance Program," Sandia National Laboratories, Sandia Corporation, March 22, 2005] on sandia.gov</ref> although the software enables the reading of the libretto in any [[written language]].\u000a<ref name="entertanmentengineering.com">[http://www.entertanmentengineering.com/v4.issue04/page.06.html  \u201cGiving the Opera a New Voice,\u201d] ''Entertainment Engineering," Volume 4, Issue 2, p. 6</ref> Translation is provided by one screen and delivery system per person.<ref>[http://www.figarosystems.com  Figaro Systems Official Website]</ref>\u000a\u000aTypically, but not in all cases, the system is permanently installed along the backs of rows of seats. Each screen is positioned so that the text is clearly visible to each user. The displays were initially available in [[vacuum fluorescent display]], ([[Vacuum fluorescent display|VFD]]) and, in 2000, [[liquid crystal display]], ([[LCD]]) was used. In 2004 the displays became available with [[organic light-emitting diode]], ([[OLED]]) screens.  Each type of display provides the same text information and program annotation on eight channels simultaneously, may be turned off by the user, and is user-operated with a single button. The software is capable of supporting venues\u2019 existing systems as well as Figaro Systems' "Simultext" system. The software enables cueing of each line as it is sung, and it appears instantly on the screen.<ref name="entertanmentengineering.com"/>\u000a\u000aThe company builds fully [[modular]] systems including its [[wireless]] [[handheld]] screens \u000a<ref>[http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=3&f=G&l=50&co1=AND&d=PTXT&s1=figaro.ASNM.&OS=AN/figaro&RS=AN/Figaro  United States Patent 6,760,010. "Wireless electronic libretto display apparatus and method," issued July 6, 2004:] United States Patent and Trademark Office Patent Full-Text and Image Database</ref> for users who cannot use seatback systems, for example people in [[wheelchair]]s, who may be viewing the opera in areas lacking seatback viewing, or people with compromised eyesight.\u000a\u000a==Venues==\u000aIn the US, the company\u2019s systems are in use in the [[Ellie Caulkins Opera House]] \u000a<ref>[http://www.highbeam.com/doc/1G1-135788390.html Marc Shulgold, "Opera dialogue shows on seat in front of you,"] ''Rocky Mountain News'' (Denver, Colorado), September 3, 2005 on highbeam.com,</ref> in [[Denver, Colorado|Denver]], Colorado, The Santa Fe Opera in Santa Fe,<ref>[http://web.archive.org/web/20080512022822/http://www.santafeopera.org/yournite/operatitles.php  Santa Fe Opera, Santa Fe, NM. Cached webpage],</ref> the [[Brooklyn Academy of Music]]<ref>[http://www.appliancemagazine.com/editorial.php?article=1768&zone=210&first=1  \u201cAn Operatic Performance,\u201d ''Appliance Magazine'', June 2007],</ref> the [[Metropolitan Opera]], New York, where it is called "MetTitles"),<ref>[http://www.figaro-systems.com/installations.php  Figaro Systems Official Website. Installations],</ref> the [[Roy E. Disney]] Theatre in [[Albuquerque]]'s [[National Hispanic Cultural Center]], [[McCaw Hall]] in [[Seattle Washington]], the [[Opera Theatre of St. Louis]] in St. Louis, Missouri, the [[Des Moines Metro Opera]] in [[Des Moines, Iowa|Des Moines]], Iowa and the Lyric Opera of Kansas City,  Missouri.<ref name="figaro-systems.com"/>\u000a\u000aIn the UK and Europe, the systems have been installed in venues including the [[Royal Opera House]] in London, the [[Teatro alla Scala]] and La Scala's [[Teatro degli Arcimboldi]] opera houses in [[Milan, Italy|Milan]], Italy, the [[Gran Teatre del Liceu]] in [[Barcelona, Spain|Barcelona]], Spain, and the [[Wiener Staatsoper]] in [[Wien]], [[Austria]].\u000a<ref>[http://www.entertainmentengineering.com/v4.issue04/page.06.html \u201cGiving the Opera a New Voice,\u201d ''Entertainment Engineering.'', Volume 4, Issue 2, p. 6], on entertainmentengineering.com</ref>\u000a\u000a==Awards==\u000aIn 2001, the company won the [[Los Alamos, New Mexico|Los Alamos]] Laboratories\u2019 Technology Commercialization Award for its Simultext system.<ref>[http://www.lanl.gov/news/index.php/fuseaction/home.story/story_id/1170 Todd Hanson, "Los Alamos announces technology commercialization awards," ''Los Alamos National Laboratory News''], Los Alamos National Security, LLC, US Department of Energy's NNSA, May 7, 2001 on lanl.gov/news.</ref>\u000aIn 2008, the company\u2019s software was one of four finalists for the Excellence Award for Commercial Software awarded by the New Mexico Information Technology and Software Association.\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Software companies based in New Mexico]]\u000a[[Category:Assistive technology]]\u000a[[Category:Educational technology]]\u000a[[Category:Companies based in New Mexico]]\u000a[[Category:Privately held companies based in New Mexico]]\u000a[[Category:Companies established in 1993]]
p110
sg6
S'Figaro Systems'
p111
ssI168
(dp112
g2
S'http://en.wikipedia.org/wiki/Agrep'
p113
sg4
S'{{lowercase|title=agrep}}\n{{Infobox software\n| name                   = agrep\n| logo                   = <!-- Image name is enough -->\n| logo caption           = \n| logo_size              = \n| logo_alt               = \n| screenshot             = <!-- Image name is enough -->\n| caption                = \n| screenshot_size        = \n| screenshot_alt         = \n| collapsible            = \n| developer              = {{Plainlist|\n* [[Udi Manber]]\n* Sun Wu\n}}\n| released               = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| discontinued           = \n| latest release version = \n| latest release date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->\n| status                 = \n| programming language   = \n| operating system       = {{Plainlist|\n* [[Unix-like]]\n* [[OS/2]]\n* [[DOS]]\n* [[Microsoft Windows|Windows]]\n}}\n| platform               = \n| size                   = \n| language               = \n| language count         = <!-- DO NOT include this parameter unless you know what it does -->\n| language footnote      = \n| genre                  = [[Pattern matching]]\n| license                = \n| website                = <!-- {{URL|example.org}} -->\n| standard               = \n}}\n\n\'\'\'agrep\'\'\' (approximate [[grep]]) is a [[proprietary software|proprietary]] [[approximate string matching]] program, developed by [[Udi Manber]] and Sun Wu between 1988 and 1991, for use with the [[Unix]] operating system. It was later ported to [[OS/2]], [[DOS]], and [[Microsoft Windows|Windows]].\n\nIt selects the best-suited algorithm for the current query from a variety of the known fastest (built-in) [[string searching algorithm]]s, including Manber and Wu\'s [[bitap algorithm]] based on [[Levenshtein distance]]s.\n\nagrep is also the [[search engine]] in the indexer program [[GLIMPSE]]. agrep is free for private and non-commercial use only, and belongs to the University of Arizona.\n\n== Alternative implementations ==\nA more recent agrep is the command-line tool provided with the [[TRE (computing)|TRE]] regular expression library. TRE agrep is more powerful than Wu-Manber agrep since it allows weights and total costs to be assigned separately to individual groups in the pattern. It can also handle Unicode.<ref>{{cite web | title=TRE - TRE regexp matching package - Features | url=http://laurikari.net/tre/about }}</ref> Unlike Wu-Manber agrep, TRE agrep is licensed under a [[BSD licenses#BSD-style licenses|2-clause BSD-like license]].\n\nFREJ (Fuzzy Regular Expressions for Java) open-source library provides command-line interface which could be used in the way similar to agrep. Unlike agrep or TRE it could be used for constructing complex substitutions for matched text.<ref>{{cite web | title=FREJ - Fuzzy Regular Expressions for Java - Guide and Examples | url=http://frej.sf.net/rules.html }}</ref> However its syntax and matching abilities differs significantly from ones of ordinary [[regular expression]]s.\n\n==References==\n{{Reflist}}\n\n==External links==\n* Wu-Manber agrep\n**[ftp://ftp.cs.arizona.edu/agrep/ For Unix]  (To compile under OSX 10.8, add <code>-Wno-return-type</code> to the <code>CFLAGs  = -O</code> line in the Makefile)\n**[http://www.tgries.de/agrep For DOS, Windows and OS/2 home page]\n*[http://wiki.christophchamp.com/index.php/Agrep_(command) Entry for "agrep" in Christoph\'s Personal Wiki]\n\n*See also\n**[http://laurikari.net/tre TRE regexp matching package]\n**[http://www.bell-labs.com/project/wwexptools/cgrep/ cgrep a command line approximate string matching tool]\n**[http://www.dcc.uchile.cl/~gnavarro/software/ nrgrep] a command line approximate string matching tool\n**[http://finzi.psych.upenn.edu/R/library/base/html/agrep.html agrep as implemented in R]\n\n[[Category:Searching]]\n[[Category:Unix text processing utilities]]'
p114
sg6
S'Agrep'
p115
ssI42
(dp116
g2
S'http://en.wikipedia.org/wiki/GLIMPSE'
p117
sg4
S"{{Other uses|Glimpse (disambiguation)}}\n{{Infobox software\n| name = Glimpse\n| logo = \n| screenshot =\n| caption =\n| developer = [[Internet WorkShop]]\n| status = \n| latest release version = 4.18.6 (source) / 4.18.5 (binary) \n| latest release date = {{release date|2012|06|09}}\n| operating system = [[Cross-platform]]\n| programming language = [[C (programming language)|C]]\n| genre = [[Search algorithm|Search]] and [[index (search engine)|index]]\n| license = \n| website = {{URL|http://webglimpse.net/}}\n}}\n'''GLIMPSE''' is a text indexing and [[text retrieval|retrieval]] [[software]] program originally developed at the [[University of Arizona]] by [[Udi Manber]], [[Sun Wu]], and [[Burra Gopal]].  It was released under the ISC [[open source]] license in September 2014.\n\nGLIMPSE stands for GLobal IMPlicit SEarch. While many text indexing schemes create quite large indexes (usually around 50% of the size of the original text), a GLIMPSE-created index is only 2-4% of the size of the original text.\n\nGLIMPSE uses and takes a great deal of inspiration from [[Agrep]], which was also developed at the University of Arizona, but GLIMPSE uses a high level index whereas Agrep parses all the text each time.\n\nThe basic algorithm is similar to other text indexing and retrieval engines, except that the text records in the index are huge, consisting of multiple files each. This index is searched using a boolean matching algorithm like most other text indexing and retrieval engines. After one or more of these large text records is matched, Agrep is used to actually scan for the exact text desired. While this is slower than traditional totally indexed approaches, the advantage of the smaller index is seen to be advantageous to the individual user. This approach would not work particularly well across websites, but it would work reasonably well for a single site, or a single workstation. In addition, the smaller index can be created more quickly than a full index.\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://webglimpse.net/ Glimpse and WebGlimpse home page]\n*[http://webglimpse.net/pubs/glimpse.pdf Original Glimpse paper] (PDF)\n\n[[Category:Information retrieval]]\n[[Category:Free search engine software]]\n[[Category:Search engine software]]"
p118
sg6
S'GLIMPSE'
p119
ssI171
(dp120
g2
S'http://en.wikipedia.org/wiki/Statistically Improbable Phrases'
p121
sg4
V'''Statistically Improbable Phrases''', '''Statimprophrases''' or '''SIPs''' constitute a system developed by [[Amazon.com]] to compare all of the books they index in the Search Inside! program and find phrases in each that are the most unlikely to be found in any other book indexed.<ref>{{cite web|url=http://www.amazon.com/gp/search-inside/sipshelp.html|title=What are Statistically Improbable Phrases?|accessdate=2007-12-18|publisher=[[Amazon.com]]}}</ref> The system is used to find the most nearly unique portions of books for use as a summary or keyword.\u000a\u000a== Example == \u000aThe Statistically Improbable Phrases of Darwin's [[On the Origin of Species]] are: ''temperate productions, genera descended, transitional gradations, unknown progenitor, fossiliferous formations, our domestic breeds, modified offspring, doubtful forms, closely allied forms, profitable variations, enormously remote, transitional grades, very distinct species'' and ''mongrel offspring''.<ref>[http://crookedtimber.org/2005/04/02/sociologically-improbable-phrases/ Sociologically Improbable Phrases] Crooked Timber April 2005</ref>\u000a\u000a==See also==\u000a*[[Googlewhack]] \u2014 a pair of words occurring on a single webpage, as indexed by Google\u000a*[[tf-idf]] \u2014 a statistic used in information retrieval and text mining.\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a{{Amazon}}\u000a\u000a[[Category:Amazon.com]]\u000a[[Category:Searching]]\u000a[[Category:Bookselling]]
p122
sg6
S'Statistically Improbable Phrases'
p123
ssI45
(dp124
g2
S'http://en.wikipedia.org/wiki/Expertise finding'
p125
sg4
V{{cleanup|date=November 2010}}{{External links|date=January 2012}}\u000a\u000a'''Expertise finding''' is the use of tools for finding and assessing individual [[expertise]], with particular focus on scientific expertise.\u000a\u000a== Importance of expertise ==\u000a\u000aIt can be argued that human expertise is more valuable than capital, means of production or intellectual property. Contrary to expertise, all other aspects of capitalism are now relatively generic: access to capital is global, as is access to means of production for many areas of manufacturing.  [[Intellectual property]] can be similarly licensed.  Furthermore, expertise finding is also a key aspect of [[institutional memory]], as without its experts an institution is effectively decapitated.  However, finding and \u201clicensing\u201d expertise, the key to the effective use of these resources, remain much harder, starting with the very first step: finding expertise that you can trust.\u000a\u000aUntil very recently, finding expertise required a mix of individual, social and collaborative practices, a haphazard process at best.  Mostly, it involved contacting individuals one trusts and asking them for referrals, while hoping that one\u2019s judgment about those individuals is justified and that their answers are thoughtful.\u000a\u000aIn the last fifteen years, a class of [[knowledge management]] software has emerged to facilitate and improve the quality of expertise finding, termed \u201cexpertise locating systems\u201d.  These software range from [[Social network service|social networking systems]] to [[knowledge base]]s.  Some software, like those in the social networking realm, rely on users to connect each other, thus using social filtering to act as [[Recommender system|\u201crecommender systems\u201d]].\u000a\u000aAt the other end of the spectrum are specialized [[knowledge base]]s that rely on experts to populate a specialized type of [[database]] with their self-determined areas of expertise and contributions, and do not rely on user recommendations.  Hybrids that feature expert-populated content in conjunction with user recommendations also exist, and are arguably more valuable for doing so.\u000a\u000aStill other expertise knowledge bases rely strictly on external manifestations of expertise, herein termed \u201cgated objects\u201d, e.g., [[citation impact]]s for scientific papers or [[data mining]] approaches wherein many of the work products of an expert are collated.  Such systems are more likely to be free of user-introduced biases (e.g., [http://researchscorecard.com/ ResearchScorecard] ), though the use of computational methods can introduce other biases.\u000a\u000aExamples of the systems outlined above are listed in Table 1.\u000a\u000a'''Table 1: A classification of expertise location systems'''\u000a\u000a{| class="wikitable" border="1"\u000a|-\u000a! Type\u000a! Application domain\u000a! Data source\u000a! Examples\u000a|-\u000a| Social networking\u000a| Professional networking\u000a| User-generated\u000a|\u000a* [[LinkedIn]]\u000a|-\u000a| [[Scientific literature]]\u000a| Identifying publications with strongest research impact\u000a| Third-party generated\u000a|\u000a* [[Science Citation Index]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/science/science_products/a-z/science_citation_index]\u000a|-\u000a| [[Scientific literature]]\u000a| Expertise search\u000a| Software\u000a|\u000a* [[Arnetminer]][http://arnetminer.org]\u000a|-\u000a| Knowledge base\u000a| Private expertise database\u000a| User-Generated\u000a|\u000a* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)\u000a* MIT ExpertFinder (ref. 3)\u000a* Decisiv Search Matters & Expertise ([[Recommind (software company)|Recommind]], Inc.)\u000a* [[Tacit Software]] (Oracle Corporation)\u000a|-\u000a| Knowledge base\u000a| Publicly accessible expertise database\u000a| User-generated\u000a|\u000a* [[Community of Science]] Expertise [http://expertise.cos.com]\u000a* [[ResearcherID]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/scientific/ResearcherID]\u000a|-\u000a| Knowledge base\u000a| Private expertise database\u000a| Third party-generated\u000a|\u000a* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)\u000a* MIT ExpertFinder (ref. 3)\u000a* MindServer Expertise ([[Recommind]], Inc.)\u000a* Tacit Software\u000a|-\u000a| Knowledge base\u000a| Publicly accessible expertise database\u000a| Third party-generated\u000a|\u000a* [http://researchscorecard.com ResearchScorecard] (ResearchScorecard Inc.)\u000a* [http://authoratory.com/ authoratory.com]\u000a* [http://biomedexperts.com BiomedExperts] (Collexis Holdings Inc.)\u000a* [http://www.hcarknowledgemesh.com/ KnowledgeMesh] (Hershey Center for Applied Research)\u000a* [http://med.stanford.edu/profiles/ Community Academic Profiles] (Stanford School of Medicine)\u000a* [http://researchcrossroads.org ResearchCrossroads.org] (Innolyst, Inc.)\u000a|-\u000a| Blog [[search engine]]s\u000a|\u000a| Third party-generated\u000a|\u000a* [[Technorati]] [http://technorati.com/]\u000a|}\u000a\u000a== Technical problems ==\u000aA number of interesting problems follow from the use of expertise finding systems:\u000a\u000a* The matching of questions from non-expert to the database of existing expertise is inherently difficult, especially when the database does not store the requisite expertise.  This problem grows even more acute with increasing ignorance on the part of the non-expert due to typical search problems involving use of keywords to search unstructured data that are not semantically normalized, as well as variability in how well an expert has set up their descriptive content pages.  Improved question matching is one reason why third-party semantically normalized systems such as [http://researchscorecard.com ResearchScorecard] and [[BiomedExperts]] should be able to provide better answers to queries from non-expert users.\u000a* Avoiding expert-fatigue due to too many questions/requests from users of the system (ref. 1).\u000a* Finding ways to avoid \u201cgaming\u201d of the system to reap unjustified expertise [[credibility]].\u000a\u000a== Expertise ranking ==\u000a\u000aMeans of classifying and ranking expertise (and therefore experts) become essential if the number of experts returned by a query is greater than a handful.  This raises the following social problems associated with such systems:\u000a\u000a* How can expertise be assessed objectively? Is that even possible?\u000a* What are the consequences of relying on unstructured social assessments of expertise, such as user recommendations?\u000a* How does one distinguish [[Authority|''authoritativeness'']] as a proxy metric of expertise from simple ''popularity'', which is often a function of one's ability to express oneself coupled with a good social sense?\u000a* What are the potential consequences of the social or professional stigma associated with the use of an authority ranking, such as used in [http://technorati.com Technorati] and [http://researchscorecard.com ResearchScorecard])?\u000a\u000a== Sources of data for assessing expertise ==\u000aMany types of data sources have been used to infer expertise.  They can be broadly categorized based on whether they measure "raw" contributions provided by the expert, or whether some sort of filter is applied to these contributions.\u000a\u000aUnfiltered data sources that have been used to assess expertise, in no particular ranking order:\u000a\u000a* user recommendations\u000a* help desk tickets: what the problem was and who fixed it\u000a* e-mail traffic between users\u000a* documents, whether private or on the web, particularly publications\u000a* user-maintained web pages\u000a* reports (technical, marketing, etc.)\u000a\u000aFiltered data sources, that is, contributions that require approval by third parties (grant committees, referees, patent office, etc.) are particularly valuable for measuring expertise in a way that minimizes biases that follow from popularity or other social factors:\u000a\u000a* [[patent]]s, particularly if issued\u000a* scientific publications\u000a* issued grants (failed grant proposals are rarely know beyond the authors)\u000a* [[clinical trial]]s\u000a* product launches\u000a* pharmaceutical drugs\u000a\u000a== Approaches for creating expertise content ==\u000a* Manual, either by experts themselves (e.g., LinkedIn) or by a curator\u000a* Automated, e.g., using [[software agent]]s (e.g., MIT's [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] and the [http://wiki.foaf-project.org/ExpertFinder ExpertFinder] initiative) or a combination of agents and human curation (e.g., [http://researchscorecard.com/ ResearchScorecard])\u000a\u000a== Interesting expertise systems over the years ==\u000aIn no particular order...\u000a\u000a* Autonomy's IDOL\u000a* AskMe\u000a* Tacit Knowledge Systems' ActiveNet\u000a* Triviumsoft's SEE-K\u000a* MIT\u2019s [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] (ref 3)\u000a* MITRE\u2019s (ref 1) [http://www.mitre.org/news/the_edge/june_98/third.html Expert Finder]\u000a* MITRE\u2019s XpertNet\u000a* Arnetminer (ref 2)\u000a* Dataware II Knowledge Directory\u000a* Thomson\u2019s tool\u000a* Hewlett-Packard\u2019s CONNEX\u000a* Microsoft\u2019s SPUD project\u000a* [http://www.xperscore.com Xperscore]\u000a* [http://intunex.fi/skillhive/ Skillhive]\u000a\u000a== Conferences ==\u000a# [http://expertfinder.info/pickme2008 The ExpertFinder Initiative]\u000a\u000a== References ==\u000a\u000a# Ackerman, Mark and McDonald, David (1998) "Just Talk to Me: A Field Study of Expertise Location" ''Proceedings of the 1998 ACM Conference on Computer Supported Cooperative Work''.\u000a# Hughes, Gareth and Crowder, Richard (2003) "Experiences in designing highly adaptable expertise finder systems"  ''Proceedings of the DETC Conference 2003''.\u000a# Maybury, M., D\u2019Amore, R., House, D. (2002). "Awareness of organizational expertise." ''International Journal of Human-Computer Interaction'' '''14'''(2): 199-217.\u000a# Maybury, M., D\u2019Amore, R., House, D. (2000). Automating Expert Finding. ''International Journal of Technology Research Management.'' 43(6): 12-15.\u000a# Maybury, M., D\u2019Amore, R, and House, D. December (2001). Expert Finding for Collaborative Virtual Environments.  ''Communications of the ACM 14''(12): 55-56. In Ragusa, J. and Bochenek, G. (eds). Special Section on Collaboration Virtual Design Environments.\u000a# Maybury, M., D\u2019Amore, R. and House, D. (2002). Automated Discovery and Mapping of Expertise.  In Ackerman, M., Cohen, A., Pipek, V. and Wulf, V. (eds.). ''Beyond Knowledge Management: Sharing Expertise.'' Cambridge: MIT Press.\u000a# Mattox, D., M. Maybury, ''et al.'' (1999). "Enterprise expert and knowledge discovery". ''Proceedings of the 8th International Conference on Human-Computer Interactions (HCI International 99)'', Munich, Germany.\u000a# Tang, J., Zhang J., Yao L., Li J., Zhang L. and Su Z.(2008) "ArnetMiner: extraction and mining of academic social networks" ''Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining''.\u000a# Viavacqua, A. (1999). "Agents for expertise location". ''Proceedings of the 1999 AAAI Spring Symposium on Intelligent Agents in Cyberspace'', Stanford, CA.\u000a\u000a[[Category:Evaluation methods]]\u000a[[Category:Metrics]]\u000a[[Category:Analysis]]\u000a[[Category:Impact assessment]]\u000a[[Category:Intellectual works]]\u000a[[Category:Knowledge sharing]]\u000a[[Category:Library science]]\u000a[[Category:Information retrieval]]\u000a[[Category:Science studies]]
p126
sg6
S'Expertise finding'
p127
ssI174
(dp128
g2
S'http://en.wikipedia.org/wiki/Hybrid search engine'
p129
sg4
S"{{Notability|date=December 2009}}\nA '''hybrid search engine''' ('''HSE''') is a type of [[computer]] [[search engine]] that uses different types of data with or without ontologies to produce the [[algorithm]]ically generated results based on [[web crawling]]. Previous types of search engines only use text to generate their results.\n\n==References==\n{{No footnotes|date=April 2010}}\n*http://eprints.ecs.soton.ac.uk/17457/\n*http://eprints.whiterose.ac.uk/3771/\n*http://www.picollator.com\n\n[[Category:Searching]]\n\n\n{{web-stub}}"
p130
sg6
S'Hybrid search engine'
p131
ssI48
(dp132
g2
S'http://en.wikipedia.org/wiki/Communication engine'
p133
sg4
S"{{Orphan|date=February 2009}}\nA '''communication engine''' is a tool that sends user requests to several other [[communication protocols]] and/or [[database]]s and aggregates the results into a single list or displays them according to their source. Communication engines enable users to enter communication account authorization once and access several communication avenues simultaneously. Communication engines operate on the premise that the [[World Wide Web]] is too large for any one engine to index it all and that more productive results can be obtained by combining the results from several engines dynamically. This may save the user from having to use multiple engines separately.\n\n[[Category:Information retrieval]]\n[[Category:Computing terminology]]\n\n\n{{Web-stub}}"
p134
sg6
S'Communication engine'
p135
ssI177
(dp136
g2
S'http://en.wikipedia.org/wiki/Category:Data search engines'
p137
sg4
S'{{Cat main|Data search engine}}\n\n[[Category:Metadata]]\n[[Category:XML]]\n[[Category:Database management systems]]\n[[Category:Searching]]'
p138
sg6
S'Category:Data search engines'
p139
ssI51
(dp140
g2
S'http://en.wikipedia.org/wiki/Index term'
p141
sg4
S'An \'\'\'index term\'\'\', \'\'\'subject term\'\'\', \'\'\'subject heading\'\'\', or \'\'\'descriptor\'\'\', in [[information retrieval]], is a term that captures the essence of the topic of a document. Index terms make up a [[controlled vocabulary]] for use in [[bibliographic record]]s. They are an integral part of bibliographic control, which is the function by which libraries collect, organize and disseminate documents. They are used as keywords to retrieve documents in an information system, for instance, a catalog or a [[search engine]].  A popular form of keywords on the web are [[tag (metadata)|tags]] which are directly visible and can be assigned by non-experts also. Index terms can consist of a word, phrase, or alphanumerical term. They are created by analyzing the document either manually with [[subject indexing]] or automatically with [[Index (search engine)|automatic indexing]] or more sophisticated methods of keyword extraction. Index terms can either come from a controlled vocabulary or be freely assigned.\n\nKeywords are stored in a [[Index (search engine)|search index]]. Common words like [[article (grammar)|articles]] (a, an, the) and conjunctions (and, or, but) are not treated as keywords because it is inefficient to do so. Almost every English-language site on the Internet has the article "\'\'the\'\'", and so it makes no sense to search for it. The most popular search engine, [[Google]] removed [[stop words]] such as "the" and "a" from its indexes for several years, but then re-introduced them, making certain types of precise search possible again.\n\nThe term "descriptor" was coined by [[Calvin Mooers]] in 1948. It is in particular used about a preferred term from a [[thesaurus]]. \n\nThe [[Simple Knowledge Organisation System]] language (SKOS) provides a way to express index terms with [[Resource Description Framework]] for use in the context of [[Semantic Web]].\n\n==Author keywords==\nMany journals and databases provides access (also) to index terms made by authors to the articles being published or represented. The relative quality of indexer-provided index terms and author provided index terms is of interest to research in information retrieval. The quality of both kinds of indexing terms depends, of course, on the qualifications of provider. In general authors have difficulties providing indexing terms that characterizes his document \'\'relative\'\' to the other documents in the database. Author keywords are an integral part of literature.\n\n==Examples==\n*[[Canadian Subject Headings]] (CSH)\n*[[Library of Congress Subject Headings]] (LCSH)\n*[[Medical Subject Headings]] (MeSH)\n*[[Polythematic Structured Subject Heading System]] (PSH)\n\n==See also==\n*[[Dynamic keyword insertion]]\n<!-- *Key-objects -->\n*[[Keyword cloud]]\n*[[Keyword density]]\n*[[Keyword optimization]]\n*[[knowledge tags|Keyword tagging]]\n*[[Subject (documents)]]\n\n== References ==\n{{commonscat|Information retrieval}}\n*{{cite book|last=Svenonius|first=Elaine|author-link=Elaine Svenonius|title=The intellectual foundation of information organization|date=2009|publisher=MIT Press|location=Cambridge, Mass.|isbn=9780262512619|edition=1st MIT Press pbk.}}\n\n[[Category:Information retrieval]]\n\n{{Library-stub}}'
p142
sg6
S'Index term'
p143
ssI180
(dp144
g2
S'http://en.wikipedia.org/wiki/OpenGrok'
p145
sg4
S'{{multiple issues|\n{{Advert|date=March 2012}}\n{{Notability|Products|date=March 2012}}\n}}\n\n{{Infobox software\n| name                   = OpenGrok\n| logo                   = [[Image:OpenGrok Logo.png|150px|OpenGrok Logo]]\n| screenshot             = \n| caption                =\n| collapsible            = yes\n| developer              = [[Sun Microsystems]]/[[Oracle Corporation]]\n| latest release version = 0.12.1\n| latest release date    = {{release date|2014|04|29}}\n| latest preview version =\n| latest preview date    =\n| operating system       = [[Cross-platform]]\n| programming language   = [[Java (programming language)|Java]]\n| genre                  = [[Index (search engine)|Index]]er and [[cross-reference]]r with [[Revision control]]\n| license                = [[CDDL]]\n| website                = http://opengrok.github.com/OpenGrok/\n}}\n\n\'\'\'OpenGrok\'\'\' is a [[source code]] search and cross reference engine. It helps programmers to search, cross-reference and navigate source code trees.\n\nIt can understand various [[program (computing)|program]] [[file formats]] and [[version control]] histories like [[Monotone (software)|Monotone]], [[Source Code Control System|SCCS]], [[Revision Control System|RCS]], [[Concurrent Versions System|CVS]], [[Subversion (software)|Subversion]], [[Mercurial (software)|Mercurial]], [[Git (software)|Git]], [[IBM Rational ClearCase|Clearcase]], [[Perforce]] and [[Bazaar (software)|Bazaar]].<ref>https://github.com/OpenGrok/OpenGrok/wiki/Supported-Revision-Control-Systems</ref>\n\nThe name comes from the term \'\'[[grok]]\'\', a [[jargon]] term used in computing to mean "profoundly understand". The term \'\'[[grok]]\'\' originated in a science fiction novel by Robert A. Heinlein called \'\'[[Stranger in a Strange Land]]\'\'.\n\nOpenGrok is being developed mainly by [[Oracle Corporation]] (former [[Sun Microsystems]]) engineers with help from its community. OpenGrok is released under the terms of the [[Common Development and Distribution License]] (CDDL).\n\n== Features ==\n\nOpenGrok\'s features include:\n\n* Full text Search\n* Definition Search\n* Identifier Search\n* Path search\n* History Search\n* Shows matching lines\n* Hierarchical Search\n* query syntax like \'\'AND\'\', \'\'OR\'\', \'\'field\'\':\n* Incremental update\n* Syntax highlighting-Xref\n* Quick navigation inside the file\n* Interface for SCM\n* Usable URLs\n* Individual file download\n* Changes at directory level\n* Multi language support\n\n== See also ==\n\n* [[LXR Cross Referencer]]\n* [[ViewVC]]\n* [[FishEye (software)]]\n\n== References ==\n\n{{reflist}}\n\n== External links ==\n* [http://opengrok.github.com/OpenGrok/ OpenGrok project page]\n* {{ohloh|opengrok}}\n* [http://code.metager.de/source/ Metager]\n* [http://BXR.SU/ Super User\'s BSD Cross Reference]\n\n{{Sun Microsystems}}\n{{Java (Sun)}}\n\n[[Category:Cross-platform free software]]\n[[Category:Free revision control software]]\n[[Category:Source code]]\n[[Category:Searching]]\n[[Category:Java platform software]]\n[[Category:Concurrent Versions System]]\n[[Category:Subversion]]\n[[Category:Code search engines]]\n\n\n{{programming-software-stub}}'
p146
sg6
S'OpenGrok'
p147
ssI54
(dp148
g2
S'http://en.wikipedia.org/wiki/List of enterprise search vendors'
p149
sg4
V== Free and open source [[enterprise search]] software ==\u000a<!--\u000a################# READ THIS\u000a\u000aPlease do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.\u000aAlso, read the definition of enterprise search before adding a new search engine\u000a-->\u000a*[[Apache Solr]]\u000a*[[DataparkSearch]]\u000a*[[ElasticSearch]]\u000a*[[Htdig|ht://Dig]]\u000a*[[ApexKB]]\u000a*[[mnoGoSearch]]\u000a*[[OpenSearchServer]]\u000a*[[Searchdaimon]]\u000a*[[Sphinx_(search_engine)|Sphinx]]\u000a*[[Zettair]]\u000a\u000a== Vendors of open source enterprise search software ==\u000a* [[30 Digits]] - Implementation, consulting, support, and value-add components for [[Lucene]] and [[Solr]]\u000a* [[Apache Software Foundation]] - The foundation is the entity behind the [[Lucene]] family of products\u000a* [[LucidWorks]] (former Lucid Imagination) - Commercial support, training and services for [[Lucene]] and [[Solr]]\u000a* Customermatrix (acquired Polyspot, CRM development and products for [[Lucene]]) \u000a* [[Searchblox]] - Commercial product for [[Lucene]] and [[ElasticSearch]]\u000a* [[Sematext]] - Consulting, development and products for [[Lucene]], [[Solr]], [[Nutch]], and [[Hadoop]]\u000a* [[FlaxUK|Flax]] - Architecture, development and support for [[Lucene]], [[Solr]] and [[Xapian]]\u000a\u000a== Vendors of proprietary enterprise search software ==\u000a<!--\u000a################# READ THIS\u000a\u000aPlease do not add web links or companies which do not have Wikipedia articles. They will be summarily deleted.\u000a\u000a-->\u000a*[[AskMeNow]]\u000a*[[Attivio]]\u000a*[[Concept Searching Limited]]\u000a*[[Content Analyst Company|Content Analyst Company LLC]]\u000a*[[Coveo]]\u000a*[[Dassault Systèmes]] (acquired [[Exalead]])\u000a*[[Denodo]]\u000a*[[Dieselpoint, Inc.]]\u000a*[[dtSearch Corp.]]\u000a*[[EMC Corp.]]\u000a*[[Exorbyte GmbH]]\u000a*[[Expert System S.p.A.]]\u000a*[[Exterro, Inc.]]\u000a*[[Fabasoft Mindbreeze|Fabasoft]]\u000a*[[Funnelback]]\u000a*[[Google Search Appliance]]\u000a*[[HP]] (acquired [[Autonomy Corporation]] which in turn acquired [[Verity]] K2 and Ultraseek)\u000a*[[IBM]] (acquired [[Vivisimo]], rebranded "[[Watson (computer)|Watson]]")\u000a*[[Inbenta]]\u000a*[[inter:gator Enterprise Search]]\u000a*[[ISYS Search Software]]\u000a*[[Lookeen]]\u000a*[[Mark Logic|MarkLogic]]\u000a*[[Microsoft]] (includes [[Microsoft Search Server]], [[Fast Search & Transfer]])\u000a*[[Fabasoft Mindbreeze|Mindbreeze]] \u000a*[[Neofonie]] (includes WeFind)\u000a*[[Omniture]] (acquired by [[Adobe Systems]])\u000a*[[Open Text Corporation]]\u000a*[[Oracle Corporation]] (includes [[Oracle_Corporation#Oracle_Secure_Enterprise_Search|Secure Enterprise Search]] and [[Endeca Technologies Inc.]])\u000a*[[Q-go]]\u000a*[[Q-Sensei]]\u000a*[[Recommind (software company)|Recommind]]\u000a*[[SAP AG|SAP]] (includes SAP NetWeaver Enterprise Search, Search Services in SAP NetWeaver AS ABAP, and Search and Classification TREX)\u000a*[[Silent Eight]]\u000a*[[Sinequa]]\u000a*[[SLI_Systems]]\u000a*[[Sophia Search Limited]]\u000a* [[Swiftype]]\u000a*[[TeraText]]\u000a*[[Thunderstone Software]]\u000a*[[X1 Technologies, Inc.]]\u000a*[[ZyLAB Technologies]]\u000a*[[ZL Technologies]]\u000a\u000a== External links ==\u000a* [http://www.dmoz.org/Computers/Software/Information_Retrieval/Fulltext/ DMOZ category Information Retrieval/Fulltext]\u000a{{Companies by industry}}\u000a\u000a{{DEFAULTSORT:Enterprise search vendors}}\u000a[[Category:Information retrieval]]\u000a[[Category:Searching]]\u000a[[Category:Search engine software|*Enterprise search vendors]]\u000a[[Category:Lists of software]]\u000a[[Category:Lists of companies by industry|Enterprise search vendors]]
p150
sg6
S'List of enterprise search vendors'
p151
ssI183
(dp152
g2
S'http://en.wikipedia.org/wiki/Indexing Service'
p153
sg4
V{{Use dmy dates|date=February 2011}}\u000a{{Infobox Windows component\u000a| name                = Indexing Service\u000a| screenshot          = Indexing Service Query Form.PNG\u000a| screenshot_size     = 300px\u000a| caption             = The Indexing Service Query Form, used to query Indexing Service catalogs, hosted in [[Microsoft Management Console]].\u000a| type                = [[Desktop search]]\u000a| service_name        = Indexing Service\u000a| service_description = Indexes contents and properties of files on local and remote computers; provides rapid access to files through flexible querying language.\u000a| replaced_by         = [[Windows Search]]\u000a| included_with       = [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]<ref name="MIS-Intro" /><br/>[[Windows 2000]]<ref name="MIS-v3" /><br/>[[Windows XP]]<ref name="TnC-144" /><br/>[[Windows Server 2003]]<ref name="TnC-144" /><br/>[[Windows Server 2008]]<ref name="WIS-Install2008" />\u000a}}\u000a\u000a'''Indexing Service''' (originally called '''Index server''') was a [[Windows service]] that maintained an index of most of the [[Computer file|files]] on a computer to improve searching performance on PCs and corporate [[computer network]]s. It updated indexes without user intervention. In [[Windows 7]], it has been replaced by [[Windows Search]].\u000a\u000a== History ==\u000aIndexing Service was a [[desktop search]] service included with [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]<ref name="MIS-Intro" /> as well as [[Windows 2000]] and later.<ref name="MIS-v3" /><ref name="TnC-144" /><ref name="WIS-What" /> The first incarnation of the indexing service was shipped in August 1996<ref name="MIS-Intro" /> as a content search system for Microsoft's web server software, [[Internet Information Services]].{{Citation needed|date=February 2011}} Its origins, however, date further back to Microsoft's [[Cairo (operating system)|Cairo operating system]] project, with the component serving as the Content Indexer for the [[Object File System]]. Cairo was eventually shelved, but the content indexing capabilities would go on to be included as a standard component of later Windows desktop and server operating systems, starting with [[Windows 2000]], which includes Indexing Service 3.0.{{Citation needed|date=February 2011}}\u000a\u000aIn [[Windows Vista]], the content indexer was replaced with the [[Windows Search]] indexer which was enabled by default. Indexing Service is still included with Windows Server 2008 but is not installed or running by default.<ref name="WIS-Install2008" />\u000a\u000aIndexing Service has been deprecated in Windows 7 and Windows Server 2008 R2.<ref>{{cite web|title=Deprecated Features for Windows 7 and Windows Server 2008 R2|url=http://technet.microsoft.com/en-us/library/ee681698%28WS.10%29.aspx|work=Windows 7 Technical Library|publisher=Microsoft Corporation|accessdate=8 November 2011|location=Indexing Service|date=October 16, 2009}}</ref> It has been removed from [[Windows 8]].\u000a\u000a== Search interfaces ==\u000a\u000aComprehensive searching is available after initial building of the index, which can take up to hours or days, depending on the size of the specified directories, the speed of the hard drive, user activity, indexer settings and other factors. Searching using Indexing service works also on [[Uniform Naming Convention|UNC]] paths and/or mapped network drives if the sharing server indexes appropriate directory and is aware of its sharing.\u000a\u000aOnce the indexing service has been turned on and has built its index it can be searched in three ways. The search option available from the [[Start Menu]] on the [[Microsoft windows|Windows]] [[Taskbar]] will use the indexing service if it is enabled and will even accept complex queries. Queries can also be performed using either the ''Indexing Service Query Form'' in the [[Microsoft Management Console#Common snap-ins|Computer Management snap-in]] of Microsoft Management Console, or, alternatively, using third-party applications such as 'Aim at File' or 'Grokker Desktop'.\u000a\u000aMicrosoft Index Server 2.0 does not detect changes to a catalog if the data is located on a [[Volume Mount Point|mounted partition]]. It does not support mounted volumes because of technical limitations in the file system.<ref>{{cite web\u000a | url = http://support.microsoft.com/kb/319506\u000a | title = INFO: Index Server Does Not Support Mounted Volumes (Revision: 1.0)\u000a | work = Microsoft Support\u000a | publisher = 10 May 2002\u000a | accessdate = 1 February 2011\u000a}}</ref>\u000a\u000a== References ==\u000a{{Reflist|refs=\u000a<ref name = "MIS-Intro">{{Cite web\u000a  |url = http://msdn.microsoft.com/en-us/library/ms951563.aspx\u000a  |title = Introduction to Microsoft Index Server\u000a  |work = [[Microsoft Developer Network]]\u000a  |publisher = Microsoft Corporation\u000a  |date = 15 October 1997\u000a  |accessdate = 1 February 2011\u000a  |first1 = Krishna\u000a  |last1 = Nareddy\u000a  }}</ref>\u000a<ref name = "MIS-v3">{{Cite web\u000a  |url = http://msdn.microsoft.com/en-us/library/ms689644.aspx\u000a  |title = Indexing Service Version 3.0\u000a  |work = [[Microsoft Developer Network]]\u000a  |publisher = Microsoft Corporation\u000a  |date =\u000a  |accessdate = 1 February 2011\u000a  |first1 =\u000a  |last1 =\u000a  }}</ref>\u000a<ref name = "WIS-What">{{Cite web\u000a  |url = http://msdn.microsoft.com/en-us/library/ms689718.aspx\u000a  |title = What is Indexing Service?\u000a  |work = [[Microsoft Developer Network]]\u000a  |publisher = Microsoft Corporation\u000a  |date =\u000a  |accessdate = 1 February 2011\u000a  |first1 =\u000a  |last1 =\u000a  }}</ref>\u000a<ref name="WIS-Install2008">{{Cite web\u000a  |url = http://support.microsoft.com/kb/954822\u000a  |title = How to install and configure the Indexing Service on a Windows Server 2008-based computer (Revision: 3.0)\u000a  |work = Microsoft Support\u000a  |publisher = Microsoft Corporation\u000a  |date = 3 May 2010\u000a  |accessdate = 1 February 2011\u000a  }}</ref>\u000a<ref name="TnC-144">{{Cite book\u000a  |url = http://www.microsoft.com/downloads/en/details.aspx?FamilyId=1B6ACF93-147A-4481-9346-F93A4081EEA8&displaylang=en\u000a  |format = Microsoft Word\u000a  |title = Threats and Countermeasures: Security Settings in Windows Server 2003 and Windows XP\u000a  |edition = 2.0\u000a  |publisher = Microsoft Corporation\u000a  |page = 144\u000a  |date=December 2005\u000a  |first1 = Mike\u000a  |last1  = Danseglio\u000a  |first2 = Kurt\u000a  |last2  = Dillard\u000a  |first3 = José\u000a  |last3  = Maldonado\u000a  |first4 = Paul\u000a  |last4  = Robichaux\u000a  |editor1-first = Reid\u000a  |editor1-last  = Bannecker\u000a  |editor2-first = John\u000a  |editor2-last  = Cobb\u000a  |editor3-first = Jon\u000a  |editor3-last  = Tobey\u000a  |editor4-first = Steve\u000a  |editor4-last  = Wacker\u000a  }}</ref>\u000a}}\u000a\u000a{{DEFAULTSORT:Indexing Service}}\u000a[[Category:Windows communication and services]]\u000a[[Category:Desktop search engines|Desktop search engines]]\u000a[[Category:Searching]]\u000a[[Category:Windows components]]
p154
sg6
S'Indexing Service'
p155
ssI57
(dp156
g2
S'http://en.wikipedia.org/wiki/Clairlib'
p157
sg4
S"{{Multiple issues|\n{{unreferenced|date=May 2009}}\n{{expert-subject|date=May 2009}}\n{{orphan|date=February 2011}}\n}}\n\n{{Infobox software\n|name                       = Clairlib\n|logo                       = [[Image:Clair logo.jpg]]\n|screenshot                 = \n|caption                    = \n|collapsible                = \n|author                     = \n|developer                  = CLAIR [[University of Michigan]]\n|released                   = \n|latest release version     = 1.0.8\n|latest release date        = {{release date and age|2009|08|1}}\n|latest preview version     = \n|latest preview date        = \n|frequently updated         = yes\n|programming language       = [[Perl]]\n|operating system           = \n|platform                   = Cross-platform\n|size                       = \n|language                   = Perl\n|status                     = Active\n|genre                      = [[Natural Language Processing]], [[Network theory|Network Analysis]], [[Information Retrieval]]\n|license                    = [[GNU General Public License]], [[Artistic License]]\n|website                    = [http://www.clairlib.org/ www.clairlib.org]\n}}\n'''Clairlib''' is a suite of open-source [[Perl]] modules developed and maintained by the Computational Linguistics And Information Retrieval (CLAIR) group at the [[University of Michigan]]. Clairlib is intended to simplify a number of generic tasks in [[natural language processing]] (NLP), [[information retrieval]] (IR), and network analysis (NA). The latest version of clairlib is 1.06 which was released on March 2009 and includes about 130 modules implementing a wide range of functionalities.\n\n==Functionality==\n\nClairlib is distributed in two forms: Clairlib-core, which has essential functionality and minimal dependence on external software, and Clairlib-ext, which has extended functionality that may be of interest to a smaller audience. Much can be done using Clairlib on its own. Some of the things that Clairlib can do are: Tokenization, Summarization, Document Clustering, Document Indexing, Web Graph Analysis, Network Generation,  [[Power law distribution]] Analysis, [[Network theory|Network Analysis]], [[Random walk]]s on graphs, [[Tf-idf]], [[Perceptron]] learning  and classification, and [[Compound term processing|Phrase Based Retrieval]] and [[Fuzzy logic|Fuzzy OR Queries]].\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://www.clairlib.org Homepage]\n*[http://tangra.si.umich.edu/clair/ Computational Linguistics And Information Retrieval (CLAIR) group]\n\n[[Category:Free computer libraries]]\n[[Category:Perl modules]]\n[[Category:University of Michigan]]\n[[Category:Information retrieval]]"
p158
sg6
S'Clairlib'
p159
ssI186
(dp160
g2
S'http://en.wikipedia.org/wiki/Social search'
p161
sg4
V'''Social search''' or a '''social search engine''' is a type of [[web search]] that takes into account the [[Social Graph]] of the person initiating the search query. When applied to web searches the Social-Graph uses established algorithmic or machine-based approaches where relevance is determined by analyzing the text of each document or the link structure of the documents.<ref>[http://searchenginewatch.com/showPage.html?page=3623153 What's the Big Deal With Social Search?], SearchEngineWatch, Aug 15, 2006</ref> Search results produced by '''social search engine''' give more visibility to content created or "touched" by users in the Social Graph.\u000a\u000aSocial search takes many forms, ranging from simple [[social bookmarking|shared bookmarks]] or [[Tag (metadata)|tagging]] of content with descriptive labels to more sophisticated approaches that combine human intelligence with computer [[algorithm]]s.<ref>[http://www2.computer.org/portal/web/csdl/doi/10.1109/MC.2009.87 Chi, Ed H. Information Seeking Can Be Social, Computer, vol. 42, no. 3, pp. 42-46, Mar. 2009, ] {{doi|10.1109/MC.2009.87}}</ref><ref>[http://blog.delver.com/index.php/2008/07/31/taxonomy-of-social-search-approaches/ A Taxonomy of Social Search Approaches], Delver company blog, Jul 31, 2008</ref>\u000a<ref>[http://www.springerlink.com/content/e12233858017h042/ Longo, Luca et al., Enhancing Social Search: A Computational Collective Intelligence Model of Behavioural Traits, Trust and Time. Transactions on Computational Collective Intelligence II, Lecture Notes in Computer Science, Volume 6450. ISBN 978-3-642-17154-3. Springer Berlin Heidelberg, 2010, p. 46 ] {{doi|10.1007/978-3-642-17155-0_3}}</ref><ref>[http://www.springerlink.com/content/gg3p6177pw6h10j8/ Longo, Luca et al., Information Foraging Theory as a Form of Collective Intelligence for Social Search. Computational Collective Intelligence. Semantic  Web, Social Networks and  Multiagent Systems Lecture Notes in Computer Science, 2009, Volume 5796/2009, 63-74] {{doi|10.1007/978-3-642-04441-0_5}}</ref>\u000a\u000aThe search experience takes into account varying sources of metadata, such as collaborative discovery of web pages, tags, social ranking, commenting on bookmarks, news, images, videos, knowledge sharing, podcasts and other web pages. Example forms of user input include social bookmarking or direct interaction with the search results such as promoting or demoting results the user feels are more or less relevant to their query.<ref>[http://venturebeat.com/2008/01/31/googles-marissa-mayer-social-search-is-the-future Google\u2019s Marissa Mayer: Social search is the future], VentureBeat, Jan 31, 2008</ref>\u000a\u000a==History==\u000a\u000aThe term social search began to emerge between 2004 and 2005. The concept of social ranking can be considered to derive from Google's [[PageRank]] algorithm,{{citation needed|date=March 2009}} which assigns importance to web pages based on analysis of the link structure of the web, because PageRank is relying on the collective judgment of webmasters linking to other content on the web. Links, in essence, are positive votes by the webmaster community for their favorite sites.\u000a\u000aIn 2008, there were a few startup companies that focused on ranking search results according to one's [[social graph]] on [[social networks]].<ref>[http://online.wsj.com/public/article/SB121063460767286631.html New Sites Make It Easier To Spy on Your Friends], Wall Street Journal, May 13. 2008</ref><ref>[http://mashable.com/2007/08/27/social-search/ Social Search Guide: 40+ Social Search Engines], Mashable, Aug 27. 2007</ref> Companies in the social search space include  Evam-SOCOTO Wajam, Slangwho, [[Sproose]], [[Mahalo.com|Mahalo]], [[Jumper 2.0]], [[Qitera]], [[Scour Inc.|Scour]], [[Wink Technologies|Wink]], [[Eurekster]], [[Baynote]], [[Delver (Social Search)|Delver]], and OneRiot. Former efforts include [[Wikia Search]]. In 2008, a story on ''[[TechCrunch]]'' showed [[Google]] potentially adding in a voting mechanism to search results similar to [[Digg]]'s methodology.<ref>[http://www.techcrunch.com/2008/07/16/is-this-the-future-of-search/ Is This The Future Of Search?], TechCrunch, July 16, 2008</ref> This suggests growing interest in how social groups can influence and potentially enhance the ability of algorithms to find meaningful data for end users. There are also other services like Sentiment that turn search personal by searching within the users' social circles.\u000a\u000aIn October 2009, [[Google]] rolled out its "Social Search" feature; after a time in [[beta]], the feature was expanded to multiple languages in May 2011. Before the expansion however in 2010 [[Bing]] and [[Google]] were already taking into account re-tweets and Likes when providing search results.<ref>{{cite web|url = http://www.marchpr.com/blog/2013/04/seo-social-media-search/|title = Retweets and Likes influencing search results|date = 10 April 2013|accessdate = 1 December 2014| publisher = March Communications}}</ref> However, after a search deal with Twitter ended without renewal, Google began to retool its Social Search. In January 2012, Google released "Search plus Your World", a further development of Social Search. The feature, which is integrated into Google's regular search as an opt-out feature, pulls references to results from [[Google+]] profiles. The goal was to deliver better, more relevant and personalized search results with this integration. This integration however had some problems in which [[Google+]] still isn't wildly adopted or has much usage among many users.<ref name="HubSpot">{{cite web|url = https://blog.hubspot.com/blog/tabid/6307/bid/34058/Facebook-Announces-New-Social-Search-Feature-Called-Graph-Search.aspx|title = Facebook Announces New Social Search Feature|date = 15 January 2013|accessdate = 1 December 2014| publisher = HubSpot}}</ref>\u000a\u000aIn January 2013, [[Facebook]] announced a new search engine called [[Graph Search]] still in the beta stages. The goal in mind was to accomplish what [[Google]] failed at, skipping the results that are popular to the internet, in favor of the results that are popular within your social circle. Unlike [[Google]], [[Facebook]]'s Graph search differed in two large areas, first, people use Facebook frequently. This allows [[Facebook]] to use all it's user generated content that is uploaded everyday to improve the [[Facebook]] search experience.<ref name="HubSpot"/> Secondly, [[Facebook]] did not incorporate Google into Facebook search, instead Graph Search is powered by [[Bing]].This allows [[Bing]] results to show when Facebook's Graph Search can't find a match.<ref>{{cite web|url = http://www.forbes.com/sites/tomiogeron/2013/01/15/live-facebook-announces-graph-search/|title = Graph Search powered by Bing|date = 15 January 2013|accessdate = 1 December 2014| publisher = Forbes}}</ref>\u000a\u000a==Concerns==\u000a\u000aWhen Google announced "Search plus Your World" the reaction was mixed among tech companies. The company was subsequently criticized by [[Twitter]] for the perceived potential impact of "Search plus Your World" upon web publishers, describing the feature's release to the public as a "bad day for the web", while Google replied that Twitter refused to allow deep search crawling by Google of Twitter's content.<ref>{{cite web|url = http://www.cnbc.com/id/100381337#.|title = Twitter unhappy about Google's social search changes|date = 11 January 2012|accessdate = 11 January 2012|publisher = BBC News}}</ref> The criticism from [[Twitter]] wasn't without merits however, by [[Google]] integrating [[Google+]], they were essentially forcing people to switch from a social network on to theirs in order to improve search results. One famous example occurred when [[Google]] showed a link to Mark Zuckerberg's dormant [[Google+]] account rather than the active [[Facebook]] profile.<ref name="Google pushing Google">{{cite web|url = http://searchengineland.com/googles-knowledge-graph-finally-shows-social-networks-named-google-209171.|title = Google pushing Google+|date = 18 November 2014|accessdate = 1 December 2012|publisher = Third Door Media}}</ref> Further more this affected businesses in which if they do not have time to leverage all other social media sites, they knew they should use [[Google+]] to maximize their efforts since the data shows it impacts rankings more than [[Twitter]] and [[Facebook]].<ref>{{cite web|url = http://www.quicksprout.com/2014/01/31/how-social-signals-impact-search-engine-rankings/#.|title = Google+ impacts ranking more|date = 31 January 2014|accessdate = 1 December 2014|publisher = Quick Sprout}}</ref> in November 2014 these accusations started to die down because Google's Knowledge Graph started to finally show links to [[Facebook]], [[Twitter]], and other social media sites.<ref name="Google pushing Google"/>\u000a\u000a[[Google]] was not the only one that garnished concerns over social search. After the introduction of [[Graph Search]] by [[Facebook]] many pointed out how [[Graph Search]] showed private information that isn't in web search.<ref>{{cite web|url = http://www.forbes.com/sites/tomiogeron/2013/01/15/live-facebook-announces-graph-search/|title = Graph Search results|date = 1 January 2013|accessdate = 1 December 2014|publisher = Forbes}}</ref> Information that was once obscure is now easier to dig up, which is why Facebook urges users to monitor post and pictures users are tagged in and filter and filter any content that users would not want to make public.<ref>{{cite web|url = http://www.forbes.com/sites/larrymagid/2013/01/15/facebooks-new-social-search-what-it-is-and-how-it-affects-your-privacy/|title = Graph Search Privacy Concerns|date = 15 January 2013|accessdate = 1 December 2014|publisher = Forbes}}</ref>\u000a\u000aThis in large points towards the biggest concern toward social search which is that social media networks don't have a vested interest in working with search engines. [[LinkedIn]] for example has taken steps to improve its own individual search functions in order to stray users from external search engines. Even [[Microsoft]] started working with [[Twitter]] in order to integrate some tweets into [[Bing]]'s search results in November 2013. Yet [[Twitter]] has its own search engine which points out how much value their data has and why they'd like to keep it in house.<ref>{{cite web|url = http://venturebeat.com/2014/06/30/microsoft-and-twitter-make-bing-a-better-social-search-engine/|title = Bing's twitter integration|date = 30 June 2014|accessdate = 1 December 2014|publisher = Venture Beat}}</ref> In the end though social search will never be truly comprehensive of the subjects that matter to people unless users opt to be completely public with their information.<ref>{{cite web|url = https://blog.hubspot.com/blog/tabid/6307/bid/34058/Facebook-Announces-New-Social-Search-Feature-Called-Graph-Search.aspx|title = User data will never be competently public|date = 15 January 2013|accessdate = 1 December 2014|publisher = HubSpot}}</ref>\u000a\u000a==Social discovery==\u000aSocial discovery is the use of social preferences and personal information to predict what content will be desirable to the user.<ref name="Bailyn2012">{{cite book|last=Bailyn|first=Evan|title=Outsmarting Social Media: Profiting in the Age of Friendship Marketing|url=http://books.google.com/books?id=M97RiODwKHEC&pg=PT51|accessdate=20 January 2014|date=2012-04-12|publisher=Que Publishing|isbn=9780132861403|pages=51\u2013}}</ref> Technology is used to discover new people and sometimes new experiences shopping, meeting friends or even traveling.<ref>{{cite web|last=Burke|first=Amy|url=http://mashable.com/2013/07/08/social-discovery-apps/|publisher=Mashable|title=Are Social Discovery Apps Too Creepy?}}</ref>  The discovery of new people is often in real-time, enabled by [[mobile apps]]. However, social discovery is not limited to meeting people in real-time, it also leads to sales and revenue for companies via social media.<ref>{{cite web|last=Cubie|first=Gregor|url=http://www.thedrum.com/news/2013/10/02/social-discovery-sites-influence-retail-expanding-rakutens-playcom-numbers-find|publisher=The Drum|title=Social Discovery sites' influence on retail expanding}}</ref>  An example of retail would be the addition of social sharing with music, through the iTunes music store. There is a social component to discovering new music <ref>{{cite web|last=Constine|first=Josh|url=http://techcrunch.com/2013/09/10/bitcovery/|publisher=TechCrunch|title=Bitcovery Brings A Desperately Needed Social Discovery Layer To The iTunes Store}}</ref> Social discovery is at the basis of [[Facebook]]'s profitability, generating ad revenue by targeting the ads to users using the social connections to enhance the commercial appeal.<ref name="Bailyn2012"/>\u000a\u000a==Developments==\u000a\u000a[[Google]] may be falling behind in terms of social search, but in reality they see the potential and importance of this technology with [[Web 3.0]] and [[web semantics]]. The importance of social media lies within how Semantic search works. Semantic search understands much more, including where you are, the time of day, your past history, and many other factors including social connections, and social signals. The first step in order to achieve this will be to teach algorithms to understand the relationship between things.<ref>{{cite web|url = http://www.socialmediatoday.com/content/google-semantic-search|title = Google Semantic Search|date = 28 February 2014|accessdate = 1 December 2014|publisher = Social Media Today}}</ref>\u000a\u000aHowever this is not possible unless social media sites decide to work with search engines, which is difficult since everyone would like to be the main toll bridge to the internet. As we continue on, and more articles are referred by social media sites, the main concern becomes what good is a search engine without the data of users.\u000a\u000aOne development that seeks to redefine search is the combination of [[distributed search]] with social search. The goal is a basic search service whose operation is controlled and maintained by the community itself. This would largely work like Peer to Peer networks in which users provide the data they seems appropriate. Since the data used by search engines belongs to the user they should have absolute control over it. The infrastructure required for a search engine is already available in the from of thousands of idle desktops and extensive residential broadband access.<ref>{{cite web|url = http://www2009.eprints.org/242/|title = Towards Distributed Social Search Engines|accessdate = 1 December 2014|publisher = EPrints}}</ref>\u000a\u000a== See also ==\u000a* [[Collaborative filtering]]\u000a* [[Enterprise bookmarking]]\u000a* [[Human search engine]]\u000a* [[Relevance feedback]]\u000a* [[Social software]]\u000a\u000a== References ==\u000a{{reflist}}\u000a{{Internet search}}\u000a\u000a[[Category:Searching]]\u000a[[Category:Social search| ]]\u000a[[Category:Social software|Search]]
p162
sg6
S'Social search'
p163
ssI60
(dp164
g2
S'http://en.wikipedia.org/wiki/European Summer School in Information Retrieval'
p165
sg4
VThe '''European Summer School in Information Retrieval''' (ESSIR) is a scientific event founded in 1990, which starts off a series of Summer Schools to provide high quality teaching of information retrieval on advanced topics. ESSIR is typically a week-long event consisting of guest lectures and seminars from invited lecturers who are recognized experts in the field.\u000aThe aim of ESSIR is to give to its participants a common ground in different aspects of '''[[Information Retrieval]] (IR)'''. Maristella Agosti in 2008 stated that: \u201c''The term IR identifies the activities that a person \u2013 the user \u2013 has to conduct to choose, from a collection of documents, those that can be of interest to him to satisfy a specific and contingent information need.''\u201d<ref>Agosti, M.: \u201cInformation Access using the Guide of User Requirements\u201d. In: ''Information Access through Search Engines and Digital Libraries''. Agosti, M. ed., Springer-Verlag Berlin Heidelberg, pp. 1-12, (2008).</ref>\u000a\u000aIR is a discipline with many facets and at the same time influences and is influenced by many other scientific disciplines. Indeed, IR ranges from [[Computer Science]] to [[Information Science]] and beyond; moreover, a large number of IR methods and techniques are adopted and absorbed by several technologies. The IR core methods and techniques are those for designing and developing IR systems, Web search engines, and tools for information storing and querying in Digital Libraries. IR core subjects are: system architectures, algorithms, formal theoretical models, and evaluation of the diverse systems and services that implement functionalities of storing and retrieving documents from multimedia document collections, and over wide area networks such as the [[Internet]].\u000a\u000aESSIR aims to give a deep and authoritative insight of the core IR methods and subjects along these three dimensions and also for this reason it is intended for researchers starting out in IR, for industrialists who wish to know more about this increasingly important topic and for people working on topics related to management of information on the [[Internet]].\u000a\u000aTwo books have been prepared as readings in IR from editions of ESSIR, the first one is ''Lectures on Information Retrieval''\u000a,<ref>Agosti, M., Crestani, F. and Pasi, G. (Eds): \u201cLectures on Information Retrieval\u201c. Revised Lectures of Third European Summer-School, ESSIR 2000 Varenna, Italy, September 11\u201315, 2000. LNCS Vol. 1980, Springer-Verlag, Berlin Heidelberg, 2001.</ref> the second one is ''Advanced Topics in Information Retrieval''.<ref>Melucci, M., and Baeza-Yates, R. (Eds): \u201cAdvanced Topics in Information Retrieval\u201c. The Information Retrieval Series, Vol. 33, Springer-Verlag, Berlin Heidelberg, 2011.</ref>\u000a\u000a== ESSIR Editions ==\u000aESSIR series started in 1990 coming out from the successful experience of the Summer School in Information Retrieval (SSIR) conceived and designed by Nick Belkin, [[Rutgers University]], U.S.A., and Maristella Agosti, [[University of Padua]], Italy, for an Italian audience in 1989.\u000a\u000a{| class="wikitable" border="1"\u000a|-\u000a! Edition\u000a! Web Site\u000a! Location\u000a! Organiser(s)\u000a|-\u000a|  9th\u000a|  [http://www.ugr.es/~essir2013/ ESSIR 2013]\u000a|  Granada, Spain\u000a|  Juan M. Fernadez-Luna and Juan F. Huete\u000a|-\u000a|  8th\u000a|  [http://essir.uni-koblenz.de/ ESSIR 2011]\u000a|  Koblenz, Germany\u000a|  Sergej Sizov and Steffen Staab\u000a|-\u000a|  7th\u000a|  [http://essir2009.dei.unipd.it/ ESSIR 2009]\u000a|  Padua, Italy\u000a|  Massimo Melucci and Ricardo Baeza-Yates\u000a|-\u000a|  6th\u000a|  [http://www.dcs.gla.ac.uk/essir2007/ ESSIR 2007]\u000a|  Glasgow, Scotland, United Kingdom\u000a|  Iadh Ounis and Keith van Rijsbergen\u000a|-\u000a|  5th\u000a|  [http://www.cdvp.dcu.ie/ESSIR2005/ ESSIR 2005]\u000a|  Dublin, Ireland\u000a|  Alan Smeaton\u000a|-\u000a|  4th\u000a|  [http://www-clips.imag.fr/mrim/essir03/main_essir.html ESSIR 2003]\u000a|  Aussois (Savoie), France\u000a|  Catherine Berrut and Yves Chiaramella\u000a|-\u000a|  3rd\u000a|  [http://www.itim.mi.cnr.it/Eventi/essir2000/index.htm ESSIR 2000]\u000a|  Varenna, Italy\u000a|  Maristella Agosti, Fabio Crestani, and Gabriella Pasi\u000a|-\u000a|  2nd\u000a|  [http://www.dcs.gla.ac.uk/essir/ ESSIR 1995]\u000a|  Glasgow, United Kingdom\u000a|  Keith van Rijsbergen\u000a|-\u000a|  1st\u000a|  [http://ims.dei.unipd.it/websites/essir/essir1990.html ESSIR 1990]\u000a|  Brixen, Italy\u000a|  Maristella Agosti\u000a|}\u000a\u000a==Notes==\u000a{{reflist}}\u000a\u000a==External links==\u000a* [http://ims.dei.unipd.it/websites/essir/home.html ESSIR presentation page of the IMS Research Group]\u000a* [http://ims.dei.unipd.it IMS Research Group, Department of Information Engineering - University of Padua, Italy]\u000a* [http://www.dei.unipd.it/ Department of Information Engineering - University of Padua, Italy]\u000a* [http://www.unipd.it/en/index.htm University of Padua, Italy]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Summer schools]]
p166
sg6
S'European Summer School in Information Retrieval'
p167
ssI189
(dp168
g2
S'http://en.wikipedia.org/wiki/Find'
p169
sg4
V{{other uses}}\u000a{{unreferenced|date=September 2013}}\u000a{{lowercase|title=find}} \u000aIn [[Unix-like]] and some other [[operating system]]s, <code>'''find'''</code> is a [[command-line utility]] that [[Search engine (computing)|searches]] through one or more [[directory tree]]s of a [[file system]], locates [[Computer file|file]]s based on some [[user (computing)|user]]-specified criteria and applies a user-specified action on each matched file. The possible search criteria include a [[pattern matching|pattern]] to match against the [[file name]] or a time range to match against the modification time or access time of the file. By default, <code>find</code> returns a list of all files below the current [[working directory]].\u000a\u000aThe related <code>[[locate (Unix)|locate]]</code> programs use a database of indexed files obtained through <code>find</code> (updated at regular intervals, typically by <code>[[cron]]</code> job) to provide a faster method of searching the entire filesystem for files by name.\u000a\u000a==History==\u000a<code>find</code> appeared in [[Version 5 Unix]] as part of the [[PWB/UNIX|Programmer's Workbench]] project.<ref name="reader">{{cite techreport |first1=M. D. |last1=McIlroy |authorlink1=Doug McIlroy |year=1987 |url=http://www.cs.dartmouth.edu/~doug/reader.pdf |title=A Research Unix reader: annotated excerpts from the Programmer's Manual, 1971\u20131986 |series=CSTR |number=139 |institution=Bell Labs}}</ref>\u000a\u000a== Find syntax ==\u000a{{expand section|date=August 2008}}\u000a\u000a<code>find [-H] [-L] [-P] path... [expression]</code>\u000a\u000aThe three options control how the <code>find</code> command should treat symbolic links. The default behaviour is never to follow symbolic links. This can be explicitly specified using the -P flag. The -L flag will cause the <code>find</code> command to follow symbolic links. The -H flag will only follow symbolic links while processing the command line arguments. These flags are not available with some older versions of <code>find</code>.\u000a\u000aAt least one path must precede the expression. <code>find</code> is capable of interpreting [[Wildcard character|wildcards]] internally and commands must be constructed carefully in order to control [[Glob (programming)|shell globbing]].\u000a\u000aExpression elements are whitespace-separated and evaluated from left to right.  They can contain logical elements such as AND (&#x2011;and or &#x2011;a) and OR (&#x2011;or &#x2011;o) as well as more complex predicates.\u000a\u000aThe [[GNU Find Utilities|GNU]] <code>find</code> has a large number of additional features not specified by POSIX.\u000a\u000a== POSIX protection from infinite output ==\u000a\u000aReal-world filesystems often contain looped structures created through the use of [[hard link|hard]] or [[symbolic link|soft links]].  The [[POSIX|POSIX standard]] requires that\u000a{{Quotation|\u000aThe <code>find</code> utility shall detect infinite loops; that is, entering a previously visited\u000adirectory that is an ancestor of the last file encountered. When it detects an infinite\u000aloop, <code>find</code> shall write a diagnostic message to standard error and shall either recover\u000aits position in the hierarchy or terminate.\u000a}}\u000a\u000a==Operators ==\u000aOperators can be used to enhance the expressions of the find command. Operators are listed in order of decreasing precedence:\u000a\u000a*'''( expr )''' Force precedence. \u000a*'''! expr''' True if expr is false.\u000a*'''expr1 expr2''' And (implied); expr2 is not evaluated if expr1 is false. \u000a*'''expr1 -a expr2''' Same as expr1 expr2.  \u000a*'''expr1 -o expr2''' Or; expr2 is not evaluated if expr1 is true.\u000a\u000a find . -name 'fileA_*' -o -name 'fileB_*'\u000a\u000aThis command searches files whose name has a prefix of "fileA_" or "fileB_" in the current directory.\u000a\u000a find . -name 'foo.cpp' '!' -path '.svn'\u000a\u000aThis command searches for files with the name "foo.cpp" in all subdirectories of the current directory (current directory itself included) other than ".svn".   We quote the ! so that it's not interpreted by the shell as the history substitution character.\u000a\u000a==Type filter explanation==\u000a\u000a'''-type''' ''option used to specify search for only file, link or directory.''\u000aVarious type filters are supported by find. They are activated using the\u000a\u000a find -type c\u000a\u000aconfiguration switch where c may be any of:\u000a* '''b '''[[Device file|block (buffered) special]]\u000a* '''c '''[[Device file|character (unbuffered special)]]\u000a* '''d [[Directory (computing)|directory]]'''\u000a* '''p '''[[Named pipe|named pipe (FIFO)]]\u000a* '''f [[regular file]]'''\u000a* '''l '''[[symbolic link]]; this is never true if the -L option or the -follow option is in effect, unless the symbolic link is broken. If you want to search for symbolic links when -L is in effect, use -xtype (though that is a GNU extension).\u000a* '''s '''[[Unix domain socket|socket]]\u000a* '''D '''[[Doors (computing)|door (Solaris)]]\u000a\u000aThe configuration switches listed in bold are most commonly used.\u000a\u000a==Examples==\u000a\u000a===From current directory===\u000a find . -name 'my*'\u000a\u000aThis searches in the current directory (represented by the dot character) and below it, for files and directories with names starting with ''my''. The quotes avoid the [[shell (computing)|shell]] expansion \u2014 without them the shell would replace ''my*'' with the list of files whose names begin with ''my'' in the current directory. In newer versions of the program, the directory may be omitted, and it will imply the current directory.\u000a\u000a===Files only===\u000a find . -name 'my*' -type f\u000aThis limits the results of the above search to only regular files, therefore excluding directories, special files, pipes, symbolic links, etc. ''my*'' is enclosed in single quotes (apostrophes) as otherwise the shell would replace it with the list of  files in the current directory starting with ''my''......\u000a\u000a===Commands===\u000aThe previous examples created listings of results because, by default, <code>find</code> executes the '-print' action.   (Note that early versions of the <code>find</code> command had no default action at all; therefore the resulting list of files would be discarded, to the bewilderment of users.)\u000a\u000a find . -name 'my*' -type f -ls\u000aThis prints extended file information.\u000a\u000a===Search all directories===\u000a find / -name myfile -type f -print\u000aThis searches every file on the computer for a file with the name ''myfile'' and prints it to the screen. It is generally not a good idea to look for data files this way.  This can take a considerable amount of time, so it is best to specify the directory more precisely.  Some operating systems may mount dynamic filesystems that are not congenial to <code>find</code>.   More complex filenames including characters special to the shell may need to be enclosed in single quotes.\u000a\u000a===Search all but one directory subtree===\u000a find / -path excluded_path -prune -o -type f -name myfile -print\u000aThis searches every folder on the computer except the subtree ''excluded_path'' (full path including the leading /), for a file with the name ''myfile''.  It will not detect directories, devices, links, doors, or other "special" filetypes.\u000a\u000a===Specify a directory===\u000a find /home/weedly -name 'myfile' -type f -print\u000aThis searches for files named ''myfile'' in the ''/home/weedly'' directory, the home directory for userid ''weedly''.  You should always specify the directory to the deepest level you can remember.  The quotes are optional in this example because "myfile" contains no characters special to the shell.\u000a\u000a===Search several directories===\u000a find local /tmp -name mydir -type d -print\u000aThis searches for directories named ''mydir'' in the ''local'' subdirectory of the current working directory and the ''/tmp'' directory.\u000a\u000a===Ignore errors===\u000aIf you're doing this as a user other than root, you might want to ignore permission denied (and any other) errors.  Since errors are printed to [[stderr]], they can be suppressed by redirecting the output to /dev/null.  The following example shows how to do this in the bash shell: \u000a find / -name 'myfile' -type f -print 2>/dev/null\u000a\u000aIf you are a [[C shell|csh]] or [[tcsh]] user, you cannot redirect [[stderr]] without redirecting [[stdout]] as well.  You can use sh to run the <code>find</code> command to get around this:\u000a sh -c find / -name 'myfile' -type f -print 2>/dev/null\u000a\u000aAn alternate method when using [[C shell|csh]] or [[tcsh]] is to pipe the output from [[stdout]] and [[stderr]] into a [[grep]] command. This example shows how to suppress lines that contain permission denied errors.\u000a find . -name 'myfile' |& grep -v 'Permission denied'\u000a\u000a===Find any one of differently named files===\u000a find . \u005c( -name '*jsp' -o -name '*java' \u005c) -type f -ls\u000a\u000aThe <code>-ls</code> option prints extended information, and the example finds any file whose name ends with either 'jsp' or 'java'. Note that the parentheses are required. Also note that the operator "or" can be abbreviated as "o". The "and" operator is assumed where no operator is given.  In many shells the parentheses must be escaped with a backslash, "\u005c(" and "\u005c)", to prevent them from being interpreted as special shell characters. The <code>-ls</code> option and the <code>-or</code> operator are not available on all versions of <code>find</code>.\u000a\u000a===Execute an action===\u000a find /var/ftp/mp3 -name '*.mp3' -type f -exec chmod 644 {} \u005c;\u000aThis command changes the [[File system permissions|permissions]] of all files with a name ending in ''.mp3'' in the directory ''/var/ftp/mp3''. The  action is carried out by specifying the option <code>-exec [[chmod]] 644 {} \u005c;</code> in the command. For every file whose name ends in <code>.mp3</code>, the command <code>chmod 644 {}</code> is executed replacing <code>{}</code> with the name of the file. The semicolon (backslashed to avoid the shell interpreting it as a command separator) indicates the end of the command. Permission <code>644</code>, usually shown as <code>rw-r--r--</code>, gives the file owner full permission to read and write the file, while other users have read-only access. In some shells, the <code>{}</code> must be quoted.  The trailing ";" is customarily quoted with a leading "\u005c", but could just as effectively be enclosed in single quotes.\u000a\u000aNote that the command itself should *not* be quoted; otherwise you get error messages like\u000a\u000a find: echo "mv ./3bfn rel071204": No such file or directory\u000a\u000awhich means that <code>find</code> is trying to run a file called 'echo "mv ./3bfn rel071204"' and failing.\u000a\u000aIf you will be executing over many results, it is more efficient to use a variant of the exec primary that collects filenames up to ARG_MAX and then executes COMMAND with a list of filenames.\u000a\u000a find . -exec COMMAND {} +\u000a\u000aThis will ensure that filenames with whitespaces are passed to the executed COMMAND without being split up by the shell.\u000a\u000a===Delete files and directories===\u000a'''Caveats''': the -delete action is a GNU extension, and using it turns on -depth.   So, if you are testing a find command with -print instead of -delete in order to figure out what will happen before going for it, you need to use -depth -print.\u000a\u000aDelete empty files and directories and print the names (note that -empty is a vendor unique extension from GNU find that may not be available in all find implementations)\u000a find /foo -empty -delete -print\u000a\u000aDelete empty files\u000a find /foo -type f -empty -delete\u000a\u000aDelete empty directories\u000a find /foo -type d -empty -delete\u000a\u000aDelete files and directories (if empty) named <code>bad</code> \u000a find /foo -name bad -empty -delete\u000a\u000a'''Warning''': <code>-delete</code> should be used with other operators such as\u000a<code>-empty</code> or <code>-name</code>.\u000a\u000a find /foo -delete  # this deletes '''all''' in /foo\u000a\u000a===Search for a string===\u000aThis command will search for a string in all files from the /tmp directory and below:\u000a<source lang="bash">\u000a $ find /tmp -type f -exec grep 'search string' '{}' /dev/null \u005c+\u000a</source>\u000aThe <tt>[[/dev/null]]</tt> argument is used to show the name of the file before the text that is found. Without it, only the text found is printed.  An equivalent mechanism is to use the "-H" or "--with-filename" option to grep:\u000a<source lang="bash">\u000a $ find /tmp -type f -exec grep -H 'search string' '{}' '+' \u000a</source>\u000aGNU grep can be used on its own to perform this task:\u000a\u000a $ grep -r 'search string' /tmp\u000a\u000aExample of search for "LOG" in jsmith's home directory\u000a<source lang="bash" highlight="1">\u000a $ find ~jsmith -exec grep LOG '{}' /dev/null \u005c; -print\u000a /home/jsmith/scripts/errpt.sh:cp $LOG $FIXEDLOGNAME\u000a /home/jsmith/scripts/errpt.sh:cat $LOG\u000a /home/jsmith/scripts/title:USER=$LOGNAME\u000a</source>\u000aExample of search for the string "ERROR" in all XML files in the current directory and all sub-directories\u000a<source lang="bash">\u000a\u000a $ find . -name "*.xml" -exec grep "ERROR" /dev/null '{}' \u005c+ \u000a</source>\u000aThe double quotes (" ") surrounding the search string and single quotes (<nowiki>' '</nowiki>) surrounding the braces are optional in this example, but needed to allow spaces and some other special characters in the string.  Note with more complex text (notably in most popular shells descended from `sh` and `csh`) single quotes are often the easier choice, since '''double quotes do not prevent all special interpretation'''. Quoting filenames which have English contractions demonstrates how this can get rather complicated, since a string with an apostrophe in it is easier to protect with double quotes.  Example:\u000a<source lang="bash">\u000a\u000a $ find . -name "file-containing-can't" -exec grep "can't" '{}' \u005c; -print\u000a</source>\u000a\u000a===Search for all files owned by a user===\u000a find . -user <userid>\u000a\u000a===Search in case insensitive mode===\u000aNote that -iname is not in the standard and may not be supported by all implementations.\u000a\u000a find . -iname ''''MyFile'''*'\u000a\u000aIf the <code>-iname</code> switch is not supported on your system then workaround techniques may be possible such as:\u000a\u000a find . -name '[m'''M''']['''y'''Y][f'''F''']['''i'''I]['''l'''L]['''e'''E]*'\u000a\u000aThis uses [[Perl]] to build the above command for you (though in general this kind of usage is dangerous, since special characters are not properly quoted before being fed into the standard input of `sh`):\u000a\u000a echo "''''MyFile'''*'" |perl -pe 's/([a-zA-Z])/[\u005cL\u005c1\u005cU\u005c1]/g;s/(.*)/find . -name \u005c1/'|sh\u000a\u000a===Search files by size===\u000aExample of searching files with size between 100 kilobytes and 500 kilobytes.\u000a find . -size +100k -a -size -500k\u000aExample of searching empty files.\u000a find . -size 0k\u000aExample of searching non-empty files.\u000a find . ! -size 0k\u000a\u000a===Search files by name and size ===\u000a '''find''' /usr/src {{abbr|!|the negation of the expression that follows}} {{abbr|\u005c(|the start of a complex expression.}} -name '*,v' {{abbr|-o|a logical or of a complex expression. In this case the complex expression is all files like '*,v' or '.*,v'}} -name '.*,v' {{abbr|\u005c)|the end of a complex expression.}} '{}' \u005c; -print\u000a\u000aThis command will search in the /usr/src directory and all sub directories. All files that are of the form '*,v' and '.*,v' are excluded. Important arguments to note are in the [[tooltip]] that is displayed on mouse-over.\u000a\u000a<source lang="bash" enclose="div">\u000afor file in `find /opt \u005c( -name error_log -o -name 'access_log' -o -name 'ssl_engine_log' -o -name 'rewrite_log' -o\u000a -name 'catalina.out' \u005c) -size +300000k -a -size -5000000k`; do \u000a    cat /dev/null > $file\u000adone\u000a</source>\u000aThe units should be one of [bckw], 'b' means 512-byte blocks, 'c' means byte, 'k' means kilobytes and 'w' means 2-byte words. The size does not count indirect blocks, but it does count blocks in sparse files that are not actually allocated.\u000a\u000a==Related utilities==\u000a* <code>[[locate (Unix)|locate]]</code> is a Unix search tool that searches through a prebuilt database of files instead of directory trees of a file system. This is faster than <code>find</code> but less accurate because the database may not be up-to-date.\u000a* <code>[[grep]]</code> is a command-line utility for searching plain-text data sets for lines matching a regular expression and by default reporting matching lines on [[standard output]].\u000a* <code>[[tree (Unix)|tree]]</code> is a command-line utility that recursively lists files found in a directory tree, indenting the file names according to their position in the file hierarchy.\u000a* [[GNU Find Utilities]] (also known as findutils) is a [[GNU package]] which contains implementations of the tools <code>find</code> and [[xargs]].\u000a* [[BusyBox]] is a utility that provides several stripped-down Unix tools in a single executable file, intended for embedded operating systems with very limited resources. It also provides a version of <code>find</code>.\u000a* <code>[[dir (command)|dir]]</code> has the /s option that recursively searches for files or folders.\u000a\u000a==See also==\u000a*[[mdfind]], a similar utility that utilizes metadata for [[Mac OS X]] and [[Darwin (operating system)|Darwin]]\u000a*[[List of Unix programs]]\u000a*[[List of DOS commands]]\u000a*[[List of duplicate file finders]]\u000a*[[Filter (higher-order function)]]\u000a*[[find (command)]], a DOS and Windows command that is very different from UNIX <code>find</code>\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links==\u000a*{{man|cu|find|SUS|find files}}\u000a*[http://www.gnu.org/software/findutils/manual/html_mono/find.html Official webpage for GNU find]\u000a\u000a{{Unix commands}}\u000a\u000a[[Category:Searching]]\u000a[[Category:Standard Unix programs]]\u000a[[Category:Unix SUS2008 utilities]]
p170
sg6
S'Find'
p171
ssI63
(dp172
g2
S'http://en.wikipedia.org/wiki/Overlap coefficient'
p173
sg4
S"The '''overlap coefficient''' (or, '''Szymkiewicz-Simpson coefficient''') is a [[String_metric|similarity measure]] related to the [[Jaccard index]] that measures the overlap between two sets, and is defined as the size of the intersection divided by the smaller of the size of the two sets:\n\n:<math>\\mathrm{overlap}(X,Y) = \\frac{| X \\cap Y | }{\\min(|X|,|Y|)}</math>\n\nIf set ''X'' is a subset of ''Y'' or the converse then the overlap coefficient is equal to one.\n\n== External links==\n* Open Source [https://github.com/rockymadden/stringmetric/blob/master/core/src/main/scala/com/rockymadden/stringmetric/similarity/OverlapMetric.scala Overlap] [[Scala programming language|Scala]] implementation as part of the larger [http://rockymadden.com/stringmetric/ stringmetric project]\n\n[[Category:Information retrieval]]\n[[Category:String similarity measures]]\n[[Category:Measure theory]]"
p174
sg6
S'Overlap coefficient'
p175
ssI192
(dp176
g2
S'http://en.wikipedia.org/wiki/Search/Retrieve via URL'
p177
sg4
S"'''Search/Retrieve via URL''' ('''SRU''') is a standard search protocol for [[Internet search]] queries, utilizing [[Contextual Query Language]] (CQL), a standard query syntax for representing queries.\n\n==See also==\n* [[Search/Retrieve Web Service]]\n\n==External links==\n* [http://www.loc.gov/standards/sru/ Search/Retrieve via URL] at [[Library of Congress]]\n\n{{Internet search}}\n\n{{DEFAULTSORT:Search Retrieve via URL}}\n[[Category:Data search engines]]\n[[Category:Searching]]\n[[Category:Uniform resource locator]]\n\n{{web-stub}}"
p178
sg6
S'Search/Retrieve via URL'
p179
ssI66
(dp180
g2
S'http://en.wikipedia.org/wiki/Instant indexing'
p181
sg4
V{{multiple issues|\u000a{{Orphan|date=February 2009}}\u000a{{Refimprove|article|date=November 2006}}\u000a}}\u000a\u000a'''Instant indexing''' is a feature offered by [[Internet]] [[search engine]]s that enables users to submit content for immediate inclusion into the [[search engine indexing|index]].\u000a\u000a==Delayed inclusion==\u000aCertain search engine services may require an extended period of time for inclusion, which is seen as a delay and a frustration by [[website]] administrators who wish to have their websites appear in [[search engine results page|search engine results]]{{Citation needed|date=February 2007}}.\u000a\u000aDelayed inclusion may due to the size of the index that the service must maintain or due to corporate, political or social policies{{Citation needed|date=February 2007}}. Some services only index content collected by a [[web crawling|crawler program]] which does not allow for manual adding of content to index{{Citation needed|date=February 2007}}.\u000a\u000a==Criticisms==\u000aA criticism of instant indexing is that certain services filter results manually or via algorithms that prevent instant inclusion to avoid inclusion of content that violates the service's policies.{{Citation needed|date=February 2007}}\u000a\u000aInstant indexing impacts the timeliness of the content included in the index. Given the manner in which many [[web crawling|crawlers]] operate in the case of Internet search engines, websites are only visited if a some other website links to them. Unlinked web sites are never visited (see [[invisible web]]) by the crawler because it cannot reach the website during its traversal. It is assumed that unlinked websites are less authoritative and less popular, and therefore of less quality. Over time, if a website is popular or authoritative, it is assumed that other websites will eventually link to it. If a search engine service provides instant indexing, it bypasses this quality control mechanism by not requiring incoming links. This infers that the search engine's service produces lower quality results.\u000a\u000aSelect search services that offer such a service typically also offer [[paid inclusion]], also referred to as [[pay per click|inorganic search]]. This may reduce the quality of search results.\u000a\u000a==External links==\u000a* {{cite web | url = http://www.web-cite.com/search_marketing/000078.html | title = Don't Blink: Instant Indexing? | publisher = Web-Cite Exposure | date = 2003-03-26 | accessdate = 2006-09-23 |archiveurl = http://web.archive.org/web/20060427184004/http://www.web-cite.com/search_marketing/000078.html <!-- Bot retrieved archive --> |archivedate = 2006-04-27}}\u000a* {{cite web | url = http://www.earthstation9.com/index.html?us_searc.htm | title = The Wonderful World of Search Engines and Web Directories \u2014 A Search Engine Guide | author = Stan Daniloski | publisher = Earth Station 9 | date = 2004-09-17 | accessdate = 2006-09-23}}\u000a\u000a== See also ==\u000a* [[Search engine]]\u000a* [[Search engine indexing]]\u000a* [[Web crawling]]\u000a\u000a[[Category:Internet terminology]]\u000a[[Category:Information retrieval]]\u000a\u000a\u000a{{website-stub}}
p182
sg6
S'Instant indexing'
p183
ssI195
(dp184
g2
S'http://en.wikipedia.org/wiki/Lookeen'
p185
sg4
V{{Infobox Software\u000a| name = Lookeen\u000a| screenshot =\u000a| caption =\u000a| developer = [[Axonic Informationssysteme GmbH]]\u000a| latest_release_version = 8.3.1.5156\u000a| latest_release_date = May 21, 2013\u000a| latest_preview_version =\u000a| latest_preview_date =\u000a| operating_system = [[Microsoft Windows]]\u000a| genre = [[Search Tool|Email search]]\u000a| company_type   = Private (venture-backed)\u000a| foundation     = 2006\u000a| location       = [[Karlsruhe]], [[Germany]]\u000a| key_people     = [[Martin Welker]], CEO<br>[[Peter Oehler]], COO\u000a| industry       = Email Applications\u000a| website = [http://www.lookeen.com www.lookeen.com]\u000a}}\u000a'''Lookeen''' is a business search [[Plug-in (computing)|add-on]] for [[Microsoft Outlook]], produced under shareware license. The program uses [[Apache Software Foundation|Apache]]'s search engine [[Lucene]] and helps searching for [[Computer file|files]], [[emails]], [[contacts]], [[Email attachment|attachements]] as well as [[desktop environment|desktop]] elements on [[personal computers]] as well as in large [[Terminal Server]] or [[Citrix]] environments.<ref>[http://email.about.com/od/outlookaddons/gr/lookeen.htm ''Lookeen 2010'']. Editor Review on about.com. Retrieved on August 22, 2014.</ref>\u000a==Using==\u000aLookeen is an add-on for Microsoft Outlook. The [[shareware]] program is developed according to the Microsoft company recommendation on the add-ons design. After installation the program automatically integrates into Microsoft Outlook workspace. After the indexing process, Lookeen easily allows to search whole [[Personal Storage Table|Outlook archives]] and the [[My Documents]] folder.<ref>[http://www.pcworld.com/article/233114/lookeen.html ''Lookeen'']. Editor Review on pcworld.com. Retrieved on August 22, 2014.</ref>\u000aIn contrast to the Microsoft Outlook [[native (computing)|native]] search engine, Lookeen indexes the complete [[folder (computing)|folder]] structure. Whereas the native Outlook search only allows searches within the presently used and active folder, Lookeen searches in complete Outlook archives for needed information. \u000a\u000a===Supported mailbox storages===\u000aLookeen supports the following types of mail accounts: [[POP3]], [[IMAP]], [[HTTP]] and [[Microsoft Exchange Server]]. Both uncached and [[cache (computing)|cached]] exchange server modes are supported.\u000a===Supported filetypes===\u000aThe following filetypes can be indexed and searched for with Lookeen (in alphabetical order]: [[.bmp]], [[.doc]], [[.docx]], [[.gif]], [[.htm]], [[.html]], [[.jpeg]], [[.jpg]], [[.msg]], [[.pdf]], [[.php]], [[.png]], [[.pps]], [[.ppsx]], [[.ppt]], [[.pptx]], [[.rtf]], [[.txt]], [[.tif]], [[.tiff]], [[.xls]], [[.xlsm]], [[.xlsx]], [[.xml]]. \u000a===Central indexing===\u000aLookeen 8 supports central indexing of shared resources (e.g. network files, public exchange folders). This shared index is created once and integrated by the clients via its URL. Goal is to reduce network- and server-traffic and reduce the index storage cost for local indexes.<ref>[http://www.techmynd.com/outlook-search-tool-lookeen-licenses-giveaway/ ''Excellent Outlook Search Tool \u2013 Lookeen'']. Editor Review on Techmynd.com. Retrieved on August 22, 2014.</ref>\u000a===Enterprise Roll-Out Support===\u000aLookeen 8 supports [[Group Policies]] for advanced software distributions in companies. Many options (e.g. index location, settings location, included sources, index intervals, license keys, etc.) can be defined by the administrator. That enables enterprises to use Lookeen in large [[Terminal Server]] or [[Citrix]] environments.<ref>[http://www.itwire.com/featured-news/54892-lookeen-8-accelerates-outlook-e-mail-search ''Lookeen 8 accelerates Outlook E-Mail-Search'']. Official Press Release on ITwire.com. Retrieved on August 22, 2014.</ref>\u000a\u000a==History==\u000aStructure and Design of the first version strongly resembled the e-mail search software Lookout as developed by the [[Silicon Valley]] [[Startup company|Startup]] [[Lookout Software LCC]]. In 2004, Microsoft bought Lookout for allegedly 6 Million US-Dollars in order to integrate the search technology into its [[Windows Desktop Search]].<ref>[http://www.microsoft.com/presspass/press/2004/jul04/07-16lookoutpr.mspx ''MSN Announces Investment in Search Technology'']. Press Release on Microsoft.com. Retrieved on August 12, 2014.</ref> Lookout continued being available as [[Freeware]], but was not compatible anymore with Microsoft Outlook with the Release of [[Microsoft Windows Vista]] in January 2007. \u000aIn 2007, the German IT company [[Axonic Informationssysteme GmbH]] started working on a follow-up software for Lookout and finally released Lookeen in January 2008 as a professional solution for file and e-mail searches.<ref>[http://unternehmen.wikia.com/wiki/Axonic ''Company history of the creators of Lookeen'']. Official company registry entry on unternehmens.wikia.com. Retrieved on August 22, 2014.</ref> Within eight months, Lookeen was then sold in more than 40 countries.\u000a\u000a==Lookeen Server Enterprise Search==\u000aIn Juli 2011, a corresponding [[enterprise search]] version has been released. The [[Lookeen Server]] supports global indexing functions taking privacy and data security concerns into account by totally centralizing control options.<ref>[http://www.lookeen-server.com/en/product/overview ''Overview: Lookeen Server'']. From lookeen-server.com. Retrieved on August 20, 2014.</ref> \u000a\u000a==See also==\u000a* [[Comparison of enterprise search software]]\u000a* [[List of enterprise search vendors]]\u000a* [[List of Search Engines]]\u000a\u000a==References==\u000a<references />\u000a== External links ==\u000a* [http://www.crunchbase.com/company/lookeen CrunchBase: Lookeen Profile]\u000a* [http://www.lookeen.net Lookeen homepage]\u000a* [http://www.lookeen-server.com Lookeen Server homepage]\u000a* [http://www.axonic.net Creators of Lookeen]\u000a\u000a[[Category:Shareware]]\u000a[[Category:Software]]\u000a[[Category:Microsoft Office-related software]]\u000a[[Category:Desktop search engines]]\u000a[[Category:Searching]]
p186
sg6
S'Lookeen'
p187
ssI69
(dp188
g2
S'http://en.wikipedia.org/wiki/Category:Citation indices'
p189
sg4
S'{{Cat main|Citation index}}\n{{cat see also|Bibliographic databases|Bibliographic indexes}}\n\n[[Category:Bibliometrics]]\n[[Category:Reference works]]\n[[Category:Indexes]]\n[[Category:Information retrieval]]\n[[Category:Bibliographic databases| ]]'
p190
sg6
S'Category:Citation indices'
p191
ssI198
(dp192
g2
S'http://en.wikipedia.org/wiki/Search by sound'
p193
sg4
VSearching by sound for now has limited uses. There are a handful of applications, specifically for mobile devises that utilizes searching by sound. [[Shazam (service)]], [[Soundhound]], Midomi, and others has seen considerable success by using a simple algorithm to match an acoustic fingerprint to a song in a library. These applications takes a sample clip of a song, or a user generated melody and checks a music library to see where the clip matches with the song. From there, song information will be pulled up and displayed to the user. \u000a\u000aThese kind of applications is mainly used for finding a song that the user does not already know. \u000a\u000aSearching by sound is not limited so just identifying [[songs]], but also for identifying [[melodies]], [[Music|tunes]] or [[advertisements]], [[sound library management]] and [[video files]].\u000a\u000a==Acoustic Fingerprinting==\u000aThe way these apps search by sound is through generating an acoustic fingerprint; a digital summary of the sound. A microphone is used to pick up an audio sample, which is then broken down into a simple numeric signature, a code unique to each track. Using the same method of fingerprinting sounds, when Shazam picks up a sound clip, it will generate a signature for that clip. Then it\u2019s simple pattern matching from there using an extensive audio music database. \u000a\u000aThe practice of using [[acoustic fingerprints]] is not limited to just music however, but other areas of the entertainment business as well. Shazam also can identify television shows with the same technique of acoustic fingerprinting. Of course, this method of breaking down a sound sample into a unique signature is useless unless there is an extensive database of music with keys to match with the samples. Shazam has over 11 million songs in its database. <ref> http://www.slate.com/articles/technology/technology/2009/10/that_tune_named.html </ref>\u000a\u000aOther services such as Midomi and Soundhound allow users to add to that library of music in order to expand the chances to match a sound sample with its corresponding sound. \u000a\u000a==Spectogram==\u000aGenerating a signature from the song is essential for searching by sound, and can be tricky. However, the way certain applications such as Shazam found a way around this issue by creating a spectrogram. \u000a\u000aAny piece of music can be translated to a time frequency graph called a spectrogram. For each song in its database, each song is basically a graph that plots the three dimensions of music, frequency vs amplitude (intensity) vs time. The algorithm then picks out the points which peaks in the graph, labeled as \u201chigher energy content\u201d. In practice, this seems to work out to about three points per song. <ref> http://www.soyoucode.com/2011/how-does-shazam-recognize-song </ref>\u000a\u000aThis is how a song can be identified with just two or three notes. This greatly reduces the impact that [[background noise]] has on searching by sound. The key values taken away from this would be frequency in hertz and time in seconds. Shazam builds their fingerprint catalog out as a hash table, where the key is the frequency. They do not just mark a single point in the spectrogram, rather they mark a pair of points: the \u201cpeak intensity\u201d plus a second \u201canchor point\u201d. <ref> Li-Chun Wang, Avery. "An Industrial-Strength Audio Search Algorithm." Columbia University. Web. 1 Dec. 2014. <http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf>. </ref> So their key is not just a single frequency, it is a hash of the frequencies of both points.  This leads to less hash collisions which in turn speeds up catalog searching by several orders of magnitude by allowing them to take greater advantage of the table\u2019s constant (O(1)) look-up time. <ref> "How Shazam Works." Free Wont. Web. 1 Dec. 2014. <http://laplacian.wordpress.com/2009/01/10/how-shazam-works/>. </ref>\u000a\u000aThis method of acoustic fingerprinting allows applications such as Shazam to have the ability to differentiate between two closely related covers, as well as not having to account for popularity of a certain song. \u000a\u000a==Query by Humming==\u000aMidomi and Soundhound both utilize Query by Humming, or QbH. This is a branch off of acoustic fingerprints, but is still a musical retrieval system. After receiving a user generated hummed melody, which is the input query, and returns a ranked list of songs that is closest to the user query. \u000a\u000a==References==\u000a{{reflist}}\u000a\u000a\u000a\u000a\u000a[[Category:Searching]]
p194
sg6
S'Search by sound'
p195
ssI72
(dp196
g2
S'http://en.wikipedia.org/wiki/Search engine (computing)'
p197
sg4
S"{{more footnotes|date=August 2014}}\n{{one source|date=August 2014}}\nA '''search engine''' is an [[information retrieval|information retrieval system]] designed to help find information stored on a [[computer system]]. The search results are usually presented in a list and are commonly called ''hits''. Search engines help to minimize the time required to find information and the amount of information which must be consulted, akin to other techniques for managing [[information overload]]. {{Citation needed|date=December 2007}}\n\nThe most public, visible form of a search engine is a [[Web search engine]] which searches for information on the [[World Wide Web]].\n\n==How search engines work==\nSearch engines provide an [[interface (computer science)|interface]] to a group of items that enables users to specify criteria about an item of interest and have the engine find the matching items. The criteria are referred to as a [[search query]]. In the case of text search engines, the search query is typically expressed as a set of words that identify the desired [[concept]] that one or more [[document]]s may contain.<ref>Voorhees, E.M. [http://www.indexnist.gov/itl/iad/894.02/works/papers/nlp_ir.ps Natural Language Processing and Information Retrieval]. National Institute of Standards and Technology. March 2000.</ref> There are several styles of search query [[syntax]] that vary in strictness. It can also switch names within the search engines from previous sites.  Whereas some text search engines require users to enter two or three words separated by [[Whitespace (computer science)|white space]], other search engines may enable users to specify entire documents, pictures, sounds, and various forms of [[natural language]]. Some search engines apply improvements to search queries to increase the likelihood of providing a quality set of items through a process known as [[query expansion]].\n\n[[Image:search-engine-diagram-en.svg|right|thumb|Index-based search engine]]\n\nThe list of items that meet the criteria specified by the query is typically sorted, or ranked. Ranking items by relevance (from highest to lowest) reduces the time required to find the desired information. [[probability|Probabilistic]] search engines rank items based on measures of [[String metric|similarity]] (between each item and the query, typically on a scale of 1 to 0, 1 being most similar) and sometimes [[popularity]] or [[authority]] (see [[Bibliometrics]]) or use [[relevance feedback]]. [[Boolean logic|Boolean]] search engines typically only return items which match exactly without regard to order, although the term ''boolean search engine'' may simply refer to the use of boolean-style syntax (the use of operators [[Logical_conjunction|AND]], [[Logical_disjunction|OR]], NOT, and [[Exclusive_or|XOR]]) in a probabilistic context.\n\nTo provide a set of matching items that are sorted according to some criteria quickly, a search engine will typically collect [[metadata]] about the group of items under consideration beforehand through a process referred to as [[Index (search engine)|indexing]]. The index typically requires a smaller amount of [[computer storage]], which is why some search engines only store the indexed information and not the full content of each item, and instead provide a method of navigating to the items in the [[serp|search engine result page]]. Alternatively, the search engine may store a copy of each item in a [[cache (computing)|cache]] so that users can see the state of the item at the time it was indexed or for archive purposes or to make repetitive processes work more efficiently and quickly.\n\nOther types of search engines do not store an index. Crawler, or spider type search engines (a.k.a. real-time search engines) may collect and assess items at the time of the search query, dynamically considering additional items based on the contents of a starting item (known as a seed, or seed URL in the case of an Internet crawler). [[Meta search engine]]s store neither an index nor a cache and instead simply reuse the index or results of one or more other search engines to provide an aggregated, final set of results.\n\n==See also==\n{{Portal|Computer Science}}\n{{div col|colwidth=30em}}\n*[[Automatic summarization]]\n*[[Bibliographic database]]\n*[[Desktop search]]\n*[[Emanuel Goldberg]] (inventor of early search engine)\n*[[Enterprise search]]\n*[[Federated search]]\n*[[Full text search]]\n*[[Human search engine]]\n*[[Image search]]\n*[[Index (search engine)]]\n*[[Inverted index]]\n*[[List of search engines]]\n*[[List of enterprise search vendors]]\n*[[Medical literature retrieval]]\n*[[Metasearch engine]]\n*[[Search engine optimization]]\n*[[Search suggest drop-down list]]\n*[[Selection-based search]]\n*[[Semantic search]]\n* [[Solver (computer science)]]\n*[[Spamdexing]]\n*[[SQL]]\n*[[Text mining]]\n*[[Vertical search]]\n*[[Video search engine]]\n*[[Web search engine]]\n{{div col end}}\n\n==References==\n{{Reflist}}\n{{Internet search}}\n\n{{DEFAULTSORT:Search Engine (Computing)}}\n[[Category:Information retrieval]]\n[[Category:Data search engines| Search engine]]"
p198
sg6
S'Search engine (computing)'
p199
ssI201
(dp200
g2
S'http://en.wikipedia.org/wiki/Lee distance'
p201
sg4
V{{No footnotes|date=July 2011}}\u000aIn [[coding theory]], the '''Lee distance''' is a [[distance]] between two [[String (computer science)|string]]s <math>x_1 x_2 \u005cdots x_n</math> and <math>y_1 y_2 \u005cdots y_n</math> of equal length ''n'' over the ''q''-ary [[alphabet]] {0,&nbsp;1,&nbsp;\u2026,&nbsp;''q''&nbsp;&minus;&nbsp;1} of size ''q''&nbsp;\u2265&nbsp;2.\u000aIt is a [[Metric (mathematics)|metric]], defined as\u000a\u000a: <math>\u005csum_{i=1}^n \u005cmin(|x_i-y_i|,q-|x_i-y_i|).</math>\u000a\u000aIf ''q''&nbsp;=&nbsp;2 the Lee distance coincides with the [[Hamming distance]].\u000a\u000aThe [[metric space]] induced by the Lee distance is a discrete analog of the [[Elliptic geometry|elliptic space]].\u000a\u000a==Example==\u000aIf ''q''&nbsp;=&nbsp;6, then the Lee distance between 3140 and 2543 is 1&nbsp;+&nbsp;2&nbsp;+&nbsp;0&nbsp;+&nbsp;3&nbsp;=&nbsp;6.\u000a\u000a==History and application==\u000aThe Lee distance is named after [[C. Y. Lee (mathematician)|C. Y. Lee]]. It is applied for phase [[modulation]] while the Hamming distance is used in case of orthogonal modulation.\u000a\u000a==References==\u000a* {{Citation|first=C. Y.|last=Lee|title=Some properties of nonbinary [[error-correcting codes]]|journal=[[IEEE Transactions on Information Theory|IRE Transactions on Information Theory]]|volume=4|year=1958|pages=77\u201382|issue=2|doi=10.1109/TIT.1958.1057446}}.\u000a* {{Citation|first=E. R.|last=Berlekamp|authorlink=Elwyn Berlekamp|title=Algebraic Coding Theory|publisher=McGraw-Hill|year=1968}}.\u000a* {{Citation|last1=Deza|first1=E.|first2=M.|last2=Deza|author2-link=Michel Deza|title=Dictionary of Distances|year=2006|publisher=Elsevier|isbn=0-444-52087-2}}.\u000a\u000a[[Category:Coding theory]]\u000a[[Category:String similarity measures]]
p202
sg6
S'Lee distance'
p203
ssI75
(dp204
g2
S'http://en.wikipedia.org/wiki/Learning to rank'
p205
sg4
V'''Learning to rank'''<ref name="liu">{{citation\u000a|author=Tie-Yan Liu\u000a|title=Learning to Rank for Information Retrieval\u000a|series=Foundations and Trends in Information Retrieval: Vol. 3: No 3\u000a|year=2009\u000a|isbn=978-1-60198-244-5\u000a|doi=10.1561/1500000016\u000a|pages=225\u2013331\u000a|journal=Foundations and Trends® in Information Retrieval\u000a|volume=3\u000a|issue=3\u000a}}. Slides from Tie-Yan Liu's talk at [[World Wide Web Conference|WWW]] 2009 conference are [http://www2009.org/pdf/T7A-LEARNING%20TO%20RANK%20TUTORIAL.pdf available online]\u000a</ref> or '''machine-learned ranking''' (MLR) is the application of [[machine learning]], typically [[Supervised learning|supervised]], [[Semi-supervised learning|semi-supervised]] or [[reinforcement learning]], in the construction of [[ranking function|ranking models]] for [[information retrieval]] systems.<ref>[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The\u000aMIT Press ISBN 9780262018258.</ref> Training data consists of lists of items with some [[partial order]] specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. "relevant" or "not relevant") for each item. The ranking model's purpose is to rank, i.e. produce a [[permutation]] of items in new, unseen lists in a way which is "similar" to rankings in the training data in some sense.\u000a\u000aLearning to rank is a relatively new research area which has emerged in the past decade.\u000a\u000a== Applications ==\u000a\u000a=== In information retrieval ===\u000a[[File:MLR-search-engine-example.png|250px|thumb|A possible architecture of a machine-learned search engine.]]\u000aRanking is a central part of many [[information retrieval]] problems, such as [[document retrieval]], [[collaborative filtering]], [[sentiment analysis]], [[computational advertising]] (online ad placement).\u000a\u000aA possible architecture of a machine-learned search engine is shown in the figure to the right.\u000a\u000aTraining data consists of queries and documents matching them together with relevance degree of each match. It may be prepared manually by human ''assessors'' (or ''raters'', as [[Google]] calls them),\u000a<!-- "assessor" is the more standard term, used e.g. by TREC conference -->\u000awho check results for some queries and determine [[Relevance (information retrieval)|relevance]] of each result. It is not feasible to check relevance of all documents, and so typically a technique called [[pooling (information retrieval)|pooling]] is used \u2014 only the top few documents, retrieved by some existing ranking models are checked. <!--\u000a  TODO: write something about selection bias caused by pooling\u000a--> Alternatively, training data may be derived automatically by analyzing ''clickthrough logs'' (i.e. search results which got clicks from users),<ref name="Joachims2002">{{citation\u000a | author=Joachims, T.\u000a | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]\u000a | url=http://www.cs.cornell.edu/people/tj/publications/joachims_02c.pdf\u000a | title=Optimizing Search Engines using Clickthrough Data\u000a | year=2002\u000a}}</ref> ''query chains'',<ref>{{citation\u000a | author=Joachims T., Radlinski F.\u000a | title=Query Chains: Learning to Rank from Implicit Feedback\u000a | url=http://radlinski.org/papers/Radlinski05QueryChains.pdf\u000a | year=2005\u000a | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]\u000a}}</ref> or such search engines' features as Google's [[Google SearchWiki|SearchWiki]].\u000a\u000aTraining data is used by a learning algorithm to produce a ranking model which computes relevance of documents for actual queries.\u000a\u000aTypically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used.<ref>{{citation\u000a | author=B. Cambazoglu, H. Zaragoza, O. Chapelle, J. Chen, C. Liao, Z. Zheng, and J. Degenhardt.\u000a | title=Early exit optimizations for additive machine learned ranking systems\u000a | journal=WSDM '10: Proceedings of the Third ACM International Conference on Web Search and Data Mining, 2010. (to appear)\u000a | url=http://olivier.chapelle.cc/pub/wsdm2010.pdf\u000a}}</ref> First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as [[vector space model]], [[Standard Boolean model|boolean model]], weighted AND,<ref>{{citation\u000a | author=Broder A., Carmel D., Herscovici M., Soffer A., Zien J.\u000a | title=Efficient query evaluation using a two-level retrieval process\u000a | journal=Proceedings of the twelfth international conference on Information and knowledge management\u000a | year=2003\u000a | pages=426\u2013434\u000a | isbn=1-58113-723-0\u000a | url=http://cis.poly.edu/westlab/papers/cntdstrb/p426-broder.pdf\u000a }}</ref> [[Okapi BM25|BM25]]. This phase is called ''top-<math>k</math> document retrieval'' and many good heuristics were proposed in the literature to accelerate it, such as using document's static quality score and tiered indexes.<ref name="manning-q-eval">{{citation\u000a | author=Manning C.,  Raghavan P. and Schütze H.\u000a | title=Introduction to Information Retrieval\u000a | publisher=Cambridge University Press\u000a | year=2008}}. Section [http://nlp.stanford.edu/IR-book/html/htmledition/efficient-scoring-and-ranking-1.html 7.1]</ref> In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents.\u000a\u000a=== In other areas ===\u000aLearning to rank algorithms have been applied in areas other than information retrieval:\u000a* In [[machine translation]] for ranking a set of hypothesized translations;<ref name="Duh09">{{citation\u000a | author=Kevin K. Duh\u000a | title=Learning to Rank with {{sic|hide=y|Partially|-}}Labeled Data\u000a | year=2009\u000a | url=http://ssli.ee.washington.edu/people/duh/thesis/uwthesis.pdf\u000a}}</ref>\u000a* In [[computational biology]] for ranking candidate 3-D structures in protein structure prediction problem.<ref name="Duh09" />\u000a* In [[proteomics]] for the identification of frequent top scoring peptides.<ref name="Hen09">{{citation\u000a | author=Henneges C., Hinselmann G., Jung S., Madlung J., Schütz W., Nordheim A., Zell A.\u000a | title=Ranking Methods for the Prediction of Frequent Top Scoring Peptides from Proteomics Data\u000a | year=2009\u000a | url=http://www.omicsonline.com/ArchiveJPB/2009/May/01/JPB2.226.pdf\u000a}}</ref>\u000a* In [[Recommender system]]s for identifying a ranked list of related news articles to recommend to a user after he or she has read a current news article.<ref>Yuanhua Lv, Taesup Moon, Pranam Kolari, Zhaohui Zheng, Xuanhui Wang, and Yi Chang, [http://sifaka.cs.uiuc.edu/~ylv2/pub/www11-relatedness.pdf ''Learning to Model Relatedness for News Recommendation''], in International Conference on World Wide Web (WWW), 2011.</ref>\u000a\u000a== Feature vectors ==\u000aFor convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called ''[[feature vector]]s''. Such approach is sometimes called ''bag of features'' and is analogous to [[bag of words]] and [[vector space model]] used in information retrieval for representation of documents.\u000a\u000aComponents of such vectors are called ''[[feature (machine learning)|feature]]s'', ''factors'' or ''ranking signals''. They may be divided into three groups (features from [[document retrieval]] are shown as examples):\u000a* ''Query-independent'' or ''static'' features \u2014 those features, which depend only on the document, but not on the query. For example, [[PageRank]] or document's length. Such features can be precomputed in off-line mode during indexing. They may be used to compute document's ''static quality score'' (or ''static rank''), which is often used to speed up search query evaluation.<ref name="manning-q-eval" /><ref>\u000a{{cite conference\u000a | first=M. |last=Richardson\u000a | coauthors=Prakash, A. and Brill, E.\u000a | title=Beyond PageRank: Machine Learning for Static Ranking\u000a | booktitle=Proceedings of the 15th International World Wide Web Conference\u000a | pages=707\u2013715\u000a | publisher=\u000a | year=2006\u000a | url=http://research.microsoft.com/en-us/um/people/mattri/papers/www2006/staticrank.pdf\u000a | accessdate=\u000a }}</ref>\u000a* ''Query-dependent'' or ''dynamic'' features \u2014 those features, which depend both on the contents of the document and the query, such as [[TF-IDF]] score or other non-machine-learned ranking functions.\u000a* ''Query level features'' or ''query features'', which depend only on the query. For example, the number of words in a query. ''Further information: [[query level feature]]''\u000a\u000aSome examples of features, which were used in the well-known [[LETOR]] dataset:<ref name="letor3">[http://research.microsoft.com/en-us/people/taoqin/letor3.pdf LETOR 3.0. A Benchmark Collection for Learning to Rank for Information Retrieval]</ref>\u000a* TF, [[TF-IDF]], [[Okapi BM25|BM25]], and [[language modeling]] scores of document's [[Zone (information retrieval)|zone]]s (title, body, anchors text, URL) for a given query;\u000a* Lengths and [[Inverse document frequency|IDF]] sums of document's zones;\u000a* Document's [[PageRank]], [[HITS algorithm|HITS]] ranks and their variants.\u000a\u000aSelecting and designing good features is an important area in machine learning, which is called [[feature engineering]].\u000a\u000a== Evaluation measures ==\u000aThere are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics.\u000a\u000aExamples of ranking quality measures:\u000a* [[Mean average precision]] (MAP);\u000a* [[Discounted cumulative gain|DCG]] and [[Normalized discounted cumulative gain|NDCG]];\u000a* [[Precision (information retrieval)|Precision]]@''n'', NDCG@''n'', where "@''n''" denotes that the metrics are evaluated only on top ''n'' documents;\u000a* [[Mean reciprocal rank]];\u000a* [[Kendall's tau]]\u000a* [[Spearman's rank correlation coefficient|Spearman's Rho]]\u000a\u000aDCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used.<ref>http://www.stanford.edu/class/cs276/handouts/lecture15-learning-ranking.ppt</ref> Other metrics such as MAP, MRR and precision, are defined only for binary judgements.\u000a\u000aRecently, there have been proposed several new evaluation metrics which claim to model user's satisfaction with search results better than the DCG metric:\u000a* [[Expected reciprocal rank]] (ERR);<ref>{{citation\u000a|author=Olivier Chapelle, Donald Metzler, Ya Zhang, Pierre Grinspan\u000a|title=Expected Reciprocal Rank for Graded Relevance\u000a|url=http://research.yahoo.com/files/err.pdf\u000a|journal=CIKM\u000a|year=2009\u000a|pages=\u000a}}</ref>\u000a* [[Yandex]]'s pfound.<ref>{{citation\u000a|author=Gulin A., Karpovich P., Raskovalov D., Segalovich I.\u000a|title=Yandex at ROMIP'2009: optimization of ranking algorithms by machine learning methods\u000a|url=http://romip.ru/romip2009/15_yandex.pdf\u000a|journal=Proceedings of ROMIP'2009\u000a|year=2009\u000a|pages=163\u2013168\u000a}} (in Russian)</ref>\u000aBoth of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document.\u000a\u000a== Approaches ==\u000a{{Expand section|date=December 2009}}\u000aTie-Yan Liu of [[Microsoft Research Asia]] in his paper "Learning to Rank for Information Retrieval"<ref name="liu" /> and talks at several leading conferences has analyzed existing algorithms for learning to rank problems and categorized them into three groups by their input representation and [[loss function]]:\u000a\u000a=== Pointwise approach ===\u000aIn this case it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then learning-to-rank problem can be approximated by a regression problem \u2014 given a single query-document pair, predict its score.\u000a\u000aA number of existing [[Supervised learning|supervised]] machine learning algorithms can be readily used for this purpose. [[Ordinal regression]] and [[classification (machine learning)|classification]] algorithms can also be used in pointwise approach when they are used to predict score of a single query-document pair, and it takes a small, finite number of values.\u000a\u000a=== Pairwise approach ===\u000aIn this case learning-to-rank problem is approximated by a classification problem \u2014 learning a [[binary classifier]] that can tell which document is better in a given pair of documents. The goal is to minimize average number of [[Permutation#Inversions|inversions]] in ranking.\u000a\u000a=== Listwise approach ===\u000aThese algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to ranking model's parameters, and so continuous approximations or bounds on evaluation measures have to be used.\u000a\u000a=== List of methods ===\u000aA partial list of published learning-to-rank algorithms is shown below with years of first publication of each method:\u000a:{|class="wikitable sortable"\u000a! Year || Name || Type || Notes\u000a|-\u000a| 1989 || OPRF <ref name="Fuhr1989">{{citation\u000a | last=Fuhr\u000a | first=Norbert\u000a | journal=ACM Transactions on Information Systems\u000a | title=Optimum polynomial retrieval functions based on the probability ranking principle\u000a | volume=7\u000a | number=3\u000a | pages=183\u2013204 \u000a | year=1989\u000a | doi=10.1145/65943.65944\u000a}}</ref> || <span style="display:none">2</span> pointwise || Polynomial regression (instead of machine learning, this work refers to pattern recognition, but the idea is the same)\u000a|-\u000a| 1992 || SLR <ref name="Cooperetal1992">{{citation\u000a | author=Cooper, William S.; Gey, Frederic C.; Dabney, Daniel P.\u000a | journal=SIGIR '92 Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval \u000a | title=Probabilistic retrieval based on staged logistic regression\u000a | pages=198\u2013210 \u000a | year=1992\u000a | doi=10.1145/133160.133199\u000a}}</ref>   || <span style="display:none">2</span> pointwise || Staged logistic regression\u000a|-\u000a| 2000 || [http://research.microsoft.com/apps/pubs/default.aspx?id=65610 Ranking SVM] (RankSVM) || <span style="display:none">2</span> pairwise ||  A more recent exposition is in,<ref name="Joachims2002" /> which describes an application to ranking using clickthrough logs.\u000a|-\u000a| 2002 || Pranking<ref>{{cite paper | id = {{citeseerx|10.1.1.20.378}} | title = Pranking }}</ref> || <span style="display:none">1</span> pointwise || Ordinal regression.\u000a|-\u000a| 2003 <!-- or 1998? --> || [http://jmlr.csail.mit.edu/papers/volume4/freund03a/freund03a.pdf RankBoost] || <span style="display:none">2</span> pairwise ||\u000a|-\u000a| 2005 || [http://research.microsoft.com/en-us/um/people/cburges/papers/ICML_ranking.pdf RankNet] || <span style="display:none">2</span> pairwise ||\u000a|-\u000a| 2006 || [http://research.microsoft.com/en-us/people/tyliu/cao-et-al-sigir2006.pdf IR-SVM] || <span style="display:none">2</span> pairwise || Ranking SVM with query-level normalization in the loss function.\u000a|-\u000a| 2006 || [http://research.microsoft.com/en-us/um/people/cburges/papers/lambdarank.pdf LambdaRank] || <span style="display:none">3</span> pairwise || RankNet in which pairwise loss function is multiplied by the change in the IR metric caused by a swap.\u000a|-\u000a| 2007 || [http://research.microsoft.com/en-us/people/junxu/sigir2007-adarank.pdf AdaRank] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70364 FRank] || <span style="display:none">2</span> pairwise || Based on RankNet, uses a different loss function - fidelity loss.\u000a|-\u000a| 2007 || [http://www.cc.gatech.edu/~zha/papers/fp086-zheng.pdf GBRank] || <span style="display:none">2</span> pairwise || \u000a|-\u000a| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70428 ListNet] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=68128 McRank] || <span style="display:none">1</span> pointwise ||\u000a|-\u000a| 2007 || [http://www.stat.rutgers.edu/~tzhang/papers/nips07-ranking.pdf QBRank] || <span style="display:none">2</span> pairwise ||\u000a|-\u000a| 2007 || [http://research.microsoft.com/en-us/people/hangli/qin_ipm_2008.pdf RankCosine] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2007 || RankGP<ref>{{cite paper | id = {{citeseerx|10.1.1.90.220}} | title = RankGP }}</ref> || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2007 || [http://staff.cs.utu.fi/~aatapa/publications/inpPaTsAiBoSa07a.pdf RankRLS] || <span style="display:none">2</span> pairwise ||\u000aRegularized least-squares based ranking. The work is extended in\u000a<ref name=pahikkala2009efficient>{{Citation|last=Pahikkala|first=Tapio|coauthors=Tsivtsivadze, Evgeni, Airola, Antti, Järvinen, Jouni, Boberg, Jorma|title=An efficient algorithm for learning to rank from preference graphs|journal=Machine Learning|year=2009|volume=75|issue=1|pages=129\u2013165|doi=10.1007/s10994-008-5097-z|postscript=.}}</ref> to learning to rank from general preference graphs.\u000a|-\u000a| 2007 || [http://www.cs.cornell.edu/People/tj/publications/yue_etal_07a.pdf SVM<sup>map</sup>] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2008 || [http://research.microsoft.com/pubs/69536/tr-2008-109.pdf LambdaMART] || <span style="display:none">3</span> listwise || Winning entry in the recent Yahoo Learning to Rank competition used an ensemble of LambdaMART models.<ref>C. Burges. (2010). [http://research.microsoft.com/en-us/um/people/cburges/tech_reports/MSR-TR-2010-82.pdf From RankNet to LambdaRank to LambdaMART: An Overview].</ref>\u000a|-\u000a| 2008 || [http://research.microsoft.com/en-us/people/tyliu/icml-listmle.pdf ListMLE] || <span style="display:none">3</span> listwise || Based on ListNet.\u000a|-\u000a| 2008 || [http://research.microsoft.com/en-us/people/junxu/sigir2008-directoptimize.pdf PermuRank] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2008 || [http://research.microsoft.com/apps/pubs/?id=63585 SoftRank] || <span style="display:none">3</span> listwise ||\u000a|-\u000a| 2008 || [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf Ranking Refinement]<ref>Rong Jin, Hamed Valizadegan, Hang Li, [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf ''Ranking Refinement and Its Application for Information Retrieval''], in International Conference on World Wide Web (WWW), 2008.</ref> || <span style="display:none">2</span> pairwise || A semi-supervised approach to learning to rank that uses Boosting.\u000a|-\u000a| 2008 || [http://www-connex.lip6.fr/~amini/SSRankBoost/ SSRankBoost]<ref>Massih-Reza Amini, Vinh Truong, Cyril Goutte, [http://www-connex.lip6.fr/~amini/Publis/SemiSupRanking_sigir08.pdf ''A Boosting Algorithm for Learning Bipartite Ranking Functions with Partially Labeled Data''], International ACM SIGIR conference, 2008. The [http://www-connex.lip6.fr/~amini/SSRankBoost/ code] is available for research purposes.</ref>  || <span style="display:none">2</span> pairwise|| An extension of RankBoost to learn with partially labeled data (semi-supervised learning to rank)\u000a|-\u000a| 2008 || [http://phd.dii.unisi.it/PosterDay/2009/Tiziano_Papini.pdf SortNet]<ref>Leonardo Rigutini, Tiziano Papini, Marco Maggini, Franco Scarselli, [http://research.microsoft.com/en-us/um/beijing/events/lr4ir-2008/PROCEEDINGS-LR4IR%202008.PDF "SortNet: learning to rank by a neural-based sorting algorithm"], SIGIR 2008 workshop: Learning to Rank for Information Retrieval, 2008</ref> || <span style="display:none">2</span> pairwise|| SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. \u000a|-\u000a| 2009 || [http://itcs.tsinghua.edu.cn/papers/2009/2009031.pdf MPBoost] || <span style="display:none">2</span> pairwise || Magnitude-preserving variant of RankBoost. The idea is that the more unequal are labels of a pair of documents, the harder should the algorithm try to rank them.\u000a|-\u000a| 2009 || [http://www.machinelearning.org/archive/icml2009/papers/498.pdf BoltzRank] || <span style="display:none">3</span> listwise || Unlike earlier methods, BoltzRank produces a ranking model that looks during query time not just at a single document, but also at pairs of documents.\u000a|-\u000a| 2009 || [http://www.iis.sinica.edu.tw/papers/whm/8820-F.pdf BayesRank] || <span style="display:none">3</span> listwise || Based on ListNet.\u000a|-\u000a| 2010 || [http://www.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf NDCG Boost]<ref>Hamed Valizadegan, Rong Jin, Ruofei Zhang, Jianchang Mao, [http://www.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf ''Learning to Rank by Optimizing NDCG Measure''], in Proceeding of Neural Information Processing Systems (NIPS), 2010.</ref> || <span style="display:none">3</span> listwise || A boosting approach to optimize NDCG.\u000a|-\u000a| 2010 || [http://arxiv.org/abs/1001.4597 GBlend] || <span style="display:none">2</span> pairwise || Extends GBRank to the learning-to-blend problem of jointly solving multiple learning-to-rank problems with some shared features.\u000a|-\u000a| 2010 || [http://wume.cse.lehigh.edu/~ovd209/wsdm/proceedings/docs/p151.pdf IntervalRank] || <span style="display:none">2</span> pairwise & listwise || \u000a|-\u000a| 2010 || [http://www.eecs.tufts.edu/~dsculley/papers/combined-ranking-and-regression.pdf CRR] || <span style="display:none">2</span> pointwise & pairwise || Combined Regression and Ranking. Uses [[stochastic gradient descent]] to optimize a linear combination of a pointwise quadratic loss and a pairwise hinge loss from Ranking SVM.\u000a|}\u000a\u000aNote: as most [[supervised learning]] algorithms can be applied to pointwise case, only those methods which are specifically designed with ranking in mind are shown above.\u000a\u000a== History ==\u000a[[Norbert Fuhr]] introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation;<ref name="Fuhr1992">{{citation\u000a | last=Fuhr\u000a | first=Norbert\u000a | journal=Computer Journal\u000a | title=Probabilistic Models in Information Retrieval\u000a | volume=35\u000a | number=3\u000a | pages=243\u2013255\u000a | year=1992\u000a | doi=10.1093/comjnl/35.3.243\u000a}}</ref> a specific variant of this approach (using [[polynomial regression]]) had been published by him three years earlier.<ref name="Fuhr1989" /> Bill Cooper proposed [[logistic regression]] for the same purpose in 1992 <ref name="Cooperetal1992" /> and used it with his  [[University of California at Berkeley|Berkeley]] research group to train a successful ranking function for [[Text Retrieval Conference|TREC]].  Manning et al.<ref>{{citation |author=Manning C.,  Raghavan P. and Schütze H. |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008}}. Sections [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-7.html 7.4] and [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-15.html 15.5]</ref>  suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques.\u000a\u000aSeveral conferences, such as [[Neural Information Processing Systems|NIPS]], [[Special Interest Group on Information Retrieval|SIGIR]] and [[International Conference on Machine Learning|ICML]] had workshops devoted to the learning-to-rank problem since mid-2000s (decade).\u000a\u000a=== Practical usage by search engines ===\u000aCommercial [[web search engine]]s began using machine learned ranking systems since the 2000s (decade). One of the first search engines to start using it was [[AltaVista]] (later its technology was acquired by [[Overture Services, Inc.|Overture]], and then [[Yahoo]]), which launched a [[gradient boosting]]-trained ranking function in April 2003.<ref>Jan O. Pedersen. [http://jopedersen.com/Presentations/The_MLR_Story.pdf The MLR Story]</ref><ref>{{US Patent|7197497}}</ref>\u000a\u000a[[Bing (search engine)|Bing]]'s search is said to be powered by [[RankNet]] algorithm,<ref>[http://www.bing.com/community/blogs/search/archive/2009/06/01/user-needs-features-and-the-science-behind-bing.aspx?PageIndex=4 Bing Search Blog: User Needs, Features and the Science behind Bing]</ref>{{when|date=February 2014}} which was invented at [[Microsoft Research]] in 2005.\u000a\u000aIn November 2009 a Russian search engine [[Yandex]] announced<ref name="snezhinsk">[http://webmaster.ya.ru/replies.xml?item_no=5707&ncrnd=5118 Yandex corporate blog entry about new ranking model "Snezhinsk"] (in Russian)</ref> that it had significantly increased its [[search quality]] due to deployment of a new proprietary MatrixNet algorithm, a variant of [[gradient boosting]] method which uses [[oblivious decision tree]]s.<ref>The algorithm wasn't disclosed, but a few details were made public in [http://download.yandex.ru/company/experience/GDD/Zadnie_algoritmy_Karpovich.pdf] and [http://download.yandex.ru/company/experience/searchconf/Searchconf_Algoritm_MatrixNet_Gulin.pdf].</ref> Recently they have also sponsored a machine-learned ranking competition "Internet Mathematics 2009"<ref>[http://imat2009.yandex.ru/academic/mathematic/2009/en/ Yandex's Internet Mathematics 2009 competition page]</ref> based on their own search engine's production data. Yahoo has announced a similar competition in 2010.<ref>[http://learningtorankchallenge.yahoo.com/ Yahoo Learning to Rank Challenge]</ref>\u000a\u000aAs of 2008, [[Google]]'s [[Peter Norvig]] denied that their search engine exclusively relies on machine-learned ranking.<ref>{{cite web\u000a  | url = http://anand.typepad.com/datawocky/2008/05/are-human-experts-less-prone-to-catastrophic-errors-than-machine-learned-models.html\u000a  | archiveurl = http://www.webcitation.org/5sq8irWNM\u000a  | archivedate = 2010-09-18\u000a  | title = Are Machine-Learned Models Prone to Catastrophic Errors?\u000a  | date = 2008-05-24\u000a  | last = Rajaraman\u000a  | first = Anand\u000a  | authorlink = Anand Rajaraman}}</ref> [[Cuil]]'s CEO, [[Tom Costello (businessman)|Tom Costello]], suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models "learn what people say they like, not what people actually like".<ref>{{cite web\u000a  | url = http://www.cuil.com/info/blog/2009/06/26/so-how-is-bing-doing\u000a  | archiveurl = http://www.webcitation.org/5sq7DX3Pj\u000a  | archivedate = 2010-09-15\u000a  | title = Cuil Blog: So how is Bing doing?\u000a  | date = 2009-06-26\u000a  | last = Costello\u000a  | first = Tom}}</ref>\u000a\u000a== References ==\u000a{{reflist|2}}\u000a\u000a== External links ==\u000a; Competitions and public datasets\u000a* [http://research.microsoft.com/en-us/um/people/letor/ LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval]\u000a* [http://imat2009.yandex.ru/en/ Yandex's Internet Mathematics 2009]\u000a* [http://learningtorankchallenge.yahoo.com/ Yahoo! Learning to Rank Challenge]\u000a* [http://research.microsoft.com/en-us/projects/mslr/default.aspx Microsoft Learning to Rank Datasets]\u000a\u000a; Open Source code\u000a* [https://mloss.org/software/view/332/ Parallel C++/MPI implementation of Gradient Boosted Regression Trees for ranking, released September 2011]\u000a* [https://sites.google.com/site/rtranking/ C++ implementation of Gradient Boosted Regression Trees and Random Forests for ranking]\u000a* [http://dlib.net/ml.html#svm_rank_trainer C++ and Python tools for using the SVM-Rank algorithm]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Machine learning]]\u000a[[Category:Ranking functions]]
p206
sg6
S'Learning to rank'
p207
ssI204
(dp208
g2
S'http://en.wikipedia.org/wiki/Jaccard index'
p209
sg4
VThe '''Jaccard index''', also known as the '''Jaccard similarity coefficient''' (originally coined ''coefficient de communauté'' by [[Paul Jaccard]]), is a [[statistic]] used for comparing the [[Similarity measure|similarity]] and [[diversity index|diversity]] of [[Sample (statistics)|sample]] sets. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the [[intersection (set theory)|intersection]] divided by the size of the [[Union (set theory)|union]] of the sample sets:\u000a\u000a:<math> J(A,B) = {{|A \u005ccap B|}\u005cover{|A \u005ccup B|}}.</math>\u000a\u000a(If ''A'' and ''B'' are both empty, we define ''J''(''A'',''B'')&nbsp;=&nbsp;1.) Clearly, \u000a:<math> 0\u005cle J(A,B)\u005cle 1.</math>\u000a\u000aThe [[MinHash]] min-wise independent permutations [[locality sensitive hashing]] scheme may be used to efficiently compute an accurate estimate of the Jaccard similarity coefficient of pairs of sets, where each set is represented by a constant-sized signature derived from the minimum values of a [[hash function]].\u000a\u000aThe '''Jaccard distance''', which measures ''dis''similarity between sample sets, is complementary to the Jaccard coefficient and is obtained by subtracting the Jaccard coefficient from 1, or, equivalently, by dividing the difference of the sizes of the union and the intersection of two sets by the size of the union:\u000a\u000a:<math> d_J(A,B) = 1 - J(A,B) = { { |A \u005ccup B| - |A \u005ccap B| } \u005cover |A \u005ccup B| }.</math>\u000a\u000aAn alternate interpretation of the Jaccard distance is as the ratio of the size of the [[symmetric difference]] <math>A \u005ctriangle B = (A \u005ccup B) - (A \u005ccap B)</math> to the union. \u000a\u000aThis distance is a [[Distance function|metric]] on the collection of all finite sets.<ref name="lipkus">{{citation |last=Lipkus |first=Alan H\u000a|title=A proof of the triangle inequality for the Tanimoto distance\u000a|journal=J Math Chem |volume=26 |number=1-3 |year=1999 |pages=263\u2013265 }}</ref><ref>{{citation |last1=Levandowsky |first1=Michael |last2=Winter |first2=David |title=Distance between sets|journal=Nature |volume=234 |number=5 |year=1971 |pages=34\u201335 |doi=10.1038/234034a0}}</ref>\u000a\u000aThere is also a version of the Jaccard distance for [[measure (mathematics)|measures]], including [[probability measure]]s. If <math>\u005cmu</math> is a measure on a [[measurable space]] <math>X</math>, then we define the Jaccard coefficient by <math>J_\u005cmu(A,B) = {{\u005cmu(A \u005ccap B)} \u005cover {\u005cmu(A \u005ccup B)}}</math>, and the Jaccard distance by <math>d_\u005cmu(A,B) = 1 - J_\u005cmu(A,B) = {{\u005cmu(A \u005ctriangle B)} \u005cover {\u005cmu(A \u005ccup B)}}</math>. Care must be taken if <math>\u005cmu(A \u005ccup B) = 0</math> or <math>\u005cinfty</math>, since these formulas are not well defined in that case.\u000a\u000a== Similarity of asymmetric binary attributes ==\u000aGiven two objects, ''A'' and ''B'', each with ''n'' [[binary numeral system|binary]] attributes, the Jaccard coefficient is a useful measure of the overlap that ''A'' and ''B'' share with their attributes.  Each attribute of ''A'' and ''B'' can either be 0 or 1.  The total number of each combination of attributes for both ''A'' and ''B'' are specified as follows:\u000a:<math>M_{11}</math> represents the total number of attributes where ''A'' and ''B'' both have a value of 1.\u000a:<math>M_{01}</math> represents the total number of attributes where the attribute of ''A'' is 0 and the attribute of ''B'' is 1.\u000a:<math>M_{10}</math> represents the total number of attributes where the attribute of ''A'' is 1 and the attribute of ''B'' is 0.\u000a:<math>M_{00}</math> represents the total number of attributes where ''A'' and ''B'' both have a value of 0.\u000aEach attribute must fall into one of these four categories, meaning that\u000a:<math>M_{11} + M_{01} + M_{10} + M_{00} = n.</math>\u000a\u000aThe Jaccard similarity coefficient, ''J'', is given as\u000a:<math>J = {M_{11} \u005cover M_{01} + M_{10} + M_{11}}.</math>\u000a\u000aThe Jaccard distance, ''d''<sub>''J''</sub>, is given as\u000a:<math>d_J = {M_{01} + M_{10} \u005cover M_{01} + M_{10} + M_{11}}.</math>\u000a\u000a== Generalized Jaccard similarity and distance ==\u000a\u000aIf <math>\u005cmathbf{x} = (x_1, x_2, \u005cldots, x_n)</math> and <math>\u005cmathbf{y} = (y_1, y_2, \u005cldots, y_n)</math> are two vectors with all real <math>x_i, y_i \u005cgeq 0</math>, then their Jaccard similarity coefficient is defined as\u000a:<math>J(\u005cmathbf{x}, \u005cmathbf{y}) = \u005cfrac{\u005csum_i \u005cmin(x_i, y_i)}{\u005csum_i \u005cmax(x_i, y_i)},</math>\u000aand Jaccard distance\u000a:<math>d_J(\u005cmathbf{x}, \u005cmathbf{y}) = 1 - J(\u005cmathbf{x}, \u005cmathbf{y}).</math>\u000a\u000aWith even more generality, if <math>f</math> and <math>g</math> are two non-negative measurable functions on a measurable space <math>X</math> with measure <math>\u005cmu</math>, then we can define\u000a:<math>J(f, g) = \u005cfrac{\u005cint\u005cmin(f, g) d\u005cmu}{\u005cint \u005cmax(f, g)  d\u005cmu},</math>\u000awhere <math>\u005cmax</math> and <math>\u005cmin</math> are pointwise operators. Then Jaccard distance is\u000a:<math>d_J(f, g) = 1 - J(f, g).</math>\u000a\u000aThen, for example, for two measurable sets <math>A, B \u005csubseteq X</math>, we have <math>J_\u005cmu(A,B) = J(\u005cchi_A, \u005cchi_B),</math> where <math>\u005cchi_A</math> and <math>\u005cchi_B</math> are the characteristic functions of the corresponding set.\u000a\u000a== Tanimoto similarity and distance ==\u000a\u000a<!-- [[Tanimoto score]] redirects here, please change that redirect if you change this section title -->\u000a\u000aVarious forms of functions described as  Tanimoto similarity  and Tanimoto distance occur  in the literature and on the Internet. Most of these are synonyms for Jaccard similarity and Jaccard distance, but some are mathematically different. Many sources<ref>For example {{cite book |first=Huihuan |last=Qian |first2=Xinyu |last2=Wu |first3=Yangsheng |last3=Xu |title=Intelligent Surveillance Systems |publisher=Springer |year=2011 |page=161 |isbn=978-94-007-1137-2 }}</ref> cite an  unavailable IBM Technical Report<ref>{{cite journal |last=Tanimoto |first=T. |title=An Elementary Mathematical theory of Classification and Prediction |journal=Internal IBM Technical Report |date=17 Nov 1957 |issue=8? |volume=1957 }}</ref> as the seminal reference.\u000a\u000aIn "A Computer Program for Classifying Plants", published in October 1960,<ref>{{cite journal |first=David J. |last=Rogers |first2=Taffee T. |last2=Tanimoto |title=A Computer Program for Classifying Plants |journal=[[Science (journal)|Science]] |volume=132 |issue=3434 |pages=1115\u20131118 |year=1960 |doi=10.1126/science.132.3434.1115 }}</ref> a method of classification based on a similarity ratio, and a derived distance function, is given. It seems that this is  the most authoritative  source for the meaning of the terms "Tanimoto similarity" and "Tanimoto Distance". The similarity ratio is equivalent to Jaccard similarity, but the distance function is ''not'' the same as Jaccard distance.\u000a\u000a=== Tanimoto's definitions of similarity and distance ===\u000a\u000aIn that paper, a "similarity ratio" is  given over [[Bit array|bitmaps]], where each bit of a fixed-size array represents the presence or absence of a characteristic in the plant being modelled. The definition of the ratio is the number of common bits, divided by the number of bits set (i.e. nonzero) in either sample.\u000a\u000aPresented in mathematical terms, if samples ''X'' and ''Y'' are bitmaps, <math>X_i</math> is the ''i''th bit of ''X'', and <math> \u005cland , \u005clor </math> are [[bitwise operation|bitwise]] ''[[logical conjunction|and]]'', ''[[logical disjunction|or]]'' operators respectively, then the similarity ratio <math>T_s</math> is\u000a\u000a: <math> T_s(X,Y) =  \u005cfrac{\u005csum_i ( X_i \u005cland Y_i)}{\u005csum_i ( X_i \u005clor Y_i)}</math>\u000a\u000aIf each sample is modelled instead as a set of attributes, this value is  equal to the Jaccard coefficient of the two sets. Jaccard is not cited in the paper, and it seems likely that the authors were not aware of it.\u000a\u000aTanimoto goes on to define a "distance coefficient" based on this ratio, defined for bitmaps with non-zero similarity:\u000a\u000a: <math>T_d(X,Y) = -\u005clog_2 ( T_s(X,Y) ) </math>\u000a\u000aThis coefficient is, deliberately, not a distance metric. It is chosen to allow the possibility of two specimens, which are quite different from each other, to both be similar to a third. It is  easy to construct an example which disproves the property of [[Triangle inequality#Metric space|triangle inequality]].\u000a\u000a=== Other definitions of Tanimoto distance ===\u000a\u000aTanimoto distance is often referred to, erroneously, as a synonym for Jaccard distance <math> 1 - T_s</math>. This function is a proper distance metric. "Tanimoto Distance" is often stated as being a proper distance metric, probably because of its confusion with Jaccard distance.\u000a\u000aIf Jaccard or Tanimoto similarity is expressed over a bit vector, then it can be written as\u000a\u000a: <math>\u000af(A,B) =\u005cfrac{ A \u005ccdot B}{\u005cvert A\u005cvert^2 +\u005cvert B\u005cvert^2 -  A \u005ccdot B }\u000a</math>\u000a\u000awhere the same calculation is expressed in terms of vector scalar product and magnitude. This representation relies on the fact that, for a bit vector (where the value of each dimension is either 0 or 1) then <math>A \u005ccdot B = \u005csum_i A_iB_i = \u005csum_i ( A_i \u005cland B_i)</math> and <math>{\u005cvert A\u005cvert}^2 = \u005csum_i A_i^2 = \u005csum_i A_i </math>.\u000a\u000aThis is a potentially confusing representation, because the function as expressed over vectors is more general, unless its domain is explicitly restricted. Properties of <math> T_s </math> do not necessarily extend to <math>f</math>. In particular, the difference function <math>1 - f</math> does not preserve [[triangle inequality]], and is not therefore a proper distance metric, whereas <math>1 - T_s </math> is.\u000a\u000aThere is a real danger that the combination of "Tanimoto Distance" being defined using this formula, along with the statement "Tanimoto Distance is a proper distance metric" will lead to the false conclusion that the function <math>1 - f</math> is in fact a distance metric over vectors or multisets in general, whereas its use in similarity search or clustering algorithms may fail to produce correct results.\u000a\u000aLipkus<ref name="lipkus" /> uses a definition of Tanimoto similarity which is equivalent to <math>f</math>, and refers to Tanimoto distance as the function <math> 1 - f</math>. It is however made clear within the paper that the context is restricted by the use of a (positive) weighting vector <math>W</math> such that, for any vector ''A'' being considered, <math> A_i \u005cin \u005c{0,W_i\u005c} </math>. Under these circumstances, the  function  is a proper distance metric, and so a set of vectors governed by such a weighting vector forms a metric space under this function.\u000a\u000a== See also ==\u000a* [[Sørensen similarity index]]\u000a* [[simple matching coefficient]]\u000a* [[Mountford's index of similarity]]\u000a* [[Most frequent k characters]]\u000a* [[Hamming distance]]\u000a* [[Dice's coefficient]], which is equivalent: <math>J=D/(2-D)</math> and <math>D=2J/(1+J)</math>\u000a* [[Tversky index]]\u000a* [[Correlation]]\u000a* [[Mutual information]], a normalized [[Mutual information#Metric|metricated]] variant of which is an entropic Jaccard distance.\u000a\u000a==Notes==\u000a{{reflist}}\u000a\u000a{{More footnotes|date=March 2011}}\u000a\u000a== References ==\u000a*{{citation|first1=Pang-Ning|last1=Tan|first2=Michael|last2=Steinbach|first3=Vipin|last3=Kumar|title=Introduction to Data Mining|year=2005|isbn=0-321-32136-7}}.\u000a*{{citation|first=Paul|last=Jaccard|authorlink=Paul Jaccard|year=1901|title=Étude comparative de la distribution florale dans une portion des Alpes et des Jura|journal=Bulletin de la Société Vaudoise des Sciences Naturelles|volume=37|pages=547\u2013579}}.\u000a*{{citation|first=Paul|last=Jaccard|authorlink=Paul Jaccard|year=1912|title=The distribution of the flora in the alpine zone|journal=New Phytologist|volume=11|pages=37\u201350|doi=10.1111/j.1469-8137.1912.tb05611.x}}.\u000a\u000a== External links ==\u000a* [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap2_data.pdf Introduction to Data Mining lecture notes from Tan, Steinbach, Kumar]\u000a* [http://sourceforge.net/projects/simmetrics/ SimMetrics a sourceforge implementation of Jaccard index and many other similarity metrics]\u000a* [http://www.idea-miner.de/cgi-bin/INT_Tools/ver_vergleich_0_1/cmp_menu2.cgi Web based tool for comparing texts using Jaccard coefficient]\u000a* [http://www.gettingcirrius.com/2011/01/calculating-similarity-part-2-jaccard.html Tutorial on how to calculate different similarities]\u000a* Open Source [https://github.com/rockymadden/stringmetric/blob/master/core/src/main/scala/com/rockymadden/stringmetric/similarity/JaccardMetric.scala Jaccard] [[Scala programming language|Scala]] implementation as part of the larger [http://rockymadden.com/stringmetric/ stringmetric project]\u000a\u000a{{DEFAULTSORT:Jaccard Index}}\u000a[[Category:Index numbers]]\u000a[[Category:Measure theory]]\u000a[[Category:Clustering criteria]]\u000a[[Category:String similarity measures]]
p210
sg6
S'Jaccard index'
p211
ssI78
(dp212
g2
S'http://en.wikipedia.org/wiki/Information Retrieval Specialist Group'
p213
sg4
S'{{Unreferenced|date=January 2010}}\n\nThe \'\'\'Information Retrieval Specialist Group\'\'\' (\'\'\'IRSG\'\'\') or \'\'\'BCS-IRSG\'\'\' is a Specialist Group of the [[British Computer Society]] concerned with supporting communication between researchers and practitioners, promoting the use of [[Information Retrieval]] (IR) methods in industry and raising public awareness. There is a newsletter called \'\'The Informer\'\', an annual European Conference (ECIR), and continual organisation and sponsorship of conferences, workshops and seminars. The current chair is Dr. Andy MacFarlane.{{Citation needed|date=January 2010}}\n\n==European Conference on Information Retrieval==\nOrganising [[European Conference on Information Retrieval|ECIR]] is one of the major activities of the Information Retrieval Specialist Group. The conference began in 1979 and has grown to become one of the major Information Retrieval conferences alongside [[Special Interest Group on Information Retrieval|SIGIR]] receiving hundreds of paper and poster submissions every year from around the world.{{Citation needed|date=January 2010}} ECIR was initially established by the IRSG under the name "Annual Colloquium on Information Retrieval Research", and held in the UK until 1997. It was renamed ECIR in 2003 to better reflect its status as an international conference.\n\n== External links ==\n* [http://irsg.bcs.org/ IRSG website]\n\n[[Category:Information retrieval|Specialist Group]]\n[[Category:BCS Specialist Groups]]'
p214
sg6
S'Information Retrieval Specialist Group'
p215
ssI207
(dp216
g2
S'http://en.wikipedia.org/wiki/String-to-string correction problem'
p217
sg4
V{{No footnotes|date=July 2010}}\u000aIn [[computer science]], the '''string-to-string correction problem''' refers to the minimum number of edit operations necessary to change one [[String (computer science)|string]] into another. A single edit operation may be changing a single [[Character (computing)|symbol]] of the string into another, deleting, or inserting a symbol. The length of the edit sequence provides a measure of the [[Hamming distance|distance]] between the two strings.\u000a\u000aSeveral [[algorithm]]s exist to provide an efficient way to determine string distance and specify the minimum number of transformation operations required. Such algorithms are particularly useful for [[Delta encoding|delta]] creation operations where something is stored as a set of differences relative to a base version. This allows several versions of a single object to be stored much more efficiently than storing them separately. This holds true even for single versions of several objects if they do not differ greatly, or anything in between. \u000aNotably, such difference algorithms are used in [[molecular biology]] to provide some measure of kinship between different kinds of organisms based on the similarities of their [[macromolecule]]s (such as [[protein]]s or [[DNA]]).\u000a\u000a== See also ==\u000a* [[Delta encoding]]\u000a* [[Levenshtein distance]]\u000a* [[Edit distance]]\u000a\u000a== References ==\u000a<div class="references-small">\u000a*{{cite journal |first=Robert A. |last=Wagner |first2=Michael J. |last2=Fischer |author2-link=Michael J. Fischer |title=The String-to-String Correction Problem |journal=Journal of the ACM |volume=21 |issue=1 |year=1974 |pages=168\u2013173 |doi= 10.1145/321796.321811}}\u000a*{{cite journal |first=Walter F. |last=Tichy |title=The string-to-string correction problem with block moves |journal=ACM Transactions on Computer Systems |volume=2 |issue=4 |year=1984 |pages=309\u2013321 |doi= 10.1145/357401.357404}}\u000a</div>\u000a\u000a[[Category:Problems on strings]]\u000a[[Category:String similarity measures]]
p218
sg6
S'String-to-string correction problem'
p219
ssI81
(dp220
g2
S'http://en.wikipedia.org/wiki/Probabilistic relevance model'
p221
sg4
V{{Underlinked|date=August 2014}}\u000a\u000aThe '''probabilistic relevance model'''<ref>{{citation | author=S. E. Robertson | coauthors=K. S. Jones | title=Relevance weighting of search terms | publisher=Journal of the American Society for Information Science | pages=129\u2013146 | date=May\u2013June 1976 | url=http://portal.acm.org/citation.cfm?id=106783 }}</ref> was devised by Robertson and Jones as a framework for probabilistic models to come.\u000a \u000aIt makes an estimation of the probability of finding if a document ''d<sub>j</sub>'' is relevant to a query ''q''. This model assumes that this probability of relevance depends on the query and document representations. Furthermore, it assumes that there is a portion of all documents that is preferred by the user as the answer set for query ''q''. Such an ideal answer set is called ''R'' and should maximize the overall probability of relevance to that user. The prediction is that documents in this set ''R'' are relevant to the query, while documents not present in the set are non-relevant.\u000a\u000a<math>sim(d_{j},q) = \u005cfrac{P(R|\u005cvec{d}_j)}{P(\u005cbar{R}|\u005cvec{d}_j)}</math>\u000a\u000a==Related models==\u000aThere are some limitations to this framework that need to be addressed by further development:\u000a* There is no accurate estimate for the first run probabilities\u000a* Index terms are not weighted\u000a* Terms are assumed mutually independent\u000a\u000aTo address these and other concerns there are some developed models from the probabilistic relevance framework. The [[Binary Independence Model]] for one, as it is from the same author. The most known derivative of this framework is the [[Probabilistic relevance model (BM25)|Okapi(BM25)]] weighting scheme and it's BM25F brother.\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Probabilistic models]]
p222
sg6
S'Probabilistic relevance model'
p223
ssI210
(dp224
g2
Vhttp://en.wikipedia.org/wiki/Damerau\u2013Levenshtein distance
p225
sg4
VIn [[information theory]] and [[computer science]], the '''Damerau\u2013Levenshtein distance''' (named after [[Frederick J. Damerau]] and [[Vladimir I. Levenshtein]]<ref>{{cite conference |last1=Brill |first1=Eric |last2=Moore |first2=Robert C. |year=2000 |title=An Improved Error Model for Noisy Channel Spelling Correction |conference=Proceedings of the 38th Annual Meeting on Association for Computational Linguistics |pages=286\u2013293 |doi=10.3115/1075218.1075255 |url=http://acl.ldc.upenn.edu/P/P00/P00-1037.pdf}}</ref><ref name="bard"/><ref>{{cite conference |last1=Li |last2=et al. |year=2006|title=Exploring distributional similarity based models for query spelling correction |conference=Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics |pages=1025\u20131032 |doi=10.3115/1220175.1220304 |url=http://acl.ldc.upenn.edu/P/P06/P06-1129.pdf}}</ref>) is a [[Metric (mathematics)|distance]] ([[string metric]]) between two [[string (computer science)|strings]], i.e., finite sequence of symbols, given by counting the minimum number of operations needed to transform one string into the other, where an operation is defined as an insertion, deletion, or substitution of a single character, or a [[transposition (mathematics)|transposition]] of two '''adjacent''' characters.  In his seminal paper,<ref>{{Citation |last=Damerau |first=Fred J. |author-link=Frederick J. Damerau |title=A technique for computer detection and correction of spelling errors |journal=Communications of the ACM |publisher=ACM |volume=7 |issue=3 |pages=171\u2013176 |date=March 1964 |doi=10.1145/363958.363994}}</ref> Damerau not only distinguished these four edit operations but also stated that they correspond to more than 80% of all human misspellings. Damerau's paper considered only misspellings that could be corrected with at most one edit operation.\u000a\u000aThe Damerau\u2013Levenshtein distance differs from the classical [[Levenshtein distance]] by including transpositions among its allowable operations. The classical Levenshtein distance only allows insertion, deletion, and substitution operations.<ref>{{citation |url= <!-- copylink? http://profs.sci.univr.it/~liptak/ALBioinfo/files/levenshtein66.pdf--> |first=Vladimir I. |last=Levenshtein |title=Binary codes capable of correcting deletions, insertions, and reversals |journal=Soviet Physics Doklady |date=February 1966 |volume=10 |issue=8 |pages=707&ndash;710}}</ref> Modifying this distance by including transpositions of adjacent symbols produces a different distance measure, known as the Damerau\u2013Levenshtein distance.<ref name="bard">{{citation\u000a | last = Bard | first = Gregory V.\u000a | contribution = Spelling-error tolerant, order-independent pass-phrases via the Damerau\u2013Levenshtein string-edit distance metric\u000a | isbn = 1-920-68285-6\u000a | location = Darlinghurst, Australia\u000a | pages = 117\u2013124\u000a | publisher = Australian Computer Society, Inc.\u000a | series = Conferences in Research and Practice in Information Technology\u000a | title = Proceedings of the Fifth Australasian Symposium on ACSW Frontiers : 2007, Ballarat, Australia, January 30 - February 2, 2007\u000a | url = http://dl.acm.org/citation.cfm?id=1274531.1274545\u000a | volume = 68\u000a | year = 2007}}. The isbn produces two hits: a 2007 work and a 2010 work at World Cat.</ref>\u000a\u000aWhile the original motivation was to measure distance between human misspellings to improve applications such as [[spell checker]]s, Damerau\u2013Levenshtein distance has also seen uses in biology to measure the variation between [[DNA]].<ref>The method used in: {{Citation\u000a| last1  = Majorek         | first1 = Karolina A.\u000a| last2  = Dunin-Horkawicz | first2 = Stanis\u0142aw\u000a| last3  = Steczkiewicz    | first3 = Kamil\u000a| last4  = Muszewska       | first4 = Anna\u000a| last5  = Nowotny         | first5 = Marcin\u000a| last6  = Ginalski        | first6 = Krzysztof\u000a| last7  = Bujnicki        | first7 = Janusz M.\u000a| display-authors = 2\u000a| title   = The RNase H-like superfamily: new members, comparative structural analysis and evolutionary classification\u000a| journal = Nucleic Acids Research\u000a| volume  = 42\u000a| issue   = 7\u000a| pages   = 4160\u20134179\u000a| year    = 2013\u000a| doi     = 10.1093/nar/gkt1414\u000a| url     = http://nar.oxfordjournals.org/content/42/7/4160.full\u000a}}</ref>\u000a\u000a== Definition ==\u000aThe Damerau\u2013Levenshtein distance between two strings <math>a</math> and <math>b</math> is given by <math>d_{a,b}(|a|,|b|)</math> where:\u000a\u000a<math>\u005cqquad d_{a,b}(i,j) = \u005cbegin{cases}\u000a  \u005cmax(i,j) & \u005ctext{ if} \u005cmin(i,j)=0, \u005c\u005c\u000a\u005cmin \u005cbegin{cases}\u000a          d_{a,b}(i-1,j) + 1 \u005c\u005c\u000a          d_{a,b}(i,j-1) + 1 \u005c\u005c\u000a          d_{a,b}(i-1,j-1) + 1_{(a_i \u005cneq b_j)} \u005c\u005c\u000a          d_{a,b}(i-2,j-2) + 1 \u000a       \u005cend{cases} & \u005ctext{ if } i,j > 1 \u005ctext{ and } a_i = b_{j-1} \u005ctext{ and } a_{i-1} = b_j \u005c\u005c\u000a  \u005cmin \u005cbegin{cases}\u000a          d_{a,b}(i-1,j) + 1 \u005c\u005c\u000a          d_{a,b}(i,j-1) + 1 \u005c\u005c\u000a          d_{a,b}(i-1,j-1) + 1_{(a_i \u005cneq b_j)}\u000a       \u005cend{cases} & \u005ctext{ otherwise.}\u000a\u005cend{cases}</math>\u000a\u000awhere  <math>1_{(a_i \u005cneq b_j)}</math> is the [[indicator function]] equal to 0 when  <math>a_i = b_j</math> and equal to 1 otherwise.\u000a\u000aEach recursive call matches one of the cases covered by the Damerau\u2013Levenshtein distance:\u000a* <math>d_{a,b}(i-1,j) + 1</math> corresponds to a deletion (from a to b).\u000a* <math>d_{a,b}(i,j-1) + 1</math> corresponds to an insertion (from a to b).\u000a* <math>d_{a,b}(i-1,j-1) + 1_{(a_i \u005cneq b_j)} </math> corresponds to a match or mismatch, depending on whether the respective symbols are the same.\u000a* <math>d_{a,b}(i-2,j-2) + 1 </math> corresponds to a [[transposition (mathematics)|transposition]] between two successive symbols.\u000a\u000a== Algorithm ==\u000aPresented here are two algorithms: the first,<ref>{{cite paper | author1 = B. J. Oommen | author2 = R. K. S. Loke | title = Pattern recognition of strings with substitutions, insertions, deletions and generalized transpositions | id = {{citeseerx|10.1.1.50.1459}} | doi=10.1016/S0031-3203(96)00101-X }}</ref> simpler one, computes what is known as the [[optimal string alignment]]{{Citation needed|date=May 2013}} (sometimes called the ''restricted edit distance''{{Citation needed|date=May 2013}}), while the second one<ref name="LW75">{{Citation |first1=Roy |last1=Lowrance |first2=Robert A. |last2=Wagner |title=An Extension of the String-to-String Correction Problem |journal=JACM |volume=22 |issue=2 |pages=177\u2013183 |date=April 1975 |doi=10.1145/321879.321880}}</ref> computes the Damerau\u2013Levenshtein distance with adjacent transpositions. Adding transpositions adds significant complexity. The difference between the two algorithms consists in that the ''optimal string alignment algorithm'' computes the number of edit operations needed to make the strings equal under the condition that '''no substring is edited more than once''', whereas the second one presents no such restriction.\u000a\u000aTake for example the edit distance between '''CA''' and '''ABC'''. The Damerau\u2013Levenshtein distance LD('''CA''','''ABC''') = 2 because '''CA''' \u2192 '''AC''' \u2192 '''ABC''', but the optimal string alignment distance OSA('''CA''','''ABC''') = 3 because if the operation '''CA''' \u2192 '''AC''' is used, it is not possible to use '''AC''' \u2192 '''ABC''' because that would require the substring to be edited more than once, which is not allowed in OSA, and therefore the shortest sequence of operations is '''CA''' \u2192 '''A''' \u2192 '''AB''' \u2192 '''ABC'''. Note that for the optimal string alignment distance, the [[triangle inequality]] does not hold: OSA('''CA''','''AC''') + OSA('''AC''','''ABC''') < OSA('''CA''','''ABC'''), and so it is not a true metric.\u000a \u000a===Optimal string alignment distance===\u000aFirstly, let us consider a direct extension of the formula used to calculate [[Levenshtein distance]].  Below is [[pseudocode]] for a function ''OptimalStringAlignmentDistance'' that takes two strings, ''str1'' of length ''lenStr1'', and ''str2'' of length ''lenStr2'', and computes the optimal string alignment distance between them:\u000a\u000a<syntaxhighlight lang="pascal">\u000a int OptimalStringAlignmentDistance(char str1[1..lenStr1], char str2[1..lenStr2])\u000a    // d is a table with lenStr1+1 rows and lenStr2+1 columns\u000a    declare int d[0..lenStr1, 0..lenStr2]\u000a\u000a    // i and j are used to iterate over str1 and str2\u000a    declare int i, j, cost\u000a\u000a    // for loop is inclusive, need table 1 row/column larger than string length\u000a    for i from 0 to lenStr1\u000a        d[i, 0] := i\u000a    for j from 1 to lenStr2\u000a        d[0, j] := j\u000a\u000a    // pseudo-code assumes string indices start at 1, not 0\u000a    // if implemented, make sure to start comparing at 1st letter of strings\u000a    for i from 1 to lenStr1\u000a        for j from 1 to lenStr2\u000a            if str1[i] = str2[j] then cost := 0\u000a                                 else cost := 1\u000a            d[i, j] := minimum(\u000a                                 d[i-1, j  ] + 1,     // deletion\u000a                                 d[i  , j-1] + 1,     // insertion\u000a                                 d[i-1, j-1] + cost   // substitution\u000a                             )\u000a            if(i > 1 and j > 1 and str1[i] = str2[j-1] and str1[i-1] = str2[j]) then\u000a                d[i, j] := minimum(\u000a                                 d[i, j],\u000a                                 d[i-2, j-2] + cost   // transposition\u000a                              )                        \u000a  \u000a    return d[lenStr1, lenStr2]\u000a</syntaxhighlight>\u000a\u000aBasically this is the algorithm to compute [[Levenshtein distance]] with one additional recurrence:\u000a\u000a<syntaxhighlight lang="pascal">\u000a            if(i > 1 and j > 1 and str1[i] = str2[j-1] and str1[i-1] = str2[j]) then\u000a                d[i, j] := minimum(\u000a                                 d[i, j],\u000a                                 d[i-2, j-2] + cost   // transposition\u000a                              )\u000a</syntaxhighlight>\u000a\u000a===Distance with adjacent transpositions===\u000aHere is the second algorithm that computes the true Damerau\u2013Levenshtein distance with adjacent transpositions (ActionScript 3.0); this function requires as an additional parameter the size of the alphabet (''C''), so that all entries of the arrays are in 0..(''C''&minus;1):\u000a\u000a<syntaxhighlight lang='actionscript3'>static public function damerauLevenshteinDistance(a:Array, b:Array, C:uint):uint\u000a{\u000a    // "infinite" distance is just the max possible distance\u000a    var INF:uint = a.length + b.length;\u000a\u000a    // make and initialize the character array indices            \u000a    var DA:Array = new Array(C);\u000a    for (var k:uint = 0; k < C; ++k) DA[k]=0;\u000a\u000a    // make the distance matrix H[-1..a.length][-1..b.length]\u000a    var H:matrix = new matrix(a.length+2,b.length+2);\u000a    \u000a    // initialize the left and top edges of H\u000a    H[-1][-1] = INF;\u000a    for (var i:uint = 0; i <= a.length; ++i)\u000a    {\u000a        H[i][-1] = INF;\u000a        H[i][ 0] = i;\u000a    }\u000a    for (var j:uint = 0; j <= b.length; ++j)\u000a    {\u000a        H[-1][j] = INF;\u000a        H[ 0][j] = j;\u000a    }\u000a\u000a    // fill in the distance matrix H\u000a    // look at each character in a\u000a    for (var i:uint = 1; i <= a.length; ++i)\u000a    {\u000a        var DB:uint = 0;\u000a        // look at each character in b\u000a        for (var j:uint = 1; j <= b.length; ++j)\u000a        {\u000a            var i1:uint = DA[b[j-1]];\u000a            var j1:uint = DB;\u000a            var cost:uint;\u000a            if (a[i-1] == b[j-1])\u000a               {\u000a                 cost = 0;\u000a                 DB   = j;\u000a               }\u000a            else\u000a               cost = 1;\u000a            H[i][j] = Math.min(    H[i-1 ][j-1 ] + cost,  // substitution\u000a                                   H[i   ][j-1 ] + 1,     // insertion\u000a                                   H[i-1 ][j   ] + 1,     // deletion\u000a                                   H[i1-1][j1-1] + (i-i1-1) + 1 + (j-j1-1));\u000a        }\u000a        DA[a[i-1]] = i;\u000a    }\u000a    return H[a.length][b.length];\u000a}\u000a</syntaxhighlight>\u000a\u000a:<small>'''Note''': the algorithm given in the paper uses alphabet 1..C rather than the 0..''C''&minus;1 used here; the paper indexes arrays: H[&minus;1..|A|,&minus;1..|B|] and DA[1..C]; here DA[0..C&minus;1] is used; the paper seems to be missing the necessary line H[&minus;1,&minus;1]&nbsp;=&nbsp;INF</small>\u000a\u000aTo devise a proper algorithm to calculate unrestricted Damerau\u2013Levenshtein distance note that there always exists an optimal sequence of edit operations, where once-transposed letters are never modified afterwards. (This holds as long as the cost of a transposition, <math>W_T</math>, is at least the average of the cost of an insertion and deletion, i.e., <math>2W_T \u005cge W_I+W_D</math>.<ref name="LW75"/>) Thus, we need to consider only two symmetric ways of modifying a substring more than once: (1) transpose letters and insert an arbitrary number of characters between them, or (2) delete a sequence of characters and transpose letters that become adjacent after deletion. The straightforward implementation of this idea gives an algorithm of cubic complexity: <math>O\u005cleft (M \u005ccdot N \u005ccdot \u005cmax(M, N) \u005cright )</math>, where ''M'' and ''N'' are string lengths. Using the ideas of Lowrance and Wagner,<ref name="LW75"/> this naive algorithm can be improved to be <math>O\u005cleft (M \u005ccdot N \u005cright)</math> in the worst case.\u000a\u000aIt is interesting that the [[bitap algorithm]] can be modified to process transposition. See the information retrieval section of{{ref|itman}} for an example of such an adaptation.\u000a\u000a== Applications ==\u000aDamerau\u2013Levenshtein distance plays an important role in [[natural language processing]]. In natural languages, strings are short and the number of errors (misspellings) rarely exceeds 2. In such circumstances, restricted and real edit distance differ very rarely. Oommen and Loke{{ref|OO}} even mitigated the limitation of the restricted edit distance by introducing ''generalized transpositions''. Nevertheless, one must remember that the restricted edit distance usually does not satisfy the [[triangle inequality]] and, thus, cannot be used with [[metric tree]]s.\u000a\u000a=== DNA ===\u000aSince [[DNA]] frequently undergoes insertions, deletions, substitutions, and transpositions, and each of these operations occurs on approximately the same timescale, the Damerau\u2013Levenshtein distance is an appropriate metric of the variation between two strands of DNA. More common in DNA, protein, and other bioinformatics related alignment tasks is the use of closely related algorithms such as [[Needleman\u2013Wunsch algorithm]] or [[Smith\u2013Waterman algorithm]].\u000a\u000a=== Fraud detection ===\u000aThe algorithm can be used with any set of words, like vendor names. Since entry is manual by nature there is a risk of entering a false vendor. A fraudster employee may enter one real vendor such as "Rich Heir Estate Services" versus a false vendor "Rich Hier State Services". The fraudster would then create a false bank account and have the company route checks to the real vendor and false vendor. The Damerau\u2013Levenshtein algorithm will detect the transposed and dropped letter and bring attention of the items to a fraud examiner.\u000a\u000a== See also ==\u000a* [[Approximate string matching]]\u000a* [[Levenshtein automata]]\u000a* [[Typosquatting]]\u000a\u000a== References ==\u000a{{Reflist|30em}}\u000a\u000a== Further reading ==\u000a* {{Citation |first=Gonzalo |last=Navarro |title=A guided tour to approximate string matching |journal=ACM Computing Surveys |volume=33 |issue=1 |pages=31\u201388 |date=March 2001 |doi=10.1145/375360.375365 }}\u000a\u000a{{DEFAULTSORT:Damerau-Levenshtein Distance}}\u000a[[Category:String similarity measures]]\u000a[[Category:Information theory]]\u000a[[Category:Dynamic programming]]
p226
sg6
VDamerau\u2013Levenshtein distance
p227
ssI84
(dp228
g2
S'http://en.wikipedia.org/wiki/Queries per second'
p229
sg4
S"{{refimprove|date=February 2010}}\n'''Queries Per Second''' (QPS) is a common measure of the amount of search traffic an [[information retrieval]] system, such as a [[search engine]] or a [[database]], receives during one second.<ref>[http://www.microsoft.com/enterprisesearch/en/us/search-glossary.aspx#Q Microsoft's search glossary]</ref><ref>[http://www.answers.com/topic/qps QPS definition at answers.com]</ref>\n\nHigh-traffic systems must watch their QPS in order to know when to scale the system to handle more load.\n\n== References ==\n{{reflist}}\n\n[[Category:Units of measurement]]\n[[Category:Information retrieval]]\n\n{{computer-stub}}"
p230
sg6
S'Queries per second'
p231
ssI213
(dp232
g2
S'http://en.wikipedia.org/wiki/Hellinger distance'
p233
sg4
VIn [[probability theory|probability]] and [[mathematical statistics|statistics]], the '''Hellinger distance''' (also called [[Bhattacharyya distance]] as this was originally introduced by [[Anil Kumar Bhattacharya]]) is used to quantify the similarity between two [[probability distributions]]. It is a type of [[f-divergence|''f''-divergence]].  The Hellinger distance is defined in terms of the [[Hellinger integral]], which was introduced by [[Ernst Hellinger]] in 1909.<ref>{{SpringerEOM|title=Hellinger distance|id=h/h046890|first=M.S. |last=Nikulin}}</ref><ref>{{Citation \u000a| last = Hellinger \u000a| first = Ernst\u000a| author-link = Ernst Hellinger\u000a| title = Neue Begründung der Theorie quadratischer Formen von unendlichvielen Veränderlichen \u000a| url = http://resolver.sub.uni-goettingen.de/purl?GDZPPN002166941 \u000a| year = 1909 \u000a| journal = [[Journal für die reine und angewandte Mathematik]]\u000a| language = German\u000a| volume = 136 \u000a| pages = 210\u2013271\u000a| jfm = 40.0393.01\u000a| doi=10.1515/crll.1909.136.210\u000a}}</ref>\u000a\u000a==Definition==\u000a\u000a===Measure theory===\u000aTo define the Hellinger distance in terms of [[measure theory]], let ''P'' and ''Q'' denote two [[probability measure]]s that are [[absolute continuity|absolutely continuous]] with respect to a third probability measure &lambda;.  The square of the Hellinger distance between ''P'' and ''Q'' is defined as the quantity\u000a\u000a:<math>H^2(P,Q) = \u005cfrac{1}{2}\u005cdisplaystyle \u005cint \u005cleft(\u005csqrt{\u005cfrac{dP}{d\u005clambda}} - \u005csqrt{\u005cfrac{dQ}{d\u005clambda}}\u005cright)^2 d\u005clambda. </math>\u000a\u000aHere, ''dP''&nbsp;/&nbsp;''d&lambda;'' and ''dQ''&nbsp;/&nbsp;''d''&lambda; are the [[Radon\u2013Nikodym derivative]]s of ''P'' and ''Q'' respectively.  This definition does not depend on &lambda;, so the Hellinger distance between ''P'' and ''Q'' does not change if &lambda; is replaced with a different probability measure with respect to which both  ''P'' and ''Q'' are absolutely continuous.  For compactness, the above formula is often written as\u000a\u000a:<math>H^2(P,Q) = \u005cfrac{1}{2}\u005cint \u005cleft(\u005csqrt{dP} - \u005csqrt{dQ}\u005cright)^2. </math>\u000a\u000a===Probability theory using Lebesgue measure===\u000aTo define the Hellinger distance in terms of elementary probability theory, we take &lambda; to be [[Lebesgue measure]], so that ''dP''&nbsp;/&nbsp;''d&lambda;'' and ''dQ''&nbsp;/&nbsp;''d''&lambda; are simply [[probability density function]]s.  If we denote the densities as ''f'' and ''g'', respectively, the squared Hellinger distance can be expressed as a standard calculus integral\u000a\u000a:<math>\u005cfrac{1}{2}\u005cint \u005cleft(\u005csqrt{f(x)} - \u005csqrt{g(x)}\u005cright)^2 dx = 1 - \u005cint \u005csqrt{f(x) g(x)} \u005c, dx,</math>\u000a\u000awhere the second form can be obtained by expanding the square and using the fact that the integral of a probability density over its domain must be one.\u000a\u000aThe Hellinger distance ''H''(''P'',&nbsp;''Q'') satisfies the property (derivable from the [[Cauchy-Schwarz inequality#L2|Cauchy-Schwarz inequality]])\u000a\u000a: <math>0\u005cle H(P,Q) \u005cle 1.</math>\u000a\u000a===Discrete distributions===\u000aFor two discrete probability distributions <math>P=(p_1 \u005cldots p_k)</math> and <math>Q=(q_1 \u005cldots q_k)</math>,\u000atheir Hellinger distance is defined as\u000a\u000a: <math>\u000a  H(P, Q) = \u005cfrac{1}{\u005csqrt{2}} \u005c; \u005csqrt{\u005csum_{i=1}^{k} (\u005csqrt{p_i} - \u005csqrt{q_i})^2},\u000a</math>\u000a\u000awhich is directly related to the [[Euclidean distance|Euclidean norm]] of the difference of the square root vectors, i.e.\u000a: <math>\u000aH(P, Q) = \u005cfrac{1}{\u005csqrt{2}} \u005c; \u005cbigl\u005c|\u005csqrt{P} - \u005csqrt{Q} \u005cbigr\u005c|_2 .\u000a</math>\u000a\u000a== Connection with the statistical distance ==\u000a\u000aThe Hellinger distance <math>H(P,Q)</math> and the [[total variation distance]] (or statistical distance) <math>\u005cdelta(P,Q)</math> are related as follows:<ref>[http://www.tcs.tifr.res.in/~prahladh/teaching/2011-12/comm/lectures/l12.pdf Harsha's lecture notes on communication complexity]</ref>\u000a\u000a: <math>\u000aH^2(P,Q) \u005cleq \u005cdelta(P,Q) \u005cleq \u005csqrt 2 H(P,Q)\u005c,.\u000a</math>\u000a\u000aThese inequalities follow immediately from the inequalities between the [[Lp space#The p-norm in finite dimensions|1-norm]] and the [[Lp space#The p-norm in finite dimensions|2-norm]].\u000a\u000a==Properties==\u000aThe maximum distance 1 is achieved when ''P'' assigns probability zero to every set to which ''Q'' assigns a positive probability, and vice versa.\u000a\u000aSometimes the factor 1/2 in front of the integral is omitted, in which case the Hellinger distance ranges from zero to the square root of two.\u000a\u000aThe Hellinger distance is related to the [[Bhattacharyya distance|Bhattacharyya coefficient]] <math>BC(P,Q)</math> as it can be defined as\u000a\u000a: <math>H(P,Q) = \u005csqrt{1 - BC(P,Q)}.</math>\u000a\u000aHellinger distances are used in the theory of [[sequential analysis|sequential]] and [[asymptotic statistics]].<ref>Erik Torgerson (1991) ''Comparison of Statistical Experiments'', volume 36 of Encyclopedia of Mathematics. Cambridge University Press.\u000a</ref><ref>{{cite book\u000a  | author = Liese, Friedrich and Miescke, Klaus-J.\u000a  | title = Statistical Decision Theory: Estimation, Testing, and Selection\u000a  | year = 2008\u000a  | publisher = Springer\u000a  | isbn = 0-387-73193-8\u000a  }}\u000a</ref>\u000a\u000a==Examples==\u000aThe squared Hellinger distance between two [[normal distribution]]s <math>\u005cscriptstyle P\u005c,\u005csim\u005c,\u005cmathcal{N}(\u005cmu_1,\u005csigma_1^2)</math> and  <math>\u005cscriptstyle Q\u005c,\u005csim\u005c,\u005cmathcal{N}(\u005cmu_2,\u005csigma_2^2)</math> is:\u000a: <math>\u000a  H^2(P, Q) = 1 - \u005csqrt{\u005cfrac{2\u005csigma_1\u005csigma_2}{\u005csigma_1^2+\u005csigma_2^2}} \u005c,  e^{-\u005cfrac{1}{4}\u005cfrac{(\u005cmu_1-\u005cmu_2)^2}{\u005csigma_1^2+\u005csigma_2^2}}.\u000a  </math>\u000a\u000aThe squared Hellinger distance  between two [[exponential distribution]]s <math>\u005cscriptstyle P\u005c,\u005csim \u005c,\u005crm{Exp}(\u005calpha)</math> and <math>\u005cscriptstyle Q\u005c,\u005csim\u005c,\u005crm{Exp}(\u005cbeta)</math> is:\u000a: <math>\u000a  H^2(P, Q) = 1 - \u005cfrac{2 \u005csqrt{\u005calpha \u005cbeta}}{\u005calpha + \u005cbeta}.\u000a  </math>\u000a\u000aThe squared Hellinger distance  between two [[Weibull distribution]]s <math>\u005cscriptstyle P\u005c,\u005csim \u005c,\u005crm{W}(k,\u005calpha)</math> and <math>\u005cscriptstyle Q\u005c,\u005csim\u005c,\u005crm{W}(k,\u005cbeta)</math> (where <math> k </math> is a common shape parameter and <math> \u005calpha\u005c, , \u005cbeta </math> are the scale parameters respectively):\u000a: <math>\u000a  H^2(P, Q) = 1 - \u005cfrac{2 (\u005calpha \u005cbeta)^{k/2}}{\u005calpha^k + \u005cbeta^k}.\u000a  </math>\u000a\u000aThe squared Hellinger distance between two [[Poisson distribution]]s with rate parameters <math>\u005calpha</math> and <math>\u005cbeta</math>, so that <math>\u005cscriptstyle P\u005c,\u005csim \u005c,\u005crm{Poisson}(\u005calpha)</math> and <math>\u005cscriptstyle Q\u005c,\u005csim\u005c,\u005crm{Poisson}(\u005cbeta)</math>, is:\u000a: <math>\u000a  H^2(P,Q) = 1-e^{-\u005cfrac{1}{2}(\u005csqrt{\u005calpha} - \u005csqrt{\u005cbeta})^2}.\u000a  </math>\u000a\u000aThe squared Hellinger distance between two [[Beta distribution]]s <math>\u005cscriptstyle P\u005c,\u005csim\u005c,\u005ctext{Beta}(a_1,b_1)</math> and  <math>\u005cscriptstyle Q\u005c,\u005csim\u005c,\u005ctext{Beta}(a_2, b_2)</math> is:\u000a: <math>\u000aH^{2}(P,Q)	=1-\u005cfrac{B\u005cleft(\u005cfrac{a_{1}+a_{2}}{2},\u005cfrac{b_{1}+b_{2}}{2}\u005cright)}{\u005csqrt{B(a_{1},b_{1})B(a_{2},b_{2})}}\u000a  </math>\u000awhere <math>B</math> is the [[Beta function]].\u000a\u000a==See also==\u000a* [[Kullback Leibler divergence]]\u000a* [[Fisher information metric]]\u000a\u000a==Notes==\u000a{{reflist}}\u000a\u000a==References==\u000a* {{cite book |author=Yang, Grace Lo; Le Cam, Lucien M. |title=Asymptotics in Statistics: Some Basic Concepts |publisher=Springer |location=Berlin |year=2000 |pages= |isbn=0-387-95036-2 |oclc= |doi=}}\u000a* {{cite book |author=Vaart, A. W. van der |title=Asymptotic Statistics (Cambridge Series in Statistical and Probabilistic Mathematics) |publisher=Cambridge University Press |location=Cambridge, UK |year= |pages= |isbn=0-521-78450-6 |oclc= |doi=}}\u000a* {{cite book |author=Pollard, David E. |title=A user's guide to measure theoretic probability |publisher=Cambridge University Press |location=Cambridge, UK |year=2002 |pages= |isbn=0-521-00289-3 |oclc= |doi=}}\u000a\u000a[[Category:Probability theory]]\u000a[[Category:F-divergences]]\u000a[[Category:Statistical distance measures]]\u000a[[Category:String similarity measures]]
p234
sg6
S'Hellinger distance'
p235
ssI87
(dp236
g2
S'http://en.wikipedia.org/wiki/Anchor text'
p237
sg4
S'{{Use dmy dates|date=February 2013}}\nThe \'\'\'anchor text\'\'\', \'\'\'link label\'\'\', \'\'\'link text\'\'\', or \'\'\'link title\'\'\' is the visible, clickable text in a [[hyperlink]]. The words contained in the anchor text can determine the ranking that the page will receive by search engines. Since 1998, some [[web browser]]s have added the ability to show a [[tooltip]] for a hyperlink before it is selected. Not all links have anchor texts because it may be obvious where the link will lead due to the context in which it is used. Anchor texts normally remain below 60 [[Character (computing)|characters]]. Different browsers will display anchor texts differently. Usually, Web Search Engines analyze anchor text from hyperlinks on web pages. Other services apply the basic principles of anchor text analysis as well. For instance, [[List of academic databases and search engines|academic search engines]] may use [[citation]] context to classify [[Academic publishing|academic articles]],<ref>{{cite web|last=Bader Aljaber, Nicola Stokes, James Bailey and Jian Pei|url=http://www.springerlink.com/content/p278617582u5x3x1/|title=Document clustering of scientific texts using citation contexts |date=1 April 2010|publisher=Springer}}</ref> and anchor text from documents linked in [[mind maps]] may be used too.<ref>Needs new reference link</ref> [[File:Anchor text.png|thumb|Visual implementation of anchor text]]\n\n==Overview==\nAnchor text usually gives the user relevant descriptive or contextual information about the content of the link\'s destination. The anchor text may or may not be related to the actual text of the [[Uniform Resource Locator|URL]] of the link. For example, a hyperlink to the [[English Wikipedia|English-language Wikipedia]]\'s [[homepage]] might take this form:\n\n:<code><nowiki><a href="http://en.wikipedia.org/wiki/Main_Page">Wikipedia</a></nowiki></code>\n\nThe anchor text in this example is "Wikipedia"; the longer, but vital, URL <code><nowiki>http://en.wikipedia.org/wiki/Main_Page</nowiki></code> needed to locate the target page, displays on the web page as {{srlink|Main Page|Wikipedia}}, contributing to clean, easy-to-read text.\n\n==Common misunderstanding of the concept==\n\nThis proper method of linking is beneficial to users and [[webmaster]]s as anchor text holds [[significant]] [[weight]] in [[search engine]] rankings. The limit of the [[concept]] is building [[Sentence (linguistics)|sentence]]s only composed with linked [[word]]s.{{citation needed|date=September 2011}}\n\n==Search engine algorithms==\nAnchor text is weighted (ranked) highly in [[search engine]] [[algorithm]]s, because the linked text is usually relevant to the [[landing page]]. The objective of search engines is to provide highly relevant search results; this is where anchor text helps, as the tendency was, more often than not, to hyperlink words relevant to the landing page. Anchor text can also serve the purpose of directing the user to internal pages on the site, which can also help to rank the website higher in the search rankings.<ref name="Search Engine Watch">{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2169750/How-the-Web-Uses-Anchor-Text-in-Internal-Linking-Study|title=\nHow the Web Uses Anchor Text in Internal Linking [Study]|accessdate=6 July 2012}}</ref>\n\n[[Webmaster]]s may use anchor text to procure high results in [[search engine results page]]s. [[Google]]\'s [[Google Webmaster Tools|Webmaster Tools]] facilitate this optimization by letting [[website]] owners view the most common words in anchor text linking to their site.<ref>{{cite web\n|last=Fox\n|first=Vanessa\n|url=http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html\n|title=Get a more complete picture about how other sites link to you\n|date=15 March 2007\n|publisher=Official Google Webmaster Central Blog\n|accessdate=2007-03-27\n| archiveurl= http://web.archive.org/web/20070331195216/http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html| archivedate= 31 March 2007 <!--DASHBot-->| deadurl= no}}</ref>\nIn the past, [[Google bomb]]ing was possible through anchor text manipulation; however, in January 2007, Google announced it had updated its algorithm to minimize the impact of Google bombs, which refers to a prank where people attempt to cause someone else\'s site to rank for an obscure or meaningless query.<ref>{{cite web\n|last=Cutts\n|first=Matt\n|url=http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html\n|title=A quick word about Googlebombs\n|date=25 January 2007\n|publisher=Official Google Webmaster Central Blog\n|accessdate=2007-03-27\n| archiveurl= http://web.archive.org/web/20070324043013/http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html| archivedate= 24 March 2007 <!--DASHBot-->| deadurl= no}}</ref>\n\nIn April 2012, Google announced in its March "Penguin" update that it would be changing the way it handled anchor text, implying that anchor text would no longer be as important an element for their ranking metrics.<ref>{{cite web|url=http://insidesearch.blogspot.co.uk/2012/04/search-quality-highlights-50-changes.html|title=Google\'s March Update|publisher=Google}}</ref><ref>{{cite web|first=Simon|last=Dalley|accessdate=2012-04-04|date=4 April 2012|url=http://www.growtraffic.co.uk/google-changes-the-way-it-handles-anchor-text|title=Google Changes The way It Handles Anchor Text|publisher=Grow Traffic}}</ref> Moving forward, Google would be paying more attention to a diversified link profile which has a mix of anchor text and other types of links.\n.<ref name="Search Engine Watch">{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2172839/Google-Penguin-Update-Impact-of-Anchor-Text-Diversity-Link-Relevancy|title=\nGoogle Penguin Update: Impact of Anchor Text Diversity & Link Relevancy|accessdate=6 July 2012}}</ref>\n\n==Anchor Text Terminology==\nThere are different classifications of anchor text that are used within the search engine optimization community such as the following:\n\n\'\'\'Exact Match:\'\'\' whenever an anchor is used with a keyword that mirrors the page that is being linked to. Example: "[[search engine optimization]]" is an exact match anchor because it\'s linking to a page about "search engine optimization.\n\n\'\'\'Branded:\'\'\' whenever a brand is used as the anchor. "[[Wikipedia]]" is a branded anchor text.\n\n\'\'\'Naked Link:\'\'\' whenever a URL is used as an anchor. "[[www.wikipedia.com]]" is a naked link anchor.\n\n\'\'\'Generic:\'\'\' whenever a generic word or phrase is used as the anchor. "Click here" is a generic anchor. Other variations may include "go here", "visit this website", etc.\n\n\'\'\'Images:\'\'\' whenever an image is linked, Google will use the "ALT" tag as the anchor text\n.<ref>{{cite web\n|last=Gotch\n|first=Nathan\n|url=http://www.gotchseo.com/anchor-text/\n|title=The Epic Guide to Anchor Text\n|date=26 October 2014}}</ref>\n\n==References==\n\n{{reflist|colwidth=30em}}\n\n[[Category:Information retrieval]]\n[[Category:Internet search engines]]\n[[Category:Internet terminology]]\n[[Category:Search engine optimization]]\n[[Category:Hypertext]]'
p238
sg6
S'Anchor text'
p239
ssI216
(dp240
g2
S'http://en.wikipedia.org/wiki/Social Sciences Citation Index'
p241
sg4
VThe '''Social Sciences Citation Index''' ('''SSCI''') is an interdisciplinary  [[citation index]] product of  [[Thomson Reuters]]' Healthcare & Science division. It was developed by the [[Institute for Scientific Information]] (ISI) from the [[Science Citation Index]].\u000a\u000aThis citation database covers some 2,474 of the world's leading [[academic journal|journals]] of [[social sciences]] across more than 50 [[academic discipline|disciplines]].<ref>{{cite web\u000a  | title = Social Sciences Citation Index \u000a  | url = http://scientific.thomson.com/products/ssci/\u000a  | accessdate = 2008-06-11 }}</ref> It is made available online through the [[Web of Science]] service for a fee.  This database product provides information to identify the articles  cited most frequently and by what publisher and author.\u000a\u000a== Criticism ==\u000aIn 2004 economists [[Daniel B. Klein]] and Eric Chiang conducted a survey of the Social Sciences Citation Index and identified a bias against free market oriented research. In addition to an ideological bias, Klein and Chiang also identified several methodological deficiencies that encouraged the over-counting of citations, and they argue that the Social Sciences Citation Index does a poor job reflecting the relevance and accuracy of articles.<ref>Daniel Klein and Eric Chiang. [http://econjwatch.org/articles/the-social-science-citation-index-a-black-box-with-an-ideological-bias The Social Science Citation Index: A Black Box\u2014with an Ideological Bias?] ''Econ Journal Watch'', Volume 1, Number 1, April 2004, pp 134-165.</ref>\u000a\u000a==See also==\u000a* [[Arts and Humanities Citation Index]]\u000a* [[Science Citation Index]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a* [http://thomsonreuters.com/products_services/science/science_products/a-z/social_sciences_citation_index Introduction to SSCI]\u000a\u000a{{Thomson Reuters}}\u000a[[Category:Thomson family]]\u000a[[Category:Thomson Reuters]]\u000a[[Category:Social sciences literature]]\u000a[[Category:Citation indices]]\u000a\u000a{{database-stub}}\u000a{{sci-stub}}
p242
sg6
S'Social Sciences Citation Index'
p243
ssI90
(dp244
g2
S'http://en.wikipedia.org/wiki/Federated search'
p245
sg4
V{{Citations missing|date=June 2008}}\u000a\u000a'''Federated search''' is an [[information retrieval]] technology that allows the simultaneous search of multiple searchable resources.  A user makes a single query request which is distributed to the [[search engine]]s participating in the federation.  The federated search then aggregates the results that are received from the [[search engine]]s for presentation to the user.\u000a\u000a==Purpose==\u000aFederated search came about to meet the need of searching multiple  disparate content sources with one query.  This allows a user to search multiple databases at once in real time, arrange the results from the various databases into a useful form and then present the results to the user.\u000a\u000a==Process==\u000aAs described by Peter Jacso (2004<ref>Thoughts About Federated Searching.  Jacsó, Péter, Information Today,  Oct 2004, Vol. 21, Issue 9</ref>), federated searching consists of (1) transforming a [[Web search query|query]] and broadcasting it to a group of disparate databases or other web resources, with the appropriate syntax, (2) merging the results collected from the databases, (3) presenting them in a succinct and unified format with minimal duplication, and (4) providing a means, performed either automatically or by the portal user, to sort the merged result set.\u000a\u000aFederated search portals, either commercial or open access, generally search public access [[bibliographic databases]], public access Web-based library catalogues ([[OPAC]]s), Web-based search engines like [[Google]] and/or open-access, government-operated or corporate data collections. These individual information sources send back to the portal's interface a list of results from the search query. The user can review this hit list.  Some portals will merely [[screen scrape]] the actual database results and not directly allow a user to enter the information source's application. More sophisticated ones will de-dupe the results list by merging and removing duplicates. There are additional features available in many portals, but the basic idea is the same: to improve the accuracy and relevance of individual searches as well as reduce the amount of time required to search for resources.\u000a\u000aThis process allows federated search some key advantages when compared with existing crawler-based search engines.  Federated search need not place any requirements or burdens on owners of the individual information sources, other than handling increased traffic.  Federated searches are inherently as current as the individual information sources, as they are searched in real time.\u000a\u000a==Implementation==\u000a[[File:Fed_search.png|thumb|alt=federated search engine|Federating across three search engines]]\u000a\u000aOne application of federated searching is the [[metasearch engine]]; however, this is not a complete solution as many documents are not currently indexed. These documents are on what is known as the [[deep Web]], or invisible Web. Many more information sources are not yet stored in electronic form. [[Google Scholar]] is one example of many projects trying to address this.\u000a\u000aWhen the search vocabulary or [[data model]] of the search system is different from the data model of one or more of the foreign target systems the query must be translated into each of the foreign target systems.  This can be done using simple data-element translation or may require [[semantic translation]].\u000a\u000aA challenge faced in the implementation of federated search engines is scalability, in other words, the performance of the site as the number of information sources comprising the federated search engine increase. One federated search engine that has begun to address this issue is [[WorldWideScience]], hosted by the [[U.S. Department of Energy]]'s [[Office of Scientific and Technical Information]].  WorldWideScience <ref>[http://www.worldwidescience.org WorldWideScience]</ref> is composed of more than 40 information sources, several of which are federated search portals themselves.  One such portal is Science.gov <ref name="Science.gov">[http://www.science.gov Science.gov]</ref> which itself federates more than 30 information sources representing most of the R&D output of the U.S. Federal government.  Science.gov returns its highest ranked results to WorldWideScience, which then merges and ranks these results with the search returned by the other information sources that comprise WorldWideScience.<ref name="Science.gov"/> This approach of cascaded federated search enables large number of information sources to be searched via a single query.\u000a\u000aAnother application [[Sesam]] running in both Norway and Sweden has been built on top of an open sourced platform specialised for federated search solutions. Sesat,<ref>[http://sesat.no Sesat]</ref> an acronym for [[Sesam Search Application Toolkit]], is a platform that provides much of the framework and functionality required for handling parallel and pipelined searches and displaying them elegantly in a user interface, allowing engineers to focus on the index/database configuration tuning.\u000a\u000a==Challenges==\u000a\u000aWhen federated search is performed against secure data sources, the users' credentials must be passed on\u000ato each underlying search engine, so that appropriate security is maintained.  If the user has different\u000alogin credentials for different systems, there must be a means to map their login ID to each search\u000aengine's security domain.<ref>[http://www.ideaeng.com/tabId/98/itemId/124/Mapping-Security-Requirements-to-Enterprise-Search.aspx Mapping Security Requirements to Enterprise Search]</ref>\u000a\u000aAnother challenge is mapping results list navigators into a common form.  Suppose 3 real-estate sites are searched, each provides a list of hyperlinked city names to click on, to see matches only in each city.  Ideally these facets would be combined into one set, but that presents additional technical challenges.<ref>[http://www.ideaeng.com/tabId/98/itemId/154/20-Differences-Between-Internet-vs-Enterprise-Se.aspx#fed_facets 20+ Differences Between Internet vs. Enterprise Search - part 1]</ref>  The system also needs to understand "next page" links if it's going to allow the user to page through the combined results.\u000a\u000a==Further reading==\u000a*[http://www.libraryjournal.com/article/CA6571320.html Federated Search 101. Linoski, Alexis, Walczyk, Tine, Library Journal, Summer 2008 Net Connect, Vol. 133]{{Dead link|date=November 2010}} Note: this content has been moved [http://www.accessmylibrary.com/article-1G1-182034526/federated-search-101-alexis.html here], but you will need a remote access account through your local library to get the whole article.\u000a\u000a*Cox, Christopher N. Federated Search: Solution or Setback for Online Library Services. Binghamton, NY: Haworth Information Press, 2007.[http://lccn.loc.gov/2006101753 Table of Contents]\u000a*[http://www.altsearchengines.com/2009/01/11/federated-search-finds-content-that-google-cant-reach-part-i-of-iii/ Federated Search Primer. Lederman, S., AltSearchEngines, January 2009] {{Dead link|date=July 2010}} Note: This material has been reposted [http://deepwebtechblog.com/federated-search-finds-content-that-google-can%E2%80%99t-reach-part-i-of-iii/ here], on the blog of a commercial search engine company.\u000a\u000a* Milad Shokouhi and Luo Si, Federated Search, Foundations and Trends® in Information Retrieval: Vol. 5: No 1, pp 1-102., [http://dx.doi.org/10.1561/1500000010 http://dx.doi.org/10.1561/1500000010]\u000a\u000a==See also==\u000a* [[Search aggregator]]\u000a* [[Deep Web]]\u000a\u000a==References==\u000a{{Reflist}}\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Federated Search}}\u000a[[Category:Information retrieval]]\u000a[[Category:Internet terminology]]\u000a[[Category:Searching]]\u000a[[Category:Internet search algorithms]]\u000a[[Category:Applications of distributed computing]]
p246
sg6
S'Federated search'
p247
ssI219
(dp248
g2
S'http://en.wikipedia.org/wiki/Science Citation Index'
p249
sg4
V{{incomplete|date=January 2014}}\u000a{{ infobox bibliographic database\u000a| title = Science Citation Index\u000a| image = \u000a| caption = \u000a| producer = [[Thomson Reuters]]\u000a| country = United States\u000a| history = 1964-present\u000a| languages = \u000a| providers = \u000a| cost = \u000a| disciplines = Science, medicine, and technology\u000a| depth = \u000a| formats = \u000a| temporal = \u000a| geospatial = \u000a| number = \u000a| updates = \u000a| p_title = \u000a| p_dates = \u000a| ISSN = 0036-827X\u000a| web = http://thomsonreuters.com/science-citation-index-expanded/\u000a| titles = \u000a}}\u000aThe '''Science Citation Index''' ('''SCI''') is a [[citation index]] originally produced by the [[Institute for Scientific Information]] (ISI) and created by [[Eugene Garfield]]. It was officially launched in 1964. It is now owned by [[Thomson Reuters]].<ref name=dimension>\u000a{{cite journal\u000a|doi=10.1126/science.122.3159.108\u000a|title=Citation Indexes for Science: A New Dimension in Documentation through Association of Ideas\u000a|url=http://ije.oxfordjournals.org/content/35/5/1123.full\u000a|format=Free web article download\u000a|year=1955\u000a|last1=Garfield\u000a|first1=E.\u000a|journal=Science\u000a|volume=122\u000a|issue=3159\u000a|pages=108\u201311\u000a|pmid=14385826|bibcode=1955Sci...122..108G\u000a}}</ref><ref name=evolve>\u000a{{cite journal\u000a |last = Garfield \u000a |first = Eugene\u000a |doi=10.2436/20.1501.01.10\u000a |url=http://garfield.library.upenn.edu/papers/barcelona2007a.pdf\u000a |format=Free PDF download\u000a |title=The evolution of the Science Citation Index|doi_brokendate = 2015-01-21\u000a }} International microbiology '''10.'''1 (2010): 65-69.</ref><ref name=gOverview>\u000a{{cite web\u000a | last = Garfield \u000a | first = Eugene\u000a | authorlink =\u000a | coauthors =\u000a | title = Science Citation Index\u000a | work = Science Citation Index 1961\u000a | publisher = Garfield Library - UPenn\u000a | date = 1963\u000a | url = http://garfield.library.upenn.edu/papers/80.pdf\u000a | format = Free PDF download\u000a | doi =\u000a | accessdate = 2013-05-27}} \u000a* Originally published by the Institute of Scientific Information in 1964\u000a* Other titles in this document are: What is a Citation Index? , How is the Citation Index Prepared? , How is the Citation Index Used? , Applications of the Science Citation Index , Source Coverage and Statistics , and a Glossary.</ref><ref name=history-cite-indexing>\u000a{{cite web\u000a | title =History of Citation Indexing \u000a | work =Needs of researchers create demand for citation indexing \u000a | publisher =Thomson Ruters \u000a | date =November 2010 \u000a | url =http://thomsonreuters.com/products_services/science/free/essays/history_of_citation_indexing/ \u000a | format =Free HTML download \u000a | accessdate =2010-11-04}}</ref> The larger version ('''Science Citation Index Expanded''') covers more than 6,500 notable and significant [[Scientific journal|journals]], across 150 disciplines, from 1900 to the present. These are alternately described as the world's leading journals of [[science]] and [[technology]], because of a rigorous selection process.{{citation needed|date=August 2013}}<ref name=Expanded>\u000a{{cite web \u000a|url=http://thomsonreuters.com/products_services/science/science_products/a-z/science_citation_index_expanded/ \u000a|title=Science Citation Index Expanded \u000a|work= |accessdate=2009-08-30}}</ref>\u000a<ref name=wetland>{{cite journal| doi= 10.1007/s12665-012-2193-y|title= The Top-cited Wetland Articles in Science Citation Index Expanded: characteristics and hotspots|url=http://dns2.asia.edu.tw/~ysho/YSHO-English/Publications/PDF/Env%20Ear%20Sci-Ma.pdf|date= December 2012| last1= Ma| first1= Jiupeng| last2= Fu| first2= Hui-Zhen| last3= Ho| first3= Yuh-Shan| journal= Environmental Earth Sciences|volume= 70|issue= 3|pages= 1039}} (Springer-Verlag)</ref><ref name=shan>\u000a{{cite journal \u000a| doi= 10.1007/s11192-012-0837-z \u000a|title= The top-cited research works in the Science Citation Index Expanded \u000a|url= http://trend.asia.edu.tw/Publications/PDF/Scientometrics94,%201297.pdf \u000a| year= 2012 \u000a| last1= Ho \u000a| first1= Yuh-Shan \u000a| journal= Scientometrics \u000a| volume= 94 \u000a| issue= 3 \u000a| page= 1297}}</ref>\u000a\u000aThe index is made available online through different platforms, such as the [[Web of Science]]<ref name=AtoZ>{{cite web |last=ISI Web of Knowledge platform |title =Available databases A to Z |publisher=Thomson Reuters |year=2010 |url=http://wokinfo.com/products_tools/products/ |format=Choose databases on method of discovery and analysis |accessdate=2010-06-24}}</ref><ref>[http://wokinfo.com/media/pdf/SSR1103443WoK5-2_web3.pdf Thomson Reuters Web of Knowledge. Thomson Reuters, 2013.]</ref> and SciSearch.<ref>{{cite web |url=http://library.dialog.com/bluesheets/html/bl0034.html |title=SCISEARCH - A CITED REFERENCE SCIENCE DATABASE |publisher=Library.dialog.com |date= |accessdate=2014-04-17}}</ref> (There are also CD and printed editions, covering a smaller number of journals). This database allows a researcher to identify which later articles have cited any particular earlier article, or have cited the articles of any particular author, or have been cited most frequently. Thomson Reuters also markets several subsets of this database, termed "Specialty Citation Indexes",<ref name=SpCI>\u000a{{cite web \u000a|url=http://thomsonreuters.com/products_services/science/science_products/a-z/specialty_citation_indexes/ \u000a|title=Specialty Citation Indexes \u000a|work= |accessdate=2009-08-30}}</ref> \u000asuch as the '''Neuroscience Citation Index'''<ref name=NCI>\u000a{{cite web \u000a|url=http://science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=MD \u000a|title=Journal Search - Science - |work= |accessdate=2009-08-30}}</ref> and the '''Chemistry Citation Index'''.<ref>{{cite web |url=http://science.thomsonreuters.com/cgi-bin/jrnlst/jloptions.cgi?PC=CD \u000a|title=Journal Search - Science - Thomson Reuters |accessdate=14 January 2011}}</ref>\u000a\u000a==Chemistry Citation Index==\u000a\u000aThe Chemistry Citation Index was first introduced by Eugene Garfield, a chemist. His original "search examples were based on [his] experience as a chemist".<ref name=Garcci/> In 1992 an electronic and print form of the index was derived from a core of 330 chemistry journals, within which all areas were covered. Additional information was provided from articles selected from 4,000 other journals. All chemistry subdisciplines were covered: organic, inorganic, analytical, physical chemistry, polymer, computational, organometallic, materials chemistry, and electrochemistry.<ref name=Garcci>Garfield, Eugene. "[http://garfield.library.upenn.edu/essays/v15p007y1992-93.pdf New Chemistry Citation Index On CD-ROM Comes With Abstracts, Related Records, and Key-Words-Plus]." Current Contents 3 (1992): 5-9.</ref>\u000a\u000aBy 2002 the core journal coverage increased to 500 and related article coverage increased to 8,000 other journals.<ref>\u000a[http://www.chinweb.com/cgi-bin/chemport/getfiler.cgi?ID=k4l7vyYF5FimYvScsOm3pxWVmEhBoH0ZuYgxjLdKBfqdmURDHLrjuVv78i16JLPX&VER=E Chemistry Citation Index]. Institute of Process Engineering of the Chinese Academy of Sciences. 2003.</ref>\u000a\u000aOne 1980 study reported the overall citation indexing benefits for chemistry, examining the use of citations as a tool for the study of the sociology of chemistry and illustrating the use of citation data to "observe" chemistry subfields over time.<ref>\u000a{{cite journal\u000a|doi=10.1007/BF02016348\u000a|title=Science citation index and chemistry\u000a|year=1980\u000a|last1=Dewitt\u000a|first1=T. W.\u000a|last2=Nicholson\u000a|first2=R. S.\u000a|last3=Wilson\u000a|first3=M. K.\u000a|journal=Scientometrics\u000a|volume=2\u000a|issue=4\u000a|page=265}}</ref>\u000a\u000a==See also==\u000a* [[Arts and Humanities Citation Index]], which covers 1130 journals, beginning with 1975.\u000a* [[Impact factor]]\u000a* [[List of academic databases and search engines]]\u000a* [[Social Sciences Citation Index]], which covers 1700 journals, beginning with 1956.\u000a\u000a== References ==\u000a{{Reflist|2}}\u000a\u000a==Further reading==\u000a*{{cite journal\u000a|doi= 10.1002/aris.1440360102\u000a|url= http://polaris.gseis.ucla.edu/cborgman/pubs/borgmanfurnerarist2002.pdf\u000a|title=Scholarly Communication and Bibliometrics\u000a|year= 2005\u000a|last1= Borgman\u000a|first1= Christine L.\u000a|last2= Furner\u000a|first2= Jonathan\u000a|journal= Annual Review of Information Science and Technology\u000a|volume= 36\u000a|issue= 1 \u000a|pages=3\u201372}}\u000a\u000a*{{cite journal\u000a|doi= 10.1002/asi.20677\u000a|url= http://staff.aub.edu.lb/~lmeho/meho-yang-impact-of-data-sources.pdf\u000a|format= Free PDF download\u000a|title= Impact of data sources on citation counts and rankings of LIS faculty: Web of science versus scopus and google scholar\u000a|year= 2007\u000a|last1= Meho\u000a|first1= Lokman I.\u000a|last2= Yang\u000a|first2= Kiduk\u000a|journal= Journal of the American Society for Information Science and Technology\u000a|volume= 58\u000a|issue= 13\u000a|page= 2105}}\u000a\u000a*{{cite journal\u000a|doi= 10.1002/asi.5090140304\u000a|url= http://www.garfield.library.upenn.edu/essays/v6p492y1983.pdf\u000a|format= Free PDF download\u000a|title= New factors in the evaluation of scientific literature through citation indexing\u000a|year= 1963\u000a|last1= Garfield\u000a|first1= E.\u000a|last2= Sher\u000a|first2= I. H.\u000a|journal= American Documentation\u000a|volume= 14\u000a|issue= 3\u000a|page= 195}}\u000a\u000a*{{cite journal\u000a|doi= 10.1038/227669a0\u000a|url= http://www.garfield.library.upenn.edu/essays/V1p133y1962-73.pdf\u000a|format= Free PDF download\u000a|title= Citation Indexing for Studying Science\u000a|year= 1970\u000a|last1= Garfield\u000a|first1= E.\u000a|journal= Nature\u000a|volume= 227\u000a|issue= 5259\u000a|pages= 669\u201371\u000a|pmid= 4914589|bibcode= 1970Natur.227..669G\u000a}}\u000a\u000a*{{cite book\u000a | last =Garfield\u000a | first =Eugene \u000a | authorlink =\u000a | title =Citation Indexing: Its Theory and Application in Science, Technology, and Humanities\u000a | publisher =Wiley-Interscience\u000a | series = Information Sciences Series\u000a | edition = 1st\u000a | origyear = 1979| year = 1983\u000a | location = New York\u000a | isbn =9780894950247}}\u000a\u000a==External links==\u000a* [http://scientific.thomson.com/products/wos/ Introduction to SCI]\u000a* [http://science.thomsonreuters.com/mjl/ Master journal list]\u000a* [https://en.wikibooks.org/wiki/Chemical_Information_Sources/Author_and_Citation_Searches Chemical Information Sources/ Author and Citation Searches]. on WikiBooks. \u000a* [http://scientific.thomson.com/tutorials/citedreference/crs1.htm Cited Reference Searching: An Introduction]. Thomson Reuters. \u000a* [http://www.chinweb.com/cgi-bin/chemport/getfiler.cgi?ID=k4l7vyYF5FimYvScsOm3pxWVmEhBoH0ZuYgxjLdKBfqdmURDHLrjuVv78i16JLPX&VER=E Chemistry Citation Index]. Chinweb.\u000a\u000a{{Thomson Reuters}}\u000a\u000a[[Category:Online databases]]\u000a[[Category:Citation indices]]\u000a[[Category:Thomson Reuters]]
p250
sg6
S'Science Citation Index'
p251
ssI93
(dp252
g2
S'http://en.wikipedia.org/wiki/Category:Music search engines'
p253
sg4
S'[[Category:Information retrieval]]\n[[Category:Music software|Search engines]]\n[[Category:Internet search engines]]\n[[Category:Online music and lyrics databases]]'
p254
sg6
S'Category:Music search engines'
p255
ssI222
(dp256
g2
S'http://en.wikipedia.org/wiki/Materials Science Citation Index'
p257
sg4
S'{{Third-party|date=February 2013}}\n\'\'\'The Materials Science Citation Index\'\'\' is a [[citation index]], established in 1992, by [[Thomson ISI]] ([[Thomson Reuters]]). Its overall focus is [[citation|cited reference]] searching of the notable and significant [[science journal|journal literature]] in [[materials science]]. The database makes accessible the various [[physical properties|properties]], behaviors, and materials in the materials science discipline. This then encompasses [[applied physics]], [[ceramic engineering|ceramics]], [[Advanced composite materials (science & engineering)|composite materials]], [[metals]] and [[metallurgy]], [[polymer engineering]], [[semiconductors]], [[thin films]], [[biomaterial]]s, [[Dentistry|dental technology]], as well as [[optics]]. The [[database]] indexes relevant materials science information from over 6,000 [[scientific journal]]s that are part of the ISI database which is [[multidisciplinary]]. Author abstracts are searchable, which links articles sharing one or more [[bibliographic]] references. The database also allows a researcher to use an appropriate (or related to research) article as a base to search forward in time to discover more recently published articles that cite it.<ref name=msci-est>Pemberton, Julia K. "\'\'Two new databases from ISI\'\'." CD-ROM Professional 5.4 (1992): 107+. General OneFile. Web. 20 June 2010.</ref>\n\n\'\'Materials Science Citation Index\'\' lists 625 high impact journals, and is accessible via the [[Science Citation Index Expanded]] collection of databases.<ref name=msci-jnlList>[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=MS Materials Science Citation Index journal list]. Thomson Reuters. July 2010.</ref>\n\n==Editions==\nCoverage of Materials science is accomplished with the following editions:<ref name=MS-indexes>[http://science.thomsonreuters.com/mjl/scope/scope_scie/ Scope Notes]. Science Citation Index, Science Citation Index Expanded. Thomson Reuters. 2010.</ref><ref>[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=D Subject categories]. Science Citation Index Expanded. Thomson Reuters. 2010</ref>\n*Materials Science, Ceramics\n*Materials Science, Characterization & Testing\n*Materials Science, Biomaterials\n*Materials Science, Coatings & Films\n*Materials Science, Composites\n*Materials Science, Paper & Wood\n*Materials Science, Multidisciplinary\n*Materials Science, Textiles\n\n==See also==\n* [[Science Citation Index]]\n* [[Academic publishing]]\n* [[List of academic databases and search engines]]\n* [[Social Sciences Citation Index]], which covers over 1500 journals, beginning with 1956\n* [[Arts and Humanities Citation Index]], which covers over 1000 journals, beginning with 1975\n* [[Impact factor]]\n* [[VINITI Database RAS]]\n\n==References==\n{{Reflist}}\n\n{{Thomson Reuters}}\n\n[[Category:Thomson family]]\n[[Category:Bibliographic databases]]\n[[Category:Online databases]]\n[[Category:Citation indices]]\n\n\n{{science-journal-stub}}'
p258
sg6
S'Materials Science Citation Index'
p259
ssI96
(dp260
g2
S'http://en.wikipedia.org/wiki/Multi-document summarization'
p261
sg4
V'''Multi-document summarization''' is an automatic procedure aimed at [[information extraction|extraction of information]] from multiple texts written about the same topic. Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the [[news aggregators]] performing the next step down the road of coping with [[information overload]].\u000a\u000a==Key benefits==\u000aMulti-[[document summarization]] creates information reports that are both concise and comprehensive.\u000aWith different opinions being put together & outlined, every topic is described from multiple perspectives within a single document.\u000aWhile the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.\u000aAutomatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.\u000a\u000a==Technological challenges==\u000aThe multi-document summarization task has turned out to be much more complex than [[automatic summarization|summarizing a single document]], even a very large one. This difficulty arises from inevitable thematic diversity within a large set of documents. A good summarization technology aims to combine the main themes with completeness, readability, and conciseness. Document Understanding Conferences,<ref>http://www-nlpir.nist.gov/projects/duc/index.html</ref> conducted annually by [[NIST]], have developed sophisticated evaluation criteria for techniques accepting the multi-document summarization challenge.\u000a\u000aAn ideal multi-document summarization system does not simply shorten the source texts but presents information organized around the key aspects to represent a wider diversity of views on the topic. When such quality is achieved, an automatic multi-document summary is perceived more like an overview of a given topic. The latter implies that such text compilations should also meet other basic requirements for an overview text compiled by a human. The multi-document summary quality criteria are as follows:\u000a*clear structure, including an outline of the main content, from which it is easy to navigate to the full text sections\u000a*text within sections is divided into meaningful paragraphs\u000a*gradual transition from more general to more specific thematic aspects\u000a*good [[readability]]\u000a\u000aThe latter point deserves additional note - special care is taken in order to ensure that the automatic overview shows:\u000a*no paper-unrelated "[[communication noise|information noise]]" from the respective documents (e.g., web pages)\u000a*no dangling references to what is not mentioned or explained in the overview\u000a*no text breaks across a sentence\u000a*no semantic [[Redundancy (information theory)|redundancy]].\u000a\u000a==Real-life systems==\u000aThe multi-document summarization technology is now coming of age - a view supported by a choice of advanced web-based systems that are currently available.\u000a* Ultimate Research Assistant<ref>http://ultimate-research-assistant.com/</ref> - performs text mining on Internet search results to help summarize and organize them and make it easier for the user to perform online research. Specific text mining techniques used by the tool include concept extraction, text summarization, hierarchical concept clustering (e.g., automated taxonomy generation), and various visualization techniques, including tag clouds and mind maps. \u000a* iResearch Reporter<ref>http://www.iresearch-reporter.com/</ref> - Commercial Text Extraction and Text Summarization system, free demo site accepts user-entered query, passes it on to Google search engine, retrieves multiple relevant documents, produces categorized, easily  readable natural language summary reports covering multiple documents in retrieved set, all extracts linked to original documents on the Web, post-processing, entity extraction, event and relationship extraction, text extraction, extract clustering, linguistic analysis, multi-document, full text, natural language processing, categorization rules, clustering, linguistic analysis, text summary construction tool set.\u000a* Newsblaster<ref>http://newsblaster.cs.columbia.edu</ref> is a system that helps users find news that is of the most interest to them. The system automatically collects, clusters, categorizes, and summarizes news from several sites on the web ([[CNN]], [[Reuters]], [[Fox News]], etc.) on a daily basis, and it provides users an interface to browse the results.\u000a* NewsInEssence<ref>http://www.newsinessence.com</ref> may be used to retrieve and summarize a cluster of articles from the web. It can start from a [[Uniform Resource Locator|URL]] and retrieve documents that are similar, or it can retrieve documents that match a given set of keywords. NewsInEssence also downloads news articles daily and produces news clusters from them.\u000a* NewsFeed Researcher<ref>http://newsfeedresearcher.com</ref> is a news portal performing continuous [[automatic summarization]] of documents initially clustered by the [[news aggregators]] (e.g., [[Google News]]). NewsFeed Researcher is backed by a free online engine covering major events related to business, technology, U.S. and international news. This tool is also available in on-demand mode allowing a user to build a summaries on selected topics.\u000a* Scrape This<ref>http://www.scrapethis.com</ref> is like a search engine, but instead of providing links to the most relevant websites based on a query, it scrapes the pertinent information off of the relevant websites and provides the user with a consolidated multi-document summary, along with dictionary definitions, images, and videos.\u000a* JistWeb<ref>http://www.jastatechnologies.com/productList.html</ref> is a query specific multiple document summariser.\u000a\u000aAs auto-generated multi-document summaries increasingly resemble the overviews written by a human, their use of extracted text snippets may one day face [[copyright]] issues in relation to the [[fair use]] copyright concept.\u000a\u000a==Bibliography==\u000a* Günes Erkan and Dragomir R. Radev. Lexrank: Graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR), 2004. [http://clair.si.umich.edu/~radev/papers/lprj.pdf]\u000a* Dragomir R. Radev, Hongyan Jing, Malgorzata Sty\u015b, and Daniel Tam. Centroid-based summarization of multiple documents. Information Processing and Management, 40:919\u2013938, December 2004. [http://clair.si.umich.edu/~radev/papers/centroid.pdf]\u000a* Kathleen R. McKeown and Dragomir R. Radev. Generating summaries of multiple news articles. In Proceedings, ACM Conference on Research and Development in Information Retrieval SIGIR'95, pages 74\u201382, Seattle, Washington, July 1995. [http://clair.si.umich.edu/~radev/papers/sigir95.pdf]\u000a* C.-Y. Lin, E. Hovy, "From single to multi-document summarization: A prototype system and its evaluation", In "Proceedings of the ACL", pp.&nbsp;457\u2013464, 2002\u000a*Kathleen McKeown, Rebecca J. Passonneau, David K. Elson, Ani Nenkova, Julia Hirschberg, "Do Summaries Help? A Task-Based Evaluation of Multi-Document Summarization", SIGIR\u201905, Salvador, Brazil, August 15\u201319, 2005 [http://www.cs.columbia.edu/~ani/papers/f98-mckeown.pdf]\u000a*R. Barzilay, N. Elhadad, K. R. McKeown, "Inferring strategies for sentence ordering in multidocument news summarization", Journal of Artificial Intelligence Research, v. 17, pp.&nbsp;35\u201355, 2002\u000a*M. Soubbotin, S. Soubbotin, "Trade-Off Between Factors Influencing Quality of the Summary", Document Understanding Workshop (DUC), Vancouver, B.C., Canada, October 9\u201310, 2005 [http://duc.nist.gov/pubs/2005papers/freetext.sergei.pdf]\u000a* C Ravindranath Chowdary, and P. Sreenivasa Kumar. "Esum: an efficient system for query-specific multi-document summarization." In ECIR (Advances in Information Retrieval), pp.&nbsp;724\u2013728. Springer Berlin Heidelberg, 2009.\u000a\u000a==See also==\u000a* [[Automatic summarization]]\u000a* [[Text mining]]\u000a* [[News aggregators]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a{{External links|date=September 2010}}\u000a*[http://www-nlpir.nist.gov/projects/duc/index.html Document Understanding Conferences]\u000a*[http://www1.cs.columbia.edu/nlp/projects.html Columbia NLP Projects]\u000a*[http://lada.si.umich.edu:8080/clair/nie1/nie.cgi NewsInEssence: Web-based News Summarization]\u000a\u000a{{Natural Language Processing}}\u000a\u000a{{DEFAULTSORT:Multi-Document Summarization}}\u000a[[Category:Natural language processing]]\u000a[[Category:Information retrieval]]
p262
sg6
S'Multi-document summarization'
p263
ssI225
(dp264
g2
S'http://en.wikipedia.org/wiki/Citation index'
p265
sg4
VA '''citation index''' is a kind of [[bibliographic database]], an index of [[citation]]s between publications, allowing the user to easily establish which later documents cite which earlier documents. A form of citation index is first found in 12th-century Hebrew religious literature. Legal citation indexes are found in the 18th century and were made popular by [[citator]]s such as [[Shepard's Citations]] (1873). In 1960, [[Eugene Garfield]]'s [[Institute for Scientific Information]] (ISI) introduced the first citation index for papers published in [[academic journal]]s, first the ''[[Science Citation Index]]'' (SCI), and later the ''[[Social Sciences Citation Index]]'' (SSCI) and the ''[[Arts and Humanities Citation Index]]'' (AHCI). The first automated citation indexing was done by [[CiteSeer]] in 1997. Other sources for such data include [[Google Scholar]].\u000a\u000a==History==\u000a\u000aThe earliest known citation index is an index of biblical citations in [[rabbinic literature]], the ''Mafteah ha-Derashot'', attributed to [[Maimonides]] and probably dating to the 12th century. It is organized alphabetically by biblical phrase. Later biblical citation indexes are in the order of the canonical text. These citation indices were used both for general and for legal study.  The Talmudic citation index ''En Mishpat'' (1714) even included a symbol to indicate whether a Talmudic decision had been overridden, just as in the 19th-century ''Shepard's Citations''.<ref>Bella Hass Weinberg, "The Earliest Hebrew Citation Indexes" in Trudi Bellardo Hahn, Michael Keeble Buckland, eds., ''Historical Studies in Information Science'', 1998,  p. 51''ff''</ref><ref>Bella Hass Weinberg, "Predecessors of Scientific Indexing Structures in the Domain of Religion" in W. Boyden Rayward, Mary Ellen Bowden, ''The History and Heritage of Scientific and Technological Information Systems'', Proceedings of the 2002 Conference, 2004, p. 126''ff''</ref> Unlike modern scholarly citation indexes, only references to one work, the Bible, were indexed.\u000a\u000aIn English legal literature, volumes of judicial reports included lists of cases cited in that volume starting with ''Raymond's Reports'' (1743) and followed by ''Douglas's Reports'' (1783). Simon Greenleaf (1821) published an alphabetical list of cases with notes on later decisions affecting the precedential authority of the original decision.<ref name='shapiro'/>\u000a\u000aThe first true citation index dates to the 1860 publication of Labatt's ''Table of Cases...California...'', followed in 1872 by Wait's ''Table of Cases...New York...''. But the most important and best-known citation index came with the 1873 publication of [[Shepard's Citations]].<ref name='shapiro'>Fred R. Shapiro, "Origins of Bibliometrics, Citation Indexing, and Citation Analysis: The Neglected Legal Literature" ''Journal of the American Society of Information Science'' '''43''':5:337-339 (1992)</ref>\u000a\u000a==Major citation indexing services==\u000a{{main|Indexing and abstracting service}}\u000a{{main cat|Citation indices}}\u000a\u000aGeneral-purpose academic citation indexes include:\u000a\u000a*ISI (now part of [[Thomson Reuters]]) publishes the ISI citation indexes in print and [[compact disc]]. They are now generally accessed through the Web under the name '' [[Web of Science]]'', which is in turn part of the group of databases in the ''[[Web of Knowledge]].''\u000a*[[Elsevier]] publishes [[Scopus]], available online only, which similarly combines subject searching with citation browsing and tracking in the sciences and [[social sciences]].\u000a*[[Indian Citation Index (ICI)|Indian Citation Index]] is an online citation data which covers [[peer review]]ed journals published from India. It covers major subject areas such as scientific, technical, medical, and [[social sciences]] and includes arts and humanities. The citation database is the first of its kind in India.\u000aEach of these offer an index of citations between publications and a mechanism to establish which documents cite which other documents. They differ widely in cost: the ISI databases and Scopus are available by subscription (generally to libraries).\u000a\u000aIn addition, [[CiteSeer]] and [[Google Scholar]] are freely available online.\u000a\u000a==Citation analysis==\u000a{{main|Citation analysis}}\u000a{{merge|section=yes|Citation analysis|date=December 2013}}\u000a{{duplication|dupe=Citation analysis|date=December 2013}}\u000a\u000aWhile citation indexes were originally designed for [[information retrieval]], they are increasingly used for [[bibliometrics]] and other studies involving research evaluation. Citation data is also the basis of the popular [[journal impact factor]].\u000a\u000aThere is a large body of literature on [[citation analysis]], sometimes called [[scientometrics]], a term invented by [[Vasily Nalimov]], or more specifically [[bibliometrics]]. The field blossomed with the advent of the [[Science Citation Index]], which now covers source literature from 1900 on. The leading journals of the field are ''[[Scientometrics]],'' ''Informetrics,'' and the ''[[Journal of the American Society of Information Science and Technology]]''. [[American Society for Information Science and Technology|ASIST]] also hosts an [[electronic mailing list]] called SIGMETRICS at  ASIST.<ref>{{cite web | title=The American Society for Information Science & Technology | work=The Information Society for the Information Age | url=http://www.asis.org| accessdate=2006-05-21}}</ref> This method is undergoing a resurgence based on the wide dissemination of the Web of Science and Scopus subscription databases in many universities, and the universally available free citation tools such as  [[CiteBase]],  [[CiteSeerX]], [[Google Scholar]], and the former [[Windows Live Academic]] (now available with extra features as [[Microsoft Academic Search]]).\u000a\u000a[[Legal citation]] analysis is a citation analysis technique for analyzing [[legal documents]] to facilitate the understanding of the inter-related regulatory compliance documents by the exploration the citations that connect provisions to other provisions within the same document or between different documents. Legal citation analysis uses a [[citation graph]] extracted from a regulatory document, which could supplement [[E-discovery]] - a process that leverages on technological innovations in [[big data analytics]].<ref>[http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=5070630&tag=1 ]{{dead link|date=December 2013}}</ref><ref>Mohammad Hamdaqa and A. Hamou-Lhadj, "Citation Analysis: An Approach for Facilitating the Understanding and the Analysis of Regulatory Compliance Documents",  In Proc. of the 6th International Conference on Information Technology, Las Vegas, USA</ref><ref name=BD-HB-R-01>{{cite web|title=E-Discovery Special Report: The Rising Tide of Nonlinear Review|url=http://hudsonlegalblog.com/e-discovery/e-discovery-special-report-rising-tide-nonlinear-review.html|publisher=[[Hudson Global]]|accessdate=1 July 2012}} by Cat Casey and Alejandra Perez</ref><ref name=BD-HB-R-02>{{cite web|title=What Technology-Assisted Electronic Discovery Teaches Us About The Role Of Humans In Technology - Re-Humanizing Technology-Assisted Review|url=http://www.forbes.com/sites/benkerschberg/2012/01/09/what-technology-assisted-electronic-discovery-teaches-us-about-the-role-of-humans-in-technology/|publisher=[[Forbes]]|accessdate=1 July 2012}}</ref>\u000a\u000a===History===\u000aIn a 1965 paper, [[Derek J. de Solla Price]] described the inherent linking characteristic of the SCI as "Networks of Scientific Papers".<ref>{{cite journal | author=Derek J. de Solla Price | title=Networks of Scientific Papers | journal=[[Science (journal)|SCIENCE]] | date=July 30, 1965 | volume=149 | issue=3683| pages=510&ndash;515 | url=http://garfield.library.upenn.edu/papers/pricenetworks1965.pdf | pmid=14325149 | doi=10.1126/science.149.3683.510|format=PDF}}</ref> The links between citing and cited papers became dynamic when the SCI began to be published online. The [[Social Sciences Citation Index]] became one of the first databases to be mounted on the [[Dialog]] system<ref>{{cite web | title=Dialog, A Thomson Business | work="Dialog invented online information services" | url=http://www.dialog.com| accessdate=2006-05-21}}</ref> in 1972. With the advent of the [[CD-ROM]] edition, linking became even easier and enabled the use of [[bibliographic coupling]] for finding related records. In 1973, Henry Small published his classic work on [[Co-Citation analysis]] which became a [[self-organizing]] classification system that led to [[document clustering]] experiments and eventually an "Atlas of Science" later called "Research Reviews".\u000a\u000aThe inherent topological and graphical nature of the worldwide citation network which is an inherent property of the [[scientific literature]] was described by [[Ralph Garner]] ([[Drexel University]]) in 1965.<ref>http://www.garfield.library.upenn.edu/rgarner.pdf</ref>\u000a\u000aThe use of citation counts to rank journals was a technique used in the early part of the nineteenth century but the systematic ongoing measurement of these counts for scientific journals was initiated by Eugene Garfield at the Institute for Scientific Information who also pioneered the use of these counts  to rank authors and [[academic paper|papers]].  In a landmark paper of 1965 he and [[Irving Sher]] showed the correlation between citation frequency and eminence in demonstrating that [[Nobel Prize]] winners published five times the average number of papers while their work was cited 30 to 50 times the average. In a long series of essays on the Nobel and other prizes Garfield reported this phenomenon.  The usual summary measure is known as [[impact factor]], the number of citations to a journal for the previous two years, divided by the number of articles published in those years. It is widely used, both for appropriate and inappropriate purposes\u2014in particular, the use of this measure alone for  ranking authors and papers is therefore [[Impact factor#|quite controversial.]]\u000a\u000aIn an early study in 1964 of the use of Citation Analysis in writing the history of [[DNA]], Garfield and Sher demonstrated the potential for generating [[historiograph]]s, [[topological map]]s of the most important steps in the history of scientific topics. This work was later automated by E. Garfield, [[A. I. Pudovkin]] of the [[Institute of Marine Biology]], [[Russian Academy of Sciences]] and [[V. S. Istomin]] of [[Center for Teaching, Learning, and Technology]], [[Washington State University]] and led to the creation of the [[Histcite|HistCite]] <ref>{{cite web | author=Eugene Garfield, A. I. Pudovkin,  V. S. Istomin | year=2002 | title=Algorithmic Citation-Linked Historiography\u2014Mapping the Literature of Science | work=Presented the ASIS&T 2002: Information, Connections and Community. 65th Annual Meeting of ASIST in Philadelphia, PA. November 18\u201321, 2002  | url=http://www.garfield.library.upenn.edu/papers/asis2002/asis2002presentation.html | accessdate=2006-05-21}}</ref> software around 2002.\u000a\u000aAutomatic citation indexing was introduced in 1998 by [[Lee Giles]], [[Steve Lawrence]] and [[Kurt Bollacker]] <ref>C.L. Giles, K. Bollacker, S. Lawrence, "CiteSeer: An Automatic Citation Indexing System," DL'98 Digital Libraries, 3rd ACM Conference on Digital Libraries, pp. 89-98, 1998.</ref> and enabled automatic algorithmic extraction and grouping of citations for any digital academic and scientific document. Where previous citation extraction was a manual process, citation measures could now scale up and be computed for any scholarly and scientific field and document venue, not just those selected by organizations such as ISI. This led to the creation of new systems for public and automated citation indexing, the first being [[CiteSeer]] (now [[CiteSeerX]], soon followed by Cora, which focused primarily on the field of [[computer science]] and [[information science]]. These were later followed by large scale academic domain citation systems such as the Google Scholar and Microsoft Academic. Such autonomous citation indexing is not yet perfect in citation extraction or citation clustering with an error rate estimated by some at 10% though a careful statistical sampling has yet to be done. This has resulted in such authors as [[Ann Arbor, Michigan|Ann Arbor]], [[Milton Keynes]], and [[Walton Hall, Milton Keynes|Walton Hall]] being credited with extensive academic output.<ref name="pmid18354457">{{cite journal |author=Postellon DC |title=Hall and Keynes join Arbor in the citation indexes |journal=[[Nature (journal)|Nature]] |volume=452 |issue=7185 |page=282 |date=March 2008 |pmid=18354457 |doi=10.1038/452282b}}</ref>  SCI claims to create automatic citation indexing through purely programmatic methods. Even the older records have a similar magnitude of error.\u000a\u000a==See also==\u000a* [[Impact factor]]\u000a* [[Citation impact]]\u000a* [[Eigenfactor]]\u000a* [[Microsoft Academic Search]]\u000a* [[Google Scholar]]\u000a* [[Scopus]]\u000a* [[H-index]] or [[Hirsch number]]\u000a* [[Citation analysis]]\u000a* [[Acknowledgment index]]\u000a* [[CiteSeer]]\u000a* [[CiteSeerX]]\u000a* [[Scientific journal]]\u000a* [[Science Citation Index]]\u000a* [[Indian Citation Index]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links ==\u000a* Official [http://admin-apps.isiknowledge.com/JCR/JCR Journal Citation Report] from the [http://www.isinet.com ISI website]\u000a* [http://www.librijournal.org/2005-4toc.html Google Scholar: The New Generation of Citation Indexes]\u000a* [http://www.atlasofscience.net/ Atlas of Science: Mapping Science by means of citation relations]\u000a* [http://www.dlib.org/dlib/september05/bauer/09bauer.html An Examination of Citation Counts in a New Scholarly Communication Environment]\u000a* [http://cids.fc.ul.pt/ CIDS] online tool that calculates the h-index and [[g-index]] based on [[Google Scholar]] data and discerning self-citations\u000a\u000a{{DEFAULTSORT:Citation Index}}\u000a[[Category:Academic publishing]]\u000a[[Category:Bibliometrics]]\u000a[[Category:Library science]]\u000a[[Category:Reputation management]]\u000a[[Category:Citation indices]]
p266
sg6
S'Citation index'
p267
ssI99
(dp268
g2
S'http://en.wikipedia.org/wiki/Conference and Labs of the Evaluation Forum'
p269
sg4
VThe '''Conference and Labs of the Evaluation Forum''' (formerly '''Cross-Language Evaluation Forum'''), or '''CLEF''', is an organization promoting research  in multilingual [[information access]] (currently focusing on [[European Commissioner for Multilingualism|European languages]]). Its specific functions are to  maintain an underlying framework for testing [[information retrieval]] systems, and creating [[digital library|repositories]] of data for researchers to use in developing  comparable [[Technical standard|standards]].<ref name="Peters">{{cite conference | first1 = Carol | last1 = Peters| first2 = Martin | last2 = Braschler | first3 = Khalid | last3 = Choukri | first4 = Julio | last4 = Gonzalo | first5 = Michael | last5 = Kluck | title = The Future of Evaluation for Cross-Language Information Retrieval Systems | conference = Second Workshop of the Cross-Language Evaluation Forum, CLEF 2001 | id = {{citeseerx|10.1.1.109.7647}} }}</ref>\u000aThe organization holds a forum meeting   every September in Europe. Prior to each forum, participants receive a set of challenge tasks. The tasks  are designed to test various aspects of information retrieval systems and encourage their development. Groups of researchers propose and organize campaigns to satisfy those tasks. The results are used as [[benchmark (computing)|benchmarks]] for the state of the art  in the specific areas.,<ref>{{cite journal | url = http://www.springerlink.com/content/l7v0354471u53385/ | title = Special Issue on CLEF | journal = Information Retrieval | volume = 7 | issue = 1\u20132 | year = 2004 }}</ref><ref>Fredric C. Gey, Noriko Kando, and Carol Peters "Cross-Language Information Retrieval: the way ahead" in ''Information Processing & Management''\u000avol. 41, no. 3,  p.415-431 May 2005, {{doi|10.1016/j.ipm.2004.06.006}}</ref>  \u000a\u000aFor example, the 2010 medical retrieval task focuses on retrieval of computed tomography,  MRI, and radiographic images.<ref name="ImageCLEFmed">{{cite web | last = Mueller| first = Henning| authorlink = | coauthors = | title = Medical Retrieval Task| work = | publisher =ImageCLEF - Cross-language image retrieval evaluations | date = 20 May 2010| url =http://www.imageclef.org/2010/medical | format = | doi = | accessdate = 27 May 2010 }}</ref>\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a== External links ==\u000a* [http://www.clef-campaign.org CLEF homepage]\u000a\u000a[[Category:Information retrieval]]\u000a\u000a{{Compu-conference-stub}}
p270
sg6
S'Conference and Labs of the Evaluation Forum'
p271
ssI228
(dp272
g2
S'http://en.wikipedia.org/wiki/Latin American Bibliography'
p273
sg4
V{{Multiple issues|\u000a{{orphan|date=March 2010}}\u000a{{advert|date=August 2010}}\u000a}}\u000a\u000aThe '''Latin American Bibliography''' refers to the set of [[databases]] and information services on [[academic journals]] from [[Latin America]] and the [[Caribbean]] created by the [[National Autonomous University of Mexico]] (UNAM) in the decade of the seventies.{{Clarify|date=August 2009}}\u000a\u000aNowadays, the Latin-American Bibliography is composed by the following databases: CLASE (''Latin-American Citations in [[Social Sciences]] and [[Humanities]]''); PERIODICA (''Index of Latin-American Journals in [[Science]]''); [[Latindex]] (''Regional Co-operative Information System for Scholarly Journals from [[Latin America]], the Caribbean, Spain and Portugal'').\u000a\u000aThese databases were created by a group of information professionals, who identified the need to register, preserve and give access to the Latin-American knowledge published in the main academic [[Academic journal|journals]] of the region. Within UNAM, the fostering institution of these information products was the Science and Humanities Information Center (CICH) created in 1971.\u000a\u000aFor the size of its collection of Latin-American journals, for the quantity of compiled records and for the duration and consistency of the project, the Latin-American Bibliography produced in the UNAM constitutes one of the most valuable resources for scholars and experts specializing in Latin-American affairs.{{Citation needed|date=August 2009}}\u000a\u000a==Products==\u000a\u000aThree databases are available through the web site of UNAM\u2019s General Directorate for Libraries [http://dgb.unam.mx General Directorate for Libraries]:\u000a\u000a'''CLASE''' (''Latin-American Citations in Social Sciences and Humanities''). Bibliographical database, with more than 280,000 records, of which nearly 14,000 provide abstracts and links to the full text of the documents. It includes more than 1,400 journals specializing in Social Sciences, Humanities and Arts, from more than 20 countries of Latin America and the Caribbean. Documents not available in full text can be retrieved through the Document Supply Service of the Latin-American Serials Collection (Hemeroteca Latinoamericana) of the DGB. Direct link: [http://dgb.unam.mx/clase.html CLASE website]\u000a\u000a'''PERIODICA''' (Index of Latin-American Journals in Science). Bibliographical database with more than 315,000 records, of which near 60,000 provide abstracts and links to the full text of the documents. The database indexes more than 1,500 journals specializing in Science and Technology, from more than 20 countries of Latin America and the Caribbean. Documents not available in full text can be retrieved through the Document Supply Service of the Latin-American Serials Collection (Hemeroteca Latinoamericana) of the DGB. Direct link: [http://dgb.unam.mx/periodica.html PERIODICA website]\u000a\u000a'''[[Latindex]]''' (Regional Co-operative Information System for Scholarly Journals from Latin America, the Caribbean, Spain and Portugal). This initiative provides relevant information and data of the scholarly journals edited in the [[Iberoamerica]]n region. Three databases are produced through the collaborative work of the member institutions: '''Directory''',: with more than 17,000 records; '''Catalogue''', with more than 3,500 selected journals that fulfill international quality criteria and an '''Index of Electronic Journals''', offering nearly 3,000 links to available resources in full text. Direct link: [http://www.latindex.org Latindex website]\u000a\u000aCurrently, the Department of Latin-American Bibliography contributes to the production of two other Latin-American information products:\u000a\u000a'''ASFA''' (''Aquatic Sciences and Fisheries Abstracts''). Bibliographical international database on Aquatic Sciences and [[Fisheries]], covering subject areas such as [[technology]] and [[Public administration|administration]] of the marine environments and its resources (salt and sweet waters), including its socioeconomic and juridical aspects. It offers abstracts of articles published in approximately 7,000 periodic publications, besides thesis, monographs and other not conventional literature. The contribution relative to the Mexican journals is produced in the Department of Latin-American Bibliography from 1981. Link: [http://www.fao.org/fishery/asfa ASFA website]\u000a\u000a'''[[SciELO]] Mexico''' (''Scientific Electronic Library Online''). Open access electronic journals collection that includes a selection of the most recognized academic publications of the country in all areas of knowledge, previously selected accordingly to the most accepted criteria related to content and editorial standards. Currently it offers the full text of more than 2,500 articles from 28 academic Mexican journals. Direct link: [http://www.scielo.org.mx/scielo.php Scielo México website]\u000a\u000aOver the time, other databases were produced by the Department of Latin-American Bibliography during its more than 30 years of existence, namely:\u000a\u000a'''BLAT''' (''Latin-American Bibliography I and II''), with information compiled from international sources, mainly documents from Latin-American origin (produced by Latin American authors and institutions) or those in which their object of study was related to the region. The database ceased in 1997. Another one was '''MEXINV''', as a subset of CLASE, offered bibliographical records of documents relative only to [[Mexico]]. This database ceased in the decade of the nineties.\u000a\u000a===Institution===\u000a\u000aCurrently, the databases described above are produced by the Department of Latin-American Bibliography, part of the Assistant Office for Information Services of the General Directorate for Libraries (DGB) of the National Autonomous University of Mexico (UNAM). The original databases (BLAT, CLASE, PERIODICA, MEXINV and Latindex) were created by the Science and Humanities Information Center (CICH). Since the incorporation of the CICH to UNAM\u2019s General Directorate for Libraries in 1997, this institution acts as Responsible Editor.\u000a\u000a==References==\u000a\u000a*Alonso Gamboa, José Octavio. Servicios, productos, docencia e investigación en información: la experiencia del Centro de Información Científica y Humanística de la Universidad Nacional Autónoma de México. Ciencias de la Información, vol. 24, no. 4, diciembre, 1993. p.&nbsp;201-208. URL: [http://www.bibliociencias.cu/gsdl/cgi-bin/library?e=d-000-00---0revistas--00-0-0--0prompt-10---4------0-1l--1-es-50---20-about---00031-001-1-0utfZz-8-00&cl=CL2.772&d=HASH01caacf727585263378aa110&x=1]\u000a\u000a*Alonso Gamboa, José Octavio. Accesso a revistas latinoamericanas en Internet. Una opción a través de las bases de datos Clase y Periódica. Ciencia da Informação, vol. 27, no. 1, Janeiro-abril, 1998, p.&nbsp;90-95. URL: http://www.scielo.br/pdf/ci/v27n1/12.pdf\u000a\u000a*Alonso Gamboa, José Octavio y Felipe Rafael Reyna Espinosa. Compilación de datos bibliométricos regionales usando las bases de datos CLASE y PERIÓDICA. Revista Interamericana de Bibliotecología, 2005. Vol. 28, no. 1, enero-junio: 63-78. URL: http://bibliotecologia.udea.edu.co/revinbi/Numeros/2801/doc3_28.html\u000a\u000a*Russell, Jane M.; Madera-Jaramillo, María J.; Hernández- García, Yoscelina y Ainsworth, Shirley. Mexican collaboration networks in the international and regional arenas. En: Kretschmer, H. & Havemann, F. (Eds.): Proceedings of WIS 2008, Berlin. Fourth International Conference on Webometrics, Informetrics and Scientometrics & Ninth COLLNET Meeting, Humboldt-Universität zu Berlin, Institute for Library and Information Science (IBI). URL: http://www.collnet.de/Berlin-2008/RussellWIS2008mcn.pdf\u000a\u000a[[Category:Bibliographic databases]]\u000a[[Category:Scientific databases]]\u000a[[Category:Citation indices]]
p274
sg6
S'Latin American Bibliography'
p275
ssI102
(dp276
g2
S'http://en.wikipedia.org/wiki/Cosine similarity'
p277
sg4
V'''Cosine similarity''' is a measure of similarity between two vectors of an [[inner product space]] that measures the [[cosine]] of the angle between them. The cosine of 0° is 1, and it is less than 1 for any other angle. It is thus a judgement of orientation and not magnitude: two vectors with the same orientation have a Cosine similarity of 1, two vectors at 90° have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude. Cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1].\u000a\u000aNote that these bounds apply for any number of dimensions, and Cosine similarity is most commonly used in high-dimensional positive spaces. For example, in [[Information Retrieval]] and [[text mining]], each term is notionally assigned a different dimension and a document is characterised by a vector where the value of each dimension corresponds to the number of times that term appears in the document. Cosine similarity then gives a useful measure of how similar two documents are likely to be in terms of their subject matter.<ref>Singhal, Amit (2001). "Modern Information Retrieval: A Brief Overview". Bulletin of the IEEE Computer Society Technical Committee on Data Engineering 24 (4): 35\u201343.</ref>\u000a\u000aThe technique is also used to measure cohesion within [[cluster (computing)|cluster]]s in the field of [[data mining]].<ref>P.-N. Tan, M. Steinbach & V. Kumar, "Introduction to Data Mining", , Addison-Wesley (2005), ISBN 0-321-32136-7, chapter 8; page 500.</ref>\u000a\u000a''Cosine distance'' is a term often used for the complement in positive space, that is: <math>D_C(A,B) = 1 - S_C(A,B)</math>. It is important to note, however, that this is not a proper [[distance metric]] as it does not have the triangle inequality property and it violates the coincidence axiom; to repair the triangle inequality property whilst maintaining the same ordering, it is necessary to convert to Angular distance (see below.)\u000a\u000aOne of the reasons for the popularity of Cosine similarity is that it is very efficient to evaluate, especially for sparse vectors, as only the non-zero dimensions need to be considered.\u000a\u000a==Definition==\u000a\u000aThe cosine of two vectors can be derived by using the [[Euclidean vector#Dot product|Euclidean dot product]] formula:\u000a\u000a:<math>\u005cmathbf{a}\u005ccdot\u005cmathbf{b}\u000a=\u005cleft\u005c|\u005cmathbf{a}\u005cright\u005c|\u005cleft\u005c|\u005cmathbf{b}\u005cright\u005c|\u005ccos\u005ctheta</math>\u000a\u000aGiven two [[Vector (geometric)|vectors]] of attributes, ''A'' and ''B'', the cosine similarity, ''cos(\u03b8)'', is represented using a [[dot product]] and [[Magnitude (mathematics)#Euclidean vectors|magnitude]] as\u000a\u000a:<math>\u005ctext{similarity} = \u005ccos(\u005ctheta) = {A \u005ccdot B \u005cover \u005c|A\u005c| \u005c|B\u005c|} = \u005cfrac{ \u005csum\u005climits_{i=1}^{n}{A_i \u005ctimes B_i} }{ \u005csqrt{\u005csum\u005climits_{i=1}^{n}{(A_i)^2}} \u005ctimes \u005csqrt{\u005csum\u005climits_{i=1}^{n}{(B_i)^2}} }</math>\u000a\u000aThe resulting similarity ranges from &minus;1 meaning exactly opposite, to 1 meaning exactly the same, with 0 usually indicating independence, and in-between values indicating intermediate similarity or dissimilarity.\u000a\u000aFor text matching, the attribute vectors ''A'' and ''B'' are usually the [[tf-idf|term frequency]] vectors of the documents.  The cosine similarity can be seen as a method of normalizing document length during comparison.\u000a\u000aIn the case of [[information retrieval]], the cosine similarity of two documents will range from 0 to 1, since the term frequencies ([[tf-idf]] weights) cannot be negative. The angle between two term frequency vectors cannot be greater than&nbsp;90°.\u000a\u000aIf the attribute vectors are normalized by subtracting the vector means (e.g., <math>A - \u005cbar{A}</math>), the measure is called centered cosine similarity and is equivalent to the [[Pearson_product-moment_correlation_coefficient#For_a_sample|Pearson Correlation Coefficient]].\u000a\u000a=== Angular similarity ===\u000a\u000aThe term "cosine similarity" has also been used on occasion to express a different coefficient, although the most common use is as defined above. Using the same calculation of similarity, the normalised angle between the vectors can be used as a bounded similarity function within [0,1], calculated from the above definition of similarity by:\u000a:<math>1 - \u005cfrac{ \u005ccos^{-1}( \u005ctext{similarity} )}{ \u005cpi} </math>\u000ain a domain where vector coefficients may be positive or negative, or\u000a:<math>1 - \u005cfrac{ 2 \u005ccdot \u005ccos^{-1}( \u005ctext{similarity} ) }{ \u005cpi }</math>\u000ain a domain where the vector coefficients are always positive.  \u000a\u000aAlthough the term "cosine similarity" has been used for this angular distance, the term is oddly used as the cosine of the angle is used only as a convenient mechanism for calculating the angle itself and is no part of the meaning. The advantage of the angular similarity coefficient is that, when used as a difference coefficient (by subtracting it from 1) the resulting function is a proper [[distance metric]], which is not the case for the first meaning. However for most uses this is not an important property. For any use where only the relative ordering of similarity or distance within a set of vectors is important, then which function is used is immaterial as the resulting order will be unaffected by the choice.\u000a\u000a=== Confusion with "Tanimoto" coefficient ===\u000a\u000aThe cosine similarity may be easily confused with the Tanimoto metric - a specialised form of a similarity coefficient with a similar algebraic form:\u000a\u000a:<math>T(A,B) = {A \u005ccdot B \u005cover \u005c|A\u005c|^2 +\u005c|B\u005c|^2 - A \u005ccdot B}</math>\u000a\u000aIn fact, this algebraic form [[Jaccard index#Tanimoto_Similarity_and_Distance|was first defined by Tanimoto]] as a mechanism for calculating the [[Jaccard coefficient]] in the case where the sets being compared are represented as [[bit vector]]s. While the formula extends to vectors in general, it has quite different properties from cosine similarity and bears little relation other than its superficial appearance.\u000a\u000a=== Ochiai coefficient ===\u000aThis coefficient is also known in biology as Ochiai coefficient, or Ochiai-Barkman coefficient, or Otsuka-Ochiai coefficient:<ref>''Ochiai A.'' Zoogeographical studies on the soleoid fishes found Japan and its neighboring regions. II // Bull. Jap. Soc. sci. Fish. 1957. V. 22. \u2116 9. P. 526-530.</ref><ref>''Barkman J.J.'' Phytosociology and ecology of cryptogamic epiphytes, including a taxonomic survey and description of their vegetation units in Europe. \u2013 Assen. Van Gorcum. 1958. 628 p.</ref>\u000a:<math>K =\u005cfrac{n(A \u005ccap B)}{\u005csqrt{n(A) \u005ctimes n(B)}}</math>\u000aHere, <math>A</math> and <math>B</math> are sets, and <math>n(A)</math> is the number of elements in <math>A</math>. If sets are represented as [[bit vector]]s, the Ochiai coefficient can be seen to be the same as the cosine similarity.\u000a\u000a== Properties ==\u000aCosine similarity is related to [[Euclidean distance]] as follows. Denote Euclidean distance by the usual <math>\u005c|A - B\u005c|</math>, and observe that\u000a\u000a:<math>\u005c|A - B\u005c|^2 = (A - B)^\u005ctop (A - B) = \u005c|A\u005c|^2 + \u005c|B\u005c|^2 - 2 A^\u005ctop B</math>\u000a\u000aby [[Polynomial expansion|expansion]]. When {{mvar|A}} and {{mvar|B}} are normalized to unit length, <math>\u005c|A\u005c|^2 = \u005c|B\u005c|^2 = 1</math> so the previous is equal to\u000a\u000a:<math>2 (1 - \u005ccos(A, B))</math>\u000a\u000a'''Null distribution:''' For data which can be negative as well as positive, the [[null distribution]] for cosine similarity is the distribution of the dot product of two independent random unit vectors. This distribution has a [[mean]] of zero and a [[variance]] of <math>1/n</math> (where <math>n</math> is the number of dimensions), and although the distribution is bounded between -1 and +1, as <math>n</math> grows large the distribution is increasingly well-approximated by the [[normal distribution]].<ref>{{cite journal\u000a | author = Spruill, Marcus C\u000a | year = 2007\u000a | title = Asymptotic distribution of coordinates on high dimensional spheres\u000a | journal = Electronic communications in probability\u000a | volume = 12 | pages = 234-247\u000a | doi = 10.1214/ECP.v12-1294\u000a}}</ref><ref>[http://stats.stackexchange.com/questions/85916/distribution-of-dot-products-between-two-random-unit-vectors-in-mathbbrd CrossValidated: Distribution of dot products between two random unit vectors in RD]</ref>\u000aFor other types of data, such as bitstreams (taking values of 0 or 1 only), the null distribution will take a different form, and may have a nonzero mean.<ref>{{cite journal\u000a | author = Graham L. Giller \u000a | year = 2012\u000a | title = The Statistical Properties of Random Bitstreams and the Sampling Distribution of Cosine Similarity\u000a | journal = Giller Investments Research Notes\u000a | number = 20121024/1\u000a | doi = 10.2139/ssrn.2167044\u000a}}</ref>\u000a\u000a== Soft Cosine Measure ==\u000a'''Soft cosine measure''' <ref>{{cite journal|last1=Sidorov|first1=Grigori|last2=Gelbukh|first2=Alexander|last3=Gómez-Adorno|first3=Helena|last4=Pinto|first4=David|title=Soft Similarity and Soft Cosine Measure: Similarity of Features in Vector Space Model|journal=Computación y Sistemas|volume=18|issue=3|pages=491\u2013504|doi=10.13053/CyS-18-3-2043|url=http://cys.cic.ipn.mx/ojs/index.php/CyS/article/view/2043|accessdate=7 October 2014}}</ref>\u000ais a measure of \u201csoft\u201d similarity between two vectors, i.e., the measure that considers similarity of pairs of features. The traditional '''cosine similarity''' considers the [[vector space model]] (VSM) features as independent or completely different, while the '''soft cosine measure''' proposes considering the similarity of features in VSM, which allows generalization of the concepts of cosine measure and also the idea of similarity (soft similarity).\u000a\u000aFor example, in the field of [[natural language processing]] (NLP) the similarity between features is quite intuitive. Features such as words, n-grams or syntactic n-grams<ref>{{cite book|last1=Sidorov|first1=Grigori|last2=Velasquez|first2=Francisco|last3=Stamatatos|first3=Efstathios|last4=Gelbukh|first4=Alexander|last5=Chanona-Hernández|first5=Liliana|title=Syntactic Dependency-based N-grams as Classification Features|publisher=LNAI 7630|isbn=978-3-642-37798-3|pages=1\u201311|url=http://link.springer.com/chapter/10.1007%2F978-3-642-37798-3_1|accessdate=7 October 2014}}</ref> can be quite similar, though formally they are considered as different features in the VSM. For example, words \u201cplay\u201d and \u201cgame\u201d are different words and thus are mapped to different dimensions in VSM; yet it is obvious that they are related semantically. In case of [[n-grams]] or syntactic n-grams, [[Levenshtein distance]] can be applied (in fact, Levenshtein distance can be applied to words as well).\u000a\u000aFor calculation of the soft cosine measure, the matrix {{math|'''s'''}} of similarity between features is introduced. It can be calculated using Levenshtein distance or other similarity measures, e.g., various [[WordNet]] similarity measures. Then we just multiply by this matrix.  \u000a\u000aGiven two {{math|''N''}}-dimension vectors a and b, the soft cosine similarity is calculated as follows:\u000a\u000a:<math>\u005cbegin{align}\u000a    \u005coperatorname{soft\u005c_cosine}_1(a,b)=\u000a    \u005cfrac{\u005csum\u005cnolimits_{i,j}^N s_{ij}a_ib_j}{\u005csqrt{\u005csum\u005cnolimits_{i,j}^N s_{ij}a_ia_j}\u005csqrt{\u005csum\u005cnolimits_{i,j}^N s_{ij}b_ib_j}},\u000a\u005cend{align}\u000a</math>\u000a\u000awhere {{math|''s<sub>ij</sub>'' {{=}} similarity(feature<sub>''i''</sub>, feature<sub>''j''</sub>)}}.\u000a\u000aIf there is no similarity between features ({{math|''s<sub>ii</sub>'' {{=}} 1}}, {{math|''s<sub>ij</sub>'' {{=}} 0}} for {{math|''i'' \u2260 ''j''}}), the given equation is equivalent to the conventional cosine similarity formula.\u000a\u000aThe complexity of this measure is quadratic, which makes it perfectly applicable to real world tasks. The complexity can be even transformed to linear.\u000a\u000a== See also ==\u000a* [[Sørensen similarity index|Sørensen's quotient of similarity]]\u000a* [[Hamming distance]]\u000a* [[Correlation]]\u000a* [[Dice's coefficient]]\u000a* [[Jaccard index]]\u000a* [[SimRank]]\u000a* [[Information retrieval]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a== External links ==\u000a* [http://www.appliedsoftwaredesign.com/archives/cosine-similarity-calculator/ Online Cosine Similarity Calculator]\u000a* [http://mathforum.org/kb/message.jspa?messageID=5658016&tstart=0 Weighted cosine measure]\u000a* [http://blog.christianperone.com/?p=2497 A tutorial on cosine similarity using Python]\u000a\u000a{{DEFAULTSORT:Cosine Similarity}}\u000a[[Category:Information retrieval]]
p278
sg6
S'Cosine similarity'
p279
ssI231
(dp280
g2
S'http://en.wikipedia.org/wiki/Google Web History'
p281
sg4
S'\'\'\'Google Web History\'\'\' (previously \'\'\'Google Search History\'\'\') is a feature of [[Google Search]] and provided by [[Google]], in which all search queries and results that a user clicks on are recorded. The feature is only available for users logged into a [[Google Account]]. The feature was renamed from Search History to Web History on April 19, 2007.<ref>[http://searchengineland.com/google-search-history-expands-becomes-web-history-11016 "Google Search History Expands, Becomes Web History"]. Like all web hostory, google web history take up space and data on your phone, which is why many people choose to clear their hostory.  Search Engine Land. Retrieved July 12, 2010.</ref> A user\'s Web History is used to personalize search results with the help of [[Google Personalized Search]]<ref>[http://www.businessweek.com/the_thread/techbeat/archives/2009/12/google_gets_real-time_personalized_search.html "Google Gets Real-Time, Personalized Search"]. \'\'Business Week\'\'. Retrieved July 12, 2010.</ref> and in [[Google Now]].\n\n==References== \nGoogle search engine searches more than just your questions, it matches the words you search with other online posts and files that have the same words, so you get more options and more answers, if you don\'t find what you\'re searching for, try rewording your question, often you will discover your question popping up after you type only a few words, and then you can go directly to the answer you were searching for. Google makes it possible and easy for everyone.\n{{Reflist}}\n\n==External links==\n* [http://history.google.com/history/ Google Web History] Also via redirect at [http://google.com/psearch]\n\n{{Google Inc.}}\n\n{{Google-stub}}\n\n[[Category:Google Search|Web History]]\n[[Category:Personalized search]]'
p282
sg6
S'Google Web History'
p283
ssI105
(dp284
g2
S'http://en.wikipedia.org/wiki/Query likelihood model'
p285
sg4
VThe '''query likelihood model''' is a [[language model]] used in [[Information Retrieval]]. A language model is constructed for each document in the collection.  It is then possible to rank each document by the probability of specific documents given a query. This is interpreted as being the [[Likelihood function|likelihood]] of a document being relevant given a query.\u000a\u000a==Calculating the likelihood==\u000aUsing [[Bayes' theorem|Bayes' rule]], the probability <math>P</math> of a document <math>d</math>, given a query <math>q</math> can be written as follows:\u000a\u000a:<math>\u000a P(d|q) = \u005cfrac{P(q|d) P(d)}{P(q)}\u000a</math>\u000a\u000aSince the probability of the query P(q) is the same for all documents, this can be ignored. Further, it is typical to assume that the probability of documents is uniform. Thus, P(d) is also ignored.\u000a\u000a:<math>\u000a P(d|q) = P(q|d)\u000a</math>\u000a\u000aDocuments are then ranked by the probability that a query is observed as a random sample from the document model. The multinomial unigram language model is commonly used to achieve this. We have:\u000a:<math>\u000a P(q|M_d) = K_q \u005cprod_{t \u005cin V} P(t|M_d)^{tf_{t,q}}\u000a</math>,where the multinomial coefficient is <math>K_q = L_q!/(tf_{t1,q}!tf_{t2,q}!...tf_{tM,q}!)</math> for query {{math|q}}.\u000a\u000aIn practice the multinomial coefficient is usually removed from the calculation. The reason is that it is a constant for a given bag of words (such as all the words from a specific document <math>d</math>). The language model <math>M_d</math> should be the true language model calculated from the distribution of words underlying each retrieved document. In practice this language model is unknown, so it is usually approximated by considering each term (unigram) from the retrieved document together with its probability of appearance. So <math>P(t|M_d)</math> is the probability of term <math>t</math> being generated by the language model <math>M_d</math> of document <math>d</math>. This probability is multiplied for all terms from query <math>q</math> to get a rank for document <math>d</math> in the interval <math>[0,1]</math>. The calculation is repeated for all documents to create a ranking of all documents in the document collection.\u000a\u000a<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: An Introduction to Information Retrieval, page 241. Cambridge University Press, 2009</ref>\u000a\u000a==References==\u000a <references/>\u000a\u000a[[Category:Information retrieval]]
p286
sg6
S'Query likelihood model'
p287
ssI234
(dp288
g2
S'http://en.wikipedia.org/wiki/Rapid Evolution'
p289
sg4
S"{{Infobox Software\n| name                   = Rapid Evolution\n| screenshot             = <!--  Commented out: [[Image:RapidEvolutionScreenshot1.jpg|thumb|right|250px]] -->\n| caption                = Screenshot of Rapid Evolution 2.9.0\n| developer              = [[Jesse Bickmore]]\n| frequently_updated     = yes\n| operating system       = Any OS that supports Java\n| genre                  = Music Software\n| website                = [http://www.mixshare.com/ Mixshare]\n}}\n'''Rapid Evolution''' (also known as RE) is an [[open source]] [[software]] tool for [[DJs]], providing filtering and searching features suitable for musicians.  It can analyze audio files and automatically determine properties such as the musical key, [[beats per minute]] (BPM), beat intensity and [[ReplayGain]]. \n\nIt supports file types [[MP3]], [[MP4]], [[WAV]], [[FLAC]], [[Ogg|OGG]], [[Advanced Audio Coding|AAC]] and [[APE tag|APE]].  It helps [[DJs]] to organize and profile their music, and assists in the process of mixing music by utilizing song metadata to be able to show harmonically compatible songs and songs of a similar style.  It allows DJs to save and remember which songs are good matches (like a personal, digital mixing journal) and to plan entire mix sets.\n\nOne of its uses is to assist in a [[DJ]] technique called [[harmonic mixing]]. Once the musical key and BPM is known for a set of songs, [[DJs]] can use [[music theory]] (such as the [[Circle of Fifths]]) to identify songs that are harmonically compatible.  The act of mixing harmonically can help eliminate [[consonance and dissonance|dissonant]] tones while mixing songs together.  Since identifying whether songs can be made harmonically compatible can be quite complex (once features such as pitch lock are introduced), the software assists DJs by being able to show them which songs in their collection can be made harmonically compatible with any particular song.  It can also assist DJs in the act of [[beatmatching]] by showing which songs are in a compatible BPM range, and the percent of BPM difference.\n\nRapid Evolution is created and released through Mixshare.com.  The metadata generated by Rapid Evolution is shared through the central servers at Mixshare.com, which can be browsed online.  There are 1 million songs added to the database sharing information such as key, BPM, styles and ratings.\n\n==History==\nRapid Evolution was developed for the [[Microsoft Windows|Windows]] environment and released in 2003.  Starting in version 2.0 it was switched to run on the Java platform, allowing it to run in virtually any environment.  It is still actively developed.\n\nSeveral improvements to the key detection algorithm have been introduced over the years.  Rapid Evolution is the only program which can detect advanced key modes, such as aeolian, ionian, dorian, phrygian, lydian and mixolydian.  To date, there has only been one serious comparison of key detection accuracy (including programs such as [[Mixed In Key]] and Mixmeister).  It was shown that Rapid Evolution is the most accurate.<ref>{{cite web|title=Key Detection Software Comparison|url=http://www.mixingonbeat.com/phpbb/viewtopic.php?t=2268|date=2006-04-26|accessdate=2008-03-21|publisher=MixingOnBeat}}</ref>\n\nThe program was open-sourced on November 2013. <ref>{{cite web |url=http://www.mixshare.com/cgi-bin/yabb2/YaBB.pl?num=1381954407|title=Open-sourcing forum thread |date=2013-10-13 |accessdate=2014-04-17 |publisher=Mixshare}}</ref>\n\n==Community interest==\nRapid Evolution was originally a freeware program.<ref>{{cite web |url=http://www.mixshare.com/wiki/doku.php?id=testimonials|title=DJ Testimonials |date=2007-01-01 |accessdate=2008-03-21 |publisher=Mixshare}}</ref>Due to its vast feature set, Rapid Evolution tends to be suited more for experienced DJs versus beginners.  \n\n== See also ==\n*[[Harmonic mixing]]\n*[[Music Theory]]\n*[[DJing]]\n\n== External links ==\n*[http://www.mixshare.com/software Download Rapid Evolution]\n*[http://www.mixshare.com Mixshare's Official website]\n*[http://www.harmonic-mixing.com Harmonic-Mixing.com]\n*[https://github.com/djqualia/RapidEvolution2 Source code for version 2]\n*[https://github.com/djqualia/RapidEvolution3 Source code for version 3]\n\n== References ==\n{{reflist}}\n\n[[Category:Music search engines]]\n[[Category:OS X multimedia software]]\n[[Category:Windows multimedia software]]\n[[Category:Audio mixing software]]"
p290
sg6
S'Rapid Evolution'
p291
ssI108
(dp292
g2
S'http://en.wikipedia.org/wiki/Datanet'
p293
sg4
V{{Use mdy dates|date=September 2011}}\u000a''This article is about the U.S. National Science Foundation Office of Cyberinfrastructure .\u000a\u000aOn September 28, 2007, the U.S. [[National Science Foundation]] Office of Cyberinfrastructure announced a request for proposals with the name '''Sustainable Digital Data Preservation and Access Network Partner (DataNet)'''.<ref name="datanetprogram">{{cite web\u000a|url=http://www.nsf.gov/funding/pgm_summ.jsp?pims_id=503141\u000a|publisher=National Science Foundation\u000a|title=Sustainable Digital Data Preservation and Access Network Partners (DataNet) Program Summary\u000a|date=September 28, 2007\u000a|accessdate=October 3, 2007\u000a}}</ref>  The lead paragraph of its synopsis describes the program as:\u000a\u000a<blockquote>Science and engineering research and education are increasingly digital and increasingly data-intensive.  Digital data are not only the output of research but provide input to new hypotheses, enabling new scientific insights and driving innovation. Therein lies one of the major challenges of this scientific generation: how to develop the new methods, management structures and technologies to manage the diversity, size, and complexity of current and future data sets and data streams.  This solicitation addresses that challenge by creating a set of exemplar national and global data research infrastructure organizations (dubbed DataNet Partners) that provide unique opportunities to communities of researchers to advance science and/or engineering research and learning.</blockquote>\u000a\u000aThe introduction in the solicitation<ref name="datanetsolicitation">{{cite web\u000a|url=http://www.nsf.gov/publications/pub_summ.jsp?ods_key=nsf07601\u000a|publisher=National Science Foundation\u000a|title=Sustainable Digital Data Preservation and Access Network Partners Program Announcements & Information\u000a|date=September 28, 2007\u000a|accessdate=October 3, 2007\u000a}}</ref> goes on to say:\u000a\u000a<blockquote>Chapter 3 (Data, Data Analysis, and Visualization) of [http://www.nsf.gov/pubs/2007/nsf0728/index.jsp NSF\u2019s Cyberinfrastructure Vision for 21st century Discovery] presents a vision in which \u201cscience and engineering digital data are routinely deposited in well-documented form, are regularly and easily consulted and analyzed by specialists and non-specialists alike, are openly accessible while suitably protected, and are reliably preserved.\u201d The goal of this solicitation is to catalyze the development of a system of science and engineering data collections that is open, extensible and evolvable.</blockquote>\u000a\u000aThe initial plan called for a $100 million initiative: five awards of $20&nbsp;million each over five years with the possibility of continuing funding.  Awards were given in two rounds. In the first round, for which  full proposals were due on March 21, 2008, two DataNet proposals were awarded. [[DataONE]],<ref>{{cite web|author=William Michener et al |url=https://www.dataone.org |title=DataONE: Observation Network for Earth |publisher=www.dataone.org | accessdate=2013-01-19}}</ref> led by William Michener at the [[University of New Mexico]] covers ecology, evolutionary, and earth science. The Data Conservancy,<ref>{{cite web|author=Sayeed Choudhury et al |url=https://dataconservancy.org |title=Data Conservancy |publisher=dataconservancy.org | accessdate=2013-01-19}}</ref> led by Sayeed Choudhury of [[Johns Hopkins University]], focuses on astronomy, earth science, life sciences, and social science. \u000a\u000aFor the second round, preliminary proposals were due on October 6, 2008 and full proposals on February 16, 2009. Awards from the second round were greatly delayed, and funding was reduced substantially from $20 million per project to $8 million.<ref>{{cite web|author=National Science Foundation |url=http://www.nsf.gov/awardsearch/simpleSearchResult?queryText=%22datanet+full+proposal%3A%22 |title=NSF DataNet Awards |publisher=www.nsf.gov | accessdate=2013-01-19}}</ref> Funding for three second round projects began in Fall 2011. SEAD: Sustainable Environment through Actionable Data,<ref>{{cite web|author=[[Margaret Hedstrom]] et al |url=http://sead-data.net/ |title=SEAD Sustainable Environment - Actionable Data |publisher=sead-data.net | accessdate=2013-01-19}}</ref> led by [[Margaret Hedstrom]] of the [[University of Michigan]], seeks to provide data curation software and services for the "long tail" of small- and medium-scale data producers in the domain of sustainability science. The DataNet Federation Consortium,<ref>{{cite web|author=[[Reagan Moore]] et al |url=http://datafed.org/ |title=DataNet Federation Consortium |publisher=datafed.org | accessdate=2013-01-19}}</ref> led by Reagan Moore of the [[University of North Carolina]], uses the integrated Rule-Oriented Data System (iRODS) to provide data grid infrastructure for science and engineering. ''Terra Populus'',<ref>{{cite web|author=[[Steven Ruggles]] et al |url=http://www.terrapop.org/ |title=Terra Populus: Integrated Data on Population and the Environment |publisher=terrapop.org | accessdate=2013-01-19}}</ref> led by [[Steven Ruggles]] of the [[University of Minnesota]] focuses on tools for data integration across the domains of social science and environmental data, allowing interoperability of the three major data formats used in these domains: microdata, areal data, and raster data.\u000a\u000a==References==\u000a{{reflist|30em}}\u000a\u000a==External links==\u000a* [http://www.dataone.org DataONE]\u000a* [http://dataconservancy.org/ Data Conservancy]\u000a* [http://sead-data.net/ SEAD Sustainable Environment - Actionable Data]\u000a* [http://datafed.org/ DataNet Federation Consortium]\u000a* [http://www.terrapop.org/ Terra Populus: Integrated Data on Population and the Environment] \u000a \u000a\u000a[[Category:National Science Foundation]]\u000a[[Category:Science and technology in the United States]]\u000a[[Category:Information retrieval]]\u000a[[Category:Digital library projects]]
p294
sg6
S'Datanet'
p295
ssI237
(dp296
g2
S'http://en.wikipedia.org/wiki/SoundHound'
p297
sg4
V{{distinguish|SoundCloud}}\u000a{{Infobox software\u000a| name                   = Soundcloud\u000a| title                  = \u000a| logo                   = [[File:SoundHound Mobile Icon.png|250px]]\u000a| logo caption           = SoundHound Mobile Icon\u000a| screenshot             = <!-- [[File: ]] -->\u000a| caption                = \u000a| collapsible            = \u000a| author                 = \u000a| developer              = SoundHound, Inc\u000a| released               = {{Start date|2009|01|29|df=yes}}\u000a| discontinued           = \u000a| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->\u000a| programming language   = \u000a| operating system       = \u000a| platform               = \u000a| size                   = \u000a| language               = \u000a| language count         = <!-- DO NOT include this parameter unless you know what it does -->\u000a| language footnote      = \u000a| status                 = \u000a| genre                  = \u000a| license                = \u000a| alexa                  = \u000a| website                = {{URL|//www.soundhound.com/}}\u000a}}\u000a'''SoundHound''' (known as '''Midomi''' until December 2009) is a [[mobile device]] service that allows users to identify music by [[Query by humming|humming]], singing or playing a recorded track. The service was launched by Melodis Corporation (now SoundHound Inc), under [[Chief Executive]] Keyvan Mohajer in 2007 and has received funding from Global Catalyst Partners, TransLink Capital and Walden Venture Capital.\u000a\u000a==Features==\u000aSoundHound is a music search engine available on the [[Apple App Store]],<ref name="Apple App Store" /> [[Google Play]],<ref name="Google Play" /> [[Windows Phone Store]], and on June 5, 2013, was available on the BlackBerry 10 platform.<ref name="Windows" /> It enables users to identify music recorded through their device's microphone.<ref name=CNETv3 /> It is also possible to speak or type the name of the artist, composer, song and piece.<ref name=CNETv3 /> Unlike competitor [[Shazam (service)|Shazam]], SoundHound can recognise tracks from singing, humming, speaking, or typing, as well as from a recording.<ref>{{cite news|last=Dolcourt|first=Jessica|title=First Look video: Shazam for iPhone|url=http://download.cnet.com/8301-2007_4-9992639-12.html|accessdate=2 October 2012|newspaper=CNET|date=16 July 2008}}</ref> Sound matching is achieved through the company's 'Sound2Sound' technology, which  can match even poorly-hummed performances to professional recordings.<ref name="CNETWVC" />\u000a\u000aThe app then returns the lyrics (if any), links to videos on YouTube, links to iTunes, ringtones, the ability to launch [[Pandora Radio]],<ref name="TNWHound">{{cite news | url=http://thenextweb.com/apps/2011/05/26/soundhounds-new-voice-app-hound-wants-to-change-the-way-we-search/ | title=SoundHound\u2019s new voice app "Hound" wants to change the way we search | date=26 May 2011 | accessdate=2 October 2012 | last=Boyd Myers | first=Courtney | newspaper=The Next Web}}</ref> as well as recommendations for other music.<ref>{{cite news|last=Dolcourt|first=Jessica|title=SoundHound for iPhone channels iTunes, recommends beats|url=http://reviews.cnet.com/8301-19512_7-20037798-233.html|accessdate=2 October 2012|newspaper=CNET|date=2 March 2011}}</ref> A feature called LiveLyrics displays a song's lyrics in time with the music, if they are available. Double-tapping on those lyrics moves the music to that point in the song.<ref>{{cite news|last=Cabebe|first=Jaymar|title=SoundHound adds LiveLyrics|url=http://download.cnet.com/8301-2007_4-20081243-12/soundhound-adds-livelyrics/|accessdate=3 October 2012|newspaper=CNET|date=20 July 2011}}</ref> It is also possible for users to play music from their iPhone's iPod library through the app. If lyrics are available for a song, it will show them as it plays.<ref name="CNETv3" />\u000a\u000aThere are three versions of the app: SoundHound, SoundHound Infinity and Hound. SoundHound is free but has banner ads, while SoundHound Infinity (styled SoundHound \u221e), priced at £4.99 in the UK or $6.99 in the US, is the premium offering and has the same functionality but without banner ads.<ref name="CNETFree" /> Hound only allows users to search for artists or songs by speaking into it. Similar to the SoundHound app, Hound then returns a song preview, lyrics, album art and videos as well as artist bios and tour dates.<ref name="TNWHound" />\u000a\u000a==History==\u000aMidomi, renamed SoundHound in December 2009 with the launch of version 3.0 of the mobile app,<ref name=CNETv3>{{cite news|last=Dolcourt|first=Jessica|title=Midomi 3.0 seeks song lyrics, knows what's hot|url=http://reviews.cnet.com/8301-19512_7-10408563-233.html|accessdate=2 October 2012|newspaper=CNET|date=3 December 2009}}</ref> was launched in [[beta]] in January 2007, as a [http://www.midomi.com website], with 2 million licensed tracks.<ref name="CNET1" /> The technology, dubbed Multimodal Adaptive Recognition System (MARS), considers pitch, tempo variation, speech content and pauses in order to recognise samples.<ref name=CNET1>{{cite news|last=Mills|first=Elinor|title=This Web site can name that tune|url=http://news.cnet.com/This-Web-site-can-name-that-tune/2100-1027_3-6153657.html|accessdate=2 October 2012|newspaper=CNET|date=26 January 2007}}</ref> The company behind the site, Melodis Corporation, was started in 2004 by [[Chief Executive]] Keyvan Mohajer, a [[PhD]] in sound-recognition from [[Stanford]].<ref name=CNET1 /> Melodis changed its name to SoundHound Inc in May 2010.<ref>{{cite press release|title=SoundHound Inc. Announces Name Change from Melodis Corporation|publisher=SoundHound Inc|date=20 May 2010|url=//www.soundhound.com/index.php?action=s.press_release&pr=15|accessdate=2 October 2012}}</ref>\u000a\u000aThe first version of the app was released on the [[Apple App Store]] in July 2008.<ref name="Apple App Store">{{cite news | url=http://news.cnet.com/8301-17938_105-9987892-1.html | title=Sing for search results with iPhone app | date=10 July 2008 | accessdate=2 October 2012 | last=Jackson | first=Holly | newspaper=CNET}}</ref> At the launch of [[Windows Marketplace for Mobile]] in October 2009, Midomi was one of the apps included in the store<ref name="Windows">{{cite news | url=http://reviews.cnet.com/8301-12261_7-10368174-10356022.html | title=Windows mobile app store, My Phone service officially opening | date=6 October 2009 | accessdate=2 October 2012 | last=Dolcourt | first=Jessica | newspaper=CNET}}</ref> and could be purchased for $4.99.<ref>{{cite news|last=Dolcourt|first=Jessica|title=Shazam debuts in Windows Marketplace for Mobile|url=http://reviews.cnet.com/8301-12261_7-10368986-10356022.html|accessdate=2 October 2012|newspaper=CNET|date=7 October 2009}}</ref> It joined the [[Android OS|Android]] app store in June 2010.<ref name="Google Play">{{cite news | url=http://www.cnet.com/8301-19736_1-20007745-251.html | title=New SoundHound names that tune--for free (Android) | date=15 June 2010 | accessdate=3 October 2012 | last=Dolcourt | first=Jessica | newspaper=CNET}}</ref> On January 2013, the [[BlackBerry]] version of the app was then available in [[BlackBerry World]] following the announcement and launch of [[BlackBerry 10]].<ref name="BlackBerry">{{cite web\u000a  |title=BlackBerry shows off some of its 70,000 new third-party apps, including Skype, Rdio, Kindle, and Whatsapp\u000a  |publisher=[[The Verge]]\u000a  |url=http://www.theverge.com/2013/1/30/3932042/blackberry-10-apps-announcement\u000a  |accessdate=2013-01-30}}</ref>\u000a\u000aA free version of the app was released in April 2010, with all the functionality of the premium version, while limiting the number of searches to five per month, and adding banners ads.<ref name="CNETFree">{{cite news | url=http://reviews.cnet.com/8301-19512_7-20003228-233.html | title=Sonic freebie: New, free SoundHound music-ID app for iPhone, iPad | date=27 April 2010 | accessdate=3 October 2012 | last=Dolcourt | first=Jessica | newspaper=CNET}}</ref> The premium version was now renamed SoundHound Infinity.<ref name="CNETFree" /> A stripped-down version, Hound, was released in May 2011.<ref name="TNWHound" />\u000a\u000aIn January 2011, Apple revealed that SoundHound was the top paid iPad app  on its [[Apple App Store|App Store]] was SoundHound, while rival Shazam was fourth in the top ten list of free iPhone apps.<ref>{{cite news|last=Reisinger|first=Don|title=Apple reveals top apps of all time|url=http://news.cnet.com/8301-13506_3-20028889-17.html|accessdate=2 October 2012|newspaper=CNET|date=19 January 2011}}</ref>\u000a\u000aIn June 2012, the firm announced that it had 80 million users while version 5.0 was released, with a new design and features that include an in-built player and integration with LiveLyrics.<ref name="TNW80m">{{cite news | url=http://thenextweb.com/apps/2012/06/07/shazam-competitor-soundhound-passes-80m-users-and-rolls-out-updated-mobile-apps/ | title=Shazam competitor SoundHound passes 80m users and rolls out updated mobile apps | work=The Next Web | date=7 June 2012 | accessdate=2 October 2012 | author=Sawers, Paul}}</ref>\u000a\u000aIn December 2013, the app passed 185 million users.<ref>{{cite news|title=SoundHound Reveals Its Top Songs of 2013|url=http://www.heraldonline.com/2013/12/16/5509030/soundhound-reveals-its-top-songs.html|accessdate=16 December 2013|newspaper=The Next Web|date=16 December 2013}}</ref>\u000a\u000aIn December 2013, the app launches iTunes Radio integration.<ref>{{cite news|title=SoundHound App Update Adds iTunes Radio Integration for iPad and iPhone Users. |url=http://www.padgadget.com/2013/12/20/soundhound-app-update-adds-itunes-radio-integration-for-ipad-and-iphone-users/|accessdate=11 February 2014|newspaper=PadGadget|date=20 December 2013}}</ref>\u000a\u000aIn September 2013, the app enables 170 million global users to sync, save, and transfer music search & discovery history across multiple devices.<ref>{{cite news|title=SoundHound adds cloud history sync on iOS and Android apps|url=http://www.intomobile.com/2013/09/25/soundhound-adds-cloud-history-sync-ios-and-android-apps/|accessdate=11 February 2014|newspaper=INTOMOBILE|date=20 September 2013}}</ref>\u000a\u000aIn January 2014, SoundHound and Hyundai Motor Group partnered to embed music search and discovery into select 2014 Hyundai & Kia models.<ref>{{cite news|title=Hyundai and Kia tap SoundHound to help you identify music in your car|url=http://www.engadget.com/2014/01/14/hyundai-kia-soundhound-music-tagging/|accessdate=11 February 2014|newspaper=Engadget|date= January 14, 2014}}</ref>\u000a\u000aIn January 2014, the app launched an "immersive second screen GRAMMYs experience".<ref>{{cite news|title=SoundHound's music search app turns its focus to the Grammys with real-time updates and more|url=http://www.engadget.com/2014/01/25/soundhound-grammys-2014/|accessdate=11 February 2014|newspaper=Engadget|date= January 24, 2014}}</ref>\u000a\u000aIn April 2014, the app passed 200 million users.<ref>//www.soundhound.com/index.php?action=s.press_release&pr=67</ref>\u000a\u000a===Funding===\u000aMelodis secured $7 million in a Series B funding round in October 2008, bringing total funds raised to $12 million. The round was led by TransLink Capital with the participation of JAIC America and [[Series A round|Series A]] investor Global Catalyst Partners.<ref>{{cite press release|title=Search and Sound Recognition Innovator MELODIS and Creator of Midomi Raises $7 Million in Series B Funding|publisher=Melodis Corporation|date=7 October 2008|url=//www.soundhound.com/index.php?action=s.press_release&pr=5|accessdate=2 October 2012}}</ref>\u000a\u000aIn 2009, Melodis attracted additional funding from Larry Marcus at Walden Venture Capital, who had previously invested in music startups [[Pandora Radio|Pandora]] and [[SNOCAP|Snocap]].<ref name=CNETWVC>{{cite news|last=Needleman|first=Rafe|title=Midomi music search gets funding and opportunities|url=http://news.cnet.com/8301-19882_3-10298068-250.html|accessdate=2 October 2012|newspaper=CNET|date=28 July 2009}}</ref> The $4 million funding round was led by Walden Venture Capital VII, with the participation of an unnamed device manufacturer.<ref>{{cite press release|title=Melodis, Sound Search Technology and Applications Innovator, Raises $4M Led by Walden Venture Capital and a Strategic Investor|publisher=Melodis Corporation|date=4 August 2009|url=//www.soundhound.com/index.php?action=s.press_release&pr=12|accessdate=2 October 2012}}</ref>\u000a\u000a==See also==\u000a*[[Query by humming]]\u000a\u000a==References==\u000a{{reflist|2}}\u000a\u000a==External links==\u000a*[http://midomi.com midomi.com]\u000a*[//www.soundhound.com/ SoundHound website]\u000a\u000a[[Category:Android (operating system) software]]\u000a[[Category:IOS software]]\u000a[[Category:Symbian software]]\u000a[[Category:Music search engines]]\u000a[[Category:Companies established in 2005]]\u000a[[Category:Companies based in California]]\u000a[[Category:BlackBerry software]]
p298
sg6
S'SoundHound'
p299
ssI111
(dp300
g2
S'http://en.wikipedia.org/wiki/XML retrieval'
p301
sg4
V{{Multiple issues|\u000a{{expert-subject|Computer science|date=January 2015}}\u000a{{COI|date=February 2009}}\u000a}}\u000a\u000a'''XML retrieval''', or XML Information Retrieval, is the content-based retrieval of documents structured with [[XML]] (eXtensible Markup Language). As such it is used for computing [[Relevance (information retrieval)|relevance]] of XML documents.<ref>{{Cite web|url=ftp://ftp.tm.informatik.uni-frankfurt.de/pub/papers/ir/An%20Architecture%20for%20XML%20Information%20Retrieval%20in%20a%20Peer-to-Peer%20Environment_2007.pdf|title=An Architecture for XML Information Retrieval in a Peer-to-Peer Environment|last=Winter|first=Judith|author2=Drobnik, Oswald |date=November 9, 2007|publisher=ACM|accessdate=2009-02-10}}</ref>\u000a\u000a==Queries==\u000aMost XML retrieval approaches do so based on techniques from the [[information retrieval]] (IR) area, e.g. by computing the similarity between a query consisting of keywords (query terms) and the document. However, in XML-Retrieval the query can also contain [[Data structure|structural]] [[Hint (SQL)|hints]]. So-called "content and structure" (CAS) queries enable users to specify what structure the requested content can or must have.\u000a\u000a==Exploiting XML structure==\u000aTaking advantage of the [[Self-documenting|self-describing]] structure of XML documents can improve the search for XML documents significantly. This includes the use of CAS queries, the weighting of different XML elements differently and the focused retrieval of subdocuments.\u000a\u000a==Ranking==\u000aRanking in XML-Retrieval can incorporate both content relevance and structural similarity, which is the resemblance between the structure given in the query and the structure of the document. Also, the retrieval units resulting from an XML query may not always be entire documents, but can be any deeply nested XML elements, i.e. dynamic documents. The aim is to find the smallest retrieval unit that is highly relevant. Relevance can be defined according to the notion of specificity, which is the extent to which a retrieval unit focuses on the topic of request.<ref name="INEX2006">{{Cite web|url=http://www.cs.otago.ac.nz/homepages/andrew/2006-10.pdf|title=Overview of INEX 2006|last=Malik|first=Saadia|author2=Trotman, Andrew |author3=Lalmas, Mounia |author4= Fuhr, Norbert |year=2007|work=Proceedings of the Fifth Workshop of the INitiative for the Evaluation of XML Retrieval|accessdate=2009-02-10}}</ref>\u000a\u000a==Existing XML search engines==\u000aAn overview of two potential approaches is available.<ref>{{Cite web|url=http://www.sigmod.org/record/issues/0612/p16-article-yahia.pdf|title=XML Search: Languages, INEX and Scoring|last=Amer-Yahia|first=Sihem|author2=Lalmas, Mounia |year=2006|publisher=SIGMOD Rec. Vol. 35, No. 4|accessdate=2009-02-10}} {{Dead link|date=October 2010|bot=H3llBot}}</ref><ref>{{Cite web|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.5986&rep=rep1&type=pdf|title=XML Retrieval: A Survey|last=Pal|first=Sukomal|date=June 30, 2006|publisher=Technical Report, CVPR|accessdate=2013-07-04}}</ref> The INitiative for the Evaluation of XML-Retrieval (''INEX'') was founded in 2002 and provides a platform for evaluating such [[algorithm]]s.<ref name="INEX2006" /> Three different areas influence XML-Retrieval:<ref name="INEX2002">{{Cite web|url=http://www.is.informatik.uni-duisburg.de/bib/pdf/ir/Fuhr_etal:02a.pdf|title=INEX: Initiative for the Evaluation of XML Retrieval|last=Fuhr|first=Norbert|author2=Gövert, N. |author3=Kazai, Gabriella |author4= Lalmas, Mounia |year=2003|work=Proceedings of the First INEX Workshop, Dagstuhl, Germany, 2002|publisher=ERCIM Workshop Proceedings, France|accessdate=2009-02-10}}</ref>\u000a\u000a===Traditional XML query languages===\u000a[[Query language]]s such as the [[W3C]] standard [[XQuery]]<ref>{{Cite web|url=http://www.w3.org/TR/2007/REC-xquery-20070123/|title=XQuery 1.0: An XML Query Language|last=Boag|first=Scott|author2=Chamberlin, Don |author3=Fernández, Mary F. |author4=Florescu, Daniela |author5=Robie, Jonathan |author6= Siméon, Jérôme |date=23 January 2007|work=W3C Recommendation|publisher=World Wide Web Consortium|accessdate=2009-02-10}}</ref> supply complex queries, but only look for exact matches. Therefore, they need to be extended to allow for vague search with relevance computing. Most XML-centered approaches imply a quite exact knowledge of the documents' [[Database schema|schemas]].<ref name="Schlieder2002">{{Cite journal|url=http://web.archive.org/web/20070610002349/http://www.cis.uni-muenchen.de/people/Meuss/Pub/JASIS02.ps.gz|title=Querying and Ranking XML Documents|last=Schlieder|first=Torsten|author2=Meuss, Holger |year=2002|work= Journal of the American Society for Information Science and Technology, Vol. 53, No. 6|accessdate=2009-02-10}}</ref>\u000a\u000a===Databases===\u000aClassic [[database]] systems have adopted the possibility to store [[Semi-structured model|semi-structured data]]<ref name="INEX2002" /> and resulted in the development of [[XML database]]s. Often, they are very formal, concentrate more on searching than on ranking, and are used by experienced users able to formulate complex queries.\u000a\u000a===Information retrieval===\u000aClassic information retrieval models such as the [[vector space model]] provide relevance ranking, but do not include document structure; only flat queries are  supported. Also, they apply a static document concept, so retrieval units usually are entire documents.<ref name="Schlieder2002"/> They can be extended to consider structural information and dynamic document retrieval. Examples for approaches extending the vector space models are available: they use document [[subtree]]s (index terms plus structure) as dimensions of the vector space.<ref>{{Cite web|url=http://www.cobase.cs.ucla.edu/tech-docs/sliu/SIGIR04.pdf|title=Configurable Indexing and Ranking for XML Information Retrieval|last=Liu|first=Shaorong|author2=Zou, Qinghua |author3=Chu, Wesley W. |year=2004|work=SIGIR'04|publisher=ACM|accessdate=2009-02-10}}</ref>\u000a\u000a==See also==\u000a*[[Document retrieval]]\u000a*[[Information retrieval applications]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a{{DEFAULTSORT:Xml-Retrieval}}\u000a[[Category:XML]]\u000a[[Category:Information retrieval]]
p302
sg6
S'XML retrieval'
p303
ssI114
(dp304
g2
S'http://en.wikipedia.org/wiki/Nearest neighbor search'
p305
sg4
V'''Nearest neighbor search''' ('''NNS'''), also known as '''proximity search''', '''similarity search''' or '''[[Closest pair of points problem|closest point search]]''',  is an [[optimization problem]] for finding closest (or most similar) points. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set ''S'' of points in a space ''M'' and a query point ''q''&nbsp;\u2208&nbsp;''M'', find the closest point in ''S'' to ''q''. [[Donald Knuth]] in vol. 3 of ''[[The Art of Computer Programming]]'' (1973) called it the '''post-office problem''', referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a ''k''-NN search, where we need to find the ''k'' closest points.\u000a\u000aMost commonly ''M'' is a  [[metric space]] and dissimilarity is expressed as a [[distance metric]], which is symmetric and satisfies the [[triangle inequality]]. Even more common, ''M'' is taken to be the ''d''-dimensional [[vector space]] where dissimilarity is measured using the [[Euclidean distance]], [[Taxicab geometry|Manhattan distance]] or other [[Statistical distance|distance metric]]. However, the dissimilarity function can be arbitrary. One example are asymmetric [[Bregman divergence]]s, for which the triangle inequality does not hold.<ref name=Cayton2008>{{Cite journal\u000a | last1 = Cayton | first1 = Lawerence\u000a | year = 2008\u000a | title =  Fast nearest neighbor retrieval for bregman divergences.\u000a | journal = Proceedings of the 25th international conference on Machine learning\u000a | pages = 112\u2013119\u000a}}</ref>\u000a\u000a==Applications==\u000a\u000aThe nearest neighbor search problem arises in numerous fields of application, including:\u000a*[[Pattern recognition]] - in particular for [[optical character recognition]]\u000a*[[Statistical classification]]- see [[k-nearest neighbor algorithm]]\u000a*[[Computer vision]]\u000a*[[Computational Geometry]] - see [[Closest pair of points problem]]\u000a*[[Database]]s - e.g. [[content-based image retrieval]]\u000a*[[Coding theory]] - see [[Decoding methods|maximum likelihood decoding]]\u000a*[[Data compression]] - see [[MPEG-2]] standard\u000a*[[Recommender system|Recommendation systems]], e.g. see [[Collaborative filtering]]\u000a*[[Internet marketing]] - see [[contextual advertising]] and [[behavioral targeting]]\u000a*[[DNA sequencing]]\u000a*[[Spell checking]] - suggesting correct spelling\u000a*[[Plagiarism detection]]\u000a*[[Contact searching algorithms in FEA]]\u000a*[[Similarity score]]s for predicting career paths of professional athletes.\u000a*[[Cluster analysis]] - assignment of a set of observations into subsets (called clusters) so that observations in the same cluster are similar in some sense, usually based on [[Euclidean distance]]\u000a*[[Chemical similarity]]\u000a*[[Motion planning#Sampling-Based Algorithms|Sampling-Based Motion Planning]]\u000a\u000a==Methods==\u000a\u000aVarious solutions to the NNS problem have been proposed.  The quality and usefulness of the algorithms are determined by the time complexity of queries as well as the space complexity of any search data structures that must be maintained. The informal observation usually referred to as the [[curse of dimensionality]] states that there is no general-purpose exact solution for NNS in high-dimensional Euclidean space using polynomial preprocessing and polylogarithmic search time.\u000a\u000a===Linear search===\u000aThe simplest solution to the NNS problem is to compute the distance from the query point to every other point in the database, keeping track of the "best so far".  This algorithm, sometimes referred to as the naive approach, has a [[running time]] of ''O''(''dN'') where ''N'' is the [[cardinality]] of ''S'' and ''d'' is the dimensionality of ''M''.  There are no search data structures to maintain, so linear search has no space complexity beyond the storage of the database. Naive search can, on average, outperform space partitioning approaches on higher dimensional spaces.<ref>{{cite web|title=A quantitative analysis and performance study for similarity search methods in high dimensional spaces|author=Weber, Schek, Blott | url=http://www.vldb.org/conf/1998/p194.pdf}}</ref>\u000a\u000a===Space partitioning===\u000aSince the 1970s, [[branch and bound]] methodology has been applied to the problem. In the case of Euclidean space this approach is known as [[spatial index]] or spatial access methods. Several [[Space partitioning|space-partitioning]] methods have been developed for solving the NNS problem.  Perhaps the simplest is the [[k-d tree]], which iteratively bisects the search space into two regions containing half of the points of the parent region.  Queries are performed via traversal of the tree from the root to a leaf by evaluating the query point at each split. Depending on the distance specified in the query, neighboring branches that might contain hits may also need to be evaluated. For constant dimension query time, average complexity is ''O''(log&nbsp;''N'') <ref>{{cite web|title=An introductory tutorial on KD trees|author=Andrew Moore | url=http://www.autonlab.com/autonweb/14665/version/2/part/5/data/moore-tutorial.pdf?branch=main&language=en}}</ref> in the case of randomly distributed points, worst case complexity analyses have been performed.<ref name=Lee1977>{{Cite journal\u000a | last1 = Lee | first1 = D. T. | author1-link = Der-Tsai Lee\u000a | last2 = Wong | first2 = C. K.\u000a | year = 1977\u000a | title = Worst-case analysis for region and partial region searches in multidimensional binary search trees and balanced quad trees\u000a | journal = Acta Informatica\u000a | volume = 9\u000a | issue = 1\u000a | pages = 23\u201329\u000a | doi = 10.1007/BF00263763\u000a | postscript = .\u000a}}</ref>\u000aAlternatively the [[R-tree]] data structure was designed to support nearest neighbor search in dynamic context, as it has efficient algorithms for insertions and deletions such as the [[R* tree]].<ref>{{cite doi|10.1145.2F223784.223794}}</ref> R-trees can yield nearest neighbors not only for Euclidean distance, but can also be used with other distances.\u000a\u000aIn case of general metric space branch and bound approach is known under the name of [[metric trees]]. Particular examples include [[vp-tree]] and [[BK-tree]].\u000a\u000aUsing a set of points taken from a 3-dimensional space and put into a [[Binary space partitioning|BSP tree]], and given a query point taken from the same space, a possible solution to the problem of finding the nearest point-cloud point to the query point is given in the following description of an algorithm.  (Strictly speaking, no such point may exist, because it may not be unique.  But in practice, usually we only care about finding any one of the subset of all point-cloud points that exist at the shortest distance to a given query point.)  The idea is, for each branching of the tree, guess that the closest point in the cloud resides in the half-space containing the query point.  This may not be the case, but it is a good heuristic.  After having recursively gone through all the trouble of solving the problem for the guessed half-space, now compare the distance returned by this result with the shortest distance from the query point to the partitioning plane.  This latter distance is that between the query point and the closest possible point that could exist in the half-space not searched.  If this distance is greater than that returned in the earlier result, then clearly there is no need to search the other half-space.  If there is such a need, then you must go through the trouble of solving the problem for the other half space, and then compare its result to the former result, and then return the proper result.  The performance of this algorithm is nearer to logarithmic time than linear time when the query point is near the cloud, because as the distance between the query point and the closest point-cloud point nears zero, the algorithm needs only perform a look-up using the query point as a key to get the correct result.\u000a\u000a===Locality sensitive hashing===\u000a\u000a[[Locality sensitive hashing]] (LSH) is a technique for grouping points in space into 'buckets' based on some distance metric operating on the points. Points that are close to each other under the chosen metric are mapped to the same bucket with high probability.<ref>{{cite web|author=A. Rajaraman and J. Ullman| url=http://infolab.stanford.edu/~ullman/mmds.html |title=Mining of Massive Datasets, Ch. 3. |year=2010}}</ref>\u000a\u000a===Nearest neighbor search in spaces with small intrinsic dimension===\u000a\u000aThe [[cover tree]] has a theoretical bound that is based on the dataset's [[doubling constant]]. The bound on search time is ''O''(''c''<sup>12</sup>&nbsp;log&nbsp;''n'') where ''c''  is the [[Expansivity constant|expansion constant]] of the dataset.\u000a\u000a===Vector approximation files===\u000a\u000aIn high dimensional spaces, tree indexing structures become useless because an increasing percentage of the nodes need to be examined anyway. To speed up linear search, a compressed version of the feature vectors stored in RAM is used to prefilter the datasets in a first run. The final candidates are determined in a second stage using the uncompressed data from the disk for distance calculation.<ref>{{cite web|title=An Approximation-Based Data Structure for Similarity Search|author=Weber, Blott}}</ref>\u000a\u000a===Compression/clustering based search===\u000aThe VA-file approach is a special case of a compression based search, where each feature component is compressed uniformly and independently. The optimal compression technique in multidimensional spaces is Vector Quantization (VQ), implemented through clustering. The database is clustered and the most "promising" clusters are retrieved. Huge gains over VA-File, tree-based indexes and sequential scan have been observed.<ref>{{cite web|title=Adaptive cluster-distance bounding for similarity search in image databases|author=Ramaswamy, Rose, ICIP 2007}}</ref><ref>{{cite web|title=Adaptive cluster-distance bounding for high-dimensional indexing|author=Ramaswamy, Rose, TKDE 2010}}</ref> Also note the parallels between clustering and LSH.\u000a\u000a===Greedy walks===\u000aOne possible way to solve NNS is to construct a graph <math>G(V,E)</math>, where every point <math>x_i \u005cin S </math> is uniquely associated with vertex <math>v_i \u005cin V </math>. The search of the point in the set ''S'' closest to the query ''q'' takes the form of the search of vertex in the graph <math>G(V,E)</math>.\u000aOne of the basic vertex search algorithms in graphs with metric objects is the greedy search algorithm. It starts from the random vertex <math>v_i \u005cin V </math>. The algorithm computes a distance value from the query q to each vertex from the neighborhood <math>\u005c{v_j:(v_i,v_j) \u005cin E\u005c}</math> of  the current vertex <math>v_i</math>, and then selects a vertex with the minimal distance value. If the distance value between the query and the selected vertex is smaller than the one between the query and the current element, then the algorithm moves to the selected vertex, and it becomes new current vertex. The algorithm stops when it reaches a local minimum: a vertex whose neighborhood does not contain a vertex that is closer to the query than the vertex itself.\u000aThis idea was exploited in VoroNet system <ref name=voroNet>{{Cite journal\u000a | last1 = Olivier | first1 = Beaumont  \u000a | last2 = Kermarrec | first2 = Anne-Marie\u000a | last3 = Marchal | first3 = Loris \u000a | last4 = Rivière | first4 = Etienne   \u000a | year = 2006\u000a | title = VoroNet: A scalable object network based on Voronoi tessellations\u000a | journal = INRIA\u000a | volume = RR-5833\u000a | issue = 1\u000a | pages = 23\u201329\u000a | doi = 10.1007/BF00263763\u000a | postscript = .\u000a}}</ref> for the plane, in RayNet system <ref name=rayNet>{{Cite journal\u000a | last1 = Olivier | first1 = Beaumont  \u000a | last2 = Kermarrec | first2 = Anne-Marie\u000a | last4 = Rivière | first4 = Etienne   \u000a | year = 2007\u000a | title = Peer to Peer Multidimensional Overlays: Approximating Complex Structures\u000a | journal = Principles of Distributed Systems\u000a | volume =  4878\u000a | issue = .\u000a | pages = 315\u2013328\u000a | doi = 10.1007/978-3-540-77096-1_23\u000a | isbn = 978-3-540-77095-4\u000a | postscript = .\u000a}}</ref> for the <math>\u005cmathbb{E}^n</math> and for the general metric space in Metrized Small World algorithm <ref name=msw2014>{{Cite journal\u000a | last1 = Malkov | first1 = Yury  \u000a | last2 = Ponomarenko | first2 = Alexander\u000a | last3 = Krylov | first3 = Vladimir \u000a | last4 = Logvinov | first4 = Andrey   \u000a | year = 2014\u000a | title = Approximate nearest neighbor algorithm based on navigable small world graphs\u000a | journal = Information Systems\u000a | volume = 45\u000a | pages = 61\u201368\u000a | doi = 10.1016/j.is.2013.10.006\u000a | postscript = .\u000a}}</ref>\u000a\u000a==Variants==\u000a\u000aThere are numerous variants of the NNS problem and the two most well-known are the [[K-nearest neighbor algorithm|''k''-nearest neighbor search]] and the [[&epsilon;-approximate nearest neighbor search]].\u000a\u000a===<span id="K-nearest neighbor"> ''k''-nearest neighbor </span>===\u000a\u000a[[K-nearest neighbor algorithm|''k''-nearest neighbor search]] identifies the top ''k'' nearest neighbors to the query.  This technique is commonly used in predictive analytics to estimate or classify a point based on the consensus of its neighbors. ''k''-nearest neighbor graphs are graphs in which every point is connected to its ''k'' nearest neighbors.\u000a\u000a===Approximate nearest neighbor===\u000aIn some applications it may be acceptable to retrieve a "good guess" of the nearest neighbor. In those cases, we can use an algorithm which doesn't guarantee to return the actual nearest neighbor in every case, in return for improved speed or memory savings. Often such an algorithm will find the nearest neighbor in a majority of cases, but this depends strongly on the dataset being queried.\u000a\u000aAlgorithms that support the approximate nearest neighbor search include [[Locality-sensitive hashing#LSH algorithm for nearest neighbor search|locality-sensitive hashing]], [[best bin first]] and [[balanced box-decomposition tree]] based search.<ref>S. Arya, [[David Mount|D. M. Mount]], [[Nathan Netanyahu|N. S. Netanyahu]], R. Silverman and A. Wu, An optimal algorithm for approximate nearest neighbor searching, Journal of the ACM, 45(6):891-923, 1998. [http://www.cse.ust.hk/faculty/arya/pub/JACM.pdf]</ref>\u000a\u000a===Nearest neighbor distance ratio===\u000a\u000a[[Nearest neighbor distance ratio]] do not apply the threshold on the direct distance from the original point to the challenger neighbor but on a ratio of it depending on the distance to the previous neighbor. It is used in [[Content-based image retrieval|CBIR]] to retrieve pictures through a "query by example" using the similarity between local features. More generally it is involved in several [[Pattern matching|matching]] problems.\u000a\u000a===Fixed-radius near neighbors===\u000a\u000a[[Fixed-radius near neighbors]] is the problem where one wants to efficiently find all points given in [[Euclidean space]] within a given fixed distance from a specified point. The data structure should work on a distance which is fixed however the query point is arbitrary.\u000a\u000a===All nearest neighbors===\u000a\u000aFor some applications (e.g. [[entropy estimation]]), we may have ''N'' data-points and wish to know which is the nearest neighbor ''for every one of those N points''. This could of course be achieved by running a nearest-neighbor search once for every point, but an improved strategy would be an algorithm that exploits the information redundancy between these ''N'' queries to produce a more efficient search. As a simple example: when we find the distance from point ''X'' to point ''Y'', that also tells us the distance from point ''Y'' to point ''X'', so the same calculation can be reused in two different queries.\u000a\u000aGiven a fixed dimension, a semi-definite positive norm (thereby including every  [[lp space|L<sup>p</sup> norm]]), and ''n'' points in this space, the nearest neighbour of every point can be found in ''O''(''n''&nbsp;log&nbsp;''n'') time and the ''m'' nearest neighbours of every point can be found in ''O''(''mn''&nbsp;log&nbsp;''n'') time.<ref>{{citation\u000a | last = Clarkson | first = Kenneth L. | author-link = Kenneth L. Clarkson\u000a | contribution = Fast algorithms for the all nearest neighbors problem\u000a | doi = 10.1109/SFCS.1983.16\u000a | pages = 226\u2013232\u000a | title = 24th IEEE Symp. Foundations of Computer Science, (FOCS '83)\u000a | year = 1983| isbn = 0-8186-0508-1 }}.</ref><ref name=Vaidya>{{Cite journal\u000a | doi = 10.1007/BF02187718\u000a | last1 = Vaidya | first1 = P. M.\u000a | year = 1989\u000a | title = An ''O''(''n''&nbsp;log&nbsp;''n'') Algorithm for the All-Nearest-Neighbors Problem \u000a | journal = [[Discrete and Computational Geometry]]\u000a | volume = 4\u000a | issue = 1\u000a | pages = 101\u2013115\u000a | url = http://www.springerlink.com/content/p4mk2608787r7281/?p=09da9252d36e4a1b8396833710ef08cc&pi=8\u000a | postscript = .\u000a}}</ref>\u000a\u000a==See also==\u000a{{div col|colwidth=20em}}\u000a* [[Range search]]\u000a* [[Set cover problem]]\u000a*[[Statistical distance]]\u000a*[[Closest pair of points problem]]\u000a*[[Ball tree]]\u000a*[[Cluster analysis]]\u000a*[[Neighbor joining]]\u000a*[[Content-based image retrieval]]\u000a*[[Curse of dimensionality]]\u000a*[[Digital signal processing]]\u000a*[[Dimension reduction]]\u000a*[[Fixed-radius near neighbors]]\u000a*[[Fourier analysis]]\u000a*[[Instance-based learning]]\u000a*[[k-nearest neighbor algorithm|''k''-nearest neighbor algorithm]]\u000a*[[Linear least squares (mathematics)|Linear least squares]]\u000a*[[Locality sensitive hashing]]\u000a*[[Multidimensional analysis]]\u000a*[[Nearest-neighbor interpolation]]\u000a*[[Principal component analysis]]\u000a*[[Singular value decomposition]]\u000a*[[Time series]]\u000a*[[Voronoi diagram]]\u000a*[[Wavelet]]\u000a*[[MinHash]]\u000a{{div col end}}\u000a\u000a==Notes==\u000a<references/>\u000a\u000a==References==\u000a*Andrews, L.. A template for the nearest neighbor problem.  ''C/C++ Users Journal'', vol. 19, no 11 (November 2001), 40 - 49, 2001, ISSN:1075-2838, [http://www.ddj.com/architect/184401449 www.ddj.com/architect/184401449]\u000a*Arya, S., D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Y. Wu.  An Optimal Algorithm for Approximate Nearest Neighbor Searching in Fixed Dimensions.  ''Journal of the ACM'', vol. 45, no. 6, pp.&nbsp;891\u2013923\u000a*Beyer, K., Goldstein, J., Ramakrishnan, R., and Shaft, U. 1999. When is nearest neighbor meaningful? In Proceedings of the 7th ICDT, Jerusalem, Israel.\u000a*Chung-Min Chen and Yibei Ling - A Sampling-Based Estimator for Top-k Query. ICDE 2002: 617-627\u000a*Samet, H. 2006. Foundations of Multidimensional and Metric Data Structures. Morgan Kaufmann. ISBN 0-12-369446-9\u000a*Zezula, P., Amato, G., Dohnal, V., and Batko, M. Similarity Search - The Metric Space Approach. Springer, 2006. ISBN 0-387-29146-6\u000a\u000a==Further reading==\u000a*{{cite book | last = Shasha | first = Dennis | title = High Performance Discovery in Time Series | publisher = Springer | location = Berlin | year = 2004 | isbn = 0-387-00857-8 }}\u000a\u000a==External links==\u000a*[http://simsearch.yury.name/tutorial.html Nearest Neighbors and Similarity Search] \u2013 a website dedicated to educational materials, software, literature, researchers, open problems and events related to NN searching. Maintained by Yury Lifshits\u000a*[http://sswiki.tierra-aoi.net Similarity Search Wiki] \u2013 a collection of links, people, ideas, keywords, papers, slides, code and data sets on nearest neighbours\u000a*[http://www.kgraph.org KGraph] \u2013 a C++ library for fast approximate nearest neighbor search with user-provided distance metric by Wei Dong.\u000a*[http://www.cs.ubc.ca/research/flann/ FLANN] \u2013 a library for performing fast approximate nearest neighbor searches in high dimensional spaces  by Marius Muja and David G. Low\u000a*[http://sisap.org/?f=library Metric Spaces Library] \u2013 An open-source C-based library for metric space indexing by Karina Figueroa, Gonzalo Navarro, Edgar Chávez\u000a*[https://github.com/searchivarius/NonMetricSpaceLib  Non-Metric Space Library] \u2013 An open-source C++ library for exact and approximate searching in non-metric and metric spaces\u000a*[http://www.cs.umd.edu/~mount/ANN/ ANN] \u2013 A library for Approximate Nearest Neighbor searching by David M. Mount and Sunil Arya\u000a*[http://www.irisa.fr/texmex/people/jegou/ann.php Product Quantization] \u2013 Matlab implementation of approximate nearest neighbor search in the compressed domain by Herve Jegou\u000a*[http://lsd.fi.muni.cz/trac/messif MESSIF] \u2013 Metric Similarity Search Implementation Framework by Michal Batko and David Novak\u000a*[http://www.obsearch.net/ OBSearch] \u2013 Similarity Search engine for Java (GPL); implementation by Arnoldo Muller, developed during Google Summer of Code 2007\u000a*[http://mrim.imag.fr/georges.quenot/freesoft/knnlsb/ KNNLSB] \u2013 K Nearest Neighbors Linear Scan Baseline (distributed, LGPL); implementation by Georges Quénot (LIG-CNRS)\u000a*[http://neartree.sourceforge.net/ NearTree] \u2013 An API for finding nearest neighbors among points in spaces of arbitrary dimensions by Lawrence C. Andrews and Herbert J. Bernstein\u000a*[http://nearpy.io/ NearPy] \u2013 Python framework for fast approximated nearest neighbor search by Ole Krause-Sparmann\u000a*[http://www.cgal.org/Pkg/SpatialSearchingD dD Spatial Searching] in [[CGAL]] \u2013 the Computational Geometry Algorithms Library\u000a*[https://github.com/ryanrhymes/panns Panns] \u2013 A Python library for searching approximate nearest neighbors, optimized for large dataset with high dimensional features, developed by Liang Wang\u000a\u000a{{DEFAULTSORT:Nearest Neighbor Search}}\u000a[[Category:Approximation algorithms]]\u000a[[Category:Classification algorithms]]\u000a[[Category:Data mining]]\u000a[[Category:Discrete geometry]]\u000a[[Category:Geometric algorithms]]\u000a[[Category:Information retrieval]]\u000a[[Category:Machine learning]]\u000a[[Category:Numerical analysis]]\u000a[[Category:Mathematical optimization]]\u000a[[Category:Searching]]\u000a[[Category:Search algorithms]]
p306
sg6
S'Nearest neighbor search'
p307
ssI117
(dp308
g2
S'http://en.wikipedia.org/wiki/Pikimal'
p309
sg4
V{{Infobox company |\u000a name   = Pikimal |\u000a logo   = [[File:Pikimal logo.jpg|200px]] |\u000a type   = [[Limited liability company|LLC]]|\u000a company_slogan = |\u000a|founder = Eric Silver|\u000a foundation     = 2010|\u000a location       = [[Pittsburgh, Pennsylvania]], [[United States]]|\u000a key_people     = Eric Silver, Chief Executive Officer|\u000a industry       = [[Search engine technology|Search]] |\u000a<!--Please fill in:-->\u000a revenue        = |\u000a operating_income = |\u000a net_income     = | \u000a num_employees  = 13<ref name="businesstimes">{{cite news|url=http://www.bizjournals.com/pittsburgh/print-edition/2011/03/25/pikimal-comparison-shopping-stand-out.html | title=Pikimal, a comparison shopping website, hopes to stand out from crowd | work= Pittsburgh Business Times | first=Malia | last=Spencer | date=25 March 2011}}</ref> |\u000a homepage       = [http://pikimal.com/ www.pikimal.com]|\u000a}}\u000a\u000a'''Pikimal''' (pronounced as pick-em-all)<ref>{{cite web|url=http://www.popcitymedia.com/innovationnews/pikimal033011.aspx | title=Shop Smarter With Pikimal \u2013 POP City Media }}</ref> is a website, designed as a [[decision engine]] that uses consumer input to provide specialized search results for products and categories.\u000a\u000aUnlike typical [[Web search engine|search engines]], Pikimal mines data to provide users with only the facts pertaining to their search, as a hopeful solution to [[SEO]] and marketing biased search results.\u000a\u000aAs of April 2011, Pikimal had 13 full-time employees in Pittsburgh, PA, interns, and various contractors around the world.<ref name="businesstimes" />\u000a\u000a== History ==\u000a\u000aPikimal was founded in January 2010 by [[Eric Silver]], previously the chief marketing officer at [[Modcloth]]. The Pikimal site was launched in [[public beta]] form in October 2010.<ref name="businesstimes" />\u000a\u000a== Functionality ==\u000a\u000aPikimal allows users to adjust sliders to express what facts of a product are particularly important to them. These percentages are combined with an algorithm to provide users with product recommendations that are rooted directly in facts, but only the facts they find most relevant.<ref>{{cite web|url=http://www.youtube.com/watch?v=imOUklpphcM&feature=player_embedded | title=What is Pikimal? Video}}</ref>\u000a\u000a== Pivot and Shutdown ==\u000a\u000aIn 2012 Pikimal changed it name to [[Webkite]] and pivoted to provide faceted search solutions to other companies. As of September 2014 pikimal.com and all associated sites has been shutdown.\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* {{official website|http://pikimal.com}}\u000a\u000a[[Category:Internet search engines]]\u000a[[Category:Information retrieval]]\u000a[[Category:Internet properties established in 2010]]\u000a[[Category:Knowledge markets]]\u000a[[Category:Companies based in Pittsburgh, Pennsylvania\u200e]]
p310
sg6
S'Pikimal'
p311
ssI120
(dp312
g2
S'http://en.wikipedia.org/wiki/Dragomir R. Radev'
p313
sg4
V'''Dragomir R. Radev''' is a [[University of Michigan]] computer science professor and [[Columbia University]] computer science adjunct professor working on [[natural language processing]] and [[information retrieval]].  \u000aHe is currently working on the fields of open domain [[question answering]],  [[multi-document summarization]], and the application of NLP in Bioinformatics and Political Science.\u000a\u000aRadev received his PhD in [[Computer Science]] from [[Columbia University]] in 1999. He is the secretary of [http://www.aclweb.org [[Association for Computational Linguistics|ACL]]] (2006\u2013present) and associate editor of [http://www.jair.org JAIR].\u000a\u000a== Awards ==\u000aAs [[NACLO]] founder, Radev shared the [[Linguistic Society of America]] 2011 [http://www.lsadc.org/info/lsa-awards.cfm ''Linguistics, Language and the Public Award'']. He is the  Co-winner of the [http://polmeth.wustl.edu/about.php?page=awards Gosnell Prize (2006)].\u000a\u000a== IOL==\u000aRadev has served as the coach and led the US national team in the [[International Linguistics Olympiad|International Linguistics Olympiad (IOL)]] to several gold medals [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073][http://www.nsf.gov/news/news_summ.jsp?cntn_id=109891].\u000a\u000a== Books ==\u000a* Puzzles in Logic, Languages and Computation (2013) <ref>{{Cite web|url = http://www.springer.com/education+%26+language/linguistics/book/978-3-642-34371-1|title = Puzzles in Logic, Languages and Computation|date = |accessdate = |website = |publisher = |last = |first = }}</ref>\u000a* Mihalcea and Radev (2011) [http://www.cambridge.org/gb/knowledge/isbn/item5980387/?site_locale=en_GB ''Graph-based methods for NLP and IR'']\u000a\u000a== Selected Papers ==\u000a* SIGIR 1995 Generating summaries of multiple news articles\u000a* ANLP 1997 Building a generation knowledge source using internet-accessible newswire\u000a* Computational Linguistics 1998 Generating natural language summaries from multiple on-line sources\u000a* ACL 1998 Learning correlations between linguistic indicators and semantic constraints: Reuse of context dependent descriptions of entities\u000a* ANLP 2000 Ranking suspected answers to natural language questions using predictive annotation\u000a* CIKM 2001 Mining the web for answers to natural language questions\u000a* AAAI 2002 Towards CST-enhanced summarization\u000a* ACL 2003 Evaluation challenges in large-scale multi-document summarization: the Mead project\u000a* Information Processing and Management 2004 Centroid-based summarization of multiple documents\u000a* J. of Artificial Intelligence Research 2004 LexRank: Graph-based lexical centrality as salience in text summarization\u000a* J. of the American Association of Information Science and Technology 2005 Probabilistic question answering on the web\u000a* Communications of the ACM 2005 NewsInEssence: summarizing online news topics\u000a* EMNLP 2007 Semi-supervised classification for extracting protein interaction sentences using dependency parsing\u000a* Bioinformatics 2008 Identifying gene-disease associations using centrality on a literature mined gene-interaction network\u000a* IEEE Intelligent Systems 2008 natural language processing and the web\u000a* NAACL 2009 Generating surveys of scientific paradigms\u000a* Nucleic Acids Research 2009 Michigan molecular interactions r2: from interacting proteins to pathways\u000a* J. of the American Association of Information Science and Technology 2009 Visual overviews for discovering key papers and influences across research fronts\u000a* KDD 2010 Divrank: the interplay of prestige and diversity in information networks\u000a* American J. of Political Science 2010 How to Analyze Political Attention with Minimal Assumptions and Costs\u000a* Arxiv 2011 The effect of linguistic constraints on the large scale organization of language\u000a* J. of Biomedical Semantics 2011 Mining of vaccine-associated ifn-gamma gene interaction networks using the vaccine ontology\u000a\u000a==External links==\u000a* [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073 Team USA Brings Home the Linguistics Gold]\u000a* [http://www.eecs.umich.edu/eecs/about/articles/2011/Radev-LSA11.html Dragomir Radev, Co-Founders Recognized as NACLO Receives Linguistics, Language and the Public Award]\u000a* [http://www.eecs.umich.edu/eecs/about/articles/2010/Radev-Linguistics.html Dragomir Radev Coaches US Linguistics Team to Multiple Wins]\u000a* [http://www.eecs.umich.edu/eecs/about/articles/2009/Radev-ACM-DM.html Dragomir Radev Honored as ACM Distinguished Scientist]\u000a* [http://www.eecs.umich.edu/eecs/etc/news/shownews.cgi?428 Prof. Dragomir Radev Receives Gosnell Prize]\u000a\u000a== References ==\u000a{{reflist}}\u000a<!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. --->\u000a*\u000a*\u000a*\u000a*\u000a\u000a{{Persondata <!-- Metadata: see [[Wikipedia:Persondata]]. -->\u000a| NAME              = Radev, Dragomir R.\u000a| ALTERNATIVE NAMES =\u000a| SHORT DESCRIPTION = American computer scientist\u000a| DATE OF BIRTH     =\u000a| PLACE OF BIRTH    =\u000a| DATE OF DEATH     =\u000a| PLACE OF DEATH    =\u000a}}\u000a\u000a{{DEFAULTSORT:Radev, Dragomir R.}}\u000a[[Category:Year of birth missing (living people)]]\u000a[[Category:Living people]]\u000a\u000a[[Category:Columbia University alumni]]\u000a[[Category:American computer scientists]]\u000a[[Category:University of Michigan faculty]]\u000a[[Category:Natural language processing]]\u000a[[Category:Information retrieval]]
p314
sg6
S'Dragomir R. Radev'
p315
ssI123
(dp316
g2
S'http://en.wikipedia.org/wiki/Multimedia Information Retrieval'
p317
sg4
S'{{COI|date=July 2014}}\n{{Original research|date=July 2014}}\n{{Use dmy dates|date=February 2012}}\n\'\'\'Multimedia Information Retrieval\'\'\' (MMIR or MIR) is a research discipline of [[computer science]] that aims at extracting semantic information from [[multimedia]] data sources.<ref>H Eidenberger. " Fundamental Media Understanding ", atpress, 2011, p. 1.</ref>{{FV|date=July 2014}} Data sources include directly perceivable media such as [[Content (media and publishing)|audio]], [[image]] and [[video]], indirectly perceivable sources such as [[Written language|text]], biosignals as well as not perceivable sources such as bioinformation, stock prices, etc. The methodology of MMIR can be organized in three groups:\n\n# Methods for the summarization of media content ([[feature extraction]]). The result of feature extraction is a description.\n# Methods for the filtering of media descriptions (for example, elimination of [[Data redundancy|redundancy]])\n# Methods for the [[categorization]] of media descriptions into classes.\n\n== Feature Extraction Methods ==\n\nFeature extraction is motivated by the sheer size of multimedia objects as well as their redundancy and, possibly, noisiness.<ref>H Eidenberger. " Fundamental Media Understanding ", atpress, 2011, p. 2.</ref>{{FV|date=July 2014}} Generally, two possible goals can be achieved by feature extraction:\n\n* Summarization of media content. Methods for summarization include in the audio domain, for example, [[Mel Frequency Cepstral Coefficients]], Zero Crossings Rate, Short-Time Energy. In the visual domain, color histograms<ref>A Del Bimbo. " Visual Information Retrieval ", Morgan Kaufmann, 1999.</ref> such as the [[MPEG-7]] Scalable Color Descriptor can be used for summarization.\n* Detection of patterns by [[auto-correlation]] and/or [[cross-correlation]]. Patterns are recurring media chunks that can either be detected by comparing chunks over the media dimensions (time, space, etc.) or comparing media chunks to templates (e.g. face templates, phrases). Typical methods include Linear Predictive Coding in the audio/biosignal domain,<ref>HG Kim , N Moreau, T Sikora. " MPEG-7 Audio and Beyond", Wiley, 2005.</ref> texture description in the visual domain and n-grams in text information retrieval.\n\n== Merging and Filtering Methods ==\n\nMultimedia Information Retrieval implies that multiple channels are employed for the understanding of media content.<ref>MS Lew (Ed.). " Principles of Visual Information Retrieval ", Springer, 2001.</ref> Each of this channels is described by media-specific feature transformations. The resulting descriptions have to be merged to one description per media object. Merging can be performed by simple concatenation if the descriptions are of fixed size. Variable-sized descriptions - as they frequently occur in motion description - have to be normalized to a fixed length first.\n\nFrequently used methods for description filtering include [[factor analysis]] (e.g. by PCA), singular value decomposition (e.g. as latent semantic indexing in text retrieval) and the extraction and testing of statistical moments. Advanced concepts such as the [[Kalman filter]] are used for merging of descriptions.\n\n== Categorization Methods ==\n\nGenerally, all forms of machine learning can be employed for the categorization of multimedia descriptions<ref>H Eidenberger. " Fundamental Media Understanding ", atpress, 2011,p. 125.</ref>{{FV|date=July 2014}} though some methods are more frequently used in one area than another. For example, [[Hidden Markov models]] are state-of-the-art in [[speech recognition]], while [[Dynamic Time Warping]] - a semantically related method - is state-of-the-art in gene sequence alignment. The list of applicable classifiers includes the following:\n\n* Metric approaches ([[Cluster Analysis]], [[Vector Space Model]], [[Minkowski]] Distances, Dynamic Alignment)\n* Nearest Neighbor methods ([[K-nearest neighbors algorithm]], K-Means, [[Self-Organizing Map]])\n* Risk Minimization (Support Vector Regression, [[Support Vector Machine]], [[Linear Discriminant Analysis]])\n* Density-based Methods (Bayes Nets, [[Markov Processes]], Mixture Models)\n* Neural Networks ([[Perceptron]], Associative Memories, Spiking Nets)\n* Heuristics ([[Decision Trees]], Random Forests, etc.)\n\nThe selection of the best classifier for a given problem (test set with descriptions and class labels, so-called [[ground truth]]) can be performed automatically, for example, using the [[Weka]] Data Miner.\n\n== Open Problems ==\n\nThe quality of MMIR Systems<ref>JC Nordbotten. "[http://nordbotten.com/ADM/ADM_book/MIRS-frame.htm Multimedia Information Retrieval Systems]". Retrieved 14 October 2011.</ref> depends heavily on the quality of the training data. Discriminative descriptions can be extracted from media sources in various forms. Machine learning provides categorization methods for all types of data. However, the classifier can only be as good as the given training data. On the other hand, it requires considerable effort to provide class labels for large databases. The future success of MMIR will depend on the provision of such data.<ref>H Eidenberger. " Frontiers of Media Understanding ", atpress, 2012.</ref> The annual [[TRECVID]] competition is currently one of the most relevant sources of high-quality ground truth.\n\n== Related Areas ==\n\nMMIR provides an overview over methods employed in the areas of information retrieval.<ref>H Eidenberger. " Professional Media Understanding ", atpress, 2012.</ref> Methods of one area are adapted and employed on other types of media. Multimedia content is merged before the classification is performed. MMIR methods are, therefore, usually reused from other areas such as:\n\n* [[Bioinformatics|Bioinformation Analysis]]\n* [[Biosignal|Biosignal Processing]]\n* [[Content-based image retrieval|Content-based Image and Video Retrieval]]\n* [[Facial recognition system|Face Recognition]]\n* [[Music information retrieval|Audio and Music Classification]]\n* [[Speech Recognition]]\n* [[Technical analysis|Technical Chart Analysis]]\n* [[Information retrieval|Text Information Retrieval]]\n\nThe Journal of Multimedia Information Retrieval<ref>"[http://www.springer.com/computer/journal/13735 Journal of Multimedia Information Retrieval]", Springer, 2011, Retrieved 21 October 2011.</ref> documents the development of MMIR as a research discipline that is independent of these areas. See also <ref>H Eidenberger. " Handbook of Multimedia Information Retrieval ", atpress, 2012.</ref> for a complete overview over this research discipline.\n\n==References==\n{{reflist}}\n\n<!--\n\n<ref>AUTH. "TITLE", PUB, YEAR.</ref>\n<ref>AUTH. "[LINK TITLE]", MEDIA, PRODDATE. Retrieved UPDATE.</ref>\n\n-->\n\n\n\n[[Category:Information retrieval]]'
p318
sg6
S'Multimedia Information Retrieval'
p319
ssI126
(dp320
g2
S'http://en.wikipedia.org/wiki/Category:Substring indices'
p321
sg4
S'{{cat main|Substring index}}\n\n[[Category:String (computer science)]]\n[[Category:Algorithms on strings]]\n[[Category:String data structures]]\n[[Category:Database index techniques]]\n[[Category:Information retrieval]]\n[[Category:Bioinformatics algorithms]]'
p322
sg6
S'Category:Substring indices'
p323
ss.